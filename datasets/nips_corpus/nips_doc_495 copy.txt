Operators and curried functions: 
Training and analysis of simple recurrent networks 
Janet Wiles 
Dept.s of Psychology and Computer Science, 
University of Queensland 
QLD 4072 Australia. 
janetw/X:s.uq.oz.au 
Anthony B!oesch, 
Dept of Computer Science, 
University of Queensland, 
QLD 4072 Australia 
anthonybCWcs.uq.oz.au 
Abstract 
We present a framework for programming the hidden unit representations of 
simple recurrent networks based on the use of hint units (additional targets at 
the output layer). We present two ways of analysing a network trained within 
this framework: Input patterns act as operators on the information encoded by 
the context units; symmetrically, patterns of activation over the context units 
act as curried functions of the input sequences. Simulations demonstrate that a 
network can learn to represent three different functions simultaneously and 
canonical discriminant analysis is used to investigate how operators and curried 
functions are represented in the space of hidden unit activations. 
1 INTRODUCTION 
Many recent papers have contributed to the understanding of recurrent networks and theft 
potential for modelling sequential phenomena (see for example Giles, Sun, Chen, Lee, & 
Chen, 1990; Elman, 1989; 1990; Jordan, 1986; Cleeremans, Servan-Schreiber & 
McClelland, 1989; Williams & Zipset, 1988). Of particular interest in these papers is 
the development of recurrent architectures and learning algorithms able to solve complex 
problems. The perspective of the work we present here has many similarities with these 
studies, however, we focus on programming a recurrent network for a specific task, and 
hence provide appropriate sequences of inputs to learn the temporal component. 
The function computed by a neural network is conventionally represenled by its weights. 
During training, the task of a network is to learn a set of weights that causes the 
appropriate action (or set of context-specific actions) for each input pattern. However, in 
325 
326 Wiles and Bloesch 
a network with recurrent connections, patterns of activation are also part of the function 
computed by a network. After training (when the weights have been fixed) each input 
pattern has a specific effect on the pattern of activation across the hidden and output units 
which is modulated by the current state of those units. That is, each input pattern is a 
context sensitive operator on the state of the system. 
To illustrate this idea, we present a task in which many sequences of the form, {F, argl, 
..., argn} are input to a network, which is required to output the value of each function, 
F(argl, ..., argn). The task is interesting since it illustrates how more than one function 
can be computed by the same network and how the function selected can be specified by 
the inputs. Viewing all the inputs (both function patterns, F, and argument patterns, 
argo as operators allows us to analyse the effect of each input on the state of the network 
(the pattern of activation in the hidden and context units). From this perspective, the 
weights in the network can be viewed as an interpreter which has been programmed to 
carry out the operations specified by each input pattern. 
We use the term programming intentionally, to convey the idea that the actions of each 
input pattern play a specific role in the processing of a sequence. In the simulations 
described in this paper, we use the simple recurrent network (SRN) proposed by Elman 
(1990). The art of programming enters the simulations in the use of extra target units, 
called hints, that are provided at the output layer. At each step in learning a sequence, 
hints specify all the information that the network must preserve in the hidden unit 
representation (the state of the system) in order to calculate outputs later in the sequence 
(for a discussion of the use of hints in training a recurrent network see Rumelhart, 
Hinton & Williams, 1986). 
2 SIMULATIONS 
Three different boolean functions and their arguments were specified as sub-sequences of 
patterns over the inputs to an SRN. The network was required to apply the function 
specified by the first pattern in each sequence to each of the subsequent arguments in 
turn. The functions provided were boolean functions of the current input and previous 
output, AND, OR and XOR (i.e., exclusive-or) and the arguments were arbitrary length 
strings of O's and l's. The context units were not reset between sub-sequences. An SRN 
with 3 input, 5 hidden, 5 context, 1 output and 5 hint units was trained using 
backpropagation with a momentum term. The 5 hint units at the output layer provided 
information about the boolean functions during training (via the backpropagation of 
errors), but not during testing. The network was trained on three data sets each 
containing 700 (ten times the number of weights in the network) randomly generated 
patterns, forming function and arguments sequences of average length 0.5, 2 and 4 
arguments respectively. The network was trained for one thousand iterations on each 
training set. 
2.1 RESULTS AND GENERALISATION 
After training, the network correctly computed every pattern in the three training sets 
(using a closest match criterion for scoring the output) and also in a test set of sequences 
generated using the same statistics. Generalisation test data consisting of all possible 
sequences composed of each function and eight arguments, and long sequences each of 50 
arguments also produced the correct output for every pattern in every sequence. To test 
Operators and curried functions: Training and analysis of simple recurrent networks 327 
First canonical component 
First canonical component 
la. lb. 
Figure la. The hidden unit patterns for the training data, projected onto the first two 
canonical components. These components separate the patterns into 3 distinct regions 
corresponding to the initial pattern (AND, OR or XOR) in each sequence. lb. The first 
and third canonical components further separate the hidden unit patterns into 6 regions 
which have been marked in the diagrams above by the corresponding output classes A1, 
A0, R1, R0, X1 and X0. These regions are effectively the computational states of the 
network. 
A 
A 
0 
o 
Figure 2. Finite state machine to compute the three-function task. 
Another way of considering sub-sequences in the input stream is to describe all the 
inputs as functions, not over the other inputs, as above, but as functions of the state (for 
which we use the term operators). Using this terminology, a sub-sequence is a 
composition of operators which act on the current state, 
G(S(t)) = argt o ... o arg2 o argl o S(0), 
where (f o g) (x) = f(g(x)), and S(0) is the initial state of the network. A consequence of 
describing the input patterns as operators is that even the 0 and 1 data bits can be seen as 
operators that transform the internal state (see Box 1). 
328 Wiles and Bloesch 
First canonical component 
3d. 
First component 
First canonical component 
Figure 3. State transitions caused by each input pattern, projected onto the first and third 
canonical components of the hidden unit patterns (generated by the training data as in 
Figure 1). 3a-c. Transitions caused by the AND, OR and XOR input patterns 
respectively. From every point in the hidden unit space, the input paRems for AND, OR 
and XOR transform the hidden units to values corresponding to a point in the regions 
marked A1, R0 and X0 respectively. 3d-e. Transitions caused by the 0 and 1 input 
patterns respectively. The 0 and 1 inputs are context sensitive operators. The 0 input 
causes changes in the hidden unit patterns corresponding to transitions from the state A1 
to A0, but does not cause transitions from the other 5 regions. Conversely, a 1 input 
does not cause the hidden unit patterns to change from the regions A1, A0 or R1, but 
causes transitions from the regions R0, X1 and X0. 
Operators and curried functions: Training and analysis of simple recurrent networks 329 
Input 
operators 
Patterns on 
the input units 
Effect on information 
encoded in the state 
AND 01 1 cf->AND 
OR 1 1 0 cf'-> OR 
XOR 1 0 1 cf'-> XOR 
i 1 1 1 x(0 --, x(t-1) 
1 
NOT(x(t-1)) 
0 000 x(0 --' 0 
x(t-1) 
x(t-1) 
if cf = AND 
if cf=OR 
if cf = XOR 
if cf = AND 
if cf = OR 
if cf = XOR 
Box 1. Operators for the 5 input patterns. The operation performed by each input 
pattern is described in terms of the effect it has on information encoded by the hidden 
unit patterns. The first and second columns specify the input operators and their 
corresponding input patterns. The third column specifies the effect that each input in a 
sub-sequence has on information encoded in the state, represented as cf, for current: 
function, and x(0 for the last output. 
For each input pattern, we plotted all the transitions in hidden unit space resulting from 
that input projected onto the canonical components used in Figure 1. Figures 3a to 3e 
show transitions for each of the five input operators. For the three function inputs, 
OR, AND, and XOR, the effect is to collapse the hidden unit patterns to a single region 
- a particular state. These are relatively context insensitive operations. For the two 
argument inputs, 0 and 1, the effect is sensitive to the context in which the input 
occurs (i.e., the previous state of the hidden units). A similar analysis of the states 
themselves focuses on the hidden unit patterns and the information that they must encode 
in order to compute the three-function task. At each timestep the weights in the network 
construct a pattern of activation over the hidden units that reduces the structured 
arguments of a complex function of several arguments by a simpler function of one less 
argument. This can be represented as follows: 
G(F, argl, ... argn) ---, F(argl, ... argn) 
-- Fargl(arg2, ... argn) 
---' Farglarg2(arg3, ... argn). 
This process of replacing structured arguments
