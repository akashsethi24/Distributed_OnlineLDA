On Stochastic Complexity and Admissible 
Models for Neural Network Classifiers 
Padhraic Smyth 
Communications Systems Research 
Jet Propulsion Laboratory 
California Institute of Technology 
Pasadena, CA 91109 
Abstract 
Given some training data how should we choose a particular network clas- 
sifier from a family of networks of different complexities? In this paper 
we discuss how the application of stochastic complexity theory to classifier 
design problems can provide some insights into this problem. In particular 
we introduce the notion of admissible models whereby the complexity of 
models under consideration is affected by (among other factors) the class 
entropy, the amount of training data, and our prior belief. In particular 
we discuss the implications of these results with respect to neural architec- 
tures and demonstrate the approach on real data from a medical diagnosis 
task. 
I Introduction and Motivation 
In this paper we examine in a general sense the application of Minimum Description 
Length (MDL) techniques to the problem of selecting a good classifier from a large 
set of candidate models or hypotheses. Pattern recognition algorithms differ from 
more conventional statistical modeling techniques in the sense that they typically 
choose from a very large number of candidate models to describe the available data. 
Hence, the problem of searching through this set of candidate models is frequently 
a formidable one, often approached in practice by the use of greedy algorithms. In 
this context, techniques which allow us to eliminate portions of the hypothesis space 
are of considerable interest. We will show in this paper that it is possible to use the 
intrinsic structure of the MDL formalism to eliminate large numbers of candidate 
models given only minimal information about the data. Our results depend on the 
818 
On Stochastic Complexity 819 
very simple notion that models which are obviously too complex for the problem 
(e.g., models whose complexity exceeds that of the data itself) can be discarded 
from further consideration in the search for the most parsimonious model. 
2 Background on Stochastic Complexity Theory 
2.1 General Principles 
Stochastic complexity prescribes a general theory of inductive inference from data, 
which, unlike more traditional inference techniques, takes into account the com- 
plexity of the proposed model in addition to the standard goodness-of-fit of the 
model to the data. For a detailed rationale the reader is referred to the work of 
Rissanen (1984) or Wallace and Freeman (1987) and the references therein. Note 
that the Minimum Description Length (MDL) technique (as Rissanen's approach 
has become known) is implicitly related to Maximum A Postertort (MAP) Bayesian 
estimation techniques if cast in the appropriate framework. 
2.2 Minimum Description Length and Stochastic Complexity 
Following the notation of Barron and Cover (1991), we have N data-points, de- 
scribed as a sequence of tuples of observations {x, ...,x,yi},l <_ i <_ N, to be 
referred to as xi, Yi} for short. The x correspond to values taken on by the K 
random variables X  (which may be continuous or discrete), while, for the purposes 
of this paper, the Yi are elements of the finite alphabet of the discrete m-ary class 
variable Y. Let FN = {M1,..., MlrN[} be the family of candidate models under 
consideration. Note that by defining FN as a function of N, the number of data 
points, we allow the possibility of considering more complicated models as more 
data arrives. For each Mj  FN let C(Mj) be non-negative numbers such that 
 2 -c(Mj) _< 1. 
The C(Mj) can be interpreted as the cost in bits of specifying model Mj -- in turn, 
2 -c(Mj) is the prior probability assigned to model Mj (suitably normalized). Let 
us use C = {C(M), ..., C(MirNi)} to refer to a particular coding scheme for 
Hence the total description length of the data plus a model Mj is defined as 
( 1 
L(M, {x,, y,)) = C(M) + log 
i.e., we first describe the model and then the class data relative to the given model 
(as a function of (x), the feature data). The stochastic complexity of the data 
{x_, Yi} relative to C and FN is the minimum description length 
I({xi,yi})= min {L(Mi,{xi,yi})}. 
Mj  PN 
The problem of finding the model of shortest description length is intractable in 
the general case -- nonetheless the idea of finding the best model we can is well 
motivated, works well in practice and is far preferable to the alternative approach 
of ignoring the complexity issue entirely. 
820 Smyth 
3 Admissible Stochastic Complexity Models 
3.1 Definition of Admissibility 
We will find it useful to define the notion of an admissible model for the classification 
problem: the set of admissible models fN (C_ FN) is defined as all models whose 
complexity is such that there exists no other model whose description length is 
known to be smaller. In other words we are saying that inadmissible models are 
those which have complexity in bits greater than any known description length -- 
clearly they cannot be better than the best known model in terms of description 
length and can be eliminated from consideration. Hence, QN is defined dynamically 
and is a function of how many description lengths we have already calculated in our 
search. Typically FN may be pre-defined, such as the class of all 3-layer feed-forward 
neural networks with particular activation functions. We would like to restrict our 
search for a good model to the set QN C_ FN as far as possible (since non-admissible 
models are of no practical use). In practice it may be difficult to determine the 
exact boundaries of fN, particularly when IFS[ is large (with decision trees or 
neural networks for example). Note that the notion of admissibility described here 
is particularly useful when we seek a minimal description length, or equivalently a 
model of mazimal a posterJori probability -- in situations where one's goal is to 
average over a number of possible models (in a Bayesian manner) a modification of 
the admissibility criterion would be necessary. 
3.2 Results for Admissible Models 
Simple techniques for eliminating obvious non-admissible models are of interest' for 
the classification problem a necessary condition that a model Mj be admissible is 
that 
C(Mj) < N. H(X) _< Nlog(m) 
where H(X) is the entropy of the m-ary class variable X. The obvious interpretation 
in words is that any admissible model must have complexity less than that of the 
data itself. It is easy to show in addition that the complexity of any admissible 
model is upper bounded by the parameters of the classification problem: 
p(Mj) _ -N.H(X) _ 2-Nlog(m) 
VMj ( QN. 
Hence, the size of the space of admissible models can also be bounded: 
[QN[ (_ 2N'H(X) (_ 2N' log(m) 
Our approach suggests that for classification at least, once we know N and the 
number of classes m, there are strict limitations on how many admissible models we 
can consider. Of course the theory does not state that considering a larger subset 
will necessarily result in a less optimal model being found, however, it is difficult to 
argue the case for including large numbers of models which are clearly too complex 
for the problem. At best, such an approach will lead to an inefficient search, whereas 
at worst a very poor model will be chosen perhaps as a result of the use of a poor 
coding scheme for the unnecessarily large hypothesis space. 
On Stochastic Complexity 821 
3.3 Admissible Models and Bayes Risk 
The notion of minimal compression (the minimum achievable goodness-of-fit) is 
intimately related in the classification problem to the minimal Bayes risk for the 
problem (Kovalevsky, 1980). Let MB be any model (not necessarily unique) which 
achieves the optimal Bayes risk (i.e., minimizes the classifier error) for the classi- 
fication problem. In particular, C({zi}lMB({Yi})) is not necessarily zero, indeed 
in most practical problems of interest it is non-zero, due to the ambiguity in the 
mapping from the feature space to the class variable. In addition, MB may not be 
defined in the set FN, and hence, MB need not even be admissible. If, in the limit 
as N -- co, MB  Foo then there is a fundamental appro:rimation error in the rep- 
resentation being used, i.e., the family of models under consideration is not flexible 
enough to optimally represent the mapping from {zi} to {yi}. Smyth (1991) has 
shown how information about the Bayes error rate for the problem (if available) 
can be used to further tighten the bounds on admissibility. 
4 
Applying Minimum Description Length Principles to 
Neural Network Design 
In principle the admissibility results can be applied to a variety of classifier design 
problems -- applications to Markov model selection and decision tree design are 
described elsewhere (Smyth, 1991). In this paper we limit our attention to the 
problem of automatically selecting a feedforward multi-layer network architecture. 
4.1 Calculation of the Goodness-of-Fit 
As is clear from the preceding discussion, application of the MDL principle to clas- 
sifier selection requires that the classifier produce a posterior probability estimate of 
the class labels. In the context of a network model this is not a problem provided the 
network is trained to provide such estimates. This requires a simple modification 
N 
of the objective function to a log-likelihood function - i=1 1ï¿½g(13(Yi[ xi)), where yi 
is the class label of the ith training datum and/50 is the network's estimate of p(). 
This function has been proposed in the literature in the past under the guise of a 
cross-entropy measure (for the special case of binary classes) and more recently it 
has been derived from the more basic arguments of Minimum Mutual Information 
(MMI) (Bridle, 1990) and Maximum Likelihood (ML) Estimation (Gish, 1990). The 
cross-entropy function for network training is nothing more that the goo
