Strong Unimodality and Exact Learning 
of Constant Depth/-Perceptron 
Networks 
Mario Marchand 
Department of Computer Science 
University of Ottawa 
Ottawa, Ont., Canada KIN 6N5 
marchand@csi.uottawa. ca 
Saeed Hadjifaradji 
Department of Physics 
University of Ottawa 
Ottawa, Ont., Canada KIN 6N5 
saeed@physics.uottawa.ca 
Abstract 
We present a statistical method that exactly learns the class of 
constant depth /-perceptron networks with weights taken from 
f-l, 0 q- 1) and arbitrary thresholds when the distribution that 
generates the input examples is member of the family of product 
distributions. These networks (also known as nonoverlapping per- 
ceptron networks or read-once formulas over a weighted threshold 
basis) are loop-free neural nets in which each node has only one 
outgoing weighl{. With arbitrary high probability, the learner is 
able to exactly identify the connectivity (or skeleton) of the target 
/-perceptron network by using a new statistical test which exploits 
the strong unimodality property of sums of independent random 
variables. 
1 INTRODUCTION 
From a computational learning theory perspective, it is well known that efficient 
learning of non trivial (or non simple) neural network function classes is possible 
only when either (1) the learner is able to use membership queries or (2) the distri- 
bution that generates the input examples is not arbitrary but member of some well 
defined family. Following several positive learnability results on different classes of 
read-once Boolean formulas, a membership query algorithm has been recently pro- 
posed [4] for learning the class of nonoverlapping perceptron networks. These net- 
works (also known as/-perceptron networks or read-once formulas over a weighted 
threshold basis) are loop-free neural nets in which each node has only one outgoing 
weight. If membership queries are not permitted (as we assume throughout this 
paper), learning this class becomes intractable [6] under arbitrary input distribu- 
Strong Unimodality and Exact Learning of Constant Depth g-Perceptron Networks 289 
tions. However, under the uniform distribution, a PAC learning algorithm has been 
proposed recently [2] for a quite restricted subclass called generalized/-perceptron 
decision lists. As an important step towards the learnability of the whole class of 
/-perceptron networks under simple distributions, we present in this paper a sta- 
tistical method that exactly learns the class of constant depth/-perceptron networks 
under the family of product distributions, i.e. distributions in which the setting of 
each input variable is chosen independently of the other variables. Eventhough the 
depth of the network must be fixed to a constant, we satisfy here a harder learning 
criterion than the one proposed by the PAC model [9]. Indeed, with arbitrary high 
probability, the proposed algorithm is able to exactly identify [1] the target function. 
Moreover, because of its statistical nature [7], the proposed algorithm can tolerate 
a classification noise rate  up to the information theoretic limit of  = 1/2. 
There exist other statistical methods to learn other classes of read-once formulas un- 
der particular distributions [1] and product distributions [8]. They all basically differ 
in the statistical tests they use to identify the gate parameters and the formula's 
skeleton. Our key novel contribution is to introduce a new test (for discovering the 
network's connectivity) which exploits the strong unimodality [5] property of sums 
of independent random variables. 
2 DEFINITIONS 
We consider the problem of learning Boolean functions of the Boolean domain 
{0,1} n. Let X = {x,x2... ,Xn} be the set of n input variables and x C {0,1} n be 
some assignment of these n variables, we denote by xv the restriction of assignment 
x on the variables in V C_ X. A perceptron g on V is defined by a vector of v = I VI 
weights wi and a single threshold t9. As usual, for any xv c {0, 1} v, the output of 
g(xv) is I whenever -ev wx > 9 and 0 otherwise. 
We restrict ourselves to the case where each wi  {-1,0, +1} but the thresholds 
are arbitrary so that, without loss of generality (w.l.o.g.), t9 E {-v - 1,-..v}. A 
perceptron is said to be positive if all its incoming weights are +1. The learning 
algorithm will use the following classification for positive perceptrons. 
T1 perceptrons: These are perceptrons which output I iff one or more of its inputs 
are set to 1. These are OR gates of multiple inputs. 
TO perceptrons: These are perceptrons which output 0 iff one or more of its inputs 
are set to 0. These are AND gates of multiple inputs. 
Tll perceptrons: These are perceptrons which output I iff two or more of its inputs 
are set to 1. These include majority gates of three inputs. 
TOO perceptrons: These are perceptrons which output 0 iff two or more of its inputs 
are set to 0. These include majority gates of four variables. 
TG perceptrons: All the perceptrons which do not belong to any one of the above 
four categories. They must, therefore, have at least five inputs. 
Each perceptron can have variables and/or other perceptrons as inputs. Hence, 
a node will denote either a variable or a perceptron. The class of/-perceptron 
networks is the set of all Boolean functions that can be represented as a loop- 
free network of percepttons where each node (including input units) has only one 
outgoing weight. The output unit of a network will often be referred as the root node. 
We say that a node is a child of the parent perceptron g if it is an immediate input 
to perceptron g. Children of the same perceptron are called siblings. A perceptron 
is said to be a bottom level perceptron if all its children are variables. The depth of a 
node is defined as the number of perceptrons (including the parent of the node and 
the root node) on the path from the parent of the node to the root. The perceptrons 
290 M. MARCHAND, S. HADJIFARADJI 
on this path are called the ancestors of that node. The descendants of perceptron 
g, denoted by desc(g), is the set of nodes that have perceptton g as an ancestor. 
The depth of a network is defined as the depth of the deepest variable in the net. 
The least common ancestor of a set V of nodes, denoted by lca(V), is defined as 
the deepest ancestor which is common to every node in V. Variables {xi, xj, xk} 
are said to meet at perceptron g, iff lca(xi,xj) - lca(xi,xk) -- lca(xj,xk) -- g. If 
there does not exist a perceptron g having this property, then variables {xi, xj, x} 
are said not to meet. 
In this paper, we use a learning criterion which is more ambitious than the PAC 
criterion introduced by Valiant [9]. We consider that each training example x 
is generated by an unknown product distribution D on 0, 1} n and then labeled 
according to an unknown target Boolean function f representable as a/-perceptron 
network. After observing a set of such examples, the goal of the learning algorithm 
is to produce an hypothesis function h which is the exact equivalent of f. More 
formally we say that algorithm A exactly learns (or exactly identifies) a class F of 
of Boolean functions iff for any 0 < 5 < 1, any product distribution D on 0, 1} n, 
and any target function f C F, algorithm A outputs, with probability at least 1 - 5 
an hypothesis function h such that h(x) - f(x) V x c 0, 1} n. 
The learning algorithm will perform several statistical tests to build its hypothesis. 
Namely, for each variable xi, it will estimate its influence, defined as: 
Infi(xi) de=f Pr(f = llxi = 1)- Pr(f = 1Ix  = 0) 
(1) 
where all probabilities (here and in the sequel) are defined with respect to the 
(unknown) training product distribution D. The empirical estimate of Pt(A) will 
be denoted as lfr(A). We will also use, Infig(xi) to denote the influence of xi on the 
subformula of f which is rooted at perceptton g. Also, Infi(xilxj = a) will denote 
the influence of xi given that variable xj is fixed to value a. To discover the skeleton 
of the target function, the learner will compute the coinfluence of several triples of 
variables, defined as: 
c/k,j de__f Infl(xjlxi = 1,x: 0) Infl(xjlx = 1, x = 1) 
- Infi(xlx, - 0, xk -- 0) - Infi(xlx, - 0, - 1) (2) 
Because h must make zero error with f, the learner must produce an hypothesis 
h which contains all the input variables and all the perceptrons of f (except those 
variables and perceptrons which are fixed to a constant value). Consequently, for a 
target f defined on n input variables xi and containing r perceptrons g, we define 
s as: 
der 
% = min {Pr(zi = a)/e{0,...n},ae{0,u,Pr(gk = b)e{0,...},be{0,}} (3) 
Hence, Vi E {0,...n} we have: %  Pr(z = 1) % l-e, andV k E {0,..-r} 
we have: % N Pr(g = 1) N 1 - %. To exactly learn the cls of constant depth 
-percepton network, the proposed algorithm needs a number of examples which 
is polynomial in l/e, (see the algorithm LearnNPN). 
3 THE LEARNING ALGORITHM 
We first perform some simplifying reductions that hold for any target/-perceptron 
net f. (1) We can assume, w.l.o.g., that only input variables have a negative 
outgoing weight. Indeed, if a perceptron g has a -1 outgoing weight, we can 
replace it by a perceptron which has all its incoming weights negated and a +1 
Strong Unimodality and Exact Learning of Constant Depth g-Perceptron Networks 291 
outgoing weight; this leaves the computation by f unchanged when we add +1 to 
the threshold of g's parent. In this manner, all -1 weights are pushed to the input 
variables. (2) T1 perceptrons do not have T1 perceptrons as children since such 
nodes can always be merged. The same remark is true for TO perceptrons. (3) 
Because the output of each node is Boolean valued, each perceptron has at least 
two inputs. This implies that f has at most n - I perceptrons. 
The first step of the algorithm is to identify the weight wi that springs out of each 
input variable xi. For this we appe
