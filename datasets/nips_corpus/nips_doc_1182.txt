Ensemble Methods for Phoneme 
Classification 
Steve Waterhouse Gary Cook 
Cambridge University Engineering Department 
Cambridge CB2 1PZ, England, Tel: [-t-44] 1223 332754 
Email: srw1001@eng.cam.ac.uk, gdc@eng.cam.ac.uk 
Abstract 
This paper investigates a number of ensemble methods for improv- 
ing the performance of phoneme classification for use in a speech 
recognition system. Two ensemble methods are described; boosting 
and mixtures of experts, both in isolation and in combination. Re- 
sults are presented on two speech recognition databases: an isolated 
word database and a large vocabulary continuous speech database. 
These results show that principled ensemble methods such as boost- 
ing and mixtures provide superior performance to more naive en- 
semble methods such as averaging. 
INTRODUCTION 
There is now considerable interest in using ensembles or committees of learning 
machines to improve the performance of the system over that of a single learning 
machine. In most neural network ensembles, the ensemble members are trained on 
either the same data (Hansen gt Salamon 1990) or different subsets of the data (Per- 
rone gt Cooper 1993). The ensemble members typically have different initial con- 
ditions and/or different architectures. The subsets of the data may be chosen at 
random, with prior knowledge or by some principled approach e.g. clustering. Addi- 
tionally, the outputs of the networks may be combined by any function which results 
in an output that is consistent with the form of the problem. The expectation of 
ensemble methods is that the member networks pick out different properties present 
in the data, thus improving the performance when their outputs are combined. 
The two techniques described here, boosting (Drucker, Schapire gt Simard 1993) 
and mixtures of experts (Jacobs, Jordan, Nowlan gt Hinton 1991), differ from simple 
ensemble methods. 
Ensemble Methods for Phoneme Classification 801 
In boosting, each member of the ensemble is trained on patterns that have been 
filtered by previously trained members of the ensemble. In mixtures, the members 
of the ensemble, or experts, are trained on data that is stochastically selected by 
a gate which additionally learns how to best combine the outputs of the experts. 
The aim of the work presented here is twofold and inspired from two differing but 
complimentary directions. Firstly, how does one select which data to train the en- 
semble members on and secondly, given these members how does one combine them 
to achieve the optimal result? The rest of the paper describes how a combination 
of boosting and mixtures may be used to improve phoneme error rates. 
PHONEME CLASSIFICATION 
Speech 
, model JJ 
MLP Ensemble 
Utterance 
hypothesis 
Figure 1: The ^SSOT hybrid connectionist-HMM speech recognition system with 
an MLP ensemble acoustic model 
The Cambridge University Engineering Department connectionist speech recogni- 
tion system (^SSOT) uses a hybrid connectionist - hidden Markov model (HMM) 
approach. This is shown in figure 1. A connectionist acoustic model is used to map 
each frame of acoustic data to posterior phone probabilities. These estimated phone 
probabilities are then used as estimates of the observation probabilities in an HMM 
framework. Given new acoustic data and the connectionist-HMM framework, the 
maximum a posterJori word sequence is then extracted using a single pass, start 
synchronous decoder. A more complete description of the system can be found 
in (Hochberg, Renals gt Robinson 1994). 
Previous work has shown how a novel boosting procedure based on utterance se- 
lection can be used to increase the performance of the recurrent network acoustic 
model (Cook gt Robinson 1996). In this work a combined boosting and mixtures- 
of-experts approach is used to improve the performance of MLP acoustic models. 
Results are presented for two speech recognition tasks. The first is phonetic clas- 
sification on a small isolated digit database. The second is a large vocabulary 
continuous speech recognition task from the Wall Street Journal corpus. 
ENSEMBLE METHODS 
Most ensemble methods can be divided into two separate methods; network selec- 
tion and network combination. Network selection addresses the question of how to 
802 S. Waterhouse and G. Cook 
choose the data each network is trained on. Network combination addresses the 
question of what is the best way to combine the outputs of these trained networks. 
The simplest method for network selection is to train separate networks on separate 
regions of the data, chosen either randomly, with prior knowledge or according to 
some other criteria, e.g. clustering. 
The simplest method of combining the outputs of several networks is to form an 
average, or simple additive merge: y(t)  K 
--' Ek--I y(t), where y(t) is the output 
of the k th network at time t. 
Boosting 
Boosting is a procedure which results in an ensemble of networks. The networks 
in a boosting ensemble are trained sequentially on data that has been filtered by 
the previously trained networks in the ensemble. This has the advantage that 
only data that is likely to result in improved generalization performance is used 
for training. The first practical application of a boosting procedure was for the 
optical character recognition task (Drucker et al. 1993). An ensemble of feedforward 
neural networks was trained using supervised learning. Using boosting the authors 
reported a reduction in error rate on ZIP codes from the United States Postal Service 
of 28% compared to a single network. The boosting procedure is as follows: train a 
network on a randomly chosen subset of the available training data. This network 
is then used to filter the remaining training data to produce a training set for a 
second network with an even distribution of cases which the first network classifies 
correctly and incorrectly. After training the second network the first and second 
networks are used to produce a training set for a third network. This training set 
is produced from cases in the remaining training data that the first two networks 
disagree on. 
The boosted networks are combined using either a voting scheme or a simple add as 
described in the previous section. The voting scheme works as follows: if the first 
two networks agree, take their answer as the output, if they disagree, use the third 
network's answer as the output. 
Mixtures of Experts 
The mixture of experts (Jacobs et al. 1991) is a different type of ensemble to the 
two considered so far. The ensemble members or experts are trained with data 
which is stochastically selected by a gate. The gate in turn learns how to best 
combine the experts given the data. The training of the experts, which are typically 
single or multi-layer networks, proceeds as for standard networks, with an additional 
weighting of the output error terms by the posterior probabilty hi(t) of selecting 
expert i given the current data point at time (t): hi(t) = gi(t).Pi(t)/-j gj(t).Pj(t), 
where gi(t) is the output of the gate for expert i, and Pi(t) is the probability of 
obtaining the correct output given expert i. In the case of classification, considered 
here, the experts use softmax output units. The gate, which is typically a single 
or multi-layered network with softmax output units is trained using the posterior 
probabilities as targets. The overall output y(t) of the mixture of experts is given by 
the weighted combination of the gate and expert outputs: y(t) K 
-- Ek=l g(t).yl(t), 
where y(t) is the output of the k th expert, and g(t) is the output of the gate for 
Ensemble Methods for Phoneme Classification 803 
expert k at time t. 
The mixture of experts is based on the principle of divide and conquer, in which a 
relatively hard problem is broken up into a series of smaller easier to solve problems. 
By using the posterior probabilities to weight the experts and provide targets for 
the gate, the effective data sets used to train each expert may overlap. 
SPEECH RECOGNITION RESULTS 
This section describes the results of experiments on two speech databases: the 
Bellcore isolated digits database and the Wall Street Journal Corpus (Paul gt Baker 
1992). The inputs to the networks consist of 9 frames of acoustic feature vectors; 
the frame on which the network is currently performing classification, plus 4 frames 
of left context and 4 frames of right context. The context frames allow the network 
to take account of the dynamical nature of speech. Each acoustic feature vector 
consists of 8th order PLP plus log energy coefficients along with the dynamic delta 
coeficients of these coefficients, computed with an analysis window of 25ms, every 
12.5 ms at a sampling rate of 8kHz. The speech is labelled with 54 phonemes 
according to the standard ABBOT phone set. 
Bellcore Digits 
The Bellcore digits database consists of 150 speakers saying the words zero 
through nine, oh, no and yes. The database was divided into a train- 
ing set of 122 speakers, a cross validation set of 13 speakers and a test set of 15 
speakers. Each method was evaluated over 10 partitions of the data into different 
training, cross validation and test sets. In all the experiments on the Bellcore digits 
multi-layer perceptrons with 200 hidden units were used as the basic network mem- 
bers in the ensembles. The gates in the mixtures were also multi-layer perceptrons 
with 20 hidden units. 
Ensemble Combination Phone Error Rate 
Method Average tr 
Simple ensemble cheat 14.7 % 0.9 
Simple ensemble vote 20.3 % 1.2 
Simple ensemble average 19.3 % 1.2 
Simple ensemble soft gated 20.9 % 1.2 
Simple ensemble hard gated 19.3 % 1.0 
Simple ensemble mixed 17.1% 1.3 
Boosted ensemble cheat 11.9 % 1.0 
Boosted ensemble vote 17.8 % 1.1 
Boosted ensemble average 17.4 % 1.1 
Boosted ensemble soft gated 17.8 % 1.0 
Boosted ensemble hard gated 17.4 % 1.2 
Boosted ensemble mixed 1
