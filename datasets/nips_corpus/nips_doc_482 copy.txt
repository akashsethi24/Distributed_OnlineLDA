Time-Warping Network: 
A Hybrid Framework for Speech Recognition 
Esther Levin 
Roberto Pieraccini 
AT&T Bell Laboratories 
Speech Research Department 
Murray Hill, NJ 07974 USA 
Enrico Bocchieri 
ABSTRACT 
Recently, much interest has been generated regarding speech 
recognition systems based on Hidden Markov Models (HMMs) and 
neural network (NN) hybrids. Such systems attempt to combine the 
best features of both models: the temporal structure of HMMs and 
the discriminative power of neural networks. In this work we define 
a time-warping (TW) neuron that extends the operation of the formal 
neuron of a back-propagation network by warping the input pattern to 
match it optimally to its weights. We show that a single-layer 
network of TW neurons is equivalent to a Gaussian nsity HMM- 
based recognition system, and we propose to unprove the 
discriminative power of this system by using back-propagation 
discriminative training, and/or by generalizing the structure of the 
recognizer to a multi-layered net. The performance of the proposed 
network was evaluated on a highly confusable, isolated word, multi 
speaker recognition task. The results indicate that not only does the 
recognition performance improve, but the separation between classes 
is enhanced also, allowing us to set up a rejection criterion to 
improve the confidence of the system. 
I. INTRODUCTION 
Since their first application in h recognition systems in the late seventies, hidden 
Markov models have been established as a most useful tool, mainly due to their ability 
to handle the sequential dynamical nature of the speech signal. With the revival of 
connectionism in the mid. eighties, considerable interest arose in applying artificial 
neural networks for speech recognition. This interest was based on the discriminafive 
power of NNs and their ability to deal with non-explicit knowledge. These two 
paradigms, namely HMM and NN, inspired by different philosophies, were seen at first 
as different and competing tools. Recently, links have been established between these 
two paradigms, aiming at a hybrid framework in which the advantages of the two 
models can be combined. For example, Boutlard and Wellekens [1] showed that neural 
151 
152 Levin, Pieraccini, and Bocchieri 
networks with proper architecture can be regarded as non-parametric models for 
computing discriminant probabilities related to HMM. Bridle [2] introduced 
Alpha-nets, a recurrent neural architecture that implements the alpha computation of 
HMM, and found connections between back-propagation [3] training and discriminative 
HMM parameter estimation. Predictive neural nets were shown to have a statistical 
interpretation [4], generalizing the conventional hidden Markov model by assuming 
that the speech signal is generated by nonlinear dynamics contaminated by noise. 
In this work we establish one more link between the two paradigms by introducing the 
time-warping network (TWN) that is a generalization of both an HMM-based 
recogmzer and a back-propagation net. The basic element of such a network, a t/me- 
warping neuron, generalizes the function of a formal neuron by warping the input 
signal in order maximize its activation. In the special case of network parameter 
values, a single-layered network of time-warping (TW) neurons is equivalent to a 
recognizer based on Gaussian HMMs. This equivalence of the HM -based recognizer 
and single-layer TWN suggests ways of using discriminafive neural tools to enhance 
the performance of the recognizer. For instance, a training algorithm, like back- 
propagation, that minimizes a quantity related to the recognition performance, can be 
used to train the recognizer instead of the standard non-discfiminative maximum 
likelihood training. Then, the architecture of the recognizer can be expanded to 
contain more than one layer of units, enabling the network to form discriminant feature 
detectors in the hidden layers. 
This paper is organized as follows: in the first pan of Section 2 we describe a simple 
HMM-based recognizer. Then we define the time-warping neuron and show that a 
single-layer network built with such neurons is equivalent to the HM recognizer. In 
Section 3 two methods are proposed to improve the discriminafive power of the 
recognizer, namely, adopting neural training algorithms and extending the structure of 
the recognizer to a multi-layer net. For special cases of such multi-layer architecture 
such net can implement a conventional or weighted [5] HMM recognizer. Results of 
experiments using a TW network for recognition of the English E-set are presented in 
Section 4. The results indicate that not only does the recognition performance 
improve, but the separation between classes is enhanced also, allowing us to set up a 
rejection criterion to improve the confidence of the system. A summary and discussion 
of this work are included in Section 5. 
II. THE MODEL 
In this section first we describe the basic HMM-based speech recognition system that 
is used in many applications, including isolated and connected word recognition [6] 
and large vocabulary subword-based recognition [7]. Though in this paper we treat the 
case of isolated word recognition, generalization to connected speech can be made like 
in [6,7]. In the second pan of this section we define a single-layered time-warping 
network and show that it is equivalent to the HMM based recognizer when certain 
conditions constraining the network parameter values apply. 
II.1 THE HIDDEN MARKOV MODEL-BASED RECOGNITION SYSTEM 
A HMM-based recognition system consists of K N-state HMMs, where K is the 
vocabulary size (number of words or subword units in the defined task). The k-th 
HMM, ', is associated with the k-th word in the vocabulary and is characterized by a 
matrix A'= {a/ } of transition probabilities between states, 
ai=Pr(st=j I s,_=i) , O<i <N , 1 <j <N, (1) 
where s, denotes the active state at time t (So =0 is a dummy initial state) and by a set 
of emission probabilities (one per state): 
Time-Warping Network: A Hybrid Framework for Speech Recognition 153 
1 k* 
1 exp [-(Xt-I&) (Z) - (Xt-I&)] i=1, .-. ,N (2) 
Pr(X, I s,=i II I I 2 ' ' 
where Xt is the d-dimensional observation vector describing some parametric 
representation of the t-th frame of the spoken token, and ()* denotes the transpose 
operation. 
For the case discussed here, we concentrate on strictly left-to-right HMMs, where 
a/: : 0 only if j=i or j=i+l, and a simplified case of (2) where all Zi=la, the 
d-Z-dimensional unit matrix. 
The system recognizes a speech token of duration T, X={X,Y2,''',Xr}, by 
classifying the token into the class k 0 with the highest likelihood L � (X), 
ko = arg maxL t (X) . (3) 
The likelihood Lk(X) is computed for the k-th HMM as 
Lt(X)= max log[Pr(X Iilt, si=i , ... ,st=i,)] (4) 
[i,. � -/4 
= max II II 2+1oga ,-log2r. 
[i, ' ,it] = 
The state sequence that maximizes (4) is found by using the Viterbi [8] algorithm. 
H.2 THE EQUIVALENT SINGLE-LAYER TIME-WARPING NETWORK 
A single-layer TW network is composed of K TW neurons, one for each word in the 
vocabulary. The TW neuron is an extension of a formal neuron that can handle 
dynamic and temporally distorted patterns. The k-th TW neuron, associated with^te 
k-l v,,ot:abulary,, kword, is chartcterized by a bias w and a set of weights, W = 
{W,Wi, � � � ,W} , where W[ is a column vector of dimensionality d+2. Given an 
input speech token of duration T, X={X,X2, � � � ,Xr}, the output activation yt of the 
k-th unit is computed as 
T ,,, N 
y'=g( ZXt'< +w{ )=g( Z ( Z ;)'l/+w{ ), (5) 
t=l j=l t:i,=j 
where g (-) is a sigrnoidal, smooth, strictly increasing nonlinearity, and Xt = [X, 1,1 ] 
is an d+2 - dimensional augmented input vector. The corresponding indices it, 
t=l, � � � ,T are determined by the following condition: 
T ,,, 
{ i,''',ir} =argmax Xt' +w. (6) 
t=l 
In other words, a TW neuron warps the input pattern to match it optimally to its 
weights (6) and computes its output using this warped version of the input (5). The 
time-warping process of (6) is a distinguishing feature of this neural model, enabling it 
to deal with the dynamic nature of a speech signal and to handle temporal distortions. 
All TW neurons in this single-layer net recognizer receive the same input speech token 
X. Recognition is performed by selecting the word class corresponding to the neuron 
with the maximal output activation. 
It is easy to show that when 
II II 2, logan./1, (7a) 
and 
154 Levin, Pieraccini, and Bocchieri 
N 
w =  log ad_ 1 -log ajl,i (7b) 
this network is equivalent to an HMM-based recognition system, with K N-state 
HMMs, as described above) 
This equivalent neural representation of an HM -based system suggests ways of 
improving the discfiminafive power of the recognizer, while preserving the temporal 
structure of the HMM, thus allowing generalization to more complicated tasks (e.g., 
continuous speech, subword units, etc.). 
HI. IMPROVING DISCRIMINATION 
There are two important differences between the HMM-based system and a neural net 
approach to speech recognition that contribute to the improved discrimination power of 
the latter, namely, training and structure. 
HI.1 DISCRIMINATIVE TRAINING 
The HMM parameters are usually estimated by applying the maximum likelihood 
approach, using only the examples of the word represented by the model and 
disregarding the rival classes completely. This is a non-discriminative approach: the 
learning criterion is not directly connected to the improvement of recognition accuracy. 
Here we propose to enhance the discfiminative power of the system by adopting a 
neural training approach. 
NN training algorithms are based on minimizing an error function E, which is related 
to the performance of the network on the training set of labeled examples, {Xt,gt}, 
/=1, � � � ,L, where gt=[z, � � � ,zc]* denotes the vector of ta
