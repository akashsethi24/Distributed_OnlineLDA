On the Computational Complexity of Networks of 
Spiking Neurons 
(Extended Abstract) 
Wolfgang Maass 
Institute for Theoretical Computer Science 
Technische Universitaet Graz 
A-8010 Graz, Austria 
e-mail: maass@igi.tu-graz.ac.at 
Abstract 
We investigate the computational power of a formal model for net- 
works of spiking neurons, both for the assumption of an unlimited 
timing precision, and for the case of a limited timing precision. We 
also prove upper and lower bounds for the number of examples that 
are needed to train such networks. 
I Introduction and Basic Definitions 
There exists substantial evidence that timing phenomena such as temporal differ- 
ences between spikes and frequencies of oscillating subsystems are integral parts 
of various information processing mechanisms in biological neural systems (for a 
survey and references see e.g. Abeles, 1991; Churchland and Sejnowski, 1992; Aert- 
sen, 1993). Furthermore simulations of a variety of specific mathematical models 
for networks of spiking neurons have shown that temporal coding offers interesting 
possibilities for solving classical benchmark-problems such as associative memory, 
binding, and pattern segmentation (for an overview see Gerstner et al., 1992). Some 
aspects of these models have also been studied analytically, but almost nothing is 
known about their computational complexity (see Judd and Aihara, 1993, for some 
first results in this direction). In this article we introduce a simple formal model 
SNN for networks of spiking neurons that allows us to model the most important 
timing phenomena of neural nets (including synaptic modulation), and we prove up- 
per and lower bounds for its computational power and learning complexity. Further 
184 Wolfgang Maass 
details to the results reported in this article may be found in Maass, 1994a,1994b, 
1994c. 
Definition of a Spiking Neuron Network (SNN): 
- a finite directed graph (V, E) (we refer to the elements of V as 
and to the elements of E as synapses) 
- a subset . C- V of input neurons 
An SNN Af consists of 
neurons 
- a subset Vout C- V of output neurons 
- for each neuron v  V -l/i, a threshold-function Ov: R + -- R t3 {oo} 
(where R + :- {x  R:x >_ 0}) 
-for each synapse (u,v)  E a response-function zu,v : R + -- R and a 
weight- function w., : R + ---. R 
We assume that the firing of the input neurons v 6 . is determined from outside 
of .Af, i.e. the sets Fv C- R + of firing times (spike trains)for the neurons v 6 
Vi, are given as the input of .Af. Furthermore we assume that a set T C_ R + of 
potential firing times has been fixed. 
For a neuron v  V - Vi, one defines its set Fv of firing times recursively. The 
first element of F is inf{t  T ' P(t) >_ 0(0)) , and for any s  F the next 
larger element of Fv is inf{t  T ' t > s and Pv(t) >_ Or(t-s)) , where the 
potential function Pv ' R+ -- R is defined by 
:= o + 
u'(u,v)E sF.'s<t 
The firing times (spike trains) Fof the output neurons v 6 Vout that result in 
this way are interpreted as the output of .Af. 
Regarding the set T of potential firing times we consider in this article the case 
T = R + (.SNN with continuous time.) and the case T = {i .p ï¿½ i 6 N) for some p 
with lip  N (SNN with discrete time). 
We assume that for each SNN Af there exists a bound r 6 R with rf > 0 such 
that Ov(x) = oo for all x 6 (0, rr) and all v  V- , (rr may be interpreted 
as the minimum of all refractory periods re! of neurons in Af). Furthermore we 
assume that all input spike trains Fv with v 6 , satisfy {Fv (3 [0, t]l < oo for 
all t  R +. On the basis of these assumptions one can also in the continuous case 
easily show that the firing times are well-defined for all v  V - , (and occur in 
distances of at least rr). 
Input- and Output-Conventions: For simulations between SNN's and Turing 
machines we assume that the SNN either gets an input (or produces an output) 
from {0, 1}* in the form of a spike-train (i.e. one bit per unit of time), or encoded 
into the phase-difference of just two spikes. Real-valued input or output for an SNN 
is always encoded into the phase-difference of two spikes. 
Remarks 
a) In models for biological neural systems one assumes that if x time-units have 
On the Computational Complexi of Networks of Spiking Neurons 185 
passed since its last firing, the current threshold O (z) of a neuron v is infinite 
for x < re! (where re! = refractory period of neuron v), and then approaches 
quite rapidly from above some constant value. A neuron v fires (i.e. it sends an 
action potential or spike along its axon) when its current membrane potential 
Pv(t) at the axon hillock exceeds its current threshold Or. Pv(t) is the sum of 
various postsynaptic potentials wu,v (s) 'zu,v (t - s). Each of these terms describes an 
ezcitatory (EPSP) or inhibitory (IPSP) poslsynaplic potential at the axon hillock of 
neuron v at time t, as a result of a spike that had been generated by a presynaptic 
neuron u at time s, and which has been transmitted through a synapse between both 
neurons. Recordings of an EPSP typically show a function that has a constant value 
e (e = resting membrane potential; e.g. e - -70mV) for some initial time-interval 
(reflecting the axonal and synaptic transmission time), then rises to a peak-value, 
and finally drops back to the same constant value e. An IPSP tends to have the 
negative shape of an EPSP. For the sake of mathematical simplicity we assume in 
the SNN-model that the constant initial and final value of all response-functions 
z, is equal to 0 (in other words: z, models the difference between a postsynaptic 
potential and the resting membrane potential e). Different presynaptic neurons u 
generate postsynaptic potentials of different sizes at the axon hillock of a neuron 
v, depending on the size, location and current state of the synapse (or synapses) 
between u an.d v. This effect is modelled by the weight-factors w,(s). 
The precise shapes of threshold-, response-, and weight-functions vary among dif- 
ferent biological neural systems, and even within the same system. Fortunately one 
can prove significant upper bounds for the computational complexity of SNN's 
without any assumptions about the specific shapes of these functions of A/'. Instead, 
we only assume that they are of a reasonably simple malhemalicai slruclure. 
b) In order to prove lower bounds for the computational complexity of an SNN 
one is forced to make more specific assumptions about these functions. All lower 
bound results that are reported in this article require only some rather weak basic 
assumplions about the response- and threshold-functions. They mainly require 
that EPSP's have some (arbitrarily short) segment where they increase linearly, 
and some (arbitrarily short) segment where they decrease linearly (for details see 
Maass, 1994a, 1994b). 
c) Although the model SNN is apparently more realistic than all models for bio- 
logical neural nets whose computational complexity has previously been analyzed, 
it deliberately sacrifices a large number of more intricate biological details for the 
sake of mathematical tractability. Our model is closely related to those of (Buh- 
mann and Schulten, 1986), and (Gerstner, 1991, 1992). Similarly as in (Buhmann 
and Schulten, 1986) we consider here only the deterministic case. 
d) The model SNN is also suitable for investigating algorithms that involve synaptic 
modulation at various time-scales. Hence one can investigate within this framework 
not only the complexity of algorithms for supervised and unsupervised learning, but 
also the potential computational power of rapid weight-changes within the course of 
a computation. In the theorems of this paper we allow that the value of a weight 
wu,v(s) at a firing time s  Fu is defined by an algebraic computation tree (see van 
Leeuwen, 1990) in terms of its value at previous firing times s'  F with s' < s, 
some preceding firing times , < s of arbitrary other neurons, and arbitrary real- 
valued parameters. In this way w, (s) can be defined by different rational functions 
186 Wolfgang Maass 
of the abovementioned arguments, depending on the numerical relationship between 
these arguments (which can be evaluated by comparing first the relative size of 
arbitrary rational functions of these arguments). As a simple special case one can 
for example increase wa,v (perhaps up to some specified saturation-value) as long 
as neurons u and v fire coherently, and decrease wa,v otherwise. 
For the sake of simplicity in the statements of our results we assume in this extended 
abstract that the algebraic computation tree for each weight wa,v involves only 
O(1) tests and rational functions of degree O(1) that depend only on O(1) of the 
abovementioned arguments. Furthermore we assume in Theorems 3, 4 and 5 that 
either each weight is an arbitrary time-invariant real, or that each current weight is 
rounded off to bit-length poly(logpr) in binary representation, and does not depend 
on the times of firings that occured longer than time O(1) ago. Furthermore we 
assume in Theorems 3 and 5 that the parameters in the algebraic computation tree 
are rationals of bit-length O(logpr). 
e) It is well-known that the Vapnik-Chervonenkis dimension (VC-dimension) of 
a neural net J (and the pseudo-dimension for the case of a neural net J with real- 
valued output, with some suitable fixed norm for measuring the error) can be used 
to bound the number of examples that are needed to train Af (see Haussler, 1992). 
Obviously these notions have to be defined differently for a network with time- 
dependent weights. We propose to define the VC-dimension (pseudo-dimension)of 
an SNN A; with time-dependent weights as the VC-dimension (pseudo-dimension) 
of the class of all functions that can be computed by Af with different assignments of 
values to the real-valued (or rationa
