317 
PARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  
Richard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch 
Center for the Neurobiology of Learning and Memory 
University of California 
Irvine, CA, 91717 
ABSTRACT
To process sensory data, sensory brain areas must preserve information about both 
the similarities and differences among learned cues: without the latter, acuity would 
be lost, whereas without the former, degraded versions of a cue would be erroneously 
thought to be distinct cues, and would not be recognized. We have constructed a 
model of piriform cortex incorporating a large number of biophysical, anatomical and 
physiological parameters, such as two-step excitatory firing thresholds, necessary and 
suicient conditions for long-term potentiation (LTP) of synapses, three distinct types 
of inhibitory currents (short IPSPs, long hyperpolarizing currents (LHP) and long cell- 
specific afterhyperpolarization (AHP)), sparse connectivity between bulb and layer-II 
cortex, caudally-fiowing excitatory collateral fibers, nonlinear dendritic summation, etc. 
We have tested the model for its ability to learn similarity- and difference-preserving 
encodings of incoming sensory cues; the biological characteristics of the model enable it 
to produce multiple encodings of each input cue in such a way that different readouts of 
the cell firing activity of the model preserve both similarity and difference-iuiormation. 
In particular, probabilistic quant al transmitter-release properties of pitiform synapses 
give rise to probabilistic postsynaptic voltage levels which, in combination with the ac- 
tivity of local patches of inhibitory interneurons in layer H, differentially select bursting 
rs. single-pulsing layer-II cells. Time-locked firing to the theta rhythm (Larson and 
Lynch, 1986) enables distinct spatial patterns to be read out against a relatively quies- 
cent background firing rate. raining trials using the physiological rules for induction of 
LTP yield stable layer-II-cell spatial firing patterns for learned cues. Multiple simulated 
olfactory input patterns (i.e., those that share many chemical features) will give rise 
to strongly-overlapping bulb firing patterns, activating many shared lateral olfactory 
tract (LOT) axons innervating layer Ia of pitiform cortex, which in turn yields highly 
overlapping layer-H-cell excitatory potentials, enabling this spatial layer-II-cell encod- 
ing to preserve the overlap (similarity) among similar inputs. At the same time, those 
synapses that are enhanced by the learning process cause stronger cell firing, yielding 
strong, cell-specific afterhyperpolarizing (AHP) currents. Local inhibitory interneurons 
effectively select alternate cells to fire once strongly-firing cells have undergone AHP. 
These alternate cells then activate their caudally-fiowing recurrent collaterals, activat- 
ing distinct populations of synapses in caudal layer Ib. Potentiation of these synapses 
in combination with those of still-active LOT axons selectively enhance the response of 
caudal cells that tend to accentuate the differences among even very-similar cues. 
Empirical tests of the computer simulation have shown that, after training, the 
initial spatial layer II cell firing responses to similar cues enhance the similarity of 
the cues, such that the overlap in response is equal to or greater than the overlap in 
Thls research was supported in part by the Otce of Naval lesearch under grants N00014-84-K-0391 
and N00014-87-K-0838 and by the National Science Foundation under grant IST-85-12419. 
ï¿½ American Institute of Physics 1988 
318 
input cell firing (in the bulb): e.g., two cues that overlap by 65% give rise to response 
patterns that overlap by 80% or more. Reciprocally, later cell firing patterns (after 
AHP), increasingly enhance the differences among even very-similar patterns, so that 
cues with 90% input overlap give rise to output responses that overlap by less than 10%. 
This difference-enhancing response can be measured with respect to its acuity; since 90% 
input overlaps are reduced to near zero response overlaps, it enables the structure to 
distinguish between even very-similar cues. On the other hand, the similarity-enhancing 
response is properly viewed as a partitioning mechanism, mapping quite-distinct input 
cues onto nearly-identical response patterns (or category indicators). We therefore use 
a statistical metric for the information value of categorizations to measure the value of 
partitionings produced by the pitiform simulation network. 
INTRODUCTION 
The three primary dimensions along which network processing models vary are their 
learning rules, their performance rules and their architectural structures. In practice, 
perfornxance rules are much the same across different models, usually being some variant 
of a 'weighted-sum' rule (in which a unit's output is calculated as some function of the 
sum of its inputs multiplied by their 'synaptic' weights). Perfornxance rules are usually 
either 'static' rules (calculating unit outputs and halting) or 'settling' rules (iteratively 
calculating outputs until a convergent solution is reached). Most learning rules are 
either variants of a 'correlation' rule, loosely based on Hebb's (1949) postulate; or a 
'delta' rule, e.g., the perceptron rule (Rosenblatt, 1962), the adaline rule (Widrow and 
Hoff, 1960) or the generalized delta or 'backpropagation' rule (Parker, 1985; Rumelhart 
et al., 1986). Finally, architectures vary by and large with learning rules: e.g., multi- 
layered feedforward nets require a generalized delta rule for convergence; bidirectional 
connections usually imply a variant of a Hebbian or correlation rule, etc. 
Architectures and learning and performance rules are typically arrived at for reasons 
of their convenient computational properties and analytical tractability. These rules 
are sometimes based in part on some results borrowed from neurobiology: e.g., 'units' 
in some network models are intended to correspond loosely to neurons, and 'weights' 
loosely to synapses; the notions of parallelism and distributed processing are based on 
metaphors derived from neural processes. 
An open question is how much of the rest of the rich literature of neurobiological 
results should or could profitably be incorporated into a network model. lrom the 
point of view of constructing mechanisms to perform certain pre-specified computatonal 
functions (e.g., correlation, optimization), there are varying answers to this question. 
However, the goal of understanding brain circuit function introduces a fundamental 
problem: there are no known, pre-specifled functions of any given cortical structures. 
We have constructed and studied a physiologically- and anatomically-accurate model of 
a particular brain structure, olfactory cortex, that is strictly based on biological data, 
with the goal of elucidating the local function of this circuit from its perforinance in a 
bottom-up' fashion. We measure our progress by the accuracy with which the model 
corresponds to known data, and predicts novel physiological results (see, e.g., Lynch 
and Granger, 1988; Lynch et al., 1988). 
Our initial analysis of the circuit reveals a mechanism consisting of a learning rule 
that is notably simple and restricted compared to most network models, a relatively 
novel architecture with some unusual properties, and a performance rule that is ex- 
319 
traordinarily complex compared to typical network-model performance rules. Taken 
together, these rules, derived directly from the known biology of the olfactory cortex, 
generate a coherent mechanism that has interesting computational properties. This pa- 
per describes the learning and performance rules and the architecture of the model; the 
relevant physiology and anatomy underlying these rules and structures, respectively; 
and an analysis of the coherent mechanism that results. 
LEARNING RULES DERIVED FROM LONG-TERM POTENTIATION 
Long-term potentiation (LTP) of synapses is a phenomenon in which a brief series 
of biochemical events gives rise to an enhancement of synaptic efficacy that is extraordi- 
narily long-lasting (Bliss and Lzmo, 1973; Lynch and Baudry, 1984; Staubli and Lynch, 
1987); it is therefore a candidate mechanism underlying certain forms of learning, in 
which few triniug trials are required for long-lasting memory. The physiological char- 
acteristics of LTP form the basis for a straightforward network learning rule. 
It is known that simultaneous pre- and post-synaptic activity (i.e., intense depolar- 
ization) result in LTP (e.g., WigstrZm et al., 1986). Since excitatory cells are embedded 
in a meshwork of inhibitory interneurons, the requisite induction of adequate levels of 
pre- and postsynaptic activity is achieved by stimulation of large numbers of afferents 
for prolonged periods, by voltage clamping the postsynaptic cell, or by chemically block- 
ing the activity of inhibitory interneurons. In the intact animal, however, the question 
of how simultaneous pre- and postsynaptic activity rnght be induced has been an open 
question. Recent work (Larson and Lynch, 1986) has shown that when hippocampal 
afferents are subjected to patterned stimulation with particular temporal and frequency 
parameters, inhibition is naturally eliminated within a specific time window, and LTP 
can arise as a result. Figure I shows that LTP naturally occurs using short (3-4 pulse) 
bursts of high-frequency (100Hz) stimulation with a 200ms interburst interval; only the 
second of a pair of two such bursts causes potentiation. This occurs because the normal 
short inhibitory currents (IPSPs), which prevent the first burst from depolarizing the 
postsynaptic cell sufficiently to produce LTP, are maximally refractory at 200ms after 
being stimulated, and therefore, although the second burst arrives against a hyperpolar- 
ized backgroun
