Agnostic PAC-Learning of Functions 
Analog Neural Nets 
(Extended Abstract) 
on 
Wolfgang Maass 
Institute for Theoretical Computer Science 
Technische Universitaet Graz 
Klosterwiesgasse 32/2 
A-8010 Graz, Austria 
e-mail: maass@igi.tu-graz.ac.at 
Abstract: 
There exist a number of negative results ([J], [BR], [KV]) about 
learning on neural nets in Valiant's model IV] for probably approx- 
imately correct learning (PAC-learning). These negative results 
are based on an asymptotic analysis where one lets the number of 
nodes in the neural net go to infinity. Hence this analysis is less ad- 
equate for the investigation of learning on a small fixed neural net 
with relatively few analog inputs (e.g. the principal components of 
some sensory data). The latter type of learning problem gives rise 
to a different kind of asymptotic question: Can the true error of the 
neural net be brought arbitrarily close to that of a neural net with 
optimal weights through sufficiently long training? In this paper 
we employ some new arguments in order to give a positive answer 
to this question in Haussler's rather realistic refinement of Valiant's 
model for PAC-learning ([H], [KSS]). In this more realistic model 
no a-priori assumptions are required about the learning target, 
noise is permitted in the training data, and the inputs and outputs 
are not restricted to boolean values. As a special case our result 
implies one of the first positive results about learning on multi-layer 
neural nets in Valiant's original PAC-learning model. At the end 
of this paper we will describe an efficient parallel implementation 
of this new learning algorithm. 
311 
312 Maass 
We consider multi-layer high order feedforward neural nets A/' with arbitrary piece- 
wise polynomial activation functions. Each node g of fan-in m > 0 in iV' is 
called a computation node. It is labelled by some polynomial Q9(yl,..., y,) 
and some piecewise polynomial activation function 79 : R - R. We assume 
that 7 g consists of finitely many polynomial pieces and that its definition in- 
volves only rational parameters. The computation node g computes the function 
{Y,...,Y,/ - 79(Q9(Y, � ..,Y,)) from R ' into R. The nodes of fan-in 0 in 
(input nodes) are labelled by variables xl,..., xk. The nodes g of fan-out 0 in 
iV' (output nodes) are labelled by 1,..., l. We assume that the range B of their 
activation functions 79 is bounded. Any parameters that occur in the definitions of 
the 79 are referred to as architectural parameters of iV'. 
The coefficients of all the polynomials Q9 are called the programmable parameters 
(or weights) of iV'. Let w be the number of programmable parameters of iV'. For any 
assignment c E R w to the programmable parameters of iV' the network computes 
a function from R k into R l which we will denote by A/'-. 
We write Qn for the set of rational numbers that can be written as quotients of 
l 
integers with bit-length _< n. For _z = Iz,...,zll E R l we write IIzl] for  Izj]. 
j=l 
Let F � R  - R l be some arbitrary functi6n, which we will view as a prediction 
rule. For any given instance (_x,y) G R  x R l we measure the error of F by 
I[F(_x) -yll. For any distribution A over some subset of R  x R t we measure the 
true error off with regard to A by E(�,y__)e.4[llF(_x ) -Yll], i.e. the expected value 
of the error of F with respect to distribution A. 
Theorem 1: Let A/' be some arbitrary high order feedforward neural net with piece- 
wise polynomial activation functions. Let w be the number of programmable para- 
meters of A/' (we assume that w = O(1)). Then one can construct from A/' some 
first order feedforward neural net J(/' with piecewise linear activation functions and 
2 
the quadratic activation function 7(x) = x , which has the following property: 
There exists a polynomial m(, �) and a learning algorithm LEARN such that for 
any given s, 5, (0,1) and s,n  N and any distribution A over Q x (Q, (3B)  
the following holds: 
For any sample  '- ((xi, Yi})i=X,...,m of m > m(1 1 
- 7, 7) points that are independently 
drawn according to A the algorithm LEARN computes in polynomially in m, s, n 
computation steps an assignment  of rational numbers to the programmable para- 
meters of J(/' such that with probability >_ 1 - 5: 
- yll] _< . + inf - Yllx], 
_ EQ 
or in other words: 
The true error of J(f5 with regard to A is within e of the least possible true error 
that can be achieved by any A/'- with c E Q. 
Remarks 
a) One can easily see (see [M 93b] for details) that Theorem 1 provides a 
positive learning result in Haussler's extension of Valiant's model for PAC- 
learning ([H], [KSS]). The touchstone class (see [KSS]) is defined as the 
Agnostic PAC-Learning of Functions on Analog Neural Nets 313 
b) 
class of function f � R k - R l that are computable on Af with program- 
mable parameters from Q. 
This fact is of some general interest, since so far only very few positive 
results are known for any learning problem in this rather realistic (but 
quite demanding) learning model. 
Consider the special case where the distribution A over Qn x (Qn n B) l is 
of the form 
{ , if_y = 
AD,_T(_x,y) = 0 , otherwise 
for some arbitrary distribution D over the domain Qn and some arbitrary 
--T E QsW. Then the term 
inf ylll] 
EQ 
is equal to 0. Hence the preceding theorem states that with learning algo- 
rithm LEARN the learning network J(/' can learn with arbitrarily small 
true error any target fimction Afar that is computable on Af with rational 
weights --T' Thus by choosing Af sufficiently large, one can guarantee 
that the associated learning network j(/' can learn any target-function 
that might arise in the context of a specific learning problem. 
In addition the theorem also applies to the more realistic situation where 
the learner receives examples (x, y) of the form (_x, Af-T (_x)+ noise), or even 
if there exists no target functih Af- that would explain the actual 
distribution A of examples (x, y) (agnostic learning). 
The proof of Theorem 1 is mathematically quite involved, 
only an outline. It consists of three steps: 
and we can give here 
(1) Construction of the auxiliary neural net .. 
(2) Reducing the optimization of weights in jrfor a given distribution A to a 
finite nonlinear optimization problem. 
(3) Reducing the resulting finite nonliuear optimization problem to a family of 
finite linear optimization problems. 
Details to step (1): If the activation fianctions  in Af are piecewise linear and 
all computation. nodes in Af have fan-out _< 1 (this occurs for example if Af has just 
one hidden layer and only one output) then one can set J(/' :- Af. If the  are 
piecewise linear but not all computation nodes in Af have fan-out _< 1 one defines 
J(f as the tree of the same depth as Af, where subcircuits of computation nodes with 
fan-out m > 1 are duplicated m times. The activation functions remain unchanged 
in this case. 
If the activation fimctions 7 g are piecewise polynomial but not piecewise linear, 
one has to apply a rather complex construction which is described in detail in the 
Journal version of [M 93a]. In any case j(f has the property that all functions that 
314 Maass 
are computable on A/' can also be computed on Jf, the depth of  is bounded by a 
constant, and the size of JQ' is bounded by a polynomial in the size of A/' (provided 
that the depth and order of iV', as well as the number and degrees of the polynomial 
pieces of the 7  are bounded by a constant). 
Details to step (2): Since the VC-dimension of a neural net is only defined 
for neural nets with boolean output, one has to consider here instead the pseudo- 
dimension of the function class c that is defined by 
Definition: (see Itaussler [It]). 
Let X be some arbitrary domain, and let jr be an arbitrary class of functions from 
X into It. Then the pseudo-dimension of Y z is defined by 
dimf,(.) := max{IS1 ' $ C- x and 3h � $-- It such that 
Vb  {0, 3f  Vx  ,S (re) >_ be) = 1)). 
Note that in the special case where . is a concept class (i.e. all f 6 . are 0- 1 
valued) the pseudo-dimension dimf,(.) coincides with the VC-dimension of.. The 
pseudo-dimension of the function class associated with network architectures J(/' with 
piecewise polynomial activation finctions can be bounded with the help of Milnor's 
Theorem [Mi] in the same way as the VC-dimension for the case of boolean network 
output (see [GJ]): 
Theorem 2: Consider arbitrary network architectures J(/' of order v with k input 
nodes, 1 output nodes, and w programmable parameters. Assume that each gate in 
Jff employs as activation fimction some piecewise polynomial (or piecewise rational) 
function of degree _< d with at most q pieces. For some arbitrary p  {1,2,...} 
we define jr := { f � R +t - R � __a  R w �_x 6 R  �y G Rt(f(__x,y) = 
I1-(_) - yll)}. The,, one has dimp(.) = O(w 2 log q) if v, d, l = O(1). 
With the hell) of the pseudo-dinension one can carry out the desired reduction of 
the optimization of weights in JQ' (with regard to an arbitrary given distribution A 
of examples {_x, y)) to a finite optimization problen. Fix some interval [b, b2] C- R 
such that B C_ [bx, b2], bx < ha, and such that the ranges of the activation functions 
of the output gates of J(f are contained in [bl,b2]. We define b := I. (ba-bx) , and 
' :---- {f � R  x [b,b2]  --+ [0, b]' __a G R w V_.x G R k Vy G [b,b2]  (f(_x,y) - 
II-(_)-yll)). Assume now that parameters s, 5 6 (0, 1) with s < b and s,n  N 
have been fixed. For convenience we assume that s is sufficiently large so that 
all architectural parameters in A/' are from Q, (we assume that all architectural 
parameters in iV' are rational). We define 
m .- 7 2-dime(). In 33eb 8 
, '- + In 
By Corollary 2 of Theorem 7 in Haussler [HI one h for m km(}, ), K :=  e 
(2,3), nd any distribution A over Q x (Q, [bl,b2])
