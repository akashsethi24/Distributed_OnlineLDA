Generalization Dynamics in 
LMS Trained Linear Networks 
Yves Chauvin* 
Psychology Department 
Stanford University 
Stanford, CA 94305 
Abstract 
For a simple linear case, a mathematical analysis of the training and gener- 
alization (validation) performance of networks trained by gradient descent 
on a Least Mean Square cost function is provided as a function of the learn- 
ing parameters and of the statistics of the training data base. The analysis 
predicts that generalization error dynamics are very dependent on a pri- 
ori initial weights. In particular, the generalization error might sometimes 
weave within a computable range during extended training. In some cases, 
the analysis provides bounds on the optimal number of training cycles for 
minimal validation error. For a speech labeling task, predicted weaving 
effects were qualitatively tested and observed by computer simulations in 
networks trained by the linear and non-linear back-propagation algorithm. 
I INTRODUCTION 
Recent progress in network design demonstrates that non-linear feedforward neu- 
ral networks can perform impressive pattern classification for a variety of real-world 
applications (e.g., Le Cun et al., 1990; Waibel et al., 1989). Various simulations and 
relationships between the neural network and machine learning theoretical litera- 
tures also suggest that too large a number of free parameters (weight overfitting) 
could substantially reduce generalization performance. (e.g., Baum, 1989 1989). 
A number of solutions have recently been proposed to decrease or eliminate the 
overfitting problem in specific situations. They range from ad hoc heuristics to 
theoretical considerations (e.g., Le Cun et al., 1990; Chauvin, 1990a; Weigend et al., 
*Also with Thomson-CSF, Inc., 630 Hansen Way, Suite 250, Palo Alto, CA 94304. 
89O 
Generalization Dynamics in LMS Trained Linear Net-works 891 
In Press). For a phoneme labeling application, Chauvin showed that the overfitting 
phenomenon was actually observed only when networks were overtrained far beyond 
their optimal performance point (Chauvin, 1990b). Furthermore, generalization 
performance of networks seemed to be independent of the size of the network during 
early training but the rate of decrease in performance with overtraining was indeed 
related the number of weights. 
The goal of this paper is to better understand training and generalization error dy- 
namics in Least-Mean-Square trained linear networks. As we will see, gradient de- 
scent training on linear networks can actually generate surprisingly rich and insight- 
ful validation dynamics. Furthermore, in numerous applications, even non-linear 
networks tend to function in their linear range, as if the networks were making use 
of non-linearities only when necessary (Weigend et al., In Press; Chauvin, 1990a). 
In Section 2, I present a theoretical illustration yielding a better understanding of 
training and validation error dynamics. In Section 3, numerical solutions to ob- 
tained analytical results make interesting predictions for validation dynamics under 
overtraining. These predictions are tested for a phonemic labeling task. The ob- 
tained simulations suggest that the results of the analysis obtained with the simple 
theoretical framework of Section 2 might remain qualitatively valid for non-linear 
complex architectures. 
2 THEORETICAL H, LUSTRATION 
2.1 ASSUMPTIONS 
Let us consider a linear network composed of n input units and n output units fully 
connected by a n.n weight matrix W. Let us suppose the network is trained to 
reproduce a noiseless output signal from a noisy input signal (the network can 
be seen as a linear filter). We write F as the signal, N the noise, X the input, Y 
the output, and D the desired output. For the considered case, we have X = F+N, 
Y = W X and D = F. 
The statistical properties of the data base are the following. The signal is zero-mean 
with covariance matrix CF. We write Ai and ei as the eigenvalues and eigenvectors 
of CF (el are the so-called principal components; we will call Ai the signal power 
spectrum). The noise is assumed to be zero-mean, with covariance matrix Cs - 
P.I where I is the identity matrix. We assume the noise is uncorrelated with the 
signal: 'rN -- O. We suppose two sets of patterns have been sampled for training 
and for validation. We write OF, Cs and CFN the resulting covariance matrices for 
the training set and C., Cv and CN the corresp_onding matrices for the validation 
set. We assume CF -- C  F, CFN  CN  CFN -- O, aN -- 12.I and Cv 
with yt > y. (Numerous of these assumptions are made for the sake of clarity of 
explanation: they can be relaxed without changing the resulting implications.) 
The problem considered is much simpler than typical realistic applications. How- 
ever, we will see below that (i) a formal analysis becomes complex very quickly 
(ii) the validation dynamics are rich, insightful and can be mapped to a number 
of results observed in simulations of realistic applications and (iii) an interesting 
number of predictions can be obtained. 
892 Chauvin 
2.2 LEARNING 
The network is trained by gradient descent on the Least Mean Square (LMS) error: 
AW -- -77wE where r/ is the usual learning rate and, in the case considered, 
E - P(Fp - Yp)T(Fp- Yp). We can write the gradient as a function of the 
various covariance matrices: VwE  (I - W)Cr  (I - 2W)Cr - WC. From 
the general assumptions, we get: 
VwE  CF - WCF - WCN (1) 
We assume now that the principal components ei are also eigenvectors of the weight 
matr W at iteration k with corresponding eigenvalue aik: Wk.ei = aikei. We can 
then compute the image of each eigenvector ei at iteration k + 1: 
W+l.ei = .Ai.ei + ai}[1 - .(Ai + y)].ei (2) 
Therefore, ei is aO an eigenvector of W}+ and ai,k+l satisfies the induction: 
Assuming W0 = 0, we can compute the alpha-dynamics of the weight matrk W: 
a,, - Ai + 7 [1 - (1 - .Ai + .))'] (4) 
As k goes to infinity, provided ? < 1/AM + , a approaches A/(A + y), which 
corresponds to the optimal (Wiener) value of the linear filter implemented by the 
network. We will write the convergence rates ai = 1 -?Ai -?y. These rates depend 
on the signal power spectrum, on the noe power and on the learning rate . 
If we now assume Wo.ei = aio.ei with ai0  0 (this assumption can be made more 
general), we get: 
Ai biaS] (5) 
where bi = 1 -io- iov/i. Figure 1 represents possible alpha dynamics for 
arbitrary vMues of i with i0  0  0. 
We can now compute the learning error dynamics by expanding the LMS error term 
E at time k. Using the general assumptions on the covariance matrices, we find: 
E =  Ei =  ,(1 -ai) 2 + Yai (6) 
i i 
Therefore, training error is a sum of err comnents, each of them being a 
quadratic function of ai. Figure 2 represents a training error component Ei as 
a function of . Knowing the alpha-dynamics, we can write these error components 
as a function of k: 
= + + Aiba) (7) 
It is easy to see that E is a monotonic decreasing function (generated by gradient 
descent) which converges to the bottom of the quadratic error surface, yielding the 
residual asymptotic error: 
(8) 
E = Ai + i 
i 
Generalization Dynamics in LMS Trained Linear Networks 893 
1.0 
Cio 
0.0 , , 
0 20 
 I  I  I  I 
40 6O 80 100 
Number of Cycles 
.i=4 
.i = .7 
'i = .2 
Figure 1: Alpha dynamics for different values of Ai with r/= .01 and ai0 = r0 y 0. 
The solid lines represent the optimal values of ri for the training data set. The 
dashed lines represent corresponding optimal values for the validation data set. 
LMS 
I I 
I I 
I I 
I I I I 
.X .X i 
0 )q+u' 
Ot ik 
Figure 2: Training and validation error dynamics as a function of ci. The dashed 
curved lines represent the error dynamics for the initial conditions ai0. Each training 
error component follows the gradient of a quadratic learning curve (bottom). Note 
the overtraining phenomenon (top curve) between a? (optimal for validation) and 
rioo (optimal for training). 
894 Chauvin 
2.3 GENERALIZATION 
Considering the general assumptions on the statistics of the data base, we can 
compute the validation error E  (Note that validation error strictly applies to the 
validation data set. Generalization error can qualify the validation data set or 
the whole population, depending on context.): 
(9) 
where the alpha-dynamics are imposed by gradient descent learning on the training 
data set. Again, the validation error is a sum of error components El, quadratic 
functions of ci. However, because the alpha-dynamics are adapted to the training 
sample, they might generate complex dynamics which will strongly depend on the 
inital values ci0 (Figure 1). Consequently, the resulting error components E are not 
monotonic decreasing functions anymore. As seen in Figure 2, each of the validation 
error components might (i) decrease (ii) decrease then increase (overtraining) or 
(iii) increase as a function of ci0. For each of these components, in the case of 
overt raining, it is possible to compute the value of ci} at which training should be 
stopped to get minimal validation error: 
Log x,+,,, + Logx,_,,o(X,+,,, ) (10) 
k = Log(1 - lAi - lv) 
However, the validation error dynamics become much more complex when we con- 
sider sums of these components. If we assume ci0 = 0, the minimum (or minima) 
of E  can be found to correspond to possible intersections of hyper-ellipsoids and 
power curves. In general, it is possible to show that there exists at least one such 
minimum. It is also possible to find simple bounds on the optimal training time for 
minimal validation error: 
 -' Log -' 
t'ï¿½g'x-Z-4'P' < k* < (11) 
;og(i -  - ) - - ;og(1 -  - ) 
These bounds are tight when the noise power is small compared to the signal power 
spectrum. For ai0  0, a formal analysis of the validation error dynamics becomes 
intractab
