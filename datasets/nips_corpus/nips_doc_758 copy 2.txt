Central and Pairwise Data Clustering by 
Competitive Neural Networks 
Joachim Buhmann & Thomas Hofmann 
Rheinische Friedrich-Wilhelms-Universitfit 
Institut f/Jr Informatik II, R6merstrage 164 
D-53117 Bonn, Fed. Rep. Germany 
Abstract 
Data clustering amounts to a combinatorial optimization problem to re- 
duce the complexity of a data representation and to increase its precision. 
Central and pairwise data clustering are studied in the maximum en- 
tropy framework. For central clustering we derive a set of reestimation 
equations and a minimization procedure which yields an optimal num- 
ber of clusters, their centers and their cluster probabilities. A meanfield 
approximation for pairwise clustering is used to estimate assignment 
probabilities. A selfconsistent solution to multidimensional scaling and 
pairwise clustering is derived which yields an optimal embedding and 
clustering of data points in a d-dimensional Euclidian space. 
1 Introduction 
A central problem in information processing is the reduction of the data complexity with 
minimal loss in precision to discard noise and to reveal basic structure of data sets. Data 
clustering addresses this tradeoff by optimizing a cost function which preserves the original 
data as complete as possible and which simultaneously favors prototypes with minimal 
complexity (Linde et al., 1980; Gray, 1984; Chou et al., 1989; Rose et al., 1990). We dis- 
cuss an objective function for the joint optimization of distortion errors and the complexity 
of a reduced data representation. A maximum entropy estimation of the cluster assign- 
ments yields a unifying framework for clustering algorithms with a number of different 
distortion and complexity measures. The close analogy of complexity optimized clustering 
with winner-take-all neural networks suggests a neural-like implementation resembling 
topological feature maps (see Fig. 1). 
104 
Central and Pairwise Data Clustering by Competitive Neural Networks 105 
xi 
Figure 1: Architecture of a three 
layer competitive neural network 
for central data clustering with 
d neurons in the input layer, K 
neurons in the clustering layer 
with activity (Mi) and G neu- 
rons in the classification layer. 
The output neurons estimate the 
conditional probability P, li of 
data point i being in class 7- 
Given is a set of data points which are characterized either by coordinates {xi [xi C d; i - 
1,..., N} or by pairwise distances {ikli, k - 1,..., N}. The goal of data clustering 
is to determine a partitioning of a data set which either minimizes the average distance 
of data points to their cluster centers or the average distance between data points of the 
same cluster. The two cases are refered to as central or pairwise clustering. Solutions 
to central clustering are represented by a set of data prototypes {YIY c = 
1 .... , K }, and the size K of that set. The assignments { Mi] o = 1,..., K;i = 1 .... , N}, 
Mi  {0, 1} denote that data point i is uniquely assigned to cluster  (, Mi, = 1). 
Rate distortion theo specifies the optimal choice of y being the cluster centroids, i.e., 
 o 
Mia oia(xi, ya) = 0. Given only a set of distances or dissimilities the solution 
to pairwise clustering is chacterized by the expected assignment viables (Mi). The 
complexity {C [ = 1 .... , K} of a clustering solution depends on the specific infomation 
processing application at hand, in pticul, we assume that C is only a function of the 
cluster probability p = i= l Mi IN. We propose the central clustering cost function 
N K 
i=1 =1 
and the pairwise clustering cost function 
N K N 
The distortion and complexity costs are adjusted in size by the weighting parameter X. e 
cost hnctions (1,2) have to be optimized in an imrafive hshion: (i) vary the assignment 
r cc.pc t { Mia }) decrease; 
variables Mi for a fixed number K of clusters such that the costs x 
(ii) increment the number of clusters K  K + 1 and optimize Mi again. 
Complexity costs which penalize small, sparsely populated clusters, i.e., C = 1/p, 
1, 2 .... , favor equal clusmr probabilities, thereby emphasizing the hardware aspect of a 
clustering solution. The special case s = 1 with constant costs per cluster coesponds 
to K-means clustering. An alternative complexity measure which estimates encoding 
costs for data compression and data transmission is the Shannon entropy of a cluster set 
(C)  E,p,C, = - E,p, logp,. 
106 Buhmann and Hofmann 
The most common choice for the distortion measure are distances Dis = 
which preserve the permutation symmetry of (1) with respect to the cluster index v. A data 
partitioning scheme without permutation invariance of cluster indices is described by the 
cost function 
The generalized distortion error ((Dis)) = -'r T,Di,(x,, y,) between data point xi and 
cluster center y quantifies the intrinsic quantization errors Di-r (xi, y,) and the additional 
errors due to transitions T, from index 7 to a. Such transitions might be caused by noise 
in communication channels. These index transitions impose a topological order on the 
set of indices { a[ a = 1, .... K} which establishes a connection to self-organizing feature 
maps (Kohonen, 1984; Ritter et al., 1992) in the case of nearest neighbor transitions in a 
d-dimensional index space. We refer to such a partitioning of the data space as topology 
preserving clustering. 
2 Maximum Entropy Estimation of Central Clustering 
Different combinations of complexity terms, distortion measures and topology constraints 
define a variety of central clustering algorithms which are relevant in very different infor- 
mation processing contexts. To derive robust, preferably parallel algorithms for these data 
clustering cases, we study the clustering optimization problem in the probabilistic frame- 
work of maximum entropy estimation. The resulting Gibbs distribution proved to be the 
most stable distribution with respect to changes in expected clustering costs (Tikochinsky 
et al., 1984) and, therefore, has to be considered optimal in the sense of robust statistics. 
Statistical physics (see e.g. (Amit, 1989; Rose et al., 1990)) states that maximizing the 
entropy at a fixed temperature T - 1/]3 is equivalent to minimizing the free energy 
� Tic = -TlnZ=-Tln( E exp(-fi�x)) 
- -N E p,20p, fi log exp C:)] (4) 
with respect to the vmiables p, y. The effective complexity costs are C  0 C)/Opt. 
For a derivation of (4) see (Buhmann, Kfihnel, 1993b). 
The resulting re-estimation equations for the expected cluster probabilities and the expected 
centroid positions are necess conditions of Wx being minimal, i.e. 
1 
i 
1 
0 '- EETT(MiT)_O T)i(xi'Y)' (6) 
Oy 
exp [--fi(((Di)) + XC;)] 
(Mi) = x (7) 
Central and Pairwise Data Clustering by Competitive Neural Networks 107 
The expectation value (Mi) of the assignment variable Mi can be interpreted as a fuzzy 
membership of data point xi in cluster a. The case of supervised clustering can be treated 
in an analogous fashion (Buhmann, Kiihnel, 1993a) which gives rise to the third layer in the 
neural network implementation (see Fig. 1). The global minimum of the free energy (4) 
with respect to p,, y determines the maximum entropy solution of the cost function (1). 
Note that the optimization problem (1) of a K v state space has been reduced to a K(d + 1 ) 
dimensional minimization of the free energy f'x (4). To find the optimal parameters p, y 
and the number of clusters K which minimize the free energy, we start with one cluster 
located at the centroid of the data distribution, split that cluster and reestimate p, y using 
equation (5,6). The new configuration is accepted as an improved solution if the free energy 
(4) has been decreased. This splitting and reestimation loop is continued until we fail to 
find a new configuration with lower free energy. The temperature determines the fuzziness 
of a clustering solution, whereas the complexity term penalizes excessively many clusters. 
3 Meanfield Approximation for Pairwise Clustering 
The maximum entropy estimation for pairwise clustering constitutes a much harder problem 
than the calculation of the free energy for central clustering. Analytical expression for 
the Gibbs distributions are not known except for the quadratic distance measure Dik = 
(xi -- xk) 2. Therefore, we approximate the free energy by a variational principle commonly 
refered to as meanfield approximation. Given the costfunction (2) we derive a lower bound 
to the free energy by a system of noninteracting assignment variables. The approximative 
costfunction with the variational parameters �w is 
K N 
Z E (8) 
v=l i=1 
The original costfunction for pairwise clustering can be written as �/ - �� K + V with a 
(small) perturbation term V -- � - �� x due to cluster interactions. The partition function 
= Zo(exp(-/SV))o > Zoexp(-/3(V)o) 
(9) 
is bound from below if terms of the order O(((V-(V)0) 3 )o) and higher are negligible com- 
pared to the quadratic term. The angular brackets denote averages over all configurations 
of the costfunction without interactions. The averaged perturbation term (V)o amounts to 
(10) 
being the averaged assignment variables 
exp(-3�m) 
(Mm): E exp(-/�w)' 
(11) 
108 Buhmann and Hofmann 
The meanfield approximation with the cost function (8) yields a lower bound to the partition 
function Z of the original pairwise clustering problem. Therefore, we vary the parameters 
�i, to maximize the quantity In Zo -/5(V)o which produces the best lower bound of Z 
based on an interaction free costfunction. Variation of & leads to the conditions 
(12) 
� being defined as 
( ' ) 
. _ i v,j (Mk)Vk + XC;. (13) 
�w - p---   
For a given distance matrix Di the transcendental equations (11,12) have to be solved 
simultaneously. 
So far the �i have been treated as independent variation parameters. An important 
problem, which is usually discussed in the context 
