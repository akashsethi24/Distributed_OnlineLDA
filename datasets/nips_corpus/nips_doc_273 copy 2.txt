516 Grossman 
The CHIR Algorithm for Feed Forward 
Networks with Binary Weights 
Tal Grossman 
Department of Electronics 
Weizmann Institute of Science 
Rehovot 76100 Israel 
ABSTRACT 
A new learning algorithm, Learning by Choice of Internal Rep- 
resetations (CHIR), was recently introduced. Whereas many algo- 
rithms reduce the learning process to minimizing a cost function 
over the weights, our method treats the internal representations as 
the fundamental entities to be determined. The algorithm applies 
a search procedure in the space of internal representations, and a 
cooperative adaptation of the weights (e.g. by using the perceptton 
learning rule). Since the introduction of its basic, single output ver- 
sion, the CHIR algorithm was generalized to train any feed forward 
network of binary neurons. Here we present the generalised version 
of the CHIR algorithm, and further demonstrate its versatility by 
describing how it can be modified in order to train networks with 
binary (+1) weights. Preliminary tests of this binary version on 
the random teacher problem are also reported. 
I. INTRODUCTION 
Learning by Choice of Internal Representations (CHIR) was recently introduced 
[1,11] as a training method for feed forward networks of binary units. 
Internal Representations are defined as the states taken by the hidden units 
of a network when patterns (e.g. from the training set) are presented to the input 
layer of the network. The CHIR algorithm views the internal representations associ- 
ated with various inputs as the basic independent variables of the learning process. 
Once such representations are formed, the weights can be found by simple and local 
learning procedures such as the Perceptron Learning Rule (PLR) [2]. Hence the 
problem of learning becomes one of searching for proper internal representations, 
The CHIR Algorithm for Feed Forward Networks with Binary Weights 517 
rather than of minimizing a cost function by varying the values of weights, which 
is the approach used by back propagation (see, however [3],[4] where back prop- 
agation of desired states is described). This basic idea, of viewing the internal 
representations as the fundamental entities, has been used since by other groups [5- 
7]. Some of these works, and the main differences between them and our approach, 
are briefly disscussed in [11]. One important difference is that the CHIR algorithm, 
as well as another similar algorithm, the MRII [8], try to solve the learning problem 
for a fixed architecture, and are not guaranteed to converge. Two other algorithms 
[5,6] always find a solution, but at the price of increasing the network size during 
learning in a manner that resembles similar algorithms developed earlier [9,10]. An- 
other approach [7] is to use an error minimizing algorithm which treat.s the internal 
representations as well as the weights as the relevant variables of the search space. 
To be more specific, consider first the single layer perceptron with its Percep- 
tron Learning Rule (PLR) [2]. This simple network consists of N input (source) 
units j, and a single target unit i. This unit is a binary linear threshold unit, i.e. 
when the source units are set in any one of It = 1, ..M patterns, i.e. $j = , the 
state of unit i, $i = :t:1 is determined according to the rule 
$i = sign( Z WijSj + Oi) (1) 
J 
Here Wij is the (unidirectional) weight assigned to the connection from unit j to 
i; Oi is a local bias. For each of the M input patterns, we require that the target 
unit (determined using (1)) will take a preassigned value . Learning takes place 
in the course of a training session. Starting from any arbitrary initial guess for the 
weights, an input y is presented, resulting in the output taking some value $. Now 
modify every weight according to the rule 
(2) 
where r/> 0 is a step size parameter g - 1 is used to modify the bias 0). Another 
� . ( j -- 
Input pattern is presented, and so on, until all inputs draw the correct output. The 
Perceptton convergence theorem states [2] that the PLR will find a solution (if one 
exists), in a finite number of steps� Nevetheless, one needs, for each unit, both the 
desired input and output states in order to apply the PLR. 
Consider now a two layer perceptron, with N input, H hidden and K output 
units (see Fig.l). The elements of the network are binary linear threshold units i, 
whose states $i = :!:1 are determined according to (1). In a typical task for such 
a network, M specified output patterns, $ut,u = ?t,u, are required in response 
to tt = 1, ..., M input patterns. If a solution is found, it first maps each input onto 
an internal representation generated on the hidden layer, which, in turn, produces 
the correct output. Now imagine that we are not supplied with the weights that 
solve the problem; however the correct internal representations are revealed. That 
is, we are given a table with M rows, one for each input. Every row has H bits ', 
for i = 1..H, specifying the state of the hidden layer obtained in response to input 
518 Grossman 
pattern u. One can now view each hidden-layer cell i as the target of the PLR, 
with the N inputs viewed as source. Given sufficient time, the PLR will converge 
to a set of weights Wij, connecting input unit j to hidden unit i, so that indeed 
the input-hidden association that appears in column i of our table will be realized. 
In order to obtain the correct output, we apply the PLR in a learning process that 
uses the hidden layer as source and each output unit as a target, so as to realize 
the correct output. In general, however, one is not supplied with a correct table of 
internal representations. Finding such a table is the goal of our approach. 
Figure 1. A typical three layered feed forward network (two layered percep- 
tron) with N input, H hidden and K output units. The unidirectional weight Wij 
connects unit j to unit i. A layer index is implicitely included in each unit's index. 
During learning, the CHIR algorithm alternates between two phases: in one it 
generates the internal representations, and in the other it uses the updated repre- 
sentations in order to search for weights, using some single layer learning rule. This 
general scheme describes a large family of possible algorithms, that use different 
ways to change the internal representations, and update the weights. 
A simple algorithm based on this general scheme was introduced recently [1,11]. 
In section II we describe the multiple output version of CHIR [11]. In section III we 
present a way to modify the algorithm so it can train networks with binary weights, 
and the preliminary results of a few tests done on this new version. In the last 
section we shortly discuss our results and describe some future directions. 
The CHIR Algorithm for Feed Forward Networks with Binary Weights 519 
II. THE CHIR ALGORITHM 
The CHIR algorithm that we describe here implements the basic idea of learn- 
ing by choice of internal representations by breaking the learning process into four 
distinct procedures that are repeated in a cyclic order: 
1. SETINREP: Generate a table of internal representations (') by presenting 
each input pattern from the training set and recording the states of the hidden 
units, using Eq.(1), with the existing couplings Wij and 
2. LEARN23: The current table of internal representations is used as the training 
set, the hidden layer cells are used as source, and each output as the target unit 
of the PLR. If weights Wij and O i that produce the desired outputs are found, the 
problem has been solved. Otherwise stop after I23 learning sweeps, and keep the 
current weights, to use in CHANGE INREP. 
3. CHANGE INREP: Generate a new table of internal representations, which 
reduces the error in the output: We present the table sequentially, row by row 
(pattern by pattern), to the hidden layer. If for pattern , the wrong output is 
obtained, the internal representation h,y is changed. 
This is done simply by choosing (at random) a hidden unit i, and checking 
the effect of flipping the sign of /h, on the total output error, i.e. the number of 
wrong bits. If the output error is not increased, the flip is accepted and the table of 
internal representations is changed accordingly. Otherwise the flip is rejected and 
we try another unit. When we have more than one output unit, it might happen 
that an error in one output unit can not be corrected without introducing an error 
in another unit. Therefore we allow only for a pre-specified number of attempted 
flips, Iin, and go on to the next pattern even if the output error was not eliminated 
completely. This procedure ends with a'modified, improved table which is our 
next guess of internal representations. Note that this new table does not necessarily 
yield a totally correct output for all the patterns. In such a case, the learning process 
will go on even if this new table is perfectly realized by the next stage - LEARN12. 
4. LEARN12: Present an input pattern; if the output is wrong, apply the PLR 
with the first layer serving as source, treating every hidden layer site separately 
as target. If input , does yield the correct output, we insert the current state 
of the hidden layer as the internal representation associated with pattern ,, and 
no learning steps are taken. We sweep in this manner the training set, modifying 
weights Wij, (between input and hidden layer), hidden-layer thresholds Oi, and, as 
explained above, internal representations. If the network has achieved error-free 
performance for the entire training set, learning is completed. Otherwise, after I12 
training sweeps (or if the current internal representation is perfectly realized), abort 
the PLR. stage, keeping the present values of Wij, Oi, and start SETINREP again. 
The idea in trying to learn the current internal representation even if it does not 
yield the perfect output is that i
