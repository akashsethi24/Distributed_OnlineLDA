Computing with Almost Optimal Size Neural 
Networks 
Kai-Yeung Siu 
Dept. of Electrical &; Comp. Engineering 
University of California, Irvine 
Irvine, CA 92717 
Vwani Roychowdhury 
School of Electrical Engineering 
Purdue University 
West Lafayette, IN 47907 
Thomas Kailath 
Information Systems Laboratory 
Stanford University 
Stanford, CA 94305 
Abstract 
Artificial neural networks are comprised of an interconnected collection 
of certain nonlinear devices; examples of commonly used devices include 
linear threshold elements, sigmoidal elements and radial-basis elements. 
We employ results from harmonic analysis and the theory of rational ap- 
proximation to obtain almost tight lower bounds on the size (i.e. number 
of elements) of neural networks. The class of neural networks to which 
our techniques can be applied is quite general; it includes any feedforward 
network in which each element can be piecewise approximated by a low 
degree rational function. For example, we prove that any depth-(d q- l) 
network of sigmoidal units or linear threshold elements computing the par- 
ity function of n variables must have f2(dn 1/d-) size, for any fixed  > 0. 
In addition, we prove that this lower bound is almost tight by showing 
that the parity function can be computed with O(dn l/d) sigmoidal units 
or linear threshold elements in a depth-(d + 1) network. These almost 
tight bounds are the first known complexity results on the size of neural 
networks with depth more than two. Our lower bound techniques yield 
a unified approach to the complexity analysis of various models of neural 
networks with feedforward structures. Moreover, our results indicate that 
in the context of computing highly oscillating symmetric Boolean func- 
19 
20 Siu, Roychowdhury, and Kailath 
tions, networks of continuous-output units such as sigmoidal elements do 
not offer significant reduction in size compared with networks of linear 
threshold elements of binary outputs. 
I Introduction 
Recently, artificial neural networks have found wide applications in many areas 
that require solutions to nonlinear problems. One reason for such success is the 
existence of good learning or training algorithms such as Backpropagation [13] 
that provide solutions to many problems for which traditional attacks have failed. 
At a more fundamental level, the computational power of neural networks comes 
from the fact that each basic processing element computes a nonlinear function 
of its inputs. Networks of these nonlinear elements can yield solutions to highly 
complex and nonlinear problems. On the other hand, because of the nonlinear 
features, it is very difficult to study the fundamental limitations and capabilities of 
neural networks. Undoubtedly, any significant progress in the applications of neural 
networks must require a deeper understanding of their computational properties. 
We employ classical tools such as harmonic analysis and rational approximation 
to derive new results on the computational complexity of neural networks. The 
class of neural networks to which our techniques can be applied is quite large; it 
includes feedforward networks of sigmoidal elements, linear threshold elements, and 
more generally, elements that can be piecewise approximated by low degree rational 
functions. 
1.1 Background, Related Work and Definitions 
A widely accepted model of neural networks is the feedforward multilayer network 
in which the basic processing element is a sigmoidal element. A sigmoidal element 
computes a function f(X) of its input variables X = (Xl,..., xn) such that 
2 1 - e -r(x) 
f(X) = a(F(X)) = 1 + e-r(X) - 1 = 1 + e-r(X) 
where 
$ 
r(x) = y] . + wo. 
i=1 
The real valued coefficients wi are commonly referred to as the weights of the sig- 
moidal function. The case that is of most interest to us is when the inputs are 
binary, i.e., X  {1,-1}. We shall refer to this model as sigmoidal network. 
Another common feedforward multilayer model is one in which each basic processing 
unit computes a binary linear threshold function s#n(F(X)), where F(X) is the 
same as above, and 
I if F(X) >_ 0 
sgn(F(X)) = -1 if F(X) < 0 
This model is often called the threshold circuit in the literature and recently has 
been studied intensively in the field of computer science. 
Computing with Almost Optimal Size Neural Networks 21 
The size of a network/circuit is the number of elements. The depth of a net- 
work/circuit is the longest path from any input gate to the output gates. We can 
arrange the gates in layers so that all gates in the same layer compute concurrently. 
(A single element can be considered as a one-layer network.) Each layer costs a 
unit delay in the computation. The depth of the network (which is the number of 
layers) can therefore be interpreted as the time for (parallel) computation. 
It has been established that threshold circuit is a very powerful model of computa- 
tion. Many functions of common interest such as multiplication, division and sort- 
ing can be computed in polynomial-size threshold circuits of small constant depth 
[19, 18, 21]. While many upper bound results for threshold circuits are known in 
the literature, lower bound results have only been established for restricted cases of 
threshold circuits. Most of the existing lower bound techniques [10, 17, 16] apply 
only to depth-2 threshold circuits. In [16], novel techniques which utilized analyti- 
cal tools from the theory of rational approximation were developed to obtain lower 
bounds on the size of depth-2 threshold circuits that compute the parity function. 
In [20], we generalized the methods of rational approximation and our earlier tech- 
niques based on harmonic analysis to obtain the first known almost tight lower 
bounds on the size of threshold circuits with depth more than two. In this paper, 
the techniques are further generalized to yield almost tight lower bounds on the 
size of a more general class of neural networks in which each element computes a 
continuous function. 
The presentation of this paper will be divided into two parts. In the first part, we 
shall focus on results concerning threshold circuits. In the second part, the lower 
bound results presented in the first part are generalized and shown to be valid even 
when the elements of the networks can assume continuous output values. The class 
of networks for which such techniques can be applied include networks of sigmoidal 
elements and radial basis elements. Due to space limitations, we shall only state 
some of the important results; further results and detailed proofs will appear in an 
extended paper. 
Before we present our main results, we shall give formal definitions of the neural 
network models and introduce some of the Boolean functions, which will be used 
to explore the computational power of the various networks. To present our results 
in a coherent fashion, we define throughout this paper a Boolean function as f: 
{1,-1} n -- {1,-1}, instead of using the usual {0, 1} notation. 
Definition 1 A threshold circuit is a Boolean circuit in which every gate com- 
putes a linear threshold function with an additional property: the weights are inte- 
gers all bounded by a polynomial in n. [] 
Remark ! The assumption that the weights in the threshold circuits are integers 
bounded by a polynomial is common in the literature. In fact, the best known lower 
bound result on depth-2 threshold circuit [10] does not apply to the case where 
exponentially large weights are allowed. On the other hand, such assumption does 
not pose any restriction as far as constant-depth and polynomial-size is concerned. 
In other words, the class of constant-depth polynomial-size threshold circuits (TC �) 
remains the same when the weights are allowed to be arbitrary. This result was 
implicit in [4] and was improved in [18] by showing that any depth-d threshold circuit 
22 Siu, Roychowdhury, and Kailath 
with arbitrary weights can be simulated by a depth-(2d q- 1) threshold circuit of 
polynomially bounded weights at the expense of a polynomial increase in size. More 
recently, it has been shown that any polynomial-size depth-d threshold circuit with 
arbitrary weights can be simulated by a polynomial-size depth-(2d q- 1) threshold 
circuit. rn 
In addition to Boolean circuits, we shall also be interested in the computation of 
Boolean functions by networks of continuous-valued elements. To formalize this 
notion, we adopt the following definitions [12]: 
Definition 2 Let y: R -4 R. A y element with weights wl, ...,w, E R and 
threshold t is defined to be an element that computes the function Y(]]i wia:-t) 
where (xt, ..., x,,) is the input. A y-network is a feedforward network of y elements 
with an additional property: the weights wi are all bounded by a polynomial in n. 
For example, when y is the sigmoidal function a(x), then we have a sigmoidal 
network, a common model of neural network. In fact, a threshold circuit can also 
be viewed as a special case of y network where y is the sgn function. 
Definition 3 A y-network C is said to compute a Boolean function f � 
{1,-1} n -4 {1,-1} with separation e > 0 if there is some tc E R such that 
for any input X = (x, ..., xm) to the network C, the output element of C outputs 
a value C(X) with the following property: If f(X) - l, then C(X) >_ tc + . If 
f(X) = -1, then C(X) _< tc- e. [] 
Remark 2 As pointed out in [12], computing with y networks without separation 
at the output element is less interesting because an infinitesimal change in the 
output of any y element may change the output bit. In this paper, we shall be 
mainly interested in computations on y networks C, with separation at least q(n -k) 
for some fixed k > 0. This together with the assumption of polynomially bounded 
weights makes the complexity class of constant-depth polynomial-size y networks 
quite robust and more interesting to study from a theoretical poi
