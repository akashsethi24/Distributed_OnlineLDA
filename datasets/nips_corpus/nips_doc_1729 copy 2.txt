Effective Learning Requires Neuronal 
Remodeling of Hebbian Synapses 
Gal Chechik Isaac Meilijson Eytan Ruppin 
School of Mathematical Sciences 
Tel-Aviv University Tel Aviv, Israel 
ggal@math.tau.ac.il isaco@math.tau.ac.il ruppin@math.tau.ac.il 
Abstract 
This paper revisits the classical neuroscience paradigm of Hebbian 
learning. We find that a necessary requirement for effective as- 
sociative memory learning is that the efcacies of the incoming 
synapses should be uncorrelated. This requirement is difficult to 
achieve in a robust manner by Hebbian synaptic learning, since it 
depends on network level information. Effective learning can yet be 
obtained by a neuronal process that maintains a zero sum of the in- 
coming synaptic efcacies. This normalization drastically improves 
the memory capacity of associative networks, from an essentially 
bounded capacity to one that linearly scales with the network's size. 
It also enables the effective storage of patterns with heterogeneous 
coding levels in a single network. Such neuronal normalization can 
be successfully carried out by activity-dependent homeostasis of the 
neuron's synaptic efcacies, which was recently observed in cortical 
tissue. Thus, our findings strongly suggest that effective associa- 
tive learning with Hebbian synapses alone is biologically implausi- 
ble and that Hebbian synapses must be continuously remodeled by 
neuronally-driven regulatory processes in the brain. 
I Introduction 
Synapse-specific changes in synaptic efcacies, carried out by long-term potentiation 
(LTP) and depression (LTD) are thought to underlie cortical self-organization and 
learning in the brain. In accordance with the Hebbian paradigm, LTP and LTD 
modify synaptic efcacies as a function of the firing of pre and post synaptic neurons. 
This paper revisits the Hebbian paradigm showing that synaptic learning alone 
cannot provide effective associative learning in a biologically plausible 
manner, and must be complemented with neuronally-driven synaptic 
remodeling. 
The importance of neuronally driven normalization processes has already been 
demonstrated in the context of self-organization of cortical maps [1, 2] and in con- 
tinuous unsupervised learning as in principal-component-analysis networks [3]. In 
these scenarios normalization is necessary to prevent the excessive growth of synap- 
Effective Learning Requires Neuronal Remodeling of Hebbian Synapses 97 
tic efficacies that occurs when learning and neuronal activity are strongly coupled. 
In contradistinction, this paper focuses on associative memory learning where this 
excessive synaptic runaway growth is mild [4], and shows that even in this simple 
learning paradigm, normalization processes are essential. Moreover, while numer- 
ous normalization procedures can prevent synaptic runaway, our analysis shows that 
only a specific neuronally-driven correction procedure that preserves the total sum 
of synaptic efiqcacies leads to effective associative memory storage. 
2 Effective Synaptic Learning rules 
We study the computational aspects of associative learning in a model of a low- 
activity associative memory network with binary firing {0, 1} neurons. M uncorre- 
t M 
lated memory patterns { }u= with coding level p (fraction of firing neurons) are 
stored in an N neurons network. The ith neuron updates its state X/t at time t by 
xt+l _ O(f[) f 1 V 1 + sign(f) (1) 
, - , = w, - r, o(f) = 2 ' 
j=l 
where fi is its input field (postsynaptic potential) and T is its firing thresh- 
old. The synaptic weight Wij between the jth (presynaptic) and ith (postsy- 
naptic) neurons is determined by a general additive synaptic learning rule de- 
pending on the neurons' activity in each of the M stored memory patterns  
presynaptic (j) 
Wij = Z A(?,?) , A(i,j) = postsynaptic(i) 1 c  , (2) 
where the synaptic learning matrix A(/, ?) governs the incremental modifications 
to a synapse as a function of the firing of the presynaptic (column) and postsynaptic 
(row) neurons. In conventional biological terms, c denotes an increment following a 
long-term potentiation (LTP) event,  denotes heterosynaptic long-term depression 
(LTD), and ' a homosynaptic LTD event. 
The parameters a, , % 5 define a four dimensional space in which all linear additive 
Hebbian learning rules reside. However, in order to visualize this space, one may 
represent these Hebbian learning rules in a reduced, two-dimensional space utilizing 
a scaling invariance constraint and the requirement that the synaptic matrix should 
have a zero mean (otherwise the synaptic values diverge, the noise overshadows the 
signal term and no retrieval is possible [5]). 
Figure 1A plots the memory capacity of the network as a function of the two free 
parameters a and . It reveals that considerable memory storage may be obtained 
only along an essentially one dimensional curve, naturally raising the possibility 
of identifying an additional constraint on the relations between (a, , % 5). Such 
a constraint is revealed by a signal-to-noise analysis of the neuronal input field fi 
during retrieval 
Signal 
Noise 
E(fi[i = 1) - E(fi[i = O) 
v/Var(fi) v/Var [Wij] + NpCOV [Wij, Wik] 
+ Vpï¿½OV ' 
(a) 
98 G. Chechik, I. Meilijson and E. Ruppin 
where averages are taken over the ensemble of stored memory patterns. 
A. So 
500 
400 ----. 
':300 
400  '; ::  
Ee 200 [ 
200 ' ' lO E 
o ha  
- _; ' ' o.o' 
o o beta 
beta 
Figure 1: A. Memory capacity of a 1000-neurons network with p = 0.05 for different 
values of a and/3 as obtained in computer simulations. Capacity is defined as the 
maximal number of memories that can be retrieved with overlap bigger than 0.95 
when presented with a degraded input cue with overlap 0.8. The overlap serves 
N  
to measure retrieval acuity and is defined as mn 1 -d=l() -p)Xj B. 
p(1-p)N ' 
Memory capacity of the effective learning rules: The peak values on the ridge of 
Figure A, are displayed by tracing their projection on the 3 coordinate. The optimal 
learning rule A(i,j) = (i -P)(j -P) [5], marked with an arrow, performs only 
slightly better than other effective learning rules. 
As evident from equation (3) and already pointed out by [6], when the postsynap- 
tic covariance COV [A(i, j), A(i, )] (determining the covariance between the 
incoming synapses of the postsynaptic neuron) is positive, the network's memory 
capacity is bounded, i.e., it does not scale with the network size. As the postsy- 
naptic covariance is non negative, effective learning rules that obtain linear scaling 
of memory capacity as a function of the network's size require a vanishing post- 
synaptic covariance. Intuitively, when the synaptic weights are correlated, adding 
any new synapse contributes only little new information, thus limiting the number 
of beneficial synapses that help the neuron estimate whether it should fire or not. 
Figure lB depicts the memory capacity of the effective synaptic learning rules which 
lie on the essentially one-dimensional ridge observed in Figure 1A. It shows that all 
these effective rules are only slightly inferior to the optimal synaptic learning rule 
calculated previously by [5, 6], which maximizes memory capacity. 
The vanishing covariance constraint on effective learning rules implies a new re- 
quirement concerning the balance between synaptic depression and facilitation: 
/3 = -_--p a. Thus, effective memory storage requires a delicate balance between 
LTP (xp) and heterosynaptic depression (/3), and is strongly dependent on the cod- 
ing level p which is a global property of the network. It is thus difficult to see how 
effective rules can be implemented at the synaptic level. Moreover, as shown in 
Figure 1A, Hebbian learning rules lack robustness as small perturbations from the 
effective rules may result in large decrease in memory capacity. 
Effective Learning Requires Neuronal Remodeling of Hebbian Synapses 99 
Furthermore, these problems cannot be circumvented by introducing a nonlinear 
Hebbian learning rule of the form Wij = g 'n(i, ?) as even for a nonlinear 
function g the covariance Cov [g('nA(, ?)), g('nA(, ))] remains positive 
if Cov(A(i,j), A(i,))is positive. These observations show that effective 
associative learning with Hebbian rules alone is implausible from a bio- 
logical standpoint requiring locality of information. 
3 Effective Learning via Neuronal Weight Correction 
The above results show that in order to obtain effective memory storage, the post- 
synaptic covariance must be kept negligible. How then may effective storage 
take place in the brain with Hebbian learning? We now proceed to show that 
a neuronally-driven procedure (essentially similar to that assumed by [2, 1] to occur 
during self-organization) can maintain a vanishing covariance and turn ineffective 
Hebbian synapses into effective ones. This enables the brain to utilize ineglcient 
learning rules which use local information only, but still attain effective learning 
capabilities. 
The solution emerges when rewriting the signal-to-noise equation (Eq. 3) as 
Noise 
Signal N 
c (4) 
/NVar[Wij] (1 - p) + pVar(.jN__l Wij) 
showing that the post synaptic covariance can be greatly diminished when the 
variance of the sum of incoming synapses is vanishing. We thus propose the following 
neuronal weight correction procedure: During learning, whenever a synapse is 
modified, its postsynaptic neuron additively modifies all its synapses to maintain 
the sum of their eglcacies at a baseline zero level. 
N 
Wij .'. Wij N Z Wij ; Vj = 1..N (5) 
j----1 
As this neuronal weight correction is additive, it can be performed either after 
several memories have been stored (as done in prescriptive learning), or during the 
storge of each memory pattern (as in developmental learning models). 
Interestingly, the joint operation of weight correction over a linear Hebbian learning 
ru
