Tight Bounds for the VC-Dimension of 
Piecewise Polynomial Networks 
Akito Sakurai 
School of Knowledge Science 
Japan Advanced Institute of Science and Technology 
Nomi-gun, Ishikawa 923-1211, Japan. 
CREST, Japan Science and Technology Corporation. 
A Sakurai@jaist. ac.jp 
Abstract 
O(ws(s log d q-log(dqh/s))) and O(ws((h/s) log q)q- log(dqh/s)) are 
upper bounds for the VC-dimension of a set of neural networks of 
units with piecewise polynomial activation functions, where s is 
the depth of the network, h is the number of hidden units, w is 
the number of adjustable parameters, q is the maximum of the 
number of polynomial segments of the activation function, and d is 
the maximum degree of the polynomials; also fl(wslog(dqh/s)) is 
a lower bound for the VC-dimension of such a network set, which 
are tight for the cases s - �(h) and s is constant. For the special 
case q -- 1, the VC-dimension is �(ws log d). 
1 Introduction 
In spite of its importance, we had been unable to obtain VC-dimension values for 
practical types of networks, until fairly tight upper and lower bounds were obtained 
([6], [8], [9], and [10]) for linear threshold element networks in which all elements 
perform a threshold function on weighted sum of inputs. Roughly, the lower bound 
for the networks is (1/2)w log h and the upper bound is w log h where h is the number 
of hidden elements and w is the number of connecting weights (for one-hidden-layer 
case w  nh where n is the input dimension of the network). 
In many applications, though, sigmoidal functions, specifically a typical sigmoid 
function 1/(1 q- exp(-x)), or piecewise linear functions for economy of calculation, 
are used instead of the threshold function. This is mainly because the differen- 
tiability of the functions is needed to perform backpropagation or other learning 
algorithms. Unfortunately explicit bounds obtained so far for the VC-dimension of 
sigmoidal networks exhibit large gaps (O(w2h 2) ([3]), (wlog h) for bounded depth 
324 .4. Sakurai 
and f/(wh) for unbounded depth) and are hard to improve. For the piecewise linear 
case, Maass obtained a result that the VG-dimension is O(w ' log q), where q is the 
number of linear pieces of the function ([5]). 
Recently Koiran and Sontag ([4]) proved a lower bound f(w ') for the piecewise 
polynomial case and they claimed that an open problem that Maass posed if there 
is a matching w ' lower bound for the type of networks is solved. But we still have 
something to do, since they showed it only for the case w = �(h) and the number 
of hidden layers being unbounded; also O(w ') bound has room to improve. 
We in this paper improve the bounds obtained by Maass, Koiran and Sontag and 
consequently show the role of polynomials, which can not be played by linear func- 
tions, and the role of the constant functions that could appear for piecewise poly- 
nomial case, which cannot be played by polynomial functions. 
After submission of the draft, we found that Bartlett, Maiorov, and Meir haA ob- 
tained similar results prior to ours (also in this proceedings). Our advantage is that 
we clarified the role played by the degree and number of segments concerning the 
both bounds. 
2 Terminology and Notation 
log stands for the logarithm base 2 throughout the paper. 
The depth of a network is the length of the longest path from its external inputs to 
its external output, where the length is the number of units on the path. Likewise 
we can assign a depth to each unit in a network as the length of the longest path 
from the external input to the output of the unit. A hidden layer is a set of units at 
the same depth other than the depth of the network. Therefore a depth L network 
has L - 1 hidden layers. 
In many cases w will stand for a vector composed of all the connection weights in 
the network (including threshold values for the threshold units) and w is the length 
of w. The number of units in the network, excluding input units, will be denoted 
by h; in other words, the number of hidden units plus one, or sometimes just the 
number of hidden units. A function whose range is {0, 1} (a set of 0 and 1) is 
called a Boolean-valued function. 
3 Upper Bounds 
To obtain upper bounds for the VC-dimension we use a region counting argument, 
developed by Goldberg and Jerrum [2]. The VC-dimension of the network, that is, 
the VC-dimension of the function set {fG(w;' ) Iwer is upper bounded by 
where Nc(.) is the number of connected components and .Af(f) is the set 
{w I f(w)=0}. 
The following two theorems are convenient. Refer [11] and [7] for the first theorem. 
The lemma followed is easily proven. 
Theorem 3.1. Let f6(w; xj) (1 _< i <_ N) be real polynomials in w, each of degree 
a or less. The number of connected components of the set gix{wlfo(w;xl)= 0} 
is bounded from above by 2(2d) w where w is the length ofw. 
Tight Bounds for the VC-Dimension of Piecewise Polynomial Networks 325 
Lemn'ta 3.2. If m _> w(log C + loglog C + 1), then 2 m > (me/w) u' for C >_ 4. 
First let us consider the polynomial activation function case. 
Theorem 3.3. Suppose that the activation function are polynomials of degree at 
most d. O(wslog d) is an upper bound of the VU-dimension for the networks with 
depth s. Wen s = O(h) the bound is O(whlogd). More precisely ws(logd + 
log loud + 2) is an upper bound. Note that if we allow a polynomial as the input 
function, dd. will replace d above where d is the maximum degree of the input 
functions and d. is that of the activation functions. 
The theorem is clear from the fcts that the network function (fa in (3.1)) is a 
polynomial of degree at most d s + d s- + .-. + d, Theorem 3.1 and Lemma 3.2. 
For the piecewise linear case, we have two types of bounds. The first one is suitable 
for bounded depth cases (i.e. the depth s - o(h)) and the second one for the 
unbounded depth case (i.e. s = �(h)). 
Theorem 3.4. Suppose that the activation functions are piecewise polynomials with 
at most q segments of polynomials degree at most d. O(ws(slogd + log(dqh/s))) 
and O(ws((h/s) log q) + log(dqh/s)) are upper bounds for the VU-dimension, where 
s is the depth of the network. More precisely, ws((s/2)logd + log(qh)) and 
ws((h/s) log q +log d) are asymptotic upper bounds. Note that if we allow a polyno- 
mial as the input function then d d. will replace d above where d is the maximum 
degree of the input functions and d. is that of the activation functions. 
Proof. 
We have two different ways to calculate the bounds. First 
s 
-< II m= - o... o 
' +... + + 
j=l 
<__ (8eNqd(S+a)/2(h/s) )os 
where hi is the number of hidden units in the i-th layer and o is an operator to 
form a new vector by concatenating the two. From this we get an asymptotic upper 
bound ws((s/2)log d + log(qh)) for the VC-dimension. 
Secondly 
Ncc(7w Jv .N' _ _ qh (8eNqad s) 
From this we get an asymptotic upper bound ws((h/s)logq + loud) for the VC- 
dimension. Combining these two bounds we get the result. Note that s in log(dqh/s) 
in it is introduced to eliminate unduly large term emerging when s = O(h). [] 
4 Lower Bounds for Polynomial Networks 
Theorem 4.1 Let us consider the case that the activation function are polynomials 
of degree at most d. f(wslogd) is a lower bound of the VU-dimension for the 
networks with depth s. When s = �(h) the bound is f/(whlogd), More precisely, 
326 ,4. Salatrai 
(1/16)w(s-6) log d is an asymptotic lower bound where d is the degree of activation 
functions and is a power of two and h is restricted to O(n ') for input dimension n. 
The proof consists of several lemmas. The network we are constructing will have 
two parts: an encoder and a decoder. We deliberately fix the N input points. The 
decoder part has fixed underlying architecture but also fixed connecting weights 
whereas the encoder part has variable weights so that for any given binary outputs 
for the input points the decoder could output the specified value from the codes in 
which the output value is encoded by the encoder. 
First we consider the decoder, which has two real inputs and one real output. One 
of the two inputs y holds a code of a binary sequence bx, b.,..., bm and the other x 
holds a code of a binary sequence cx, c.,..., cm. The elements of the latter sequence 
are all O's except for cj - 1, where cj - 1 orders the decoder to output bj from it 
and consequently from the network. 
We show two types of networks; one of which has activation functions of degree at 
most two and has the VC-dimension w(s- 1) and the other has activation functions 
of degree d a power of two and has the VC-dimension w(s - 5) log d. 
We use for convenience two functions 7/0(x) - 1 if x _> 0 and 0 otherwise and 
7/0,4 (x) = 1 if x _> b, 0 if x _< 0, and undefined otherwise. Throughout this section 
we will use a simple logistic function p(x) = (16/3)x(1 - x) which has the following 
property. 
Lemma 4.2. For any binary sequence b, b2, . . . , bin, there exists an interval [x, x2] 
such that bi = }'l/4,3/4(pi(x)) and 0 <_ pi(x) _ 1 for any x  [xx, x2]. 
The next lemmas are easily proven. 
Lemma 4.3. For any binary sequence cx,c2,...,cm which are all O's except for 
i 
cj = 1, there exists xo such that ci - Tg/4,a/4(p (xo)). Specifically we will take xo '- 
where pZ(x) is the inverse of(x) on [0, 1/2]. Then = 1/4, 
pJ(xo) = 1, pi(Xo) = 0 for all i > j, and pj-i(xo) <_ (1/4) i for all positive i _< j. 
Proof. Clear from the fact that p(x) _> 4x on [0,1/4]. 
Lemma 4.4. For any binary sequence bx, b2,...,bm, take y such that 
7tl/4,a/4(pi(y)) and 0 _< pi(y) _< 1 for all i and x0 = pZ(/-x)(1/4), 
Tl,/x2,a/4 (Ei Pi(xo)pi(y)) = bj, i.e. Tlo (Eix pi(xo)Pi(Y) - 213) = 
[] 
then 
Proof. If bj = O, Ei=x pi(xo)Pi(Y) J 1- i 
m = Ei=X Pi(Xo)Pi(Y) --< pJ(y) + Ei=X (1/4) < 
pJ(y) + (1/3) _< 7/12. If bj = 1, Ei=x pi(xo)pi(y) > pi(xo)pJ(y) _> 3/4. [] 
By the above lemmas, the network in Figure 1 (left) has the following function: 
S
