Connectionist Speaker Normalization 
with Generalized 
Resource Allocating Networks 
Cesare Furlanello 
Istituto per La Ricerca 
Scientifica e Tecnologica 
Povo (Trento), Italy 
fnrlnirst. it 
Diego Giuliani 
Istituto per La Ricerca 
Scientifica e Tecnologica 
Povo (Trento), Italy 
giuliaiirst. it 
Edmondo Trentin 
Istituto per La Ricerca 
Scientifica e Tecnologica 
Povo (Trento), Italy 
trentinirst. it 
Abstract 
The paper presents a rapid speaker-normalization technique based 
on neural network spectral mapping. The neural network is used 
as a front-end of a continuous speech recognition system (speaker- 
dependent, HMM-based) to normalize the input acoustic data from 
a new speaker. The spectral difference between speakers can be 
reduced using a limited amount of new acoustic data (40 phonet- 
ically rich sentences). Recognition error of phone units from the 
acoustic-phonetic continuous speech corpus APASCI is decreased 
with an adaptability ratio of 25%. We used local basis networks of 
elliptical Gaussian kernels, with recursive allocation of units and 
on-line optimization of parameters (GRAN model). For this ap- 
plication, the model included a linear term. The results compare 
favorably with multivariate linear mapping based on constrained 
orthonormal transformations. 
1 INTRODUCTION 
Speaker normalization methods are designed to minimize inter-speaker variations, 
one of the principal error sources in automatic speech recognition. Training a speech 
recognition system on a particular speaker (speaker-dependent or SD mode) gen- 
erally gives better performance than using a speaker-independent system, which is 
868 Cesare Furlanello, Diego Giuliani, Edmondo Trentin 
trained to recognize speech from a generic user by averaging over individual dif- 
ferences. On the other hand, performance may be dramatically worse when a SD 
system tailored on the acoustic characteristics of a speaker (the reference speaker) 
is used by another one (the new or target speaker). Training a SD system for any 
new speaker may be unfeasibte: collecting a large amount of new training data 
is time consuming for the speaker and unacceptable in some applications. Given 
a pre-trained SD speech recognition system, the goal of normalization methods is 
then to reduce to a few sentences the amount of training data required from a new 
speaker to achieve acceptable recognition performance. The inter-speaker variation 
of the acoustic data is reduced by estimating a feature vector transformation be- 
tween the acoustic parameter space of the new speaker and that of the reference 
speaker (Montacie et al., 1989; Class et at., 1990; Nakamura and Shikano, 1990; 
Huang, 1992; Matsukoto and Inoue, 1992). This multivariate transformation, also 
called spectral mapping given the type of features considered in the parameteri- 
zation of speech data, provides an acoustic front-end to the recognition system. 
Supervised speaker normalization methods require that the text of the training ut- 
terances required from the new speaker is known, while arbitrary utterances can 
be used by unsupervised methods (Furui and Sondhi, 1991). Good performance 
have been achieved with spectral mapping techniques based on MSE optimization 
(Class et at., 1990; Matsukoto and Inoue, 1992). Alternative approaches presented 
estimation of the spectral normalization mapping with Multi-Layer Perceptron neu- 
ral networks (Montacie et al., 1989; Nakamura and Shikano, 1990; Huang, 1992; 
Watrous, 1994). 
This paper introduces a supervised speaker normalization method based on neural 
network regression with a generalized local basis model of elliptical kernels (General- 
ized Resource Allocating Network: GRANmodet). Kernels are recursively allocated 
by introducing the heuristic procedure of (Platt, 1991) within the generalized RBF 
schema proposed in (Poggio and Girosi, 1989). The model includes a linear term 
and efficient on-line optimization of parameters is achieved by an automatic dif- 
ferentiation technique. Our results compare favorably with normalization by affine 
linear transformations based on orthonormal constrained pseudoinverse. In this pa- 
per, the normalization module was integrated and tested as an acoustic front-end for 
speaker-dependent continuous speech recognition systems. Experiments regarded 
phone units recognition with Hidden Markov Model (HMM) recognition systems. 
The diagram in Figure 1 outlines the general structure of the experiment with 
GRAN normalization modules. The architecture is independent from the specific 
speech recognition system and allows comparisons between different normalization 
techniques. The GRAN model and a general procedure for data standardization are 
described in Section 2 and 3. After a discussion of the spectral mapping problem 
in Section 4, the APASCI corpus used in the experiments and the characteristics 
of the acoustic data are described in Section 5. The recognition system and the 
experiment set-up are detailed in Sections 6-8. Results are presented and discussed 
in Section 9. 
Connectionist Speaker Normalization with Generalized Resource Allocating Networks 869 
Data Base: ] 
reference phrase 
Feature Extraction 
phrase S 
Dynamic Time Warping 
{Xi(k), (1 
Test 
Speech Signal 
corresponding to phrase S 
uttered by a new speaker 
Figure 1: System overview 
Neural Network 
supervised training 
GRAN normalizatio 
i- 1 ..... 
HMM Speech Recogn. 
1 
Output 
2 THE GRAN MODEL 
Feedforward artificial neural networks can be regarded as a convenient realization 
of general functional superpositions in terms of simpler kernel functions (Barron 
and Barron, 1988). With one hidden layer we can implement a multivariate su- 
perposition f(z) = 7=oojKj(z,wj) where Kj is a function depending on an 
input vector z and a parameter vector a,j, a general structure which allows to re- 
alize flexible models for multivariate regression. We are interested in the schema: 
 = HK(x)+ Ax + b with input vector x  R a and estimated output vec- 
tor 9  R 2. K = (Kj) is a n-dimensional vector of local kernels, H is the 
d2 x n real matrix of kernel coefficients, b  R a is an offset term and A is a 
d2 x d linear term. Implemented kernels are Gaussian, Hardy multiquadrics, in- 
verse of Hardy multiquadrics and Epanenchnikov kernels, also in the Nadaraya- 
Watson normalized form (Hirdle, 1990). The kernel allocation is based on a 
recurslye procedure: if appropriate novelty conditions are satisfied for the exam- 
ple (x', y'), a new kernel K,+ is allocated and the new estimate 9,+ becomes 
- 9.(x) + K+l(llx- x'llw)(y' - 9.(x)) (Hirdle, 1990). Global proper- 
ties and rates of convergence for recursive kernel regression estimates are given in 
(Krzyzak, 1992). The heuristic mechanism suggested by (Platt, 1991) has been 
extended to include the optimization of the weighted metrics as requested in the 
generalized versions of RBF networks of (Poggio and Girosi, 1989). Optimization 
regards kernel coefficients, locations and bandwidths, the offset term, the coeffi- 
cient matrix A if considered, and the W matrix defining the weighted metrics in 
the input space: IIxllv = xWWx. Automatic differentiation is used for efficient 
on-line gradient-descent procedure w.r.t. different error functions (L2, L, entropy 
fit), with different learning rates for each type of parameters. 
8 70 Cesare Furlanello, Diego Giuliani, Edmondo Trentin 
Figure 2: Commutative diagram for the speaker normalization problem. The spec- 
tral mapping  between original spaces ,r and  is estimated by  = ]l .  ï¿½ ]x, 
obtained by composition of the neural GRAN mapping  between PCA spaces 
and  with the two invertible PCA transformations ]x and 
3 NETWORKS AND PCA TRANSFORMATIONS 
The normalization module is designed to estimate a spectral mapping between the 
acoustic spaces of two different speakers. Inter-speaker variability is reflected by 
significant differences in data distribution in these multidimensional spaces (we con- 
sidered 8 dimensions); in particular it is important to take into account global data 
anisotropy. More generally, it is also crucial to decorrelate the features describing 
the data. A general recipe is to apply the well-known Principal Component Analy- 
sis (PCA) to the data, in this case implemented from standard numerical routines 
based on Singular Value Decomposition of the data covariance matrices. The net- 
work was applied to perform a mapping between the new feature spaces obtained 
from the PCA transformations, mean translation included (Figure 2). 
4 THE SPECTRAL MAPPING PROBLEM 
A sound uttered by a speaker is generally described by a sequence of feature vectors 
obtained from the speech signal via short-time spectral analysis (Sec. 5). The spec- 
tral representations of the same sequence of sounds uttered by two speakers are sub- 
ject to significant variations (e.g. differences between male and female speakers, re- 
gional accents, . .. ). To deal with acoustic differences, a suitable transformation (the 
spectral mapping) is seeked which performs the best mapping between the corre- 
sponding spectra of two speakers. Let Y - (yl, y2, ..., yJ) and X -- (x, x2, ..., x) be 
the spectral feature vector sequences of the same sentence uttered by two speakers, 
called respectively the reference and the new speaker. The desired mapping is per- 
formed by a function (xi) such that the transformed vector sequence obtained from 
X = (xi) approximates as close as possible the spectral vector sequence Y -- (yj). 
To eliminate time differences between the two acoustic realizations, a time warping 
function has to be determined yielding pairs C(k) - (i(k),j(k))k=L..K of corre- 
sponding indexes of feature vectors in X and Y, respectively. The desired spectral 
Connectionist Speaker Normalization with Generalized Resource Allocating Networks 871 
mapping o(zi) is the one which minimizes 'k I d(y
