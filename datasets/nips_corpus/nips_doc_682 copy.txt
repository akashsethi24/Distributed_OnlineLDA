A Boundary Hunting Radial Basis Function 
Classifier Which Allocates Centers 
Constructively 
Eric I. Chang and Richard P. Lippmann 
MIT Lincoln Laboratory 
Lexington, MA 02173-0073, USA 
Abstract 
A new boundary hunting radial basis function (BH-RBF) classifier 
which allocates RBF centers constructively near class boundaries is 
described. This classifier creates complex decision boundaries only in 
regions where confusions occur and corresponding RBF outputs are 
similar. A predicted square error measure is used to determine how 
many centers to add and to determine when to stop adding centers. Two 
experiments are presented which demonstrate the advantages of the BH- 
RBF classifier. One uses artificial data with two classes and two input 
features where each class contains four clusters but only one cluster is 
near a decision region boundary. The other uses a large seismic database 
with seven classes and 14 input features. In both experiments the BH- 
RBF classifier provides a lower error rate with fewer centers than are 
required by more conventional RBF, Gaussian mixture, or MLP 
classifiers. 
1 INTRODUCTION 
Radial basis function (RBF) classifiers have been successfully applied to many pattern 
classification problems (Broomhead, 1988, Ng, 1991). These classifiers have the advan- 
tages of short training times and high classification accuracy. In addition, RBF outputs 
estimate minimum-error Bayesian a posteriori probabilities (Richard, 1991). Performing 
classification with RBF outputs requires selecting the output which is highest for each 
input. In regions where one class dominates, the Bayesian a posteriori probability for that 
class will be uniformly high and near 1.0. Detailed modeling of the variation of the 
Bayesian a posteriori probability in these regions is not necessary for classification. Only 
139 
140 Chang and Lippmann 
at the boundary between different classes is accurate estimation of the Bayesian a posteri- 
ori probability necessary for high classification accuracy. If the boundary between differ- 
ent classes can be located in the input space, RBF centers can be judiciously allocated in 
those regions without wasting RBF centers in regions where accurate estimation of the 
Bayesian a posterJori probability does not improve classification performance. 
In general, having more RBF centers allows better approximation of the desired output. 
While training a RBF classifier, the number of RBF centers must be selected. The tradi- 
tional approach has been to randomly choose patterns from the training set as centers, or to 
perform K-means clustering on the data and then to use these centers as the RBF centers. 
Frequently the correct number of centers to use is not known a priori and the number of 
centers has to be tuned. Also, with K-means clustering, the centers are distributed without 
considering their usefulness in classification. In contrast, a constructive approach to add- 
ing RBF centers based on modeling Bayesian a posteriori probabilities accurately only 
near class boundaries provides good performance with fewer centers than are required to 
separately model class PDF's. 
Many algorithms have been proposed for constructively building up the structure of a RBF 
network (Mel, 1991). However, the algorithms proposed have all been designed for train- 
ing a RBF network to perform function mapping. For mapping tasks, accuracy is impor- 
tant throughout the input region and the mean squared error is the criterion that is 
minimized. In classification tasks, only boundaries between different classes are important 
and the overall mean squared error is not as important as the error in class boundaries. 
2 ALGORITHM DESCRIPTION 
A block diagram of a new boundary hunting RBF (BH-RBF) classifier that adds centers 
constmctively near class boundaries is presented in Figure 1. A simple unimodal Gaussian 
classifier is first formed by clustering the training patterns from a randomly selected class 
and assigning a center to that class. The confusion matrix generated by using this simple 
classifier is then examined to determine the pair of classes A and B, which have the most 
mutual confusion. Training patterns that are close to the boundary between these two 
classes are determined by looking at the outputs of the RBF classifier. Boundary patterns 
ONE RBF 
CENTER 
I INITIAL RBF 
NETWORK 
ADD NEW RBF CENTERS TO 
CLASS PAIR RESPONSIBLE , 
FOR MOST ERRORS & OVERLAP  
 CALCULATE 
INTERMEDIATE 
RBF NETWORKS 
PREDICTED SQUARED 
ERROR SCORE 
Figure 1: Block Diagram of Training of BH-RBF Network 
A Boundary Hunting Radial Basis Function Classifier (Allocates Centers Constructively) 141 
which produce similar high outputs for both classes that are different by less than a 
closecall threshold are used to produce new cluster centers. 
Figure 2 shows RBF outputs corresponding to class A and B as the input varies over a 
small range. This figure illustrates how network outputs are used to determine the close- 
call region between classes. Network outputs are high in regions dominated by a particu- 
lar class and therefore these regions are outside the boundary between different classes. 
Network outputs are close in the region where the absolute difference of the two highest 
network outputs is less than the closecall threshold. Training patterns which fall into this 
closecall region plus all the points that are misclassified as the other class in the class pair 
are considered to be points in the boundary. For example, a pattern in class A which is 
misclassified as class B would be considered to be in the boundary between class A and B. 
On the other hand, a pattern in class A which is misclassified as class C would not be 
placed in the boundary between class A and B. 
1 
0.9 
0.8 
0.7 
o.6 
0.5 
0.4 
0.3 
0.2 
o.1 
o 
-3 
F (A) F (B) 
CLASS A CLASS B 
CLOSECALL 
THRESHOLD 
...i ON OSEcALL 
-2 -1 0 
INPUT 
I 2 3 
Figure 2: Using the Network Output to Determine Closecall Regions 
After the patterns which belong in the boundary are determined, clustering is performed 
separately on boundary paUems from different classes using K-means clustering and a 
number of centers ranging from zero to a preset maximum number of centers. After the 
centers are found, new RBF classifiers are trained using the new sets of centers plus the 
original set of centers. The combined set of centers that provides the best performance is 
saved and the cycle repeats again by finding the next class pair which accounts for the 
most remaining confusions. Overfitting by adding too many centers at a time is avoided by 
using lhe predicted squared error (PSE) as the criterion for choosing new centers (Barron, 
1984): 
Cx 2 
PSE = RMS + -- 
N 
142 Chang and Lippmann 
In this equation, RMS is the root mean squared error on the training set, 02 estimates the 
variance of the error, C is the total number of centers in the RBF classifier, and N is the 
total number of patterns in the training set. The error variance 02 is selected empirically 
using left-out evaluation data. Different values of 02 are tried and the value which pro- 
vides the best performance on the evaluation data is chosen. On each cycle, different num- 
ber of centers are tried for each class of the selected class pair and the PSE is used to select 
the best subset of centers. The best PSE on each cycle is used to determine when training 
should be stopped to prevent overfitting. Training stops after the PSE has not decreased 
for five consecutive cycles. 
3 EXPERIMENTAL RESULTS 
Two experiments were performed using the new BH-RBF classifier, a more conventional 
RBF classifier, a Gaussian mixture classifier (Ng, 1991), and a MLP classifier. Five regu- 
lar RBF classifiers (RBF) were trained by assigning 1, 2, 3, 4, or 5 centers to each class. 
Similarly, five Gaussian mixture classifiers (GMIX) were trained with 1, 2, 3, 4, or 5 cen- 
ters in each class. The means of each center were trained individually using K-means clus- 
tering to find the centers for patterns from each class. The diagonal covariance of each 
center was set using all the patterns that were assigned to a cluster during the last pass of 
K-means clustering. The structure of the regular RBF classifier and the Gaussian mixture 
classifier are identical when the number of centers are the same. The only difference 
between the classifiers is the method used to train parameters. 
MLP classifiers were trained for 10 independent trials for each data set. The number of 
hidden nodes was varied from 2 to 30 in increments of 2. The goal of the experiment was 
to explore the relationship between the complexity of the classifier and the classification 
accuracy of the classifier. Training was stopped using cross validation to avoid overfitting. 
3.1 FOUR-CLUSTER DATABASE 
The first problem is an artificial data set designed to illustrate the difference between BH- 
RBF and other classifiers. There are two classes, each class consist of one large Gaussian 
cluster with 700 random points and three smaller clusters with 100 points each. Figure 3 
shows the distribution of the data and the ideal decision boundary if the actual centers and 
variances are used to train a Bayesian minimum error classifier. There were 2000 training 
patterns, 2000 evaluation patterns, and 2000 test patterns. The BH-RBF classifier was 
trained with the closecall threshold set to 0.75, 02 set to 0.5, and a maximum of two extra 
centers per class at between each pair of classes. The theoretically optimal Bayesian clas- 
sifter for this database provides the error rate of 1.95% on the test set. This optimal Baye- 
sian classifier is obtained using the actual centers, variances, and a priori probability used 
to generate the data in a Gaussian mixture classifier. In a real classification task, these cen- 
ter parameters are not known and have to be estimated from training data. 
Figure 4 shows the testing err
