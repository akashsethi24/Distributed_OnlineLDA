Effects of Noise on Convergence and 
Generalization in Recurrent Networks 
Kam Jim 
Bill G. Horne 
C. Lee Giles* 
NEC Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540 
{kamj ira, horne, giles}reseaxch. nj. nec. corn 
*Also with 
UMIACS, University of Maryland, College Park, MD 20742 
Abstract 
We introduce and study methods of inserting synaptic noise into 
dynamically-driven recurrent neural networks and show that ap- 
plying a controlled amount of noise during training may improve 
convergence and generalization. In addition, we analyze the effects 
of each noise parameter (additive vs. multiplicative, cumulative vs. 
non-cumulative, per time step vs. per string) and predict that best 
overall performance can be achieved by injecting additive noise at 
each time step. Extensive simulations on learning the dual parity 
grammar from temporal strings substantiate these predictions. 
1 INTRODUCTION 
There has been much research in applying noise to neural networks to improve net- 
work performance. It has been shown that using noisy hidden nodes during training 
can result in error-correcting codes which increase the tolerance of feedforward nets 
to unreliable nodes (Judd and Munro, 1992). Also, randomly disabling hidden 
nodes during the training phase increases the tolerance of MLP's to node failures 
(Sdquin and Clay, 1990). Bishop showed that training with noisy data is equivalent 
to Tikhonov Regularization and suggested directly minimizing the regularized error 
function as a practical alternative (Bishop, 1994). Hanson developed a stochastic 
version of the delta rule which adapt weight means and standard deviations instead 
650 Kam Jim, Bill G. Home, C. Lee Giles 
of clean weight values (Hanson, 1990). (Mpitsos and Burton, 1992) demonstrated 
faster learning rates by adding noise to the weight updates and adapting the mag- 
nitude of such noise to the output error. Most relevant to this paper, synaptic noise 
has been applied to MLP's during training to improve fault tolerance and training 
quality. (Murray and Edwards, 1993) 
In this paper, we extend these results by introducing several methods of inserting 
synaptic noise into recurrent networks, and demonstrate that these methods can 
improve both convergence and generalization. Previous work on improving these 
two performance measures have focused on ways of simplifying the network and 
methods of searching the coarse regions of state space before the fine regions. Our 
work shows that synaptic noise can improve convergence by searching for promising 
regions of state space, and enhance generalization by enforcing saturated states. 
2 NOISE INJECTION IN RECURRENT NETWORKS 
In this paper, we inject noise into a High Order recurrent network (Giles et al., 1992) 
consisting of N recurrent state neurons $1, L non-recurrent input neurons I, and 
N2L weights W/d. (For justification of its use see Section 4.) The recurrent network 
operation is defined by the state process $+1 = g('i, WidS I)' where g(.) is a 
sigmoid discriminant function. During training, an error function is computed as 
Zp__12 
ep, where ep -- So-dp, So is the output neuron, and dp is the target output 
value Ibr pattern p. 
Synaptic noise has been simulated on Multi-Layered-Perceptrons by inserting noise 
to the weights of each layer during training (Murray et al., 1993). Applying this 
method to recurrent networks is not straightforward because effectively the same 
weights are propagated forward in time. This can be seen by recalling the BPTT 
representation of unrolling a recurrent network in time into T layers with identical 
weights, where T is the length of the input string. In Tables 2 and 3, we introduce 
the noise injection steps for eight recurrent network noise models representing all 
combinations of the following noise parameters: additive vs. multiplicative, cu- 
mulative vs. non-cumulative, per time step vs. per string. As their name imply, 
additive and multiplicative noise add or multiply the weights by a small noise term. 
In cumulative noise, the injected noise is accumulated, while in non-cumulative 
noise the noise from the current step is removed before more noise is injected on 
the next step. Per time step and per string noise refer to when the noise is inserted: 
either at each time step or only once for each training string respectively. Table 1 
illustrates noise accumulation examples for all additive models (the multiplicative 
case is analogous). 
3 ANALYSIS ON THE EFFECTS OF SYNAPTIC NOISE 
The effects of each noise model is analyzed by taking the Taylor expansion on 
the error function around the noise-free weight set. By truncating this expansion 
to second and lower order terms, we can interpret the effect of noise as a set of 
regularization terms applied to the error function. From these terms predictions can 
be made about the effects on generalization and convergence. A similar analysis was 
Effects of Noise on Convergence and Generalization in Recurrent Networks 651 
performed on MLP's to demonstrate the effects of synaptic noise on fault tolerance 
and training quality (Murray et. al., 1993). Tables 2 and 3 list the noise injection 
step and the resulting first and second 6rder Taylor expansion terms for all noise 
models. These results are derived by assuming the noise to be zero-mean white 
with variance a2 and uncorrelated in time. 
3.1 Predictions on Generalization 
One common cause of bad generalization in recurrent networks is the presence of 
unsaturated state representations. Typically, a network cannot revisit the exact 
same point in state space, but tends to wander away from its learned state repre- 
sentation. One approach to alleviate this problem is to encourage state nodes to 
operate in the saturated regions of the sigmoid. The first order error expansion 
terms of most noise models considered are capable of encouraging the network to 
achieve saturated states. This can be shown by applying the chain rule to the 
partial derivative in the first order expansion terms: 
OWt,id} : 00---o  O -x OOr -x ' k OS[ OWt,ija ' (1) 
where O is the net input to state node i at time step t. The partial derivatives 
os favor internal representations such that the effects of perturbations to the net 
0o 
inputs O are minized. 
Multiplicative noise implements a form of weight decay because the error expansion 
terms include the weight products Wijk or Wt,ijkWu,ijk. Although weight decay 
h been shown to improve generalization on kedforward networks (Krogh and 
Hertz, 1992) we hypothesize this may not be the ce for recurrent networks that 
are learning to solve FSA problems. Large weights are necessary to saturate the 
state nodes to the upper and lower limits of the sigmoid discriminant function. 
Therefore, we predict additive noise will allow better generalization because of its 
absence of weight decay. 
Noise models whose first order error term contain the exprsion 0 os 
OW,ij 
will hvor saturated states for those partials whose sign correspond to the sign of 
a majority of the partials. It will favor unsaturated states, operating in the linear 
region of the sigmoid, for partials whose sign is the minority. Such sign-dependent 
enforcement is not optimal. 
The error terms for cumulative per time step noises sum a product with the expres- 
0s 0s where v = min(t + 1, u + 1). The effect of cumulative noise 
incre,es more rapidly because of v and thus optimal generalization and detrimental 
noise effects will occur at lower amplitudes than non-cumulative noise. 
For cumulative per string noise models, the products (t+ 1)(u+ 1) and 
in the expansion terms rapidly overwhelm the raw error term. Generalization im- 
provement is not expected for these models. 
We also re,on that all generalization enhancements will be valid only for a range 
of noise values, above which noise overwhelms the raw error information. 
652 Kam Jim, Bill G. Home, C. Lee Giles 
3.2 Predictions on Convergence 
Synaptic noise can improve convergence by favoring promising weights in the begin- 
ning stages of training. This can be demonstrated by examining the second order 
error expansion term for non-cumulative, multiplicative, per time step noise: 
k 2 / 
When ep is negative, solutions with a negative second order state-weight partial 
derivative will be de-stabilized. In other words, when the output So is too small 
the network will favor updating in a direction such that the first order partial 
derivative is increasing. A corresponding relationship can be observed for the case 
when ep is positive. Thus the second order term of the error function will allow 
a higher raw error ep to be favored if such an update will place the weights in a 
more promising area, i.e. a region where weight changes are likely to move So in 
a direction to reduce the raw error. The anticipatory effect of this term is more 
important in the beginning stages of training where ep is large, and will become 
insignificant in the finishing stages of training as ep approaches zero. 
Similar to arguments in Section 3.1, the absence of weight decay will make the 
learning task easier and improve convergence. 
From this discussion it can be inferred that additive per time step noise mod- 
els should yield the best generalization and convergence performance because of 
their sign-independent favoring of saturated states and the absence of weight decay. 
Furthermore, convergence and generalization performance is more sensitive to cu- 
mulative noise, i.e. optimal performance and detrimental effects will occur at lower 
amplitudes than in non-cumulative noise. 
4 SIMULATION RESULTS 
In order to perform many experiments in a reasonable amount of computation 
time, we attempt to learn the simple hidden-state dual parity automata from 
sample strings encoded as temporal sequences. (Dual parity is a 4-state automata 
that recognizes binary strings containing
