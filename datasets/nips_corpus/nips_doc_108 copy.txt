527 
IMPLICATIONS OF 
RECURSIVE DISTRIBUTED REPRESENTATIONS 
Jordan B. Pollack 
Laboratory for AI Research 
(),hio Slate University 
Columbus, OH 43210 
ABSTRACT 
I will describe my recent results on the automatic development of fixed- 
width recursive distributed representations of variable-sized hierarchal data 
structures. One implication of this work is that certain types of AI-style 
data-structures can now be represented in fixed-width analog vectors. Simple 
inferences can be performed using the type of pattern associations that 
neural networks excel at. Another implication arises from noting that these 
representations become self-similar in the limit. Once this door to chaos is 
opened, many interesting new questions about the representational basis of 
intelligence emerge, and can (and will) be discussed. 
INTRODUCTION 
A major problem for any co.tmitive system is the capacity for, and the induction of the 
potentially infinite structures implicated in faculties such as human language and 
memory. 
Classical coitive architectures handle this problem through finite but recursire sets of 
rules, such as formal grammars (Chomsky, 1957). Connectionist architectures, while 
yielding intriguing in.RightS into fault-tolerance and machine learning, have, thus far, not 
handled such productive systems in an adequate fashion. 
So, it is not surprising that one of the main attacks on connectionism, especially on itS 
application to language processing models, has been on the adequacy of such systems to 
deal with apparenfiy rule-based behaviors (Pinker & Prince, 1988) and systematicity 
(Fodor & Pylyshyn, 1988). 
I had earlier discussed precisely these challenges for connectionism, calling them the 
generafive capacity problem for language, and the representafional adequacy problem for 
data stmctures (Pollack, 1987b). These problems are actually intimately related, as the 
capacity to recognize or generate novel language relies on the ability to represent the 
underlying concept. 
Recently, I have developed an approach to the representation problem, at least for recur- 
sive structures like sequences and trees. Recursive auto-associative memory (RAAM) 
(Pollack, 1988a). automatically develops recursive distributed representations of finite 
training sets of such structures, using Back-Propagation (Rumelhart et al., 1986). These 
representations appear to occupy a novel position in the space of both classical and con- 
nectionist symbolic representations. 
A fixed-width representation of variable-sized symbolic trees leads immediately to the 
implication that simple forms of neural-network associative memories may be able to 
perform inferences of a type that are thought to require complex machinery such as vari- 
able binding and unification. 
But when we take seriously the infinite part of the representational adequacy problem, we 
are lead into a strange intellectual area, to which the second part of this paper is 
addressed. 
528 Pollack 
BACKGROUND 
RECURSIVE AUTO-ASSOCIATIVE MEMORY 
A RAAM is composed of two mechanisms: a compressor, and a reconstructor, which are 
simultaneously trained. The job of the compressor is to encode a small set of fixed-width 
patterns imo a single pattern of the same width. This compression can be recursively 
applied, from the bottom up, to a fixed-valence tree with distinguished labeled terminals 
(leaves), resulting in a fixed-width pattem represeming the entire structure. The job of 
the reconstructor is to accurately decode this pattern imo its parts, and then to further 
decode the parts as necessary, until the terminal patterns are found, resulting in a recon- 
struction of the original tree. 
For binary trees with k-bit binary patterns as the leaves, the compressor could be a 
single-layer feedforward network with 2k inputs and k outputs, along with additional con- 
trol machinery. The reconstructor could be a single-layer feedforward-network with k 
inputs and 2k outputs, along with a mechamsm for testing whether a pattern is a terminal. 
We simultaneously train these two networks in an auto-associative framework as follows. 
Consider the tree, ((D (A N))(V (P (D N))), as one member of a training set of such trees, 
where the lexical categories are pre-encoded as k-bit vectors. If the 2k-k-2k network is 
successfully trained (defined below) with the following patterns (among other such pat- 
terns in the training environment), the resultant compressor and reconstmctor can reliably 
form representations for these binary trees. 
input pattern 
hidden pattem 
output pattern 
A+N --) Rmv(t) --) A,+N, 
D+RAv(t) --) RDav( t) --) D,+RAv(t), 
D+N --) RDtq( t ) -+ D,+N, 
P+RDtq(t) -+ ReDs(t) --) P'+RDtq(t)' 
V+RpDN(t ) -- RVPDN(t ) --- Vt+RpDN(t ), 
RDAN(t)+RvPDN(t ) --> RDANVPDN(t ) ---> RDAN(t)t+RvPDN(t ), 
The (initially random) values of the hidden units, R;(t), are part of the training environ- 
ment, so it (and the represemations) evolve along with the weights. 1 
Because the traimng regime involves multiple compressions, but only single reconstruc- 
tions, we rely on an induction that the reconstructor works. If a reconstructed pattem, say 
RpDN t, is SUfficiently close to the original pattem, then its parts can be reconstructed as 
well. 
AN EXPERIMENT 
The tree considered above was one member of the first experiment done on RAAM's. I 
used a simple context-free parser to parse a set of lexical-category sequences into a set of 
bracketed binary trees: 
(D (A (A (A N)))) 
((D N)(P (D N))) 
(V (D N)) 
(P (D (A N))) 
((D N) V) 
This moving target strategy is also used by (Elman, 1988) and (Dyer et al., 1988). 
Implications of Recursive Distributed Representations 529 
((D N) (V (D (A N))))) 
((D (A N)) (V (P (D N)))) 
Each terminal pattem (D A N V & P) was represented as a l-bit-in-5 code padded with 5 
zeros. A 20-10-20 RAAM devised the representations shown in figure 1. 
NP 
�P 
PP 
(D N) 
(D (A (A (A N)))) 
(D (A N)) 
((D N) (P (D N))) 
(V (P (D N))) 
(v (D (^ ))) 
(v (D N)) 
(P (D N)) 
(P (D (A N))) 
P 
(A N) 
(A (A N)) 
(A (A (A N))) 
((D N) V) 
((D N) (V (D (A N)))) 
((D (^ N)) (V (P (D N)))) 
OOOo .... DO 
000D ..... [] 
oo0o ..... 0 
0.- -.oD- 
O.O- 
� .Do O.OOo 
� .0o O.O.o 
� .O. 0.o0 
..O. 
oOOOOO. o. 
' 'ODD' ' �' 
DO-OOoOoO. 
0'o .... 
.0...o.0o. 
Representations of all the binary trees in the training set, devised by a 
20-10-20 RAAM, manually clustered by phrase-type. The squares represent 
values between 0 and 1 by area. 
I labeled each tree and its representation by the phrase type in the grammar, and sorted 
them by type. The RAAM, without having any intrinsic concepts of phrase-type, has 
clearly developed a representation with similarity between members of the same type. 
For example, the third feature seems to be clearly distinguishing sentences from non- 
sentences, the fifth feature seems to be involved in separating adjective phrases from oth- 
ers, while the tenth feature appears to distinguish prepositional and noun phrases from 
others. 2 
At the same time, the representation must be keeping enough information about the sub- 
trees in order to allow the reconstructor to accurately recover the original structure. So, 
knowledge about structural regularity flows into the we, ights while constraints about con- 
text similarity guide the development of the representations. 
RECURSIVE DISTRIBUTED REPRESENTATIONS 
These vectors are a very new kind of representation, a recursive, distributed represen- 
tation, hinted at by Hintoh's (1988) notion of a reduced description. 
They combine aspects of several disparate represemations. Like feature-vectors, they are 
fixed-width, similarity-based, and their content is easily accessible. Like symbols, they 
combine only in syntactically well-formed ways. Like symbol-structures, they have con- 
stituency and compositionality. And, like pointers, they refer to larger symbol structures 
2 In fact, by these metrics, the test case ((D N)(P (D N))) should really be classified as a sentence; since it was 
not used in any other construction, there was no reason for the RAAM to believe otherwise. 
530 Pollack 
which can be efficiently retrieved. 
But, unlike feature-vectors, they compose. Unlike symbols, they can be compared. 
Unlike symbol structures, they are fixed in size. And, unlike pointers, they have content. 
Recursive distributed representations could, potentially, lead to a reintegration of syntax 
and semantics at a very low level 3. Rather than having meaning-free symbols which syn- 
tactically combine, and meanings which are recursively ascribed, we could functionally 
compose symbols which bear their own meanings. 
IMPLICATIONS 
One of the reasons for the historical split between symbolic AI and fields such as pattem 
recognition or neural networks is that the structured representations AI requires do not 
easily commingle with the representations offered by n-dimensional vectors. 
Since recursive distributed representations form a bridge from structured representations 
to n-dimensional vectors, they will allow high-level AI tasks to be accomplished with 
neural networks. 
ASSOCIATIVE INFERENCE 
There are many kinds of inferences which seem to be very easy for humans to perform. 
In fact, we must perform incredibly long chains of inferences in the act of understanding 
natural language (Bimbaum, 1986). 
And yet, when we consider performing those inferences using standard techniques which 
involve variable binding and unification, the costs seem prohibitive. For humans, how- 
ever, these inferences seem to cost no more than simple associative priming (Meyer & 
Schvaneveldt, 1971). 
Since RAAMS can devise representations of trees as analog pattems which can actually 
be associated, they may lead to very fast neuro-logical inference engines. 
For example, in a larger experiment, which was reported in (Pollack, 1988b), a 48-16-48 
RAAM developed representations for a set of temary trees, suc
