Probabilistic Visualisation of 
High-dimensional Binary Data 
Michael E. Tipping 
Microsoft Research, 
St George House, I Guildhall Street, 
Cambridge CB2 3NH, U.K. 
mtipping�microsoft.com 
Abstract 
We present a probabilistic latent-variable framework for data visu- 
alisation, a key feature of which is its applicability to binary and 
categorical data types for which few established methods exist. A 
variational approximation to the likelihood is exploited to derive a 
fast algorithm for determining the model parameters. Illustrations 
of application to real and synthetic binary data sets are given. 
I Introduction 
Visualisation is a powerful tool in the exploratory analysis of multivariate data. The 
rendering of high-dimensional data in two dimensions, while generally implying loss 
of information, often reveals interesting structure to the human eye. Standard 
dimensionality-reduction methods from multivariate analysis, notably the principal 
component projection, are often utilised for this purpose, while techniques such 
as 'projection pursuit' have been tailored specifically to this end. With the cur- 
rent trend for larger databases and the need for effective 'data mining' methods, 
visualisation is becoming increasingly topical, and recent novel developments in- 
clude nonlinear topographic methods (Lowe and Tipping 1997; Bishop, Svens6n, 
and Williams 1998) and hierarchical combinations of linear models (Bishop and 
Tipping 1998). However, a disadvantageous aspect of many proposed techniques 
is their applicability only to continuous variables; there are very few such methods 
proposed specifically for the visualisation of discrete binary data types, which are 
commonplace in real-world datasets. 
We approach this difficulty by proposing a probabilistic framework for the visualisa- 
tion of arbitrary data types, based on an underlying latent variable density model. 
This leads to an algorithm which permits the visualisation of structure within data, 
while also defining a generative observation probability model. A further, and 
Probabilistic I4sualisation of High-Dimensional Binary Data 593 
intuitively pleasing, result is that the specialisation of the model to continuous vari- 
ables recovers principal component analysis. Continuous, binary and categorical 
data types may thus be combined and visualised together within this framework, 
but for reasons of space, we concentrate on binary types alone in this paper. 
In the next section we outline the proposed latent variable approach, and in Section 
3 consider the difficulties involved in estimating the parameters in this model, giving 
an efficient variational scheme to this end in Section 4. In Section 5 we illustrate the 
application of the model and consider the accuracy of the variational approximation. 
2 Latent Variable Models for Visualisation 
In an ideal visualisation model, we would wish all of the dependencies between 
variables to be evident in the visualisation space, while the information that we lose 
in the dimensionality-reduction process should represent noise, independent to 
each variable. This principle is captured by the following probability density model 
for a dataset comprising d-dimensional observation vectors t = (t, t2,..., t): 
(1) 
where x is a two-dimensional latent variable vector, the distribution of which must 
be a priori specified, and 0 are the model parameters. Now, for a given value of x 
(or location in the visualisation space), the observations are independent under the 
model. (In general, of course, the model and conditional independence assumption 
will only hold approximately.) However, the unconditional observation model p(t) 
does not, in general, factorise and so can still capture dependencies between the 
d variables, given the constraint implied by the use of just two underlying latent 
variables. So, having estimated the parameters 0, data could be visualised by 
'inverting' the generative model using Bayes' rule: p(xlt ) = p(tlx)p(x)/p(t ). Each 
data point then induces a distribution in the latent space, which for the purposes 
of visualisation, we might summarise with the conditional mean value {xlt ). 
That this form of model can be appropriate for visualisation was demonstrated by 
Bishop and Tipping (1998), who showed that if the latent variables are defined to 
be independent and Gaussian, x 0 A/'(0, I), and the conditional observation model 
is also Gaussian, til x 0 A/'(w'x q- tti, aI), then maximum-likelihood estimation of 
the model parameters {wi, tti, or/2} leads to a model where the the posterior mean 
{xtt } is equivalent to a probabilistic principal component projection. 
A visualisation method for binary variables now follows naturally. Retaining the 
Gaussian latent distribution x ,0 A/'(0, I), we specify an appropriate conditional 
distribution for P(ti Ix, 0). Given that principal components corresponds to a linear 
model for continuous data types, we adopt the appropriate generalised linear model 
in the binary case: 
P(tilx) = a(Ai) t' {1 - a(Ai)} -t , 
(2) 
where a(A) = {1 + exp(-A)} - and Ai = w'x + bi with parameters wi and bi. 
3 Maximum-likelihood Parameter Estimation 
The proposed model for binary data already exists in the literature under various 
guises, most historically as a latent trait model (Bartholomew 1987), although it 
is not utilised for data visualisation. While in the case of probabilistic principal 
594 M. E. Tipping 
component analysis, ML parameter estimates can be obtained in closed-form, a dis- 
advantageous feature of the binary model is that, with P(tilx) defined by (2), the 
integral of (1) is analytically intractable and P(t) cannot be computed directly. Fit- 
ting a latent trait model thus necessitates a numerical integration, and recent papers 
have considered both Gauss-Hermite (Moustaki 1996) and Monte-Carlo sampling 
approximations (Mackay 1995; Sammel, Ryan, and Legler 1997). 
In this latter case, the log-likelihood for a dataset of N observation vectors 
{tl,..., tN} would be approximated by 
/2 Zln ZHP(tinlXt,wi,bi) (3) 
n=l l:l i----1 
where xt, 1 = 1... L, are samples from the two-dimensional latent distribution. 
To obtain parameter estimates we may utilise an expectation-maximisation (EM) 
approach by noting that (3) is equivalent in form to an L-component latent class 
model (Bartholomew 1987) where the component probabilities are mutually con- 
strained from (2). Applying standard methodology leads to an E-step which re- 
quires computation of N x L posterior 'responsibilities' P(xtlt,,), and a logistic 
regression M-step which is unfortunately iterative, although it can be performed 
relatively efficiently by an iteratively re-weighted least-squares algorithm. Because 
of these difficulties in implementation, in the next section we describe a variational 
approximation to the likelihood which can be maximised more efficiently. 
4 A Variational Approximation to the Likelihood 
Jaakkola and Jordan (1997) introduced a variational approximation for the predic- 
tive likelihood in a Bayesian logistic regression model and also briefly considered 
the dual problem, which is closely related to the proposed visualisation model. 
In this approach, the integral in (1) is approximated by: 
dx, (4) 
where 
with Ai = 
i are the 'variational' parameters, and this approximation has the property that 
P(ti[x,i) _< P(tilx), with equality at i: Ai, and thus it follows that P(t) _< P(t). 
Now because the exponential in (5) is quadratic in x, then the integral in (4), and 
also the likelihood, can be computed in closed form. This suggests an alterna- 
tive algorithm for finding parameter estimates where we iteratively maximise the 
variational approximation to the likelihood. Each iteration of this algorithm is guar- 
anteed to increase a lower bound on, but will not necessarily maximise, the true 
likelihood. Nevertheless, we would hope that it will be a close approximation, the 
accuracy of which is investigated later. At each step in the algorithm, then, we: 
P(til X, i): cr(i)exp {(Ai - i)/2 q- (i)(A - /2)}, (5) 
(2ti - 1)(w'x + bi) and X(i) : {0.5 - a(i)}/2i. The parameters 
1. Obtain the sufficient statistics for the approximated posterior distribution 
of latent variables given each observation, fi(x,, 
2. Optimise the variational parameters i,, in order to make the approximation 
P(t,,) as close as possible to P(t,,) for all 
3. Update the model parameters wi and bi to increase (t). 
Probabilistic Visualisation of High-Dimensional Binary Data 595 
Jaakkola and Jordan (1997) give formulae for the above computations, but these 
do not include provision for the 'biases' bi, and so the necessary expressions are 
re-derived below. Note that although we have introduced N x d additional vari- 
ational parameters, it is no longer necessary to sample from p(x) and compute 
responsibilities, and no iterative logistic regression step is needed. 
Computing the Latent Posterior Statistics. From Bayes' rule, the posterior 
approximation (xnltn,n) is Gaussian with covariance and mean given by 
Cn = I -- 2 ,(in)WiW , 
i----1 
un = Cn k tin 2 +2A(in)bi 
i=1 
(6) 
wi}. (7) 
Optimising the Variational Parameters. Because P(t) > P(t), the variational 
approximation can be optimised by maximising (tn) with respect to each in. We 
use the EM methodology to obtain updates 
w(xnx)w + (xn) + (8) 
where the angle brackets (.) denote expectations with respect to i(xnltn,,C�ld] and 
where, from (6) and (7) earlier, the necessary posterior statistics are given by: 
(Xn) =/%, (9) 
(XnX T) = Cn +/%ttT. (10) 
Since (6) and (7) depend on the variational parameters, Cn and/% are computed 
followed by the update for each in from (8). Iteration of this two-stage process 
is guaranteed to improve monotonically the approximation of P(tn) and typically 
only two iterations are necessary for convergence. 
Optimising the Model Parameters.
