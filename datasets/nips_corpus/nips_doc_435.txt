A Simple Weight Decay Can Improve 
Generalization 
Anders Krogh* 
CONNECT, The Niels Bohr Institute 
Blegdamsvej 17 
DK-2100 Copenhagen, Denmark 
krogh@cse.ucsc.edu 
John A. Hertz 
Nordita 
Blegdamsvej 17 
DK-2100 Copenhagen, Denmark 
hertz@nordita.dk 
Abstract 
It has been observed in numerical simulations that a weight decay can im- 
prove generalization in a feed-forward neural network. This paper explains 
why. It is proven that a weight decay has two effects in a linear network. 
First, it suppresses any irrelevant components of the weight vector by 
choosing the smallest vector that solves the learning problem. Second, if 
the size is chosen right, a weight decay can suppress some of the effects of 
static noise on the targets, which improves generalization quite a lot. It 
is then shown how to extend these results to networks with hidden layers 
and non-linear units. Finally the theory is confirmed by some numerical 
simulations using the data from NetTalk. 
I INTRODUCTION 
Many recent studies have shown that the generalization ability of a neural network 
(or any other 'learning machine') depends on a balance between the information in 
the training examples and the complexity of the network, see for instance [1,2,3]. 
Bad generalization occurs if the information does not match the complexity, e.g. 
if the network is very complex and there is little information in the training set. 
In this last instance the network will be over-fitting the data, and the opposite 
situation corresponds to under-fitting. 
*Present address: Computer and Information Sciences, Univ. of California Santa Cruz, 
Santa Cruz, CA 95064. 
950 
A Simple Weight Decay Can Improve Generalization 951 
Often the number of free parameters, i.e. the number of weights and thresholds, is 
used as a measure of the network complexity, and algorithms have been developed, 
which minimizes the number of weights while still keeping the error on the training 
examples small [4,5,6]. This minimization of the number of free parameters is not 
always what is needed. 
A different way to constrain a network, and thus decrease its complexity, is to limit 
the growth of the weights through some kind of weight decay. It should prevent the 
weights from growing too large unless it is really necessary. It can be realized by 
adding a term to the cost function that penalizes large weights, 
I 
i 
where E0 is one's favorite error measure (usually the sum of squared errors), and 
X is a parameter governing how strongly large weights are penalized. w is a vector 
containing all free parameters of the network, it will be called the weight vector. If 
gradient descend is used for learning, the last term in the cost function leads to a 
new term -AWl in the weight update: 
OEo 
O) i C , W i . (2) 
Owi 
Here it is formulated in continuous time. If the gradient of E0 (the 'force term') 
were not present this equation would lead to an exponential decay of the weights. 
Obviously there are infinitely many possibilities for choosing other forms of the 
additional term in (1), but here we will concentrate on this simple form. 
It has been known for a long time that a weight decay of this form can improve 
generalization [7], but until now not very widely recognized. The aim of this paper 
is to analyze this effect both theoretically and experimentally. Weight decay as a 
special kind of regularization is also discussed in [8,9]. 
2 FEED-FORWARD NETWORKS 
A feed-forward neural network implements a function of the inputs that depends 
on the weight vector w, it is called fw. For simplicity it is assumed that there is 
only one output unit. When the input is  the output is fw (). Note that the input 
vector is a vector in the N-dimensional input space, whereas the weight vector is a 
vector in the weight space which has a different dimension 14/. 
The aim of the learning is not only to learn the examples, but to learn the underlying 
function that produces the targets for the learning process. First, we assume that 
this target function can actually be implemented by the network. This means there 
exists a weight vector u such that the target function is equal to f,,. The network 
with parameters u is often called the teacher, because from input vectors it can 
produce the right targets. The sum of squared errors is 
i -.[f,,(,,)_ f (,?,)]: (3) 
Zo(,-,,) = i , 
952 Krogh and Hertz 
where p is the number of training patterns. The learning equation (2) can then be 
written 
Ofw() _Awi. (4) 
Now the idea is to expand this around the solution u, but first the linear case will 
be analyzed in some detail. 
3 THE LINEAR PERCEPTRON 
The simplest kind of 'network' is the linear perceptron characterized by 
(5) 
where the N -x/2 is just a convenient normalization factor. Here the dimension of 
the weight space (W) is the same as the dimension of the input space (N). 
The learning equation then takes the simple form 
Defining 
and 
it becomes 
tO i O( E N-1 E [ttj - wjly -- Wi. 
(6) 
Vi  ti- Wi (7) 
% = (8) 
i) i O( -- E Aijvj q- (Iti - vi)' 
(9) 
Transforming this equation to the basis where A is diagonal yields 
b,. o(-(A,. + A)vr + Au,., (10) 
where Ar are the eigenvalues of A, and a subscript r indicates transformation to this 
basis. The generalization error is defined as the error averaged over the distribution 
of input vectors 
F 
-- ([fu() -- fw()]2) -- (N-l(y]vii)2) --- N -1 vivj(ij) 
i ij 
i 
(11) 
Here it is assumed that (ij)( -- 6ij. The generalization error F is thus proportional 
to Ivl 2, which is also quite natural. 
The eigenvalues of the covariance matrix A are non-negative, and its rank can easily 
be shown to be less than or equal to p. It is also easily seen that all eigenvectors 
belonging to eigenvalues larger than 0 lies in the subspace of weight space spanned 
A Simple Weight Decay Can Improve Generalization 953 
by the input patterns l,...,p. This subspace, called the pattern subspace, will 
be denoted Vp, and the orthogonal subspace is denoted by V. � When there are 
sufficiently many examples they span the whole space, and tee will be no zero 
eigenvalues. This can only happen for p >_ N. 
When X = 0 the solution to (10) inside Vp is just a simple exponential decay to 
vr = 0. Outside the pattern subspace Ar = 0, and the corresponding part of v will 
be constant. Any weight vector which has the same projection onto the pattern 
subspace as u gives a learning error 0. One can think of this as a 'valley' in the 
error surface given by u + l/p � . 
The training set contains no information that can help us choose between all these 
solutions to the learning problem. When learning with a weight decay A > 0, the 
constant part in V. � will decay to zero asymptotically (as e -At, where t is the time). 
p 
An infinitesimal weight decay will therefore choose the solution with the smallest 
norm out of all the solutions in the valley described above. This solution can be 
shown to be the optimal one on average. 
4 LEARNING WITH AN UNRELIABLE TEACHER 
Random errors made by the teacher can be modeled by adding a random term r/to 
the targets: 
f()  f,({) + r/. (12) 
The variance of r/is called er e, and it is assumed to have zero mean. Note that these 
targets are not exactly realizable by the network (for ( > 0), and therefore this is 
a simple model for studying learning of an unrealizable function. 
With this noise the learning equation (2) becomes 
i CK -(m-1 - vj + N-1/2.Y)- wi. (la) 
 J 
Transforming it to the bis where  is diagonal  before, 
  -(A + X)v + Xu - N -/' .. (14) 
The asymptotic solution to this equation is 
� Ur -- N-I/2Y'.i litter  
v,. = X + A,. 
(15) 
The contribution to the generalization error is the square of this summed over all 
r. If averaged over the noise (shown by the bar) it becomes for each r 
The last expression has a minimum in A, which can be found by putting the deriva- 
tive with respect to A equal to zero, >ptimal -- ff2/Itr 2' Remarkably it depends only 
954 
Krogh and Hertz 
Figure 1: Generalization error as a 
function of a = piN. The full line is 
for ,X = 0-2 = 0.2, and the dashed line 
for ,X = 0. The dotted line is the gener- 
alization error with no noise and ,X = 0. 
I 
, I 
/ 
o 1 2 
p/N 
on u and the variance of the noise, and not on A. If it is assumed that u is random 
(16) can be averaged over u. This yields an optimal A independent of r, 
0-2 
opt,mal-- , (17) 
where u 2 is the average of N-JuJ 2 
In this case the weight decay to some extent prevents the network from fitting the 
noise. 
From equation (14) one can see that the noise is projected onto the pattern subspace. 
Therefore the contribution to the generalization error from Vp -L is the same as before, 
and this contribution is on average minimized by a weight decay of any size. 
Equation (17) was derived in [10] in the context of a particular eigenvalue spectrum. 
Figure fig. i shows the dramatic improvement in generalization error when the 
optimal weight decay is used in this case. The present treatment shows that (17) 
is independent of the spectrum of A. 
We conclude that a weight decay has two positive effects on generalization in a 
linear network: 1) It suppresses any irrelevant components of the weight vector by 
choosing the smallest vector that solves the learning problem. 2) If the size is chosen 
right, it can suppress some of the effect of static noise on the targets. 
15 NON-LINEAR NETWORKS 
It is not possible to analyze a general non-linear network exactly, as done above 
for the linear case. By a local linearization, it is however, possible to draw some 
interesting conclusions from the results in the previous section. 
Assume the function is realizable, f = fu. Then learning corresponds to solving the 
p equations 
(18) 
A Simple Weight Decay Can Improve Generalization 955 
in W variables, where W is the number of weights. For p < W these equations 
define a manifold i
