Fool's Gold: Extracting Finite State Machines 
From Recurrent Network Dynamics 
John E Kolen 
Laboratory for Artificial Intelligence Research 
Department of Computer and Information Science 
The Ohio State University 
Columbus, OH 43210 
kolen-j @ cis.ohio-state.edu 
Abstract 
Several recurrent networks have been proposed as representations for the 
task of formal language learning. After training a recurrent network rec- 
ognize a formal language or predict the next symbol of a sequence, the 
next logical step is to understand the information processing carried out 
by the network. Some researchers have begun to extracting finite state 
machines from the internal state trajectories of their recurrent networks. 
This paper describes how sensitivity to initial conditions and discrete 
measurements can trick these extraction methods to return illusory finite 
state descriptions. 
INTRODUCTION 
Formal language learning (Gold, 1969) has been a topic of concern for cognitive science 
and artificial intelligence. It is the task of inducing a computational description of a formal 
language from a sequence of positive and negative examples of strings in the target lan- 
guage. Neural information processing approaches to this problem involve the use of recur- 
rent networks that embody the internal state mechanisms underlying automata models 
(Cleeremans et al., 1989; Elman, 1990; Pollack, 1991; Giles et al, 1992; Watrous & Kuhn, 
1992). Unlike traditional automata-based approaches, learning systems relying on recurrent 
networks have an additional burden: we are still unsure as to what these networks are 
doing. Some researchers have assumed that the networks are learning to simulate finite state 
501 
502 Kolen 
machines (FSMs) in their state dynamics and have begun to extract FSMs from the net- 
works' state transition dynamics (Cleeremans et al., 1989; Giles et al., 1992; Watrous & 
Kuhn, 1992). These extraction methods employ various clustering techniques to partition 
the internal state space of the recurrent network into a finite number of regions correspond- 
ing to the states of a finite state automaton. 
This assumption of finite state behavior is dangerous on two accounts. First, these extrac- 
tion techniques are based on a discretization of the state space which ignores the basic def- 
inition of information processing state. Second, discretization can give rise to incomplete 
computational explanations of systems operating over a continuous state space. 
SENSITIVITY TO INITIAL CONDITIONS 
In this section, I will demonstrate how sensitivity to initial conditions can confuse an FSM 
extraction system. The basis of this claim rests upon the definition of information processing 
state. Information processing (IP) state is the foundation underlying automata theory. Two 
IP states are the same if and only if they generate the same output responses for all possible 
future inputs (Hopcroft & Ullman, 1979). This definition is the fulcrum for many proofs and 
techniques, including finite state machine minimization. Any FSM extraction technique 
should embrace this definition, in fact it grounds the standard FSM minimization methods 
and the physical system modelling of Crutchfield and Young (Crutchfield & Young, 1989). 
Some dynamical systems exhibit exponential divergence for nearby state vectors, yet 
remain confined within an attractor. This is known as sensitivity to initial conditions. If this 
divergent behavior is quantized, it appears as nondeterministic symbol sequences (Crutch- 
field & Young, 1989) even though the underlying dynamical system is completely deter- 
ministic (Figure 1). 
Consider a recurrent network with one output and three recurrent state units. The output 
unit performs a threshold at zero activation for state unit one. That is, when the activation 
of the first state unit of the current state is less than zero then the output is A. Otherwise, 
the output is B. Equation 1 presents a mathematical description. S(t) is the current state of 
the system O (t) is the current output. 
S(t+l) = tanh( 0 2 . t ) 
0 2 
A Sl(t ) < 0 
O(t) = B S l(t)>0 
(1) 
Figure 2 illustrates what happens when you run this network for many iterations. The point 
in the upper left hand state space is actually a thousand individual points all within a ball 
of radius 0.01. In one iteration these points migrate down to the lower corner of the state 
space. Notice that the ball has elongated along one dimension. After ten iterations the orig- 
inal ball shape is no longer visible. After seventeen, the points are beginning to spread 
along a two dimensional sheet within state space. And by fifty iterations, we see the net- 
work reaching the its full extent of in state space. This behavior is known as sensitivity to 
initial conditions and is one of three conditions which have been used to characterize cha- 
otic dynamical systems (Devaney, 1989). In short, sensitivity to initial conditions implies 
Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics 503 
x6-4x(1-x) 
O(x) = { A x<0.5 B 
B x>0.5 
A 
x 6- 2x mod 1 
1 
A x<� 
1 2 
s �<x<� 
C 2 
--<32 
3 Ai C 
x 6- 3.68x ( 1 - x) 
O(x) = { A x<0.5 
B x>0.5 
Figure 1: Examples of deterministic dynamical systems whose discretize trajectories 
appear nondeterministic. 
that any epsilon ball on the attractor of the dynamical will exponentially diverge, yet still 
be contained within the locus of the attractor. The rate of this divergence is illustrated in 
Figure 3 where the maximum distance between two points is plotted with respect to the 
number of iterations. Note the exponential growth before saturation. Saturation occurs as 
the point cloud envelops the attractor. 
No matter how small one partitions the state space, sensitivity to initial conditions will 
eventually force the extracted state to split into multiple trajectories independent of the 
future input sequence. This is characteristic of a nondeterministic state transition. Unfortu- 
nately, it is very difficult, and probably intractable, to differentiate between a nondetermin- 
istic system with a small number of states or a deterministic with large number of states. In 
certain cases, however, it is possible to analytically ascertain this distinction (Crutchfield & 
Young, 1989). 
THE OBSERVERS' PARADOX 
One response to this problem is to evoke more computationally complex models such as 
push-down or linear-bounded automata. Unfortunately, the act of quantization can actually 
introduce both complexion and complexity in the resulting symbol sequence. Pollack and 
I have focused on a well-hidden problems with the symbol system approach to understand- 
ing the computational powers of physical systems. This work (Kolen & Pollack, 1993; 
504 Kolen 
1 1 1 
output=A 1 output=B 1 output=A 1 
Start (e<0.01) 1 iteration 10 iterations 
1 1 1 
output=A,B 1 output=A,B 1 output=A,B 1 
17 iterations 25 iterations 50 iterations 
Figure 2: The state space of a recurrent network whose next state transitions are 
sensitive to initial conditions. The initial epsilon ball contains 1000 points. These points 
first straddle the output decision boundary at iteration seven. 
Kolen & Pollack, In press) demonstrated that computational complexity, in terms of Chom- 
sky's hierarchy of formal languages (Chomsky, 1957; Chomsky, 1965) and Newell and 
Simon's physical symbol systems (Newell & Simon, 1976), is not intrinsic to physical sys- 
tems. The demonstration below shows how apparently trivial changes in the partitioning of 
state space can produce symbol sequences from varying complexity classes. 
Consider a point moving in a circular orbit with a fixed rotational velocity, such as the end 
of a rotating rod spinning around a fixed center, or imagine watching a white dot on a spin- 
ning bicycle wheel. We measure the location of the dot by periodically sampling the loca- 
tion with a single decision boundary (Figure 4, left side). If the point is to the left of 
boundary at the time of the sample, we write down an 17'. Likewise, we write down an ' 
when the point is on the other side. (The probability of the point landing on the boundary 
is zero and can arbitrarily be assigned to either category without affecting the results 
below.) In the limit, we will have recorded an infinite sequence of symbols containing long 
sequences of s and l.'s. 
The specific ordering of symbols observed in a long sequence of multiple rotations is 
Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics 505 
2.5 
2 
10 20 30 40 5 
Iteration number 
Figure 3: Spread of initial points across the attractor as measured by maximum distance. 
1 
1 r 
Figure 4: On the left, two decision regions which induce a context free language. 0 is 
the current angle of rotation. At the time of sampling, if the point is to the left (right) of 
the dividing line, an 1 (r) is generated. On the right, three decision regions which 
induce a context sensitive language. 
dependent upon the initial rotational angle of the system. However, the sequence does pos- 
sess a number of recurring structural regularities, which we call sentences: a run of r's fol- 
lowed by a run of l's. For a fixed rotational velocity (rotations per time unit) and sampling 
rate, the observed system will generate sentences of the form rnl m (n, m > 0). (The notation 
r n indicates a sequence of n r's.) For a fixed sampling rate, each rotational velocity spec- 
ifies up to three sentences whose number of r's and l's differ by at most one. These sen- 
tences repeat in an arbitrary manner. Thus, a typical subsequence of a rotator which 
produces sentences rnl n, rnln+l,rn+11 n would look like 
506 Kolen 
rnln+lrnlnrnln+lrn+llnrnlnrnln+l ' 
A language of sentences may be constructed by examining the families of sentences gener- 
ated by a large collection of individuals, much like a natural language is induced from the 
abilities of its individual speak
