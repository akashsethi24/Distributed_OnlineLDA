Estimating Average-Case Learning Curves 
Using Bayesian, Statistical Physics and 
�C Dimension Methods 
David Haussler 
University of California 
Santa Cruz, California 
Michael Kearns* 
AT&T Bell Laboratories 
Murray Hill, New Jersey 
Manfred Opper 
Institut fiir Theoretische Physik 
Universitht Giessen, Germany 
Robert Schapire 
AT&T Bell Laboratories 
Murray Hill, New Jersey 
Abstract 
In this paper we investigate an average-case model of concept learning, and 
give results that place the popular statistical physics and VC dimension 
theories of learning curve behavior in a common framework. 
I INTRODUCTION 
In this paper we study a simple concept learning model in which the learner attempts 
to infer an unknown target concept f, chosen from a known concept class ' of {0, 1}- 
valued functions over an input space X. At each trial i, the learner is given a point 
xi  X and asked to predict the value of f(xi). If the learner predicts f(xi) 
incorrectly, we say the learner makes a mistake. After making its prediction, the 
learner is told the correct value. 
This simple theoretical paradigm applies to many areas of machine learning, includ- 
ing much of the research in neural networks. The quantity of fundamental interest 
in this setting is the learning curve, which is the function of rn defined as the prob- 
*Contact author. Address: AT&T Bell Laboratories, 600 Mountain Avenue, Room 
2A-423, Murray Hill, New Jersey 07974. Electronic mail: mkearns@research.att.com. 
855 
856 Haussler, Kearns, Opper, and Schapire 
ability the learning algorithm makes a mistake predicting f(Xm+l ), having already 
seen the examples (Xl, f(xl)), ..., (xm, f(xm)). 
In this paper we study learning curves in an average-case setting that admits a prior 
distribution over the concepts in '. We examine learning curve behavior for the 
optimal Bayes algorithm and for the related Gibbs algorithm that has been studied 
in statistical physics analyses of learning curve behavior. For both algorithms we 
give new upper and lower bounds on the learning curve in terms of the Shannon 
information gain. 
The main contribution of this research is in showing that the average-case or 
Bayesian model provides a unifying framework for the popular statistical physics 
and VC dimension theories of learning curves. By beginning in an average-case set- 
ting and deriving bounds in information-theoretic terms, we can gradually recover 
a worst-case theory by removing the averaging in favor of combinatorial parameters 
that upper bound certain expectations. 
Due to space limitations, the paper is technically dense and almost all derivations 
and proofs have been omitted. We strongly encourage the reader to refer to our 
longer and more complete versions [4, 6] for additional motivation and technical 
detail. 
2 NOTATIONAL CONVENTIONS 
Let X be a set called the instance space. A concept class :1: over X is a (possibly 
infinite) collection of subsets of X. We will find it convenient to view a concept 
f  ' as a function f: X - {0, 1}, where we interpret f(x) = 1 to mean that 
x 6 X is a positive example of f, and f(x) = 0 to mean x is a negative example. 
The symbols P and 73 are used to denote probability distributions. The distribution 
 is over ', and 73 is over X. When ' and X are countable we assume that these 
distributions are defined as probability mass functions. For uncountable ' and X 
they are assumed to be probability measures over some appropriate a-algebra. All 
of our results hold for both countable and uncountable ' and X. 
We use the notation Ele, [x(f)] for the expectation of the random variable X with 
respect to the distribution P, and Prle[cond(f)] for the probability with respect 
to the distribution P of the set of all f satisfying the predicate cond(f). Everything 
that needs to be measurable is assumed to be measurable. 
3 INFORMATION GAIN AND LEARNING 
Let . be a concept class over the instance space X. Fix a target concept f  :1: and 
an infinite sequence of instances x = Xl, ..., x,, x,+l, � � � with x,  X for all m. 
For now we assume that the fixed instance sequence x is known in advance to the 
learner, but that the target concept f is not. Let P be a probability distribution 
over the concept class '. We think of P in the Bayesian sense as representing the 
prior beliefs of the learner about which target concept it will be learning. 
In our setting, the learner receives information about f incrementally via the label 
Estimating Average-Case Learning Curves 857 
sequence f(x),...,f(x,),f(z,+), .... At time m, the learner receives the label 
j(x.). For any m >_ 1 we define (with respect to x, j) the ruth version space 
(,,,f) = {]  . ]()= f(),...,]()= f()} 
and the ruth volume V(x,f) = P[(x,f)]. We define 0(x,f) =  for all 
x and f, so Vf(x,f) = 1. The version space at time m is simply the cls of 
all concepts in W consistent with the first m labels of f (with respect to x), and 
the ruth volume is the meure of this cls under . For the first part of the 
paper, the infinite instance sequence x and the prior  are fixed; thus we simply 
write W(f) and V(f). Later, when the sequence x is chosen randomly, we will 
reintroduce this dependence explicitly. We adopt this notational practice of omitting 
any dependence on a fixed x in many other places  well. 
For each m  0 let us define the ruth posterior distribution (x, f) =  by 
restricting  to the ruth version space W(f); that is, for all (meurable) S C W, 
v[ = v[  ()]/v[(/)] = v[  (f)]/v(D. 
Having already seen f(x),..., f(x), how much information (suming the prior 
) does the learner expect to gain by seeing f(x+)? If we let +(x,f) (ab- 
breviated +(f) since x is fixed for now) be a random variable whose value is 
the (Shannon) information gained from /(x+i), then it can be shown that the 
expected information is 
rI[+i(f)] = rI -log  = rI[-logx+i(f)] (1) 
where we define the (m + 1)st volume ratio by  (x,f) X+i(f) - 
+1 ()/(). 
We now return to our learning problem, which we define to be that of predicting the 
label f(x+x) given only the previous labels f(xi),...,f(x). The first learning 
algorithm we consider is called the Bayes optimal classification algorithm, or the 
Bayes algorithm for short. For any m and b  {0, 1), define f(x, f) = W(f) = 
{f  W(x, f)' f(x+i)= b). Then the Bayes algorithm is: 
If 7.[1'1(f)] > 
If 7.[]'(f)] < 
If 7. []'l(f)] = 
(f)], predict f(x.+ ) = 1. 
(f)], predict f(Xm+l) -- 0. 
(f)], flip a fair coin to predict f(x.+l). 
It is well known that if the target concept f is drawn at random according to 
the prior distribution P, then the Bayes algorithm is optimal in the sense that it 
minimizes the probability that f(x,+l) is predicted incorrectly. Furthermore, if we 
let Bayes+ (x, f) (abbreviated Bayes+l (f) since x is fixed for now) be a random 
variable whose value is 1 if the Bayes algorithm predicts f(x,+i) correctly and 0 
otherwise, then it can be shown that the probability of a mistake for a random f is 
E.t[Bayesm+i(f)] = E.t [O ( -- Xm+i(f))] . (2) 
Despite the optimality of the Bayes algorithm, it suffers the drawback that its 
hypothesis at any time m may not be a member of the target class '. (Here we 
858 Haussler, Kearns, Opper, and Schapire 
define the hypothesis of an algorithm at time m to be the (possibly probabilistic) 
mapping f' X - {0, 1) obtained by letting ](x) be the prediction of the algorithm 
when x,,+ -- x.) This drawback is absent in our second learning algorithm, which 
we call the Gibbs algorithm [6]: 
Given f(xx),..., f(x,,), choose a hypothesis concept ] randomly from 
Given Xm+l, predict f(x,+x)= /(m+l). 
The Gibbs algorithm is the zero-temperature limit of the learning algorithm stud- 
ied in several recent papers [2, 3, 8, 9]. If we let Gibbs+i(x,f ) (abbreviated 
Gibbs+ x (f) since x is fixed for now) be a random variable whose value is 1 if the 
Gibbs algorithm predicts f(x,+) correctly and 0 otherwise, then it can be shown 
that the probability of a mistake for a random f is 
El'[Gibbs+l(f)] = Ele'[1 - Xm+(f)]. 
(3) 
Note that by the definition of the Gibbs algorithm, Equation (3) is exactly the 
average probability of mistake of a consistent hypothesis, using the distribution on 
' defined by the prior. Thus bounds on this expectation provide an interesting 
contrast to those obtained via VC dimension analysis, which always gives bounds 
on the probability of mistake of the worst consistent hypothesis. 
4 THE MAIN INEQUALITY 
In this section we state one of our main results: a chain of inequalities that upper 
and lower bounds the expected error for both the Bayes and Gibbs .algorithms 
by simple functions of the expected information gain. More precisely, using the 
characterizations of the expectations in terms of the volume ratio Xm+l (f) given 
by Equations (1), (2) and (3), we can prove the following, which we refer to as the 
main inequality: 
_ E.t,[Bayesm+l(f)] 
_< E.t,[Gibbsm+l(f)] 
1 
_  E.f7:' [m+l(f)]. (4) 
Here we have defined an inverse to the binary entropy function 7/(p) = -p log p- 
(1 - p)log(1 -p) by letting -i(q), for q  [0, 1], be the unique p 6 [0,1/2] such 
that 7-/(p) = q. Note that the bounds given depend on properties of the particular 
prior P, and on properties of the particular fixed sequence x. These upper and 
lower bounds are equal (and therefore tight) at both extremes Ele,[27,,+(f)] = 1 
(maximal information gain) and Ele,[27,,+(f)] = 0 (minimal information gain). 
To obtain a weaker but perhaps more convenient lower bound, it can also be shown 
that there is a constant co > 0 such that for all p > 0, -i(p) _ cop/log(2/p). 
Finally, if all that is wanted is a direct comparison of the performances of the Gibbs 
and Bayes algorithms, we can also show: 
El7v[Bayes,+l(f)] _< El7v[Gibbs,+(f)] _< 2El6,[Bayes,+(f)]. (5) 
Estima
