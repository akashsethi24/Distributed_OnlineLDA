Discovering hidden features with 
Gaussian processes regression 
Francesco Vivarelli 
Centro Ricerche Ambientali 
Montecatini, 
via Ciro Menotti, 48 
48023 Marina di Ravenna 
Italy 
fvivarellicramont. it 
Christopher K. I. Williams 
Division of Informatics 
The University of Edinburgh, 
5 Forrest Hill, 
Edinburgh, EH1 2QL 
United Kingdom 
ckiwdai. ed. ac. uk 
Abstract 
In Gaussian process regression the covariance between the outputs 
at input locations x and x' is usually assumed to depend on the 
distance (x - x') T W (x - x'), where W is a positive definite mat- 
rix. W is often taken to be diagonal, but if we allow W to be a 
general positive definite matrix which can be tuned on the basis of 
training data, then an eigen-analysis of W shows that we are ef- 
fectively creating hidden features, where the dimensionality of the 
hidden-feature space is determined by the data. We demonstrate 
the superiority of predictions using the general matrix over those 
based on a diagonal matrix on two test problems. 
I Introduction 
Over the last few years Bayesian approaches to prediction with neural networks have 
come to the fore. Following an argument in Neal (1996) concerning the equivalence 
between infinite neural networks and certain Gaussian processes, Gaussian process 
(GP) prediction has also become popular, and Rasmussen (1996) has demonstrated 
good performance of GP predictors on a number of tasks. 
In Gaussian process prediction as applied by Rasmussen (1996), Williams and 
Rasmussen (1996) and others, the covariance between the outputs at locations x 
and x' is usually assumed to depend on the distance (x - x')TW(x -- x'), where 
W is a positive definite, diagonal matrix. This means that different dimensions in 
the input space can have different relevances to the prediction problem (c.f. MacKay 
and Neal's idea of Automatic Relevance Determination (Neal, 1996)). However, 
some of the reasoning about the success of neural networks and methods such as 
projection pursuit regression suggests that discovering relevant directions in feature 
space is important; clearly the ARD model is a special case, where these directions 
614 F.. 4varelli and C. K. I. Williams 
are parallel to the axes in the input feature space. In this paper we allow W to be a 
general positive semidefinite matrix (defining a Mahalanobis distance in the input 
space), thereby allowing general directions in the input space to be selected. We 
then compare the performance of GP predictors using the diagonal and full distance 
matrices on some regression problems. 
The structure of the paper is as follows. GPs for regression are introduced in Section 
2, where we also explain the rSle played by the distance matrix W and the criterion 
used to compare the generalisation performances of the diagonal and the general 
distance matrices. The two methods have been compared on two regression tasks 
and the results of our experiments are shown in Section 3. A summary of the work 
done and some open questions are presented in Section 4. 
2 Gaussian processes and prediction 
In this paper we use Gaussian process models as predictors. Consider a stochastic 
process Y (x), with the input observable x belonging to some input space X C_ I e. 
Gaussian processes are a subset of stochastic processes that can be defined by 
specifying the mean and covariance functions, /(x) = �[Y (x)] and Cp (x,x ) - 
�[� (x)� (x)] respectively. For the work below we shall set / (x) -- 0. Al- 
though the GP formulation provides a prior over functions, for our purposes it 
suffices to note that the y-values � (xl), � (x2),..., � (x n) corresponding to x- 
values x 1, x2,..., x n have a multivariate Gaussian distribution A; (0, Kp), where 
(Kp)ij = Cp (xi, xJ). The specific form of the covariance function that we shall use 
is 
2exp[ 1 (x -- x')T ] 
(x,x') = % -5 w (x - x') . 
(1) 
When W is a diagonal matrix the entry wii is the inverse of the squared correla- 
tion length-scale of the process along the direction i. In particular, we note that 
this model is closely related to the Automatic Relevance Determination method of 
MacKay and Neal (Neal, 1996), as a small lengthscale along a certain direction of 
the space highlights the relevance of the corresponding input feature (assuming that 
the inputs are normalised). 
For the prediction problem, let us suppose to have n data points D, - 
{ (xl,tl), (x,t),..., (x,t ) }, where t i is the output-value corresponding to the 
input x i. The t's are assumed to be generated from the true y-values by adding 
 Given the assumption of a Gaussian process prior 
Gaussian noise of variance 
over functions, it is a standard result (e.g. Whittle, 1963) that the predictive dis- 
tribution p(t]x,Z)n)corresponding to a new input is Af ()(x),c 2 (x)), with mean 
and variance 
(x) = k T (x) K-t 
2 _ k T 
cy 2 (x) - Cp (x,x) 4- cy v (x) K-lk (x), (3) 
where K = Kp + a2I, k T (x) = (Cp (x,x ) ,Cp (x, x2),... ,Cp (x,x)) and t  = 
(,l,t2,...t n) . 
This method of prediction assumes that the process y (x) we are modelling is really 
a function of the observable x. However it is often the case that for real world 
problems the y is actually a function of a set of hidden features z  Z C q which 
arise from a combination of the manifest variables x. In particular we wish to study 
the problem in which the hidden features are a linear combination of the observable 
Discovering Hidden Features with Gaussian Processes Regression 615 
coordinates through a q x d matrix M, where q < d (i.e.z = Mx). In this case, 
the covariance of the function y is specified by Equation I but turns out to depend 
upon the estimation of the distance between hidden features (z- z')T (z- z'). 
Sincez=Mx,(z-z ')=M(x-x ) andW=MTM. 
A GP model depends on the parameters which describe the covariance function 
2 and the elements of W). The training of a GP can be carried out by 
(i.e.a, crv 
either estimating the parameters of the covariance function (for example, using the 
maximum likelihood method) or using a Bayesian approach and sampling from the 
posterior distribution over the parameters (Williams and Rasmussen, 1996). We 
follow the first approach, maximising the logarithm of the likelihood 
/2 = logp (D, I0 ) = -- 
1 log det K - -tTK-it -- n 
2  log 2r (4) 
where K -1 depends upon 0, the vector of parameters of the covariance function. 
The number of free parameters depends on the number of non-zero elements of the 
matrix W. Usually, W is chosen to be diagonal and the number of free parameters 
2 and cry,) We notice that this parametrisation 
is d + 2 (the d diagonal elements, ap . 
of W allows the discovery of relevant directions in the observed space; it does not 
lead to an estimation of a general mapping of X onto the feature space Z as the 
relevant directions are parallel to the axes in the input manifest space. 
If q is not known in advance, it is preferable to use a general symmetric posit- 
ive semidefinite matrix W. A parametrisation of such a matrix follows from the 
Choleski decomposition as W = uTu, where U is an upper triangular matrix with 
positive entries on the diagonal (Williams, 1996). Hence the factorisation of U turns 
out to be 
exp [Ul,1] Ul,2 ... t/1,d 
U = 0 exp [u2,2] ... u2, (5) 
0 0 ... ua,a ' 
......... exp 
The elements on the diagonal are positive because of the exponential. Being sym- 
metric, W has at most d (d + 1)/2 independent entries and thus the total number 
of free parameters of the GP model is 2 + d (d + 1)/2. 
We note that such a full distance matrix IV allows an estimation of the matrix M 
from an eigenvalue decomposition of W = VAV T, where A is a diagonal matrix of 
the eigenvalues of W and V is the matrix of the eigenvectors. The dimension of the 
hidden feature space Z can be inferred by the number of relevant eigenvalues of the 
matrix A (which are the inverse of the squared correlation lengths of the process 
along the directions of the hidden space). The directions of the hidden feature 
space are defined by the eigenvectors corresponding to the relevant eigenvalues; 
in particular the matrix composed by these eigenvectors gives an estimate of the 
mapping from X to Z. In the following the diagonal and the general full correlation 
matrices are designated by Wd and 
It is important to observe that the predictor obtained using Wf is not equivalent to 
an additive model (Hastie and Tibshirani, 1990), as the predictor is a multivariate 
function of z rather than being an additive function of the components of z. How- 
ever, it would be possible to produce an additive function in the GP context, using a 
covariance function which is the sum of one-dimensional covariance functions based 
on projections of x. 
616 E I4varelli and C. K. I. Williams 
2.1 Generalisation error 
Consider predicting the value of a function y(x) with a predictor (x). A commonly- 
used measure of the generalisation error given a dataset Dn is the average squared 
error 
E g (T)n) -- / (y(x) - v (x))2P (x) dx. 
(6) 
The average generalisation error E g (n) for a dataset of size n is obtained by aver- 
aging over the choice of training dataset, i.e. E g (n) = �v [Eg (/)n)]. Eg (/)n) can 
sometimes be evaluated analytically or by numerical integration, but it is usually 
necessary to use samples to perform the average over training datasets 
In order to investigate the generalisation capabilities of GPs using a diagonal and 
full distance matrices Wd and Wf, we trained the GP predictors on some regression 
tasks. The generalisation errors are compared by looking at the relative error 
(7) 
where E (/)) and E (/)) are the generalisation errors reported using a diagonal 
and a full distance matrix respectively. This ratio allow us to perform a fair compar- 
ison between the pairwise differences of the generalisation errors for each dataset 
and the actual value E (/)). The expected value p (n) is the average over th
