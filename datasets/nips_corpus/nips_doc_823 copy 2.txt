Bayesian Modeling and Classification of 
Neural Signals 
Michael S. Lewicki 
Computation and Neural Systems Program 
California Institute of Technology 216-76 
Pasadena, CA 91125 
leickicns.caltech.edu 
Abstract 
Signal processing and classification algorithms often have limited 
applicability resulting from an inaccurate model of the signal's un- 
derlying structure. We present here an efficient, Bayesian algo- 
rithm for modeling a signal composed of the superposition of brief, 
Poisson-distributed functions. This methodology is applied to the 
specific problem of modeling and classifying extracellular neural 
waveforms which are composed of a superposition of an unknown 
number of action potentials (APs). Previous approaches have had 
limited success due largely to the problems of determining the spike 
shapes, deciding how many are shapes distinct, and decomposing 
overlapping APs. A Bayesian solution to each of these problems is 
obtained by inferring a probabilistic model of the waveform. This 
approach quantifies the uncertainty of the form and number of the 
inferred AP shapes and is used to obtain an efficient method for 
decomposing complex overlaps. This algorithm can extract many 
times more information than previous methods and facilitates the 
extracellular investigation of neuronal classes and of interactions 
within neuronal circuits. 
590 
Bayesian Modeling and Classification of Neural Signals 591 
1 INTRODUCTION 
Extracellular electrodes typically record the activity of several neurons in the vicin- 
ity of the electrode tip (figure 1). Most electrophysiological data is collected by 
isolating action potentials (APs) from a single neuron by using a level detector or 
window discriminator. Methods for extracting APs from multiple neurons can, in 
addition to the obvious advantage of providing more data, provide the means to 
investigate local hentonal interactions and response properties of neuronal popula- 
tions. Determining from the voltage waveform what cell fired when is a difficult, 
ill-posed problem which is compounded by the fact that cells frequently fire simul- 
taneously resulting in large variations in the observed shapes. 
There are three major difficulties in identifying and classifying action potentials 
(APs) in a neuron waveform. The first is determining the AP shapes, the second is 
deciding the number of distinct shapes, and the third is decomposing overlapping 
spikes into their component parts. In general, these problems cannot be solved 
independently, since the solution of one will affect the solution of the others. 
Figure 1: Each neuron generates a stereotyped action potential (AP) which is observed 
through the electrode as a voltage fluctuation. This shape is primarily a function of 
the position of a neuron relative to the tip. The extracellular waveform shows several 
different APs generated by an unknown number of neurons. Note the frequent presence of 
overlapping APs which can completely obscure individual spikes. 
The approach summarized here is to model the waveform directly to obtain a prob- 
abilistic description of each action potential and, in turn, of the whole waveform. 
This method allows us to compute the class conditional probabilities of each AP. 
In addition, it is possible to quantify the certainty of both the form and number of 
spike shapes. Finally, we can use this description to decompose overlapping APs 
efficiently and assign probabilities to alternative spike sequences. 
2 MODELING SINGLE ACTION POTENTIALS 
The data from the event observed (at time zero) is modeled as resulting from 
fixed underlying spike function, s(t), plus noise: 
di -s(ti ;v)+ r/i, (1) 
592 Lewicki 
where v is the parameter vector that defines the spike function. The noise, r/, is 
modeled as Gaussian with zero mean and standard deviation 
From the Bayesian perspective, the task is to infer the posterior distribution of the 
spike function parameters (assuming, for the moment, that rr. and rrw are known): 
(2) 
The two terms specifying the posterior distribution of v are 1) the probability of 
the data given the model: 
P(DIv' a M) - ZD(a.) exp  2a2 y.(di - s(ti)) 2 , (3) 
i=1 
and 2) the prior assumptions of the structure of s(t) which are assumed to be of 
the form: 
P(vlrrw, M) orexp[-/dus(m)(u)'/r2]. (4) 
The superscript (m) denotes differentiation which for these demonstrations we as- 
sumed to be m = i corresponding to linear splines. The smoothness of s(t) is 
controlled through rrw with small values of rrw penalizing large fluctuations. 
The final step in determining the posterior distribution is to eliminate the depen- 
dence of P(vID , rr. aw, M) on rr.and aw. Here, we use the approximation: 
(s) 
The most probable values of rr.and aw were obtained using the methods of MacKay 
(1992) in which reestimation formulas are obtained from a Gaussian approximation 
of the posterior distribution for r.and aw, P(rr., rrwlD, M). Correct inference of aw 
prevents the spike function from overfitting the data. 
3 MODELING MULTIPLE ACTION POTENTIALS 
When a waveform contains multiple types of APs, determining the component spike 
shapes is more difficult because the classes are not known a priori. The uncertainty 
of which class an event belongs to can be incorporated with a mixture distribution. 
The probability of a particular event, Dn, given all spike models, M:K, is 
K 
P(D.Iv:K, = P(D. Ivy, (6) 
k--1 
where r} is the a priori probability that a spike will be an instance of M}, and 
As before, the objective is to determine the posterior distribution for the parameters 
defining a set of spike models, P(v:,{, .rlD:N , rr., O'w, M:) which is obtained again 
using Bayes' rule. 
Bayesian Modeling and Classification of Neural Signals 593 
Finding the conditions satisfied at a posterior maximum leads to the equation: 
N 
E P(M ID,, v, r, %)? . [d, -  (t, - v, 'v)] 0 (t,, v) 
n=l ' ' OqVk 
=0, (7) 
where rn is the inferred occurrence time (typically to sub-sample period accuracy) of 
the event D. This equation is solved iteratively to obtain the most probable values 
of v:K. Note that the error for each event, Dn, is weighted by P(M [Dn, v, ,r, %) 
which is the probability that the event is an instance of the kth spike model. This is 
a soft clustering procedure, since the events are not explicitly assigned to particular 
classes. Maximizing the posterior yields accurate estimates of the spike functions 
even when the clusters are highly overlapping. 
The techniques described in the previous section are used to determine the most 
probable values for % and trw and, in turn, the most probable values of v:K and 
4 DETERMINING THE NUMBER OF SPIKE SHAPES 
Choosing a set of spike models that best fit the data, would result eventually in a 
model for each event in the waveform. Heuristics might indicate whether two spike 
models are identical or distinct, but ad hoc criteria are notoriously dependent on 
particular circumstances, and it is difficult to state precisely what information the 
rules take into account. 
To determine the most probable number of spike models, we apply probability theory. 
Let Sj = {M? )} denote a set of spike models and H denote information known 
a priori. The probability of Sj, conditioned only on H and the data, is obtained 
using Bayes' rule: 
:N, ) 
(8) 
The only data-dependent term is P(D:,ISj,H ) which is the evidence for Sj 
(MacKay, 1992). With the assumption that all hypotheses S:j are equally probable 
a priori, P(D:,[Sj, H) ranks alternative spike sets in terms of their probability. 
The evidence term P(D:,ISi, H) is convenient because it is the normalizing con- 
stant for the posterior distribution of the parameters defining the spike set. Al- 
though calculation of P(D:NISj,H) is analytically intractable, it is often well- 
approximated with a Gaussian integral which was the approximation used for these 
demonstrations. 
A convenient way of collapsing the spike set is to compare spike models pairwise. 
Two models in the spike set are selected along with a sampled set of events fit by 
each model. We then evaluate P(DISx ) and P(DIS. ). S is the hypothesis that 
the data is modeled by a single spike shape, S. says there are two spike shapes. If 
P(DI$) > P(D]$2), we replace both models in S. by the one in Sx. The procedure 
terminates when no more pairs can be combined to increase the evidence. 
594 Lewicki 
5 DECOMPOSING OVERLAPPING SPIKES 
Overlaps must be decomposed into their colnponent spikes for accurate inference 
of the spike functions and accurate classification of the events. Determining the 
best-fitting decomposition is difficult because of the enormous number of possible 
spike sequences, not only all possible model combinations for each event but also 
all possible event times. 
A brute-force approach to this problem is to perform an exhaustive search of the 
space of overlapping spike functions and event times to find the sequence with 
maximum probability. This approach was used by Atiya (1992) in the case of two 
overlapping spikes with the times optimized to one sample period. Unfortunately, 
this is often computationally too demanding even for off-line analysis. 
We make this search efficient utilizing dynamic programming and k-dimensional 
trees (Friedman et al., 1977). Once the best-fitting decomposition can be obtained, 
however, it may not be optimal, since adding more spike shapes can overfit the 
data. This problem is minimized by evaluating the probability for alternative de- 
compositions to determine the most probable spike sequence (figure 2). 
Figure 2: Many spike function sequences can account for the same region of data. The 
thick lines show the data, thin lines show individual spike functions. In this case, the best- 
fitting overlap solution is not the most probable: the sequence with 4 spike functions is 
more than 8 times more probable than the other solutions, e
