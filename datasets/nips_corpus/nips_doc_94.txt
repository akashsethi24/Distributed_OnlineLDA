57 
Self Organizing Neural Networks for the 
Identification Problem 
Manoel Fernando Tenorio 
School of Electrical Engineering 
Purdue University 
W. Lafayette, IN. 47907 
tenorio@ ee.ecn.purdue.edu 
Wei-Tsih Lee 
School of Electrical Engineering 
Purdue University 
W. Lafayette, IN. 47907 
lwl@ ed.ecn.purdue.edu 
ABSTRACT 
This work introduces a new method called Self Organizing 
Neural Network (SONN) algorithm and demonstrates its use in a 
system identification task. The algorithm constructs the network, 
chooses the neuron functions, and adjusts the weights. It is compared to 
the Back-Propagation algorithm in the identification of the chaotic time 
series. The results shows that SONN constructs a simpler, more 
accurate model, requiring less training data and epochs. The algorithm 
can be applied and generalized to appilications as a classifier. 
I. INTRODUCTION 
1.1 THE SYSTEM IDENTIFICATION PROBLEM 
In various engineering applications, it is important to be able to estimate, interpolate, 
and extrapolate the behavior of an unknown system when only its input-output pairs are 
available. Algorithms which produce an estimation of the system behavior based on these 
pairs fall under the category of system identification techniques. 
1.2 SYSTEM IDENTIFICATION USING NEURAL 
NETWORKS 
A general form to represent systems, both linear and nonlinear, is the Kolmogorov- 
Garbor polynomial [Garbor, 1961 ] shown below: 
y = a0 + Z aixi + X E aijxixj +' 
i i j (1) 
58 Tenorio and Lee 
where the y is the output, and x the input to the system. [Garbor ,1961] proposed a 
learning method that adjuswat the coefficient of (1) by minimizing the mean square error 
between each desired output sample and the actual output. 
This paper describes a supervised learning algorithm for structure construction and 
adjustment. Here, systems which can be described by (1) are presented. The computation 
of the function for each neuron performs a choice from a set of possible functions 
previously assigned to the algorithm, and it is general enough to accept a wide range of 
both continuous and discrete functions. In this work, the set is taken from variants of the 
2-input quadratic polynomial for simplicity, although there is no requirement making it 
so. This approach abandons the simplistic mean-square error for performance measure in 
favor of a modified Minimum Description Length (MDL) criterion [Rissanen,1975], with 
provisions to measure the complexity of the model generated. The algorithm searches for 
the simplest model which generates the best estimate. The modified MDL, from hereon 
named the Structure Estimation Criterion (SEC), is applied hierarchically in the selection 
of the optimal neuron transfer function from the function set, and then used as an optimal 
criterion to guide the construction of the structure. The connectivity of the resulting 
structure is.arbitrary, and under the correct conditions [Geman&Geman, 84] the estimation 
of the structure is optimal in terms of the output error and low function complexity. This 
approach shares the same spirit of GMDH-type algorithms. However, the concept of 
parameter estimation from Information Theory, combined with a stochastic search 
algorithm - Simulated Annealing, was used to create a new tool for system identification. 
This work is organized as follows: section II presents the problem formulation and the 
Self Organizing Neural Network (SONN) algorithm description; section III describes the 
results of the application of SONN to a well known problem tested before using other 
neural network algorithms [Lapedes&Farber, 1987; Moody, 1988]; and finally, section IV 
presents a discussion of the results and future directions for this work. 
II. THE SELF ORGANIZING NEURAL NETWORK 
ALGORITHM 
II.1 SELF ORGANIZING STRUCTURES 
The Self Organizing Neural Network (SONN) algorithm performs a search on the model 
space by the construction of hypersurfaces. A network of nodes, each node representing a 
hypersurface, is organized to be an approximate model of the real system. SONN can be 
fully characterized by three major components, which can be modified to incorporate 
knowledge about the process: (1) a generating rule of the primitive neuron transfer 
functions, (2) an evaluation method which accesses the quality of the model, and, (3) a 
structure search strategy. Below, the components of SONN are discussed. 
II.2 THE ALGORITHM STRUCTURE 
Self Organizing Neural Networks 59 
II.2.1 The Generating Rule 
Given a set of observations S: 
S = {(Xl, Y1),(X2, Y2),...,(Xl, Vl)} 
Yi = f(Xi) + 1 
(2) 
where f(.) is represented by a Kolmogorov-Garbor polynomial, and the random variable 
'1 is normally distributed, N(O,1). The dimensions of Y is m, and the dimensions of X is 
n. Every component Yk of Y forms a hypersufface Yk = fk(X) in the space of dim (X) + 
1. The problem is to find f(.), given the observations S, which is a corrupted version of 
the desired function. In this work, the model which estimates f(.) is desired to be as 
accurate and simple (small number of parameters, and low degree of non linearity) as 
possible. 
The approach taken here is to estimate the simplest model which best describes f(.) by 
generating optimal functions for each neuron, which can be viewed as the construction of 
a hypersurface based on the observed data. It can be described as follows: given a set of 
observations S; use p components of the n dimensional space of X to create a 
hypersurface which best describes Yk = f(X), through a three step process. First, given X 
= [x 1, x 2, x 3, ..., x n] and Yk' and the mapping t'n: [x 1, x2, x3, ..., Xn] -> [xt,(1), 
xt,(2 ), xt,(3 ), ..., xt,(n)], construct the hypersurface hl(Xt,(1), xt,(2), xt,(3), ..., 
xt,(n)) (h i after the first iteration) of p+l dimensions, where t' n is a projection from n 
dimensions to p dimensions. The elements of the domain of t' n are called terminals. 
Second, If the global optimality criterion is reached by the construction of hi(xt,(l ), 
xt,(2 ), xt,(3 ), ..., xt,(n)), then stop, otherwise continue to the third step. Third, 
generate from [x 1, x 2, x 3, ..., xn,hl(Xt,(1), xt,(2), xt,(3), ..., xt,(n)) ] a new p+l 
dimensional hypersurface hi+ 1 through the extended mapping t'n+l(. ), and reapply the 
second step. The resulting model is a multilayered neural network whose topology is 
arbitrarily complex and creawxi by a stochastic search guided by a structure estimation 
criterion. For simplicity in this work, the set of prototype functions (F) is reslricWxl to be 
2-input quadratic surfaces or smaller, with only four possible types: 
y = ao+alxl+a2x2 (3) 
y = ao+alxl+a2x2+a3xlx2 (4) 
Y = ao+alxl+a2x21 (5) 
Y = ao+alxl+a2x2+a3xlx2+a4x21+asx (6) 
II.2.2 Evaluation of the Model Based on the MDL Criterion 
The selection rule (T) of the neuron transfer function was based on a modification of the 
Minimal Description Length (MDL) information criterion. In [Rissanen, 1975] the 
principle of minimal description for statistical estimation was developed. The MDL 
provides a wade-off between the accuracy and the complexity of the model by including 
the structure estimation term of the final model. The final model (with the minimal 
60 Tenorio and Lee 
MDL) is optimum in the sense of being a consistent estimate of the number of 
parameters while achieving the minimum error [Rissanen, 1980]. Given a sequence of 
observation x10t20t3,...,x N from the random variable X, the dominant term of the MDL 
in [Rissanen, 1975] is: 
MDL = - log f(xl0) + 0.5 k log N 
where f(x10) is the estimated probability density function of the model, k is the number 
of parameters, and N is the number of observations. The first term is actually the negative 
of the maximum likelihood (ML) with respect to the estimated parameter. The second 
term describes the structure of the models and it is used as a penalty for the complexity of 
the model. In the case of linear polynomial regression, the MDL is: 
MDL = - 0.5 N log S2n + 0.5 k log N 
where k is the number of coefficients in the model selected. 
In the SONN algorithm, the MDL criterion is modified to operate both recursively and 
hierarchically. First, the concept of the MDL is applied to each candidate prototype 
surface for a given neuron. Second, the acceptance of the node, based on Simulated 
Annealing, uses the MDL measure as the system energy. However, since the new neuron 
is generateA from terminals which can be the output of other neurons, the original 
definition of the MDL is unable to compute the true number of system parameters of the 
final function. Recall that due to the arbitrary connectivity, feedback loops and other 
configurations it is non trivial to compute the number of parameters in the entire 
structure. In order to reflect the hierarchical nature of the model, a modified MDL called 
Structure Estimation Criterion (SEC) is used in conjunction with an heuristic estimator 
of the number of parameters in the system at each stage of the algorithm. A 
computationally efficient heuristic for the estimation of the number of parameters in the 
model is based on the fact that SONN creates a tree-like structure with multiple roots at 
the input terminals. Then k, in expression (8), can be estimated recursively by: 
k = kL + kR + (no. of parameters of the current node) (9) 
where kL and kR are the estimated number of parameters of the left and right parents of 
the current node, respectively. This heuristic estimator is neither a lower bound nor an 
upper bound of the true number of parameter in the model. 
II.2.3 The SONN Algorithm 
To explain the algorithm, the following definitions are necessary: Node - neuron and the 
associated function, connections, and SEC; BASIC NODE - A node for the system input 
variable; FRONT NODE - A node without children; INTERMIDIATE NODE - The nodes 
that are neither front or basic nodes; STATE - The collection of nodes, and the 
co
