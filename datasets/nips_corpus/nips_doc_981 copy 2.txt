Correlation and Interpolation Networks for 
Real-time Expression Analysis/Synthesis. 
Trevor Darrell, Irfan Essa, Alex Pentland 
Perceptual Computing Group 
MIT Media Lab 
Abstract 
We describe a framework for real-time tracking of facial expressions 
that uses neurally-inspired correlation and interpolation methods. A 
distributed view-based representation is used to characterize facial state, 
and is computed using a replicated correlation network. The ensemble 
response of the set of view correlation scores is input to a network based 
interpolation method, which maps perceptual state to motor control states 
for a simulated 3-D face model. Activation levels of the motor state 
correspond to muscle activations in an anatomically derived model. By 
integrating fast and robust 2-D processing with 3-D models, we obtain a 
system that is able to quickly track and interpret complex facial motions 
in real-time. 
1 INTRODUCTION 
An important task for natural and artificial vision systems is the analysis and interpretation 
of faces. To be useful in interactive systems and in other settings where the information 
conveyed is of a time critical nature, analysis of facial expressions must occur quickly, or be 
of little value. However, many of the traditional computer vision methods for estimating and 
modeling facial state have proved difficult to perform fast enough for interactive settings. 
We have therefore investigated neurally inspired mechanisms for the analysis of facial 
expressions. We use neurally plausible distributed pattern recognition mechanisms to make 
fast and robust assessments of facial state, and multi-dimensional interpolation networks to 
connect these measurements to a facial model. 
There are many potential applications of a system for facial expression analysis. Person- 
910 Trevor Darrell, Irfan Essa, Alex Pentland 
alized interfaces which sense a users emotional state, ultra-low bitrate video conferencing 
which sends only facial muscle activations, as well as the enhanced recognition systems 
mentioned above. We have focused on a application in computer graphics which stresses 
both the analysis and synthesis components of our system: interactive facial animation. 
In the next sections we develop a computational framework for neurally plausible expression 
analysis, and the connection to a physically-based face model using a radial basis function 
method. Finally we will show the results of these methods applied to the interactive 
animation task, in which an computer graphics model of a face is rendered in real time, and 
matches the state of the users face as sensed through a conventional video camera. 
2 EXPRESSION MODELING/TRACKING 
The modeling and tracking of expressions and faces has been a topic of increasing interest 
recently. In the neural network field, several successful models of character expression 
modeling have been developed by Poggio and colleagues. These models apply multi- 
dimensional interpolation techniques, using the radial basis function method, to the task 
of interpolating 2D images of different facial expression. Librande [4] and Poggio and 
Brunelli [9] applied the Radial Basis Function (RBF) method to facial expression mod- 
eling, using a line drawing representation of cartoon faces. In this model a small set of 
canonical expressions is defined, and intermediate expressions constructed via the interpo- 
lation technique. The representation used is a generic feature vector, which in the case of 
cartoon faces consists of the contour endpoints. Recently, Beymer et al. [1] extended this 
approach to use real images, relying on optical flow and image warping techniques to solve 
the correspondence and prediction problems, respectively. 
RBF-based techniques have the advantage of allowing for the efficient and fast compu- 
tation of intermediate states in a representation. Since the representation is simple and 
the interpolation computation straight-forward, real-time implementations are practical on 
conventional systems. These methods interpolate between a set of 2D views, so the need for 
an explicit 3-D representation is sidestepped. For many applications, this is not a problem, 
and may even be desirable since it allows the extrapolation to impossible figures or ex- 
pressions, which may be of creative value. However, for realistic rendering and recognition 
tasks, the use of a 3-D model may be desirable since it can detect such impossible states. 
In the field of computer graphics, much work has been done on on the 3-D modeling of 
faces and facial expression. These models focus on the geometric and physical qualities 
of facial structure. Platt and Badler [7], Pieper [6], Waters [11] and others have developed 
models of facial structure, skin dynamics, and muscle connections, respectively, based on 
available anatomical data. These models provide strong constraints for the tracking of 
feature locations on a face. Williams et. al. [12] developed a method in which explicit 
feature marks are tracked on a 3-D face by use of two cameras. Terzopoulos and Waters [10] 
developed a similar method to track linear facial features, estimate corresponding parameters 
of a three dimensional wireframe face model, and reproduce facial expression. A significant 
limitation of these systems is that successful tracking requires facial markings. Essa and 
Pentland [3] applied optical flow methods (see also Mase [5]) for the passive tracking of 
facial motion, and integrated the flow measurement method into a dynamic system model. 
Their method allowed for completely passive estimation of facial expressions, using all the 
constraints provided by a full 3-D model of facial expression. 
Both the view based method of Beymer et. al. and the 3-D model of Essa and Pentland rely 
Correlation and Interpolation Networks for Real-Time Expression Analysis/Synthesis 911 
(a) (b) 
Figure 1: (a) Frame of video being processed to extract view model. Outlined rectangle 
indicates area of image used for model. (b) View models found via clustering method on 
training sequence consisting of neutral, smile, and surprise expressions. 
on estimates of optic flow, which are difficult to compute reliably, especially in real-time. 
Our approach here is to combine interpolated view-based measurements with physically 
based models, to take advantage of the fast interpolation capability of the RBF and the 
powerful constraints imposed by physically based models. We construct a framework in 
which perceptual states are estimated from real video sequences and are interpolated to 
control the motor control states of a physically based face model. 
3 VIEW-BASED FACE PERCEPTION 
To make reliable real-time measurements of a complex dynamic object, we use a distributed 
representation corresponding to distinct views of that object. Previously, we demonstrated 
the use of this type of representation for the tracking and recognition of hand gestures [2]. 
Like faces, hands are complex objects with both non-rigid and rigid dynamics. Direct use 
of a 3-D model for recognition has proved difficult for such objects, so we developed a 
view-based method for representation. Here we apply this technique to the problem of facial 
representation, but extend the scheme to connect to a 3-D model for high-level modeling 
and generation/animation. With this, we gain the representational power and constraints 
implied by the 3-D model as a high-level representation; however the 3-D model is only 
indirectly involved in the perception stage, so we can still have the same speed and reliability 
afforded by the view-based representation. 
In our method each view characterizes a particular aspect or pose of the object being 
represented. The view is stored iconically, that is, it is a literal image or template (but 
with some point-wise statistics) of the appearance of the object in that aspect or pose. A 
match criteria is defined between views and input images; usually a normalized correlation 
function is used, but other criteria are possible. An input image is represented by the 
ensemble of match scores from that image to the stored views. 
To achieve invariance across a range of transformations, for example translation, rotation 
and/or scale, units which compute the match score for each view are replicated at different 
values of each transformation. ! The unit which has maximal response across all values of 
the transformation is selected, and the ensemble response of the view units which share the 
In a computer implementation this exhaustive sampling may be impractical due to the num- 
ber of units needed, in which case this stage may be approximated by methods which are hybrid 
sampling/search methods. 
912 Trevor Darrell, lrfan Essa, Alex Pentland 
same transformation values as the selected unit is stored as the representation for the input 
image. We set the perceptual state X to be a vector containing this ensemble response. 
If the object to be represented is fully known a priori, then methods to generate views can 
be constructed by analysis of the aspect graph if the object is polyhedral, or in general 
by rendering images of the object at evenly spaced rotations. However, in practice good 
3-D models that are useful for describing image intensity values are rare 2, so we look to 
data-driven methods of acquiring object views. 
As described in [2] a simple clustering algorithm can find a set of views that span a 
training sequence of images, in the sense that for each image in the sequence at least one 
view is within some threshold similarity to that image. The algorithm is as follows. Let 
V be the current set of views for an object (initially one view is specified manually). For 
each frame I of a training sequence, if at least one v  V has a match value M(v, I) that is 
greater than a threshold 0, then no action is performed and the next frame is processed. If 
no view is close, then I is use
