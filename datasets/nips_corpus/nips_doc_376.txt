Integrated Segmentation and Recognition of 
Hand-Printed Numerals 
James D. Keeler* 
MCC 
3500 W. Balcones Ctr. Dr. 
Austin, TX 78759 
David E. Rumelhart 
Psychology Department 
Stanford University 
Stanford, CA 94305 
Abstract 
Wee-Kheng Leow 
MCC and 
University of Texas 
Austin, TX 78759 
Neural network algorithms have proven useful for recognition of individ- 
ual, segmented characters. However, their recognition accuracy has been 
limited by the accuracy of the underlying segmentation algorithm. Con- 
ventional, rule-based segmentation algorithms encounter difficulty if the 
characters are touching, broken, or noisy. The problem in these situations 
is that often one cannot properly segment a character until it is recog- 
nized yet one cannot properly recognize a character until it is segmented. 
We present here a neural network algorithm that simultaneously segments 
and recognizes in an integrated system. This algorithm has several novel 
features: it uses a supervised learning algorithm (backpropagation), but is 
able to take position-independent information as targets and self-organize 
the activities of the units in a competitive fashion to infer the positional 
information. We demonstrate this ability with overlapping hand-printed 
numerals. 
1 INTRODUCTION 
A major problem with standard backpropagation algorithms for pattern recognition 
is that they seem to require carefully segmented and localized input patterns for 
training. This is a problem for two reasons: first, it is often a labor intensive task 
to provide this information and second, the decision as to how to segment often 
depends on prior recognition. However, we describe below a neural network design 
and corresponding backpropagation learning algorithm that learns to simultane- 
*Reprint requests: Jim Keeler, keelermcc.com or coilamcc.com 
557 
558 Keeler, Rumelhart, and Leow 
ous]y segment and identify a pattern. 
There are two important aspects to many pattern recognition problems that we 
have built directly into our network and learning algorithm. The first is that the 
exact location of the pattern, in space or time, is irrelevant to the classification of 
the pattern; it should be recognized as a member of the same class wherever or 
whenever it occurs. This suggests that we build translation independence directly 
into our network. The second aspect is that feedback about whether or not a 
pattern of a particular class is present is all that should be required for training; 
information about the exact location and relationship to other patterns should not 
be required. The target information, thus, does not include information about 
where the patterns occur, only about whether a particular pattern occurs. 
We have incorporated two design principles into our network to deal with these 
problems. The first is to build translation independence into the network by us- 
ing linked local receptive fields. The second is to build a fixed forward model'' 
(c.f. Jordan and Rumelhart, 1990) which translates a location-specific recognition 
process into a location-independent output value. This output gives rise to a non- 
specific error signal which is propagated back through this fixed network to train 
the underlying location-specific network. 
2 NETWORK ARCHITECTURE AND ALGORITHM 
The basic organization of the network is illustrated in Figure 1. In the case of 
character recognition, the input consists of a set of pixels over which the stimulus 
patterns are displayed. We designate the stimulus pattern by the vector . In 
general, we assume that any character can be presented in any position and that 
characters may overlap. The input image then projects to a set of hidden units 
which learn to abstract features from the input field. These feature abstraction 
units are organized into sheets, one for each feature type. Each unit within a sheet 
is constrained to have the same weights as every other unit in the sheet (to enforce 
translational invariance). This is the same method used by Rumelhart, Hinton and 
Williams (1986) in solving the so-called T/C problem and the one used by LeCun 
et al. (1990) in their work on ZIP-code recognition. 
We let the activation value of hidden unit of type i at location 3' be a logistic 
sigmoidal function of its net input and designate it hij. We interpret hij as the 
probability that feature fi is present in the input at position j. The hidden units 
then project onto a set of sheets of position-specific character recognition units, one 
sheet for each character type. These units have exponential activation functions and 
each unit in the sheet receives inputs from a local receptive field block of feature 
detection units as shown in Figure 1. As with the hidden units, the weights in each 
exponential unit sheet are linked, enforcing translational invariance. We designate 
as Xi the activation of the unit for detecting character i at location j, and define 
The algorithm and network design presented here was first proposed by Rumelhart 
in a presentation entitled Learning and Generalization in Multilayer networks given at 
the NATO Advanced Research Workshop on Neuro Computing, Algorithms, Architectures 
and Applications held in Les Arcs, France in February, 1989. The algorithm can be viewed 
as a generalization and refinement of the TDNN of Lang, Hinton, & Waibel, 1990. 
Integrated Segmentation and Recognition of Hand-Printed Numerals 559 
Net input: 
i N ixy k 
TlxY--k W,y, hx,y, : 
1 ,' 
x'y  
Exponentials: 
i 
i = eTIxy 
Xxy 
OutputS: 
$ 
p= i 
i 1+$ i 
Character 
Detectors: 
Sheets of 
exponential 
units 
/ I Linear units: 
I  = i 
Targets 
Sigmoidai 
Hiddens: 
Feature 
Detectors: 
Sheets of sigmoidai units 
with linked local receptive 
fields. 
/ Grey-scale 
/ Input image 
/ 
Figure 1: The Integrated Segementation and Recognition (ISR) network. The input 
image may contain several characters and is presented to the network in a two- 
h.,y, have linked-local 
dimensional array of grey-scale values. Units in the first block k 
receptive fields to the input image, and detect features of type k. The exponential 
units in the next block receive inputs from a local receptive field of hidden sigmoidal 
h.., u, to the exponential unit 
units. The weights .. k.,, connect the hidden unit 
X i . The architecture enforces translational invariance across the sheets of units by 
liing the weights and shifting the receptive fields in each dimension. Finally, the 
activity in each individual sheet of exponential units is summed by the linear units 
and converted to a probability Pi. The two-dimensional input image can be thought 
of az a one-dimensional vector , as discussed in the text. For notational convenience 
we used one-dimensional indices (j) in the text rather than two-dimensional (xy) as 
shown in the figure. All of 4;he mathematics goes through Lf one replaces j 
560 Keeler, Rumelhart, and Leow 
Xii = �r�, where the net input to the unit is 
and wo, is the weight from hidden unit hkj to the detector Xij. As we gue in Keeler 
Rumelht and Leow (1991), ii can usefully be interpreted m the logithm of the 
likelihood ratio favoring the hypothesis that a chacter of type i is at location j of 
the input field. Since Xij is the exponential of iy  the X units e to be interpreted as 
representing the likelihood ratios themseNes. Thus we can interpret the output of 
the X units dectly  the edence favoring the sumption that there is a chacter 
of a pticul type at a pticul location. ff we were willing and able to carefully 
segment the input and tell the network the exact location of each chacter we could 
use a standd training technique to train the network to recognize chacters at 
any location with any degree of overlap. However we e interested in a training 
algorithm in which we dont have to provide the network with such specific training 
information. We e interested in simply telling the network which chters e 
present in the input - not where each chacter is. This approach saves tremendous 
time and effort in data prepation and labeling. To implement this idea, we have 
built an additional network which takes the output of the X units and computes 
through a fixed output network, the probability that a given chacter is present 
anywhere in the input field. We do this by adding two additional layers of units. 
The first layer of units the  units, simply sum the activity of each sheet of the 
X units. The activity of unit i can under certain resumptions, be interpreted m 
the likelihood ratio that a chacter of type i occued anywhere in the input field. 
Finally in the output layer we convert the likelihood ratio into a probability by the 
formula 
- + (2) 
Thus, p is interpreted  representing dectly the probability that chacter i 
occued in the input field. 
2.1 The learning Rule 
On having set up our network, it is straight-forward to compute the derivative of 
the error function with respect to r/O.. We get a particularly simple learning rule if 
we let the objective function be the cross-entropy function, 
I =  t, lnpi + (1 - ti)ln(1 - p,) (3) 
i 
where ti equals I if character i is presented and zero otherwise. In this case, we get 
the following rule: 
Ol _ (ti _ p�) Xo' (4) 
Ori , 
It should be noted that this is a kind of competitive rule in which the learning is 
proportional to the relative strength of the activation at the unit at a particular 
location in the X layer to the strength of activation in the entire layer. This is valid 
if we assume that either the character appears exactly once or not at all. This ratio 
is the conditional probability that the target was at position $' under the assumption 
that the target was, in fact, presented. It is also possible to derive a learning rule 
for the case where more than one of the same character is present[3]. 
Integrated Segmentation and Recognition of Hand-Printed Numerals 56
