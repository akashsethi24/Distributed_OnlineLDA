Geometry of Early Stopping in Linear 
Networks 
Robert Dodier * 
Dept. of Computer Science 
University of Colorado 
Boulder, CO 80309 
Abstract 
A theory of early stopping as applied to linear models is presented. 
The backpropagation learning algorithm is modeled as gradient 
descent in continuous time. Given a training set and a validation 
set, all weight vectors found by early stopping must lie on a cer- 
tain quadric surface, usually an ellipsoid. Given a training set and 
a candidate early stopping weight vector, all validation sets have 
least-squares weights lying on a certain plane. This latter fact can 
be exploited to estimate the probability of stopping at any given 
point along the trajectory from the initial weight vector to the least- 
squares weights derived from the training set, and to estimate the 
probability that training goes on indefinitely. The prospects for 
extending this theory to nonlinear models are discussed. 
I INTRODUCTION 
'Early stopping' is the following training procedure: 
Split the available data into a training set and a validation set. 
Start with initial weights close to zero. Apply gradient descent 
(backpropagation) on the training data. If the error on the valida- 
tion set increases over time, stop training. 
This training method, as applied to neural networks, is of relatively recent origin. 
The earliest references include Morgan and Bourlard [4] and Weigend et al. [7]. 
*Address correspondence to: dodiercs. colordo.edu 
366 R. DODIER 
Finnoff et al. [2] studied early stopping empirically. While the goal of a theory of 
early stopping is to analyze its application to nonlinear approximators such as sig- 
moidal networks, this paper will deal mainly with linear systems and only marginally 
with nonlinear systems. Baldi and Chauvin [1] and Wang et al. [6] have also ana- 
lyzed linear systems. 
The main result of this paper can be summarized as follows. It can be shown 
(see Sec. 5) that the most probable stopping point on a given trajectory (fixing 
the training set and initial weights) is the same no matter what the size of the 
validation set. That is, the most probable stopping point (considering all possible 
validation sets) for a finite validation set is the same as for an infinite validation 
set. (If the validation data is unlimited, then the validation error is the same as the 
true generalization error.) However, for finite validation sets there is a dispersion 
of stopping points around the best (most probable and least generalization error) 
stopping point, and this increases the expected generalization error. See Figure 1 
for an illustration of these ideas. 
2 MATHEMATICAL PRELIMINARIES 
In what follows, backpropagation will be modeled as a process in continuous time. 
This corresponds to letting the learning rate approach zero. This continuum model 
simplifies the necessary algebra while preserving the important properties of early 
stopping. Let the inputs be denoted X - (xij), so that xij is the j'th component of 
the i'th observation; there are p components of each of the n observations. Likewise, 
let y = (yi) be the (scalar) outputs observed when the inputs are X. Our regression 
model will be a linear model, yi -- wxi + el, i = 1,...,n. Here ei represents 
independent, identically distributed (i.i.d.) Gaussian noise, ei -' N(O, a2). Let 
E(w) - �[[Xw -y[[2 be one-half the usual sum of squared errors. 
The error gradient with respect to the weights is VE(w) - wXX - yX. The 
backprop algorithm is modeled as  = -VE(w). The least-squares solution, at 
which VE(w) - 0, is w�$ -- (XX)-lXy. Note the appearence here of the 
input correlation matrix, XX - (.= xkixkj). The properties of this matrix 
determine, to a large extent, the properties of the least-squares solutions we find. It 
turns out that as the number of observations n increases without bound, the matrix 
a(XX) - converges with probability one to the population covariance matrix of 
the weights. We will find that the correlation matrix plays an important role in the 
analysis of early stopping. 
We can rewrite the error E using a diagonalization of the correlation matrix XX - 
SAS . Omitting a few steps of algebra, 
p 
1 1  
E(w) =   Akv + y (y- Xw�$) (1) 
k=l 
where v = S(w-w�s) and A = diag(A1,..., Ap). In this sum we see that the mag- 
nitude of the k'th term is proportional to the corresponding characteristic value, 
so moving w toward w�$ in the direction corresponding to the largest character- 
istic value yields the greatest reduction of error. Likewise, moving in the direction 
corresponding to the smallest characteristic value gives the least reduction of error. 
Geometry of Early Stopping in Linear Networks 367 
So far, we have implicitly considered only one set of data; we have assumed all data 
is used for training. Now let us distinguish training data, Xt and Yt, from validation 
data, Xv and Yv; there are nt training and nv validation data. Now each set of 
data has its own least-squares weight vector, wt and Wv, and its own error gradient, 
VEt(w) and VEv (w). Also define Mt = X'tXt and My = XvXv for convenience. 
The early stopping method can be analyzed in terms of the these pairs of matrices, 
gradients, and least-squares weight vectors. 
3 THE MAGIC ELLIPSOID 
Consider the early stopping criterion,  (w) - 0. Applying the chain rule, 
dt 
der der dw 
-- = -- -- = VE.-VEt, (2) 
dt dw dt 
where the last equality follows from the definition of gradient descent. So the early 
stopping criterion is the same as saying 
VEt � VE, = 0, 
(3) 
that is, at an early stopping point, the training and validation error gradients are 
perpendicular, if they are not zero. 
Consider now the set of all points in the weight space such that the training and 
validation error gradients are perpendicular. These are the points at which early 
stopping may stop. It turns out that this set of points has an easily described shape. 
The condition given by Eq. 3 is equivalent to 
0 = VEt' VE, = (w- wt)'MtM,(w- w,). 
(4) 
Note that all correlation matrices are symmetric, so MtM[ - MtMv. We see that 
Eq. 4 gives a quadratic form. Let us put Eq. 4 into a standard form. Toward this 
end, let us define some useful terms. Let 
M = MtMv, (5) 
1I = + M ) -}(MtM, + M,Mt), (6) 
- 
1 
w = (wt + (7) 
/Xw = wt - w, (8) 
and 
 = -- 41-1(/I-1(M- M')Aw. 
(9) 
Now an important result can be stated. The proof is omitted. 
Proposition 1. VEt � VEv = 0 is equivalent to 
1 
(w - g')'l(/l(w - g') = 41-Aw[l(/l + ;(M - M)I(/I-i(M - M')]Aw. [] (10) 
The matrix 1QI of the quadratic form given by Eq. 10 is usually positive definite. 
As the number of observations nt and nv of training and validation data increase 
without bound, 1QI converges to a positive definite matrix. In what follows it will 
368 R. DODIER 
always be assumed that 1QI is indeed positive definite. Given this, the locus defined 
by VEt � VEv is an ellipsoid. The centroid is ,, the orientation is determined by 
the characteristic vectors of 1QI, and the length of the k'th semiaxis is cv/, where 
c is the constant on the righthand side of Eqo 10 and k is the k'th characteristic 
value of 1QI. 
4 THE MAGIC PLANE 
Given the least-squares weight vector wt derived from the training data and a 
candidate early stopping weight vector Wes, any least-squares weight vector Wv 
from a validation set must lie on a certain plane, the 'magic plane.' The proof of 
this statement is omitted. 
Proposition 2. The condition that wt, Wv, and wes all lie on the magic ellipsoid, 
(w,-�,)'(w-�,) = (w,-�')'(v -�') = (w,-�,)'(w-�') =c, (11) 
implies 
(wt- we,)'Mwv = (wt - we,)'Mwe,. [] (12) 
This shows that Wv lies on a plane, the magic plane, with normal M(wt - Wes). 
The reader will note a certain difficulty here, namely that M - MtMv depends on 
the particular validation set used, as does Wv. However, we can make progress by 
considering only a fixed correlation matrix My and letting Wv vary. Let us suppose 
the inputs (x, x2,..., Xp) are i.i.d. Gaussian random variables with mean zero and 
some covariance . (Here the inputs are random but they are observed exactly, so 
the error model y: wx + e still applies.) Then 
(M) = (XX) = nvXl, 
so in Eq. 12 let us replace M with its expected value n,,. That is, we can 
approximate Eq. 12 with 
(wt - Wes)'Mtwv = (wt - Wes)'Mtwes. 
(13) 
Now consider the probability that a particular point w(t) on the trajectory from 
w(0) to wt is an early stopping point, that is, VEt(w(t)). VE(w(t)) - 0. This is 
exactly the probability that Eq. 12 is satisfied, and approximately the probability 
that Eq. 13 is satisfied. This latter approximation is easy to calculate: it is the 
mass of an infinitesimally-thin slab cutting through the distribution of least-squares 
validation weight vectors. Given the usual additive noise model y - wx q- e with e 
being i.i.d. Gaussian distributed noise with mean zero and variance a 2, the least- 
squares weights are approximately distributed as 
W- W*  N(O,o'2(X'X) -1) 
(14) 
when the number of data is large. 
Consider now the plane f/= {w: wfi = k}. The probability mass on this plane as 
it cuts through a Gaussian distribution N(/, C) is then 
l(k-Wa) 2 
pn(k, fi) = (27rtCfl) -1/2exp(- fi'Cfi ) ds 
(15) 
where ds denotes an infinitesimal arc length. (See, for example, Sec. VIII-9.3 of 
von Mises [3].) 
Geometry of Early Stopping in Linear Networks 369 
0.25 
0.1 
0.2 
0.15 
0.05 
1 2 3 . 5 6 7 8 
Arc Length Along Trajectory 
Figure 1: Histogram of early stopping points along a trajectory, with bins of equal 
arc length. An approximation to the probability of stopping (Eq. 16) is superim- 
posed. Altogether 1000 validation sets were generated for a certain training set; of 
these, 288 gave don't start solutions, 701 gave early stopping solutions (which are 
binned here) somewhere on the trajectory, and 11 gave don't stop solutions. 
5 PROBABILIT
