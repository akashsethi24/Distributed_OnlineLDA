544 
MURPHY: A Pobot that Learns by Doing 
Bartlett W. Mel 
Center for Complex Systems Research 
University of Illinois 
508 South Sixth Street 
Champaign, IL 61820 
January 2, 1988 
Abstract 
MURPHY consists of a camera looking at a robot arm, with a connectionist network 
architecture situated in between. By moving its arm through a small, representative 
sample of the 1 billion possible joint configurations, MURPHY learns the relationships, 
backwards and forwards, between the positions of its joints and the state of its visual field. 
MURPHY can use its internal model in the forward direction to envision sequences 
of actions for planning purposes, such as in grabbing a visually presented object, or in 
the reverse direction to 'qmitate, with its arm, autonomous activity in its visual field. 
Furthermore, by taking explicit advantage of continuity in the mappings between visual 
space and joint space, MURPHY is able to learn non-linear mappings with only a single 
layer of modifiable weights. 
Background 
Current Focus Of Learning Research 
Most connectionist learning algorithms may be grouped into three general catagories, 
commonly referred to as supervised, unsupervised, and reinforcement learning. Supervised 
learning requires the explicit participation of an intelligent teacher, usually to provide the 
learning system with task-relevant input-output pairs (for two recent examples, see [1,2]). 
Unsupervised learning, exemplified by clustering algorithms, are generally concerned 
with detecting structure in a stream of input patterns [3,4,5,6,7]. In its final state, an 
unsupervised learning system will typically represent the discovered structure as a set of 
categories representing regions of the input space, or, more generally, as a mapping from 
the input space into a space of lower dimension that is somehow better suited to the task at 
hand. In reinforcement learning, a critic rewards or penalizes the learning system, until 
the system ultimately produces the correct output in response to a given input pattern 
[8]. 
It has seemed an inevitable tradeoff that systems needing to rapidly learn specific, 
behaviorally useful input-output mappings must necessarily do so under the auspices of 
an intelligent teacher with a ready supply of task-relevant training examples. This state of 
affairs has seemed somewhat paradoxical, since the processes of gerceptual and cognitive 
development in human infants, for example, do not depend on the moment by moment 
intervention of a teacher of any sort. 
Learning by Doing 
The current work has been focused on a fourth type of learning algorithm, i.e. learning-b.y- 
doing, an approach that has been very little studied from either a connectionist perspective 
American Institute of Physics 1988 
545 
or in the context of more traditional work in machine learning. In its basic form, the 
learning agent 
begins with a repertoire of actions and some form of perceptual input, 
exercises its repertoire of actions, learning to predict i) the detailed sensory conse- 
quences of its actions, and, in the other direction, it) its actions that are associated 
with incoming sensory patterns, and 
runs its internal model (in one or both directions) in a variety of behaviorally-relevant 
tasks, e.g. to envision sequences of actions for planning purposes, or to internally 
imitate, via its internal action representation, an autonomously generated pattern 
of perceptual activity. 
In comparison to standard supervised learning algorithms, the crucial property of 
learning-by-doing is that no intelligent teacher is needed to provide inpl-otpt pairs 
for learning. Laws of physics simply translate actions into their resulting percepts, both 
of which are represented internally. The learning agent need only notice and record this 
relationship for later use. In contrast to traditional unsupervised learning approaches, 
learning-by-doing allows the acquisition of specific, task-relevant mappings, such as the 
relationship between a simultaneously represented visual and joint state. Learning-by- 
doing differs as well from reinforcement paradigms in that it can operate in the absence of 
a critic, i.e. in situations where reward or penalty for a particular training instance may 
be inappropriate. 
Learning by doing may therefore by described as an ansapervised associative algorithm, 
capable of acquiring rich, task-relevant associations, but without an intelligent teacher or 
critic. 
Abridged History of the Approach 
The general concept of leaning by doing may be attributed at least to Piaget from the 
1940's (see [9] for review). Piaget, the founder of the constructivist school of cognitive 
development, argued that knowledge is not given to a child as a passive observer, but 
is rather discovered and constructed by the child, through active manipulation of the 
environment. A handful of workers in artificial intelligence have addressed the issue of 
learning-by-doing, though only in highly schematized, simulated domains, where actions 
and sensory states are represented as logical predicates [10,11,12,13]. 
Barto & Sutton [14] discuss learning-by-doing in the context of system identification 
and motor control. They demonstrated how a simple simulated automaton with two ac- 
tions and three sensory states can build a model of its environment through exploration, 
and subsequently use it to choose among behavioral alternatives. In a similar vein, Rumel- 
hart [15] has suggested this same approach could be used to learn the behavior of a robot 
arm or a set of speech articulators. Furthermore, the forward-going mental model, once 
learned, could be used internally to train an inverse model using back-propagation. 
In previous work, this author [16] described a connectionist model (VIPS) that learned 
to perform 3-D visual transformations on simulated wire-frame objects. Since in complex 
sensory-motor environments it is not possible, in general, to learn a direct relationship 
between an outgoing command state and an incoming sensory state, VIPS was designed 
to predict changes in the state of its visual field as a function of its outgoing motor com- 
mand. VIPS could then use its generic knowledge of motor-driven visual transformations 
to mentally rotate objects through a series of steps. 
546 
Rednomp/c Value-Coded 
Visua/Repmmtafi(m $oim Repre. senmi(m 
Figure 1: MURPHY's Connectionist Architecture. 4096 coarsely-tuned visual 
units are organized in a square, retinotopic grid. These units are bi-directionlly 
interconnected with a population of 273 joint units. The joint population is subdi- 
vided into 3 subpopulations, each one a vaJue-coded representation of joint angle 
for one of the three joints. During training, activity in the joint unit population 
determines the physica arm configuration. 
Inside MURPY 
The current work has sought to further explore the process of learning-by-doing in a 
complex sensory-motor domain, extending previous work in three ways. First, the learning 
of mappings between sensory and command (e.g. motor) representations should be allowed 
to proceed in both directions simultaneousll during exploratory behavior, where each 
mapping may ultimately subserve a very different behavioral goal. Secondly, MURPHY 
has been implemented with a real camera and robot arm in order to insure representational 
realism to the greatest degree possible. Third, while the specifics of MURPHY's internal 
structures are not intended as a model of a specific neural system, a serious attempt has 
been made to adhere to architectural components and operations that have either been 
directly suggested by nervous system structures, or are at least compatible with what is 
currently known. Detailed biological justification on this point awaits further work. 
MURPHY's Body 
MURPHY consists of a 512 x 512 JVC color video camera pointed at a Rhino XR-3 robotic 
arm. Only the shoulder, elbow, and wrist joints are used, such that the arm can move 
only in the image plane of the camera. (A fourth, waist joint is not used). White spots are 
stuck to the arm in convenient places; when the image is thresholded, only the white spots 
appear in the image. This arrangement allows continuous control over the complexity of 
the visual image of the arm, which in turn affects time spent both in computing visual 
features and processing weights during learning. A Datacube image processing system is 
used for the thresholding operation and to blur the image in real time with a gaussian 
mask. The degree of blur is variable and can be used to control the degree of coarse-coding 
(i.e. receptive field overlap) in the camera-topic array of visual units. The arm is software 
controllable, with a stepper motor for each joint. Arm dynamics are not considered in this 
work. 
547 
MURPHY's Mind 
MURPHY is currently based on two interconnected populations of neuron-like units. The 
first is organized as a rectangular, visuotopically-mapped 64 x 64 grid of coarsely-tuned 
visual units that each responds when a visual feature (such as a white spot on the arm) 
falls into its receptive field (fig. 1). Coarse coding insures that a single visual feature will 
activate a small population of units whose receptive fields overlap the center of stimulation. 
The upper trace in figure 2 shows the unprocessed camera view, and the center trace depicts 
the resulting pattern of activation over the grid of visual units. 
The second population of 273 units consists of three subpopulations, representing the 
angles of each of the three joints. The angle of each joint is value-coded in a line of 
units dedicated to that joint (fig. 1). Each unit in the population is centered at a 
some joint angle, and is maximally activated when the joint is to be sent to that angle. 
Neighboring joint units within a joint subpopulation have overlapping projective fields 
and progressively increasing joint-an
