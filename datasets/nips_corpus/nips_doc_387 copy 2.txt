From Speech Recognition to Spoken Language 
Understanding: The Development of the MIT 
SUMMIT and VOYAGER Systems 
Victor Zue, James Glass, David Goodine, Lynette Hirschman, 
Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneft 
Room NE43-601 
Spoken Language Systems Group 
Laboratory for Computer Science 
Massachusetts Institute of Technology 
Cambridge, MA 02139 U.S.A. 
Abstract 
Spoken language is one of the mot natural, efficient, flexible, and econom- 
ical means of communication among humans. As computers play an ever 
increasing role in our lives, it is important that we address the issue of 
providing a graceful human-machine interface through spoken language. 
In this paper, we will describe our recent efforts in moving beyond the 
scope of speech recognition into the realm of spoken-language understand- 
ing. Specifically, we report on the development of an urban navigation and 
exploration system called VOYAGER, an application which we have used as 
a basis for performing research in spoken-language understanding. 
I Introduction 
Over the past decade, research in speech coding and synthesis has matured to the 
extent that speech can now be transmitted efficiently and generated with high in- 
telligibility. Spoken input to computers, however, has yet to pass the threshold of 
practicality. Despite some recent successful demonstrations, current speech recog- 
nition systems typically fall far short of human capabilities of continuous speech 
recognition with essentially unrestricted vocabulary and speakers, under adverse 
acoustic environments. This is largely due to our incomplete knowledge of the en- 
coding of linguistic information in the speech signal, and the inherent variabilities of 
255 
256 Zue, Glass, Goodine, Hirschman, Leung, Phillips, Folifroni, and Seneft 
this process. Our approach to system development is to seek a good understanding 
of human communication through spoken language, to capture the essential features 
of the process in appropriate models, and to develop the necessary computational 
framework to make use of these models for machine understanding. 
Our research in spoken language system development is based on the premise that 
many of the applications suitable for human/machine interaction using speech typ- 
ically involve interactive problem solving. That is, in addition to converting the 
speech signal to text, the computer must also understand the user's request, in 
order to generate an appropriate response. As a result, we have focused our atten- 
tion on three main issues. First, the system must operate in a realistic application 
domain, where domain-specific information can be utilized to translate spoken in- 
put into appropriate actions. The use of a realistic application is also critical to 
collecting data on how people would like to use machines to access information and 
solve problems. Use of a constrained task also makes possible rigorous evaluations 
of system performance. Second and perhaps most importantly the system must 
integrate speech recognition and natural language technologies to achieve speech 
understanding. Finally, the system must begin to deal with interactive speech, 
where the computer is an active conversational participant, and where people pro- 
duce spontaneous speech, including false starts, hestitations, etc. 
In this paper, we will describe our recent efforts in developing a spoken language 
interface for an urban navigation system (VOYAGER). We begin by describing our 
overall system architecture, paying particular attention to the interface between 
speech and natural language. We then describe the application domain and some 
of the issues that arise in realistic interactive problem solving applications, par- 
ticulary in terms of conversational interaction. Finally, we report results of some 
performance evaluations we have made, using a spontaneous speech corpus we col- 
lected for this task. 
2 System Architecture 
Our spoken language language system contains three important components. The 
SUMMIT speech recognition system converts the speech signal into a set of word 
hypotheses. The TINA natural language system interacts with the speech recognizer 
in order to obtain a word string, as well as a linguistic' interpretation of the utterance. 
A control strategy mediates between the recognizer and the language understanding 
component, using the language understanding constraints to help control the search 
of the speech recognition system. 
2.1 Continuous Speech Recognition: The SUMMIT System 
The SUMMIT system (Zue et al., 1989) starts the recognition process by first trans- 
forming the speech signal into a representation that models some of the known 
properties of the human auditory system (Seneft, 1988). Using the output of the 
auditory model, acoustic landmarks of varying robustness are located and embed- 
ded in a hierarchical structure called a dendrogram (Glass, 1988). The acoustic 
segments in the dendrogram are then mapped to phoneme hypotheses, using a set 
of automatically determined acoustic attributes in conjunction with conventional 
From Speech Recognition to Spoken Language Understanding 257 
pattern recognition algorithms. The result is a phoneme network, in which each 
arc is characterized by a vector of probabilities for all the possible candidates. Re- 
cently, we have begun to experiment with the use of artificial neural nets for pho- 
netic classifiction. To date, we have been able to improve the system's classification 
performance by over 5% (Leung and Zue, 1990). 
Words in the lexicon are represented as pronunciation networks, which are generated 
automatically by a set of phonological rules (Zue et al., 1990). Weights derived 
from training data are assigned to each arc, using a corrective training procedure, 
to reflect the likelihood of a particular pronunciation. Presently, lexical decoding 
is accomplished by using the Viterbi algorithm to find the best path that matches 
the acoustic-phonetic network with the lexical network. 
2.2 Natural Language Processing: The TINA System 
In a spoken language system, the natural language component should perform two 
critical functions: 1) provide constraint for the recognizer component, and 2) pro- 
vide an interpretation of the meaning of the sentence to the back-end. Our natural 
language system, TINA, was specifically designed to meet these two needs. TINA 
is a probabilistic parser which operates top-down, using an agenda-based control 
strategy which favors the most likely analyses. The basic design of TINA has been 
described elsewhere (Seneft, 1989), but will be briefly reviewed. The grammar is 
entered as a set of simple context-free rules which are automatically converted to 
a shared network structure. The nodes in the network are augmented with con- 
straint filters (both syntactic and semantic) that operate only on locally available 
parameters. All arcs in the network are associated with probabilities, acquired 
automatically from a set of training sentences. Note that the probabilities are es- 
tablished not on the rule productions but rather on arcs connecting sibling pairs 
in a shared structure for a number of linked rules. The effect of such pooling 
is essentially a hierarchical bigram model. We believe this mechanism offers the 
capability of generating probabilities in a reasonable way by sharing counts on syn- 
tactically/semantically identical units in differing structural environments. 
2.3 Control Strategy 
The current interface between the SUMMIT speech recognition system and the TINA 
natural language system, uses an N-best algorithm (Chow and Schwartz, 1989; 
Soong and Huang, 1990; Zue et al., 1990), in which the recognizer can propose its 
best N complete sentence hypotheses one by one, stopping with the first sentence 
that is successfully analyzed by the natural language component TINA. In this case, 
TINA acts as a filter on whole sentence hypotheses. 
In order to produce N-best hypotheses, we use a search strategy that involves an 
initial Viterbi search all the way to the end of the sentence, to provide a best 
hypothesis, followed by an A* search to produce next-best hypotheses in turn, 
provided that the first hypothesis failed to parse. If all hypotheses fail to parse the 
system produces the rejection message, I'm sorry but I didn't understand you. 
Even with the parser acting as a filter of whole-sentence hypotheses, it is appropriate 
to also provide the recognizer with an inexpensive language model that can partially 
258 Zue, Glass, Goodine, Hirschman, Leung, Phillips, Iblifroni, and Seneff 
constrain the theories. This is currently done with a word-pair language model, in 
which each word in the vocabulary is associated with a list of words that could 
possibly follow that word anywhere in the sentence. 
3 The VOYAGER Application Domain 
VOYAGER. is an urban navigation and exploration system that enables the user to 
ask about places of interest and obtain directions. It has been under development 
since early 1989 (Zue et al., 1989; Zue et al., 1990). In this section, we describe the 
application domain, the interface between our language understanding system TINA 
and the application back-end, and the discourse capabilities of the current system. 
3.1 Domain Description 
For our first attempt at exploring issues related to a fully-interactive spoken- 
language system, we selected a task in which the system knows about the physical 
environment of a specific geographical area and can provide assistance on how to 
get from one location to another within this area. The system, which we call VOY- 
AGER, can also provide information concerning certain objects located inside this 
area. The current version of VOYAGER focuses on the geographic area of the city of 
Cambridge, MA between MIT and Harvard University. 
The application database is an enhanced version of the Direction Assistance pro-
