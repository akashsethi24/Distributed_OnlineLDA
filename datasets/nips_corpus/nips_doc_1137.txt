Interpolating Earth-science Data using RBF 
Networks and Mixtures of Experts 
E. Wan D. Bone 
Division of Information Technology 
Canberra Laboratory, CSIRO 
GPO Box 664, Canberra, ACT, 2601, Australia 
{ ernest, don } @ cbr.dit. csiro.au 
Abstract 
We present a mixture of experts (ME) approach to interpolate sparse, 
spatially correlated earth-science data. Kriging is an interpolation 
method which uses a global covariation model estimated from the data 
to take account of the spatial dependence in the data. Based on the 
close relationship between kriging and the radial basis function (RBF) 
network (Wan & Bone, 1996), we use a mixture of generalized RBF 
networks to partition the input space into statistically correlated 
regions and learn the local covariation model of the data in each 
region. Applying the ME approach to simulated and real-world data, 
we show that it is able to achieve good partitioning of the input space, 
learn the local covariation models and improve generalization. 
1. INTRODUCTION 
Kriging is an interpolation method widely used in the earth sciences, which models the 
surface to be interpolated as a stationary random field (RF) and employs a linear model. 
The value at an unsampled location is evaluated as a weighted sum of the sparse, 
spatially correlated data points. The weights take account of the spatial correlation 
between the available data points and between the unknown points and the available data 
points. The spatial dependence is specified in the form of a global covariafion model. 
Assuming global stationarity, the kriging predictor is the best unbiased linear predictor 
of the unsampled value when the true covariation model is used, in the sense that it 
minimizes the squared error variance under the unbiasedness constraint. However, in 
practice, the covariation of the data is unknown and has to be estimated from the data by 
an initial spatial data analysis. The analysis fits a covariation model to a covariation 
measure of the data such as the sample variogram or the sample covariogram, either 
graphically or by means of various least squares (LS) and maximum likelihood (ML) 
approaches. Valid covariation models are all radial basis functions. 
Optimal prediction is achieved when the true covariation model of the data is used. In 
general, prediction (or generalization) improves as the covariafion model used more 
Interpolating Earth-science Data using RBFN and Mixtures of Experts 989 
closely matches the true covariation of the data. Nevertheless, estimating the covariation 
model from earth-science data has proved to be difficult in practice due to the sparseness 
of data samples. Furthermore for many data sets the global stationarity assumption is 
not valid. To address this, data sets are commonly manually partitioned into smaller 
regions within which the stationarity assumption is valid or approximately so. 
In a previous paper, we showed that there is a close, formal relationship between kriging 
and RBF networks (Wan & Bone, 1996). In the equivalent RBF network formulation of 
kriging, the input vector is a coordinate and the output is a scalar physical quantity of 
interest. We pointed out that, under the stationarity assumption, the radial basis function 
used in an RBF network can be viewed as a covariation model of the data. We showed 
that an RBF network whose RBF units share an adaptive norm weighting matrix, can be 
used to estimate the parameters of the postulated covariation model, outperforming more 
conventional methods. In the rest of this paper we will refer to such a generalization of 
the RBF network as a generalized RBF (GRBF) network. 
In this paper, we discuss how a mixture of GRBF networks can be used to partition the 
input space into statistically correlated regions and learn the local covariation model of 
each region. We demonstrate the effectiveness of the ME approach with a simulated 
data set and an aero-magnetic data set. Comparisons are also made of prediction 
accuracy of a single GRBF network and other more traditional RBF networks. 
2 MIXTURE OF GRBF EXPERTS 
Mixture of experts (Jacobs et al , 1991) is a modular neural network architecture in 
which a number of expert networks augmented by a gating network compete to learn the 
data. The gating network learns to assign probability to the experts according to their 
performance over various parts of the input space, and combines the outputs of the 
experts accordingly. During training, each expert is made to focus on modelling the 
local mapping it performs best, improving its performance further. Competition among 
the experts achieves a soft partitioning of the input space into regions with each expert 
network learning a separate local mapping. An hierarchical generalization of ME, the 
hierarchical mixture of experts (HME), in which each expert is allowed to expand into a 
gating network and a set of sub-experts, has also been proposed (Jordan & Jacobs, 1994). 
Under the global stationarity assumption, training a GRBF network by minimizing the 
mean squared prediction error involves adjusting its norm weighting matrix. This can 
be interpreted as an attempt to match the RBF to the covariation of the data. It then 
seems natural to use a mixture of GRBF networks when only local stationarity can be 
assumed. After training, the gating network soft partitions the input space into 
statistically correlated regions and each GRBF network provides a model of the 
covariation of the data for a local region. Instead of an ME architecture, an HME 
architecture can be used. However, to simplify the discussion we restrict ourselves to the 
ME architecture. 
Each expert in the mixture is a GRBF network. The output of expert i is given by: 
j=l 
where n i is the number of RBF units, 0i = { {wi } j--o, {co }j%, Mi } are the parameters 
of the expert and Assuming zero-mean Gaussian error and 
common variance oi 2, the conditional probability ofy given x and 61 is given by: 
P(ylx, O,)=exp(-2-;?(y-9(x;O))2). 
(2.3) 
990 E. Wan and D. Bone 
Since the radial basis functions we used have compact support and each expert only 
learns a local covariation model, small GRBF networks spanning overlapping regions 
can be used to reduce computation at the expense of some resolution in locating the 
boundaries of the regions. Also, only the subset of data within and around the region 
spanned by a GRBF network is needed to train it, further reducing computational effort. 
With m experts, the i t output of the gating network gives the probability of selecting the 
expert i and is given by the normalized function: 
gi (x;x)) = P(ilx,x)) = ai exp(q(x;x)i ))/aj exp(q(x;x)j )) (2.4) 
wherex)={{a}i,{x)}i}. Using q(x;x))=x)[[x r 1]rand setting allah's to 1, the 
gating network implements the softmax function and partitions the input space into a 
smoothed planar tessellation. Alternatively, with q(x;v0=-IITi(x-u0112 (where 
v={u,Ti} consists of a location vector and an affine transformation matrix) and 
restricting the a's to be non-negative, the gating network divides the input space into 
packed anisotropic ellipsoids. These two partitionings are quite convenient and adequate 
for most earth-science applications where x is a 2D or 3D coordinate. 
The output of the experts are combined to give the overall output of the mixture: 
i=1 i=1 
(2.5) 
where 0 = {x), {0 i }i } and the conditional probability of observing y given x and 0 is: 
191 
P(ylx,O) =  P(ilx, x))P(ylx,O, ). (2.6) 
i=1 
3 THE TRAINING ALGORITHM 
The Expectation-Maximization (EM) algorithm of Jordan and Jacobs is used to train the 
mixture of GRBF networks. Instead of computing the ML estimates, we extend the 
algorithm by including priors on the parameters of the experts and compute the 
maximum a posterJori (MAP) estimates. Since an expert may be focusing on a small 
subset of the data, the priors help to prevent over-fitting and improve generalization. 
Jordan & Jacobs introduced a set of indicator random variables Z = {z(')} as missing 
data to label the experts that generate the observable data D = {(x('),y('))}. The log 
joint probability of the complete data D c = {D,Z} and parameters 0 can be written as: 
{ fi{ 
In P(Dc,01,) = In P(0lt)I-[ P(ilx('),x))P(y(')lx('),O i 
t=l 
(3.1) 
where X is a set of hyperparameters. Assuming separable priors on the parameters of the 
modeli.e. P(01X)= P x)l& P 0il& with X= {&.}i=o, (3.1)can be rewritten as: 
In P(Dc,01 ) = E E z ')In P( ilx{, x))+ In 
t=l i=1 
+{z')lnP(yt')lxt'),O)+lnP(Oil&)} 
i=l t=l 
(3.2) 
Interpolating Earth-science Data using RB FN and Mixtures of Experts 991 
Since the posterior probability of the model parameters is proportional to the joint 
probability, maximizing (3.2) is equivalent to maximizing the log posterior. In the E- 
step, the observed data and the current network parameters are used to compute the 
expected value of the complete-data log joint probability: 
N m 
Q(010 (k))= Y Y (k)(t)In P(ilx <') , x))+ In 
t=l i=1 
+ ((t)lnP(ytlx�',Oi)+lnP(Oil, 
In the M-step, Q(010 (0) is maximized with respect to 0 to obtain 0 q'+0 . As a result of 
the use of the indicator variables, the problem is alecoupled into a separate set of interim 
MAP estimations: 
N m 
x) (+') =argmaxYYhJ)(t)lnP(ilx('),x))+lnP(x)l&) (3.5) 
> t=l i:1 
N 
01 +') : arg max E I t)(t ) In P(Y t')lx�) , O, )+ In P(0il , ) 
0f /:1 
(3.6) 
We assume a flat prior for the gating network parameters and the prior 
P(0i I&) = exp(--} &   % w,q(% -%))l Z R (&) where Z R (X,) is a normalization 
r=l s:l 
cons!ant, for the experts. This smoothness prior is used on the GRBF networks because 
it can be derived from regularization theory (Girosi & Poggio, 1990) and at the same 
time is consistent with the interpretation of the radial basis function as a covariation 
model. Hence, maximizing 0i with (3.6) is equivalent to minimizing the cost function: 
where &' = & (yi2. The v
