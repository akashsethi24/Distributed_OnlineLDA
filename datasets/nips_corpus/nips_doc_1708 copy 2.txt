v-Arc: Ensemble Learning 
in the Presence of Outliers 
G. Ritsch t, B. Sch51kopf t, A. Smola*, 
K.-R. Miillert, T. Onoda**, and S. Mika* 
t GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany 
Microsoft Research, I Guildhall Street, Cambridge CB2 3NH, UK 
� Dep. of Engineering, ANU, Canberra ACT 0200, Australia 
t CRIEPI, 2-11-1, Iwado Kita, Komae-shi, Tokyo, Japan 
{raetsch, klaus, mika}first.gmd.de, bscmicrosoft. com, 
Alex. Smolaanu. edu. au, onodacriepi. denken. or. j p 
Abstract 
AdaBoost and other ensemble methods have successfully been ap- 
plied to a number of classification tasks, seemingly defying prob- 
lems of overfitting. AdaBoost performs gradient descent in an error 
function with respect to the margin, asymptotically concentrating 
on the patterns which are hardest to learn. For very noisy prob- 
lems, however, this can be disadvantageous. Indeed, theoretical 
analysis has shown that the margin distribution, as opposed to just 
the minimal margin, plays a crucial role in understanding this phe- 
nomenon. Loosely speaking, some outliers should be tolerated if 
this has the benefit of substantially increasing the margin on the 
remaining points. We propose a new boosting algorithm which al- 
lows for the possibility of a pre-specified fraction of points to lie in 
the margin area or even on the wrong side of the decision boundary. 
1 Introduction 
Boosting and related Ensemble learning methods have been recently used with great 
success in applications such as Optical Character Recognition (e.g. [8, 16]). 
The idea of a large minimum margin [17] explains the good generalization perfor- 
mance of AdaBoost in the low noise regime. However, AdaBoost performs worse 
on noisy tasks [10, 11], such as the iris and the breast cancer benchmark data sets 
[1]. On the latter tasks, a large margin on all training points cannot be achieved 
without adverse effects on the generalization error. This experimental observation 
was supported by the study of [13] where the generalization error of ensemble meth- 
ods was bounded by the sum of the fraction of training points which have a margin 
smaller than some value p, say, plus a complexity term depending on the base hy- 
potheses and p. While this bound can only capture part of what is going on in 
practice, it nevertheless already conveys the message that in some cases it pays to 
allow for some points which have a small margin, or are misclassified, if this leads 
to a larger overall margin on the remaining points. 
To cope with this problem, it was mandatory to construct regularized variants of 
AdaBoost, which traded off the number of margin errors and the size of the margin 
562 G. Riitsch, B. Sch6lkopf, A. J. Smola, K.-R. Mgller, T. Onoda andS. Mika 
[9, 11]. This goal, however, had so far been achieved in a heuristic way by introduc- 
ing regularization parameters which have no immediate interpretation and which 
cannot be adjusted easily. 
The present paper addresses this problem in two ways. Primarily, it makes an algo- 
rithmic contribution to the problem of constructing regularized boosting algorithms. 
However, compared to the previous efforts, it parameterizes the above trade-off in 
a much more intuitive way: its only free parameter directly determines the fraction 
of margin errors. This, in turn, is also appealing from a theoretical point of view, 
since it involves a parameter which controls a quantity that plays a crucial role in 
the generalization error bounds (cf. also [9, 13]). Furthermore, it allows the user 
to roughly specify this parameter once a reasonable estimate of the expected error 
(possibly from other studies) can be obtained, thus reducing the training time. 
2 Boosting and the Linear Programming Solution 
Before deriving a new algorithm, we briefly discuss the properties of the solution 
generated by standard AdaBoost and, closely related, Arc-GV [2], and show the 
relation to a linear programming (LP) solution over the class of base hypotheses G. 
Let {gt(x) � t = 1,... ,T} be a sequence of hypotheses and c = [o1...OT] their 
weights satisfying ct > 0. The hypotheses gt are elements of a hypotheses class 
G = {g' x + [-1, 1]}, which is defined by a base learning algorithm. 
The ensemble generates the label which is the weighted majority of the votes by 
sign(f(x)) where f(x)- iillgz(x). (1) 
In order to express that f and therefore also the margin p depend on c and for ease 
of notation we define 
p(z,a) := yf(x) where z := (x,y) and f is defined as in (1). (2) 
Likewise we use the normalized margin: 
p(c) := min p(zi, or) (3) 
l<i<m ' 
Ensemble learning methods have to find both, the hypotheses gt  G used for the 
combination and their weights c. In the following we will consider only AdaBoost 
algorithms (including Arcing). For more details see e.g. [4, 2]. The main idea of 
AdaBoost is to introduce weights wt(zi) on the training patterns. They are used to 
control the importance of each single pattern in learning a new hypothesis (i.e. while 
repeatedly running the base algorithm). Training patterns that are difficult to learn 
(which are misclassified repeatedly) become more important. 
The minimization objective of AdaBoost can be expressed in terms of margins as 
(c,) := y exp(-Ilc, llp(zi, c,)) . (4) 
i----1 
In every iteration, AdaBoost tries to minimize this error by a stepwise maximization 
of the margin. It is widely believed that AdaBoost tries to maximize the smallest 
margin on the training set [2, 5, 3, 13, 11]. Strictly speaking, however, a general 
proof is missing. It would imply that AdaBoost asymptotically approximates (up to 
scaling) the solution of the following linear programming problem over the complete 
hypothesis set G (cf. [7], assuming a finite number of basis hypotheses): 
maximize p 
subject to p(zi,e) >p for alll<i<rn 
ct,p>_0 for alll<t<lGI (5) 
Ilalll-- 1 
u-Arc: Ensemble Learning in the Presence of Outliers 563 
Since such a linear program cannot be solved exactly for a infinite hypothesis set 
in general, it is interesting to analyze approximation algorithms for this kind of 
problems. 
Breiman [2] proposed a modification of AriaBoost - Arc-GV - making it possible 
to show the asymptotic convergence of p(ot t) to the global solution pip: 
Theorem I (Breiman [2]). Choose at in each iteration as 
at := argmin  exp [-II*ll (P(Zi, Oft) -- P(O?-))] , (6) 
ce[O,1]  
and assume that the base learner always finds the hypothesis g  G which minimizes 
the weighted training error with respect to the weights. Then 
lira p(c t) = pip. 
Note that the algorithm above can be derived from the modified error function 
gv(O?) :: E exp [-[[otl[1 (p(zi, o t) - p(t-))]. (7) 
i 
The question one might ask now is whether to use AdaBoost or rather Arc-GV 
in practice. Does Arc-GV converge fast enough to benefit from its asymptotic 
properties? In [12] we conducted experiments to investigate this question. We 
empirically found that (a) AdaBoost has problems finding the optimal combination 
if plp < 0, (b) Arc-GV's convergence does not depend on plp, and (c) for plp > 0, 
AdaBoost usually converges to the maximum margin solution slightly faster than 
Arc-GV. Observation (a) becomes clear from (4): (c) will not converge to 0 and 
IIlll can be bounded by some value. Thus the asymptotic case cannot be reached, 
whereas for Arc-GV the optimum is always found. 
Moreover, the number of iterations necessary to converge to a good solution seems to 
be reasonable, but for a near optimal solution the number of iterations is rather high. 
This implies that for real world hypothesis sets, the number of iterations needed 
to find an almost optimal solution can become prohibitive, but we conjecture that 
in practice a reasonably good approximation to the optimum is provided by both 
AdaBoost and Arc-GV. 
3 v-Algorithms 
For the LP-AdaBoost [7] approach it has been shown for noisy problems that the 
generalization performance is usually not as good as the one of AdaBoost [7, 2, 11]. 
From Theorem 5 in [13] (cf. Theorem 3 on page 5) this fact becomes clear, as 
the minimum of the right hand side of inequality (cf. (13)) need not necessarily be 
achieved with a maximum margin. We now propose an algorithm to directly control 
the number of margin errors and therefore also the contribution of both terms in the 
inequality separately (cf. Theorem 3). We first consider a small hypothesis class 
and end up with a linear program - v-LP-AdaBoost. In subsection 3.2 we then 
combine this algorithm with the ideas from section 2 and get a new algorithm - 
v-Arc - which approximates the v-LP solution. 
3.1 v-LP-AdaBoost 
Let us consider the case where we are given a (finite) set G = {g: x + [-1, 1]} of T 
hypotheses. To find the coefficients c for the combined hypothesis f(x) we extend 
the LP-AdaBoost algorithm [7, 11] by incorporating the parameter v [15] and solve 
the following linear optimization problem: 
m 
1 Ei----1 i 
maximize P ,m 
subject to p(zi,c)_>p-i for alll<i<m 
- - (8) 
i,at,P_>0 for alll_<t_<Tandl<i<m 
I1111- 1 
564 G. Riitsch, B. SchOlkopf, A. J. Smola, K.-R. Miiller, T. Onoda andS. Mika 
This algorithm does not force all margins to be beyond zero and we get a soft 
margin classification (cf. SVMs) with a regularization constant i . The following 
proposition shows that v has an immediate interpretation: 
Proposition 2 (Rfitsch et al. [12]). Suppose we run the algorithm given in (8) 
on some data with the resulting optimal p > O. Then 
1. v upper bounds the fraction of margin errors. 
2. 1 - v upper bounds the fraction of patterns with margin larger than p. 
Since the slack variables i only enter the cost function linearly, their absolute size 
is not important. Loosely speaking, this is due to the fact that for the optimum 
of the primal objective function, only derivatives wrt. the primal variables matter, 
and the derivative of a linear function is constant. 
In the case of SVMs
