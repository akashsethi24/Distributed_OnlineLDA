442 
How Neural Nets Work 
Alan Lapedes 
Robert Father 
Theoretical Division 
Los Alamos National Laboratory 
Los Alamos, NM 87545 
Abstract: 
There is presently great interest in the abilities of neural networks to mimic 
qualitative reasoningby manipulating neural incodings of symbols. Less work 
has been performed on using neural networks to process floating point nurnbers 
and it is sometimes stated that neural networks are somehow inherently inaccu- 
rate and therefore best suited for fuzzyqualitative reasoning. Nevertheless, 
the potential speed of massively parallel operations make neural net number 
crunchingan interesting topic to explore. In this paper we discuss some of our 
work in which we demonstrate that for certain applications neural networks can 
achieve significantly higher numerical accuracy than more conventional tech- 
niques. In particular, prediction of future values of a chaotic time series can 
be performed with exceptionally high accuracy. We analyze how a neural net 
is able to do this , and in the process show that a large class of functions from 
R  -- R 'n may be accurately approximated by a backpropagation neural net 
with just two hiddenlayers. The network uses this functional approximation 
to perform either interpolation (signal processing applications) or extrapolation 
(symbol processing applications I. Neural nets therefore use quite familiar meth- 
ods to perform their tasks. The geometrical viewpoint advocated here seems to 
be a useful approach to analyzing neural network operation and relates neural 
networks to well studied topics in functional approximation. 
1. Introduction 
Although a great deal of interest has been displayed in neural network's 
capabilities to perform a kind of qualitative reasoning, relatively little work has 
been done on the ability of neural networks to process floating point nurnbers 
in a massively parallel fashion. Clearly, this is an important ability. In this 
paper we discuss some of our work in this area and show the relation between 
numerical, and symbolic processing. We will concentrate on the the subject of 
accurate prediction in a time series. Accurate prediction has applications in 
many areas of signal processing. It is also a useful, and fascinating ability, when 
dealing with natural, physical systems. Given some data from the past history 
of a system, can one accurately predict what it will do in the future? 
Many conventional signal processing tests, such as correlation function anal- 
ysis, cannot distinguish deterministic chaotic behavior from from stochastic 
noise. Particularly difficult systems to predict are those that are nonlinear and 
chaotic. Chaos has a technical definition based on nonlinear, dynamical systems 
theory, but intuitivly means that the system is deterministic but random,in 
a rather similar manner to deterministic, pseudo random number generators 
used on conventional computers. Examples of chaotic systems in nature include 
turbulence in fluids (D. Ruelie, 1971; H. Swinney, 1978), chemical reactions (K. 
Tomira, 1979), lasers (H. Haken, 1975), plasma physics (D. Russel, 1980) to 
name but a few. Typically, chaotic systems also display the full range of non- 
linear behavior (fixed points, limit cycles etc.) when parameters are varied, and 
therefore provide a good testbed in which to investigate techniques of nonlinear 
signal processing. Clearly, if one can uncover the underlying, deterministic al- 
gorithm from a chaotic time series, then one may be able to predict the future 
time series quite accurately. 
@ American Institute of Physics 1988 
443 
In this paper we review and extend our work (Lapedes and Farber,1987) 
on predicting the behavior of a particular dynamical system, the Glass-Mackey 
equation. We feel that the method will be fairly general, and use the Glass- 
Mackey equation solely for illustrative purposes. The Glass-Mackey equation 
has a strange attractor with fractal dimension controlled by a constant param- 
eter appearing in the differential equation. We present results on a neural net- 
work's ability to predict this system at two values of this parameter, one value 
corresponding to the onset of chaos, and the other value deeply in the chaotic 
regime. We also present the results of more conventional predictive methods and 
show that a neural net is able to achieve significantly better numerical accuracy. 
This particular system was chosen because of D. Farmer's and J. Sidorowich's 
(D. Farmer, J. Sidorowich, 1987) use of it in developing a new, non-neural net 
method for predicting chaos. The accuracy of this non-neural net method, and 
the neural net method, are roughly equivalent, with various advantages or dis- 
advantages accruing to one method or the other depending on one's point of 
view. We are happy to acknowledge many valuable discussions with Farmer and 
Sidorowich that has led to further improvements in each method. 
We also show that a neural net never needs more than two hidden layers to 
solve most problems. This statement arises from a more general argument that 
a neural net can approximate functions from R a - R m with only two hidden 
layers, and that the accuracy of the approximation is controlled by the number 
of neurons in each layer. The argument assumes that the global minimum to the 
backpropagation minimization problem may be found, or that a local minima 
very close in value to the global minimum may be found. This seems to be 
the case in the examples we considered, and in many examples considered by 
other researchers, but is never guaranteed. The conclusion of an upper bound 
of two hidden layers is related to a similar conclusion of R. Lipman (R. Lipman, 
1987) who has previously analyzed the number of hidden layers needed to form 
arbitrary decision regions for symbolic processing problems. Related issues are 
discussed by J. Denker (J. Denker et.al. 1987) It is easy to extend the argument 
to draw similar conclusions about an upper bound of two hidden layers for 
symbol processing and to place signal processing, and symbol processing in a 
common theoretical framework. 
2. Backpropagation 
Backpropagation is a learning algorithm for neural networks that seeks to 
find weights, TQ., such that given an input pattern from a training set of pairs 
of Input/Output patterns, the network will produce the Output of the training 
set given the Input. Having learned this mapping between I and O for the 
training set, one then applies a new, previously unseen Input, and takes the 
Output as the conclusiondrawn by the neural net based on having learned 
fundamental relationships between Input and Output from the training set. A 
popular configuration for backpropagation is a totally feedforward net (Figure 
1) where Input feeds up through hidden layersto an Output layer. 
444 
OUTPUT 
Figure 1. 
A feedforward neural 
HDEN net. Arrows schemat- 
ically indicate full 
feedforward connect- 
ivity 
PUT 
Ech neuron form a weighted sum of the inputs from previous layers to 
which it is connected, adds a threshold value, and produces a nonlinear function 
of this sum s its output value. This output value serves m input to the future 
layers to which the neuron is connected, and the proces is repeated. Ultimately 
a value is produced for the outputs of the neurons in the Output layer. Thus, 
each neuron perform.: 
(1) 
where T i are continuous valued, positive or negative weights, 8i is a constant, 
ï¿½ nd g(x) is a nonlinear function that is often chosen to be of a sigmoidal form. 
For example, one may choose 
1 
g(x): (1 + ,a.hx) (2) 
where tanh is the hyperbolic tangent, although the exact formula of the sigmoid 
is irrelevant to the results. 
If i are the target output values for the pO, Input pattern then ones trains 
the network by minire!zing 
p 
(3) 
where t is the target output values (taken from the training set) and 0 i 
is the output of the network when the pth Input pattern of the training set is 
presented on the Input layer. i indexes the number of neurons in the Output 
layer. 
An iterative procedure is used to minimize E. For example, the commonly 
used steepest descents procedure is implemented by changing Tii and 01 by AT O. 
and A0i where 
445 
E 
ATo' -- ----'. '  (4a) 
E 
Ai -- -qS-. ' e (4b) 
This implies that AE < 0 and hence E will decrease to a local minimum. 
Use of the chain rule and definition of some intermediate quantities allows the 
following expressions for ATi. to be obtained (Rumelhart, 1987): 
where 
if i is labeling a neuron in the Output layer; and 
(5b) 
(6) 
if i labels a neuron in the hidden layers. Therefore one computes 5r) for the 
Output layer first, then uses Eqn. (7) to computer 57) for the hidden layers, 
and finally uses Eqn. (5) to make an adjustment to the weights. We remark that 
the steepest descents procedure in common use is extremely slow in simulation, 
and that a better minimization procedure, such az the classic conjugate gradient 
procedure (W. Press, 1986), can offer quite significant speedups. Many appli- 
cations use bit representations (0,1) for symbols, and attempt to have a neural 
net learn fundamental relationships between the symbols. This procedure has 
been successfully used in converting text to speech (T. Sejnowski, 1986) and in 
determining whether a given fragment of DNA codes for a protein or not (A. 
Lapedes, R. Farber, 1987). 
There is no fundamental reason, however, to use integer's az values for Input 
and Output. If the Inputs and Outputs are instead a collection of floating point 
numbers, then the network, fter training, yields a specific continuous function 
in n variables (for n inputs) involving g(x) (i.e. hyperbolic tanh's) that provides 
a type of nonlinear, least mean square interpolant formula for the discrete set 
of data points in the training set. Use of this formula 0 -- f(lt,l,...l,) 
when given a new input not in the training
