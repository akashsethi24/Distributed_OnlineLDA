586 
STATISTICAL PREDICTION WITH KANERVA'S 
SPARSE DISTRIBUTED MEMORY 
David Rogers 
Research Institute for Advanced Computer Science 
MS 230-5, NASA Ames Research Center 
Moffett Field, CA 94035 
ABSTRACT 
A new viewpoint of the processing performed by Kanerva's sparse 
distributed memory (SDM) is presented. In conditions of near- or 
over- capacity, where the associative-memory behavior of the mod- 
el breaks down, the processing performed by the model can be inter- 
preted as that of a statistical predictor. Mathematical results are 
presented which serve as the framework for a new statistical view- 
point of sparse distributed memory and for which the standard for- 
mulation of SDM is a special case. This viewpoint suggests possi- 
ble enhancements to the SDM model, including a procedure for 
improving the predictiveness of the system based on Holland's 
work with 'Genetic Algorithms', and a method for improving the 
capacity of SDM even when used as an associative memory. 
OVERVIEW 
This work is the result of studies involving two seemingly separate topics that 
proved to share a common framework. The first topic, statistical prediction, is the 
task of associating extremely large perceptual state vectors with future events. The 
second topic, over-capacity in Kanerva's sparse distributed memory (SDM), is a 
study of the computation done in an SDM when presented with many more associa- 
tions than its stated capacity. 
I propose that in conditions of over-capacity, where the associative-memory behav- 
ior of an SDM breaks down, the processing performed by the SDM can be used for 
statistical prediction. A mathematical study of the prediction problem suggests a 
variant of the standard SDM architecture. This variant not only behaves as a statisti- 
cal predictor when the SDM is filled beyond capacity but is shown to double the 
capacity of an SDM when used as an associative memory. 
THE PREDICTION PROBLEM 
The earliest living creatures had an ability, albeit limited, to perceive the world 
through crude senses. This ability allowed them to react to changing conditions in 
Statistical Prediction with Kanerva's Sparse Distributed Memory 587 
the environment; for example, to move towards (or away from) light sources. As 
nervous systems developed, learning was possible; if food appeared simultaneously 
with some other perception, perhaps some odor, a creature could learn to associate 
that smell with food. 
As the creatures evolved further, a more rewarding type of learning was possible. 
Some perceptions, such as the perception of pain or the discovery of food, are very 
important to an animal. However, by the time the perception occurs, damage may 
already be done, or an opportunity for gain missed. If a creature could learn to asso- 
ciate current perceptions with future ones, it would have a much better chance to 
do something about it before damage occurs. This is the prediction problem. 
The difficulty of the prediction problem is in the extremely large number of possi- 
ble sensory inputs. For example, a simple animal might have the equivalent of 1000 
bits of sensory data at a given time; in this case, the number of possible inputs is 
greater than the number of atoms in the known universe! In essence, it is an enor- 
mous search problem: a living creature must find the subregions of the perceptual 
space which correlate with the features of interest. Most of the gigantic perceptual 
space will be uncorrelated, and hence uninteresting. 
THE OVERCAPACITY PROBLEM 
An associative memory is a memory that can recall data when addressed 'close-to' an 
address where data were previously stored. A number of designs for associative 
memories have been proposed, such as Hopfield networks (Hopfield, 1986) or the 
nearest-neighbor associative memory of Baum, Moody, and Wilczek (1987). Memo- 
ry-related standards such as capacity are usually selected to judge the relative perfor- 
mance of different models. Performance is severely degraded when these memories 
are filled beyond capacity. 
Kanerva's sparse distributed memory is an associative memory model developed from 
the mathematics of high-dimensional spaces (Kanerva, 1988) and is related to the 
work of David Mart (1969) and James Albus (1971) on the cerebellum of the brain. 
(For a detailed comparison of SDM to random-access memory, to the cerebellum, 
and to neural-networks, see (Rogers, 1988b)). Like other associative memory mod- 
els, it exhibits non-memory behavior when near- or over- capacity. 
Studies of capacity are often over-simplified by the common assumption of uncorre- 
lated random addresses and data. The capacity of some of these memories, including 
SDM, is degraded if the memory is presented with correlated addresses and data. 
Such correlations are likely if the addresses and data are from a real-world source. 
Thus, understanding the over-capacity behavior of an SDM may lead to better proce- 
dures for storing correlated data in an associative memory. 
588 Rogers 
SPARSE DISTRIBUTED MEMORY 
Sparse distributed memory can be best illustrated as a variant of random-access mem- 
ory (RAM). The structure of a twelve-location SDM with ten-bit addresses and 
ten-bit data is shown in figure 1. (Kanerva, 1988) 
Reference Address Radius InputData 
I �,�,�,O,l�l I ol'ol oi ,! ,I ,Iol ,Iol,I 
1101100111 
1010101010 
0000011110 
0011011001 
1011101100 
0010101111 
1101101101 
0100000110 
0110101001 
1011010110 
1100010111 
1111110011 
Location 
Addresses 
Sums-3 -5-3 5 
oat0  i         
Output Data 0 0 0 1 
Figure 1. Structure of a Sparse Distributed Memory 
Data 
Counters 
A memory location is a row in this figure. The location addresses are set to random 
addresses. The data counters are initialized to zero. All operations begin with 
addressing the memory; this entails finding the Hamming distance between the refer- 
ence address and each of the location addresses. If this distance is less than or equal 
to the Hamming radius, the select-vector entry is set to 1, and that location is 
termed selected. The ensemble of such selected locations is called the selected set. 
Selection is noted in the figure as non-gray rows. A radius is chosen so that only a 
small percentage of the memory locations are selected for a given reference address. 
(Later, we will refer to the fact that a memory location defines an activation set of 
addresses in the address space; the activation set corresponding to a location is the 
set of reference addresses which activate that memory location. Note the reciprocity 
Statistical Prediction with Kanerva's Sparse Distributed Memory 589 
between the selected set corresponding to a given reference address, and the activa- 
tion set corresponding to a given location.) 
When writing to the memory, all selected counters beneath elements of the input 
data equal to 1 are incremented, and all selected counters beneath elements of the 
input data equal to 0 are decremented. This completes a write operation. When 
reading from the memory, the selected data counters are summed columnwise into 
the register sums. If the value of a sum is greater than or equal to zero, we set the 
corresponding bit in the output data to 1; otherwise, we set the bit in the output 
data to 0. (When reading, the contents of the input data are ignored.) 
This example makes clear that a datum is distributed over the data counters of the 
selected locations when writing, and that the datum is reconstructed during reading 
by averaging the sums of these counters. However, depending on what additional 
data were written into some of the selected locations, and depending on how these 
data correlate with the original data, the reconstruction may contain noise. 
THE BEHAVIOR OF AN SDM WHEN AT OVER-CAPACITY 
Consider an SDM with a 1,000-bit address and a 1-bit datum. In this memory, we 
are storing associations that are samples of some binary function f on the space S of 
all possible addresses. After storing only a few associations, each data counter will 
have no explicit meaning, since the data values stored in the memory are distributed 
over many locations. However, once a sufficiently large number of associations are 
stored in the memory, the data counter gains meaning: when appropriately normal- 
ized to the interval [0, 1], it contains a value which is the conditional probability 
that the data bit is 1, given that its location was selected. This is shown in figure 2. 
[0 or 1] 
� S is the space of all possible addresses 
� L is the set of addresses in S which activate 
a given memory location 
� f is a binary function on S that we want 
to estimate using the memory 
� The data counter for L contains the average value 
of f over L, which equals P(f(X) = 1 I X  L ) 
Figure 2. The Normalized Content of a Data Counter is the Conditional 
Probability of the Value of f Being Equal to 1 Given the Reference 
Addresses are Restricted to the Sphere L. 
In the prediction problem, we want to find activation sets of the address space that 
correlate with some desired feature bit. When filled far beyond capacity, the indi- 
590 Rogers 
vidual memory locations of an SDM are collecting statistics about individual subre- 
gions of the address space. To estimate the value of f at a given address, it should 
be possible to combine the conditional probabilities in the data counters of the 
selected memory locations to make a best guess. 
In the prediction problem, S is the space of possible sensory inputs. Since most 
regions of S have no relationship with the datum we wish to predict, most of the 
memory locations will be in non-informative regions of the address space. Associa- 
tive memories are not useful for the prediction problem because the key part of the 
problem is the search for subregions of the address space that are informative. Due 
to capacity limitations and the extreme size of the address space, memories fill to 
capacity and fail before eno
