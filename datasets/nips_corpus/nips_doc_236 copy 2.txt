676 Baum 
The Perceptron Algorithm Is Fast for 
Non-Malicious Distributions 
Erice B. Baum 
NEC Research Institute 
4 Independence Way 
Princeton, NJ 08540 
Abstract
Within the context of Valiant's protocol for learning, the Perceptron 
algorithm is shown to learn an arbitrary half-space in time O(�r) if D, the proba- 
bility distribution of examples, is taken uniform over the unit sphere $n. Here  is 
the accuracy parameter. This is surprisingly fast, as standard approaches involve 
solution of a linear programming problem involving () constraints in n dimen- 
sions. A modification of Valiant's distribution independent protocol for learning 
is proposed in which the distribution and the function to be learned may be cho- 
sen by adversaries, however these adversaries may not communicate. It is argued 
that this definition is more reasonable and applicable to real world learning than 
Valiant's. Under this definition, the Perceptron algorithm is shown to be a distri- 
bution independent learning algorithm. In an appendix we show that, for uniform 
distributions, some classes of infinite V-C dimension including convex sets and a 
class of nested differences of convex sets are learnable. 
�1: Introduction 
The Perceptton algorithm was proved in the early 1960s[Rosenblatt,1962] to 
converge and yield a half space separating any set of linearly separable classified 
examples. Interest in this algorithm waned in the 1970's after it was empha- 
sized[Minsky and Papert, 1969] (1) that the class of problems solvable by a single 
half space was limited, and (2) that the Perceptton algorithm, although converg- 
ing in finite time, did not converge in polynomial time. In the 1980's, however, it 
has become evident that there is no hope of providing a learning algorithm which 
can learn arbitrary functions in polynomial time and much research has thus been 
restricted to algorithms which learn a function drawn from a particular class of 
functions. Moreover, learning theory has focused on protocols like that of [Valiant, 
1984] where we seek to classify, not a fixed set of examples, but examples drawn 
from a probability distribution. This allows a natural notion of generalization. 
There are very few classes which have yet been proven learnable in polynomial time, 
and one of these is the class of half spaces. Thus there is considerable theoretical 
interest now in studying the problem of learning a single half space, and so it is 
natural to reexamine the Perceptron algorithm within the formalism of Valiant. 
The Perceptron Algorithm Is Fast for Non-Malicious Distributions 677 
In Valiant's protocol, a class of functions is called learnable if there is a learn- 
ing algorithm which works in polynomial time independent of the distribution D 
generating the examples. Under this definition the Perceptron learning algorithm 
is not a polynomial time learning algorithm. However we will argue in section 2 
that this definition is too restrictive. We will consider in section 3 the behavior of 
the Perceptron algorithm if D is taken to be the uniform distribution on the unit 
sphere $n. In this case, we will see that the Perceptron algorithm converges re- 
markably rapidly. Indeed we will give a time bound which is faster than any bound 
known to us for any algorithm solving this problem. Then, in section 4, we will 
present what we believe to be a more natural definition of distribution independent 
learning in this context, which we will call Nonmalicious distribution independent 
learning. We will see that the Perceptton algorithm is indeed a polynomial time non- 
malicious distribution independent learning algorithm. In Appendix A, we sketch 
proofs that, if one restricts attention to the uniform distribution, some classes with 
infinite Vapnik-Chervonenkis dimension such as the class of convex sets and the 
class of nested differences of convex sets (which we define) are learnable. These 
results support our assertion that distribution independence is too much to ask for, 
and may also be of independent interest. 
�2: Distribution Independent Learning 
In Valiant's protocol[Valiant, 1984], a class F of Boolean functions on n is 
called learnable if a learning algorithm A exists which satisfies the following condi- 
tions. Pick some probability distribution D on . A is allowed to call examples, 
which are pairs (x, f(x)), where x is drawn according to the distribution D. A is a 
valid learning algorithm for F if for any probability distribution D on , for any 
0 < 6, e < 1, for any f  F, A calls examples and, with probability at least 1 - 6 
outputs in time bounded by a polynomial in n, 6 -, and e- a hypothesis g such 
that the probability that f(x)  g(x) is less than e for x drawn according to D. 
This protocol includes a natural formalization of 'generalization' as predic- 
tion. For more discussion see [Valiant, 1984]. The definition is restrictive in de- 
manding that A work for an arbitrary probability distribution D. This demand 
is suggested by results on uniform convergence of the empirical distribution to the 
actual distribution. In particular, if F has Vapnik-Chervonenkis (V-C) dimension ; 
d, then it has been proved[Blumer et al, 1987] that all A needs to do to be a valid 
learning algorithm is to call Mo(e, 5, d) = rnac(log,-log) examples and to 
find in polynomial time a function #  F which correctly classifies these. 
Thus, for example, it is simple to show that the class H of half spaces is 
Valiant learnable[Blumer et al, 1987]. The V-C dimension of H is n + 1. All we 
need to do to learn H is to call Mo(e, 6, n + 1) examples and find a separating half 
space using Karmarkar's algorithm [Karmarkar, 1984]. Note that the Perceptton 
algorithm would not work here, since one can readily find distributions for which 
the Perceptron algorithm would be expected to take arbitrarily long times to find 
a separating half space. 
I We say a set S C R is shattered by a class F of Boolean functions if F 
induces all Boolean functions on S. The V-C dimension of F is the cardinality of 
the largest set S which F shatters. 
678 Baum 
Now, however, it seems from three points of view that the distribution inde- 
pendent definition is too strong. First, although the results of [Blumer et al., 1987] 
tell us we can gather enough information for learning in polynomial time, they say 
nothing about when we can actually find an algorithm A which learns in polynomial 
time. So far, such algorithms have only been found in a few cases, and (see, e.g. 
[Baum, 1989a]) these cases may be argued to be trivial. 
Second, a few classes of functions have been proved (modulo strong but plau- 
sible complexity theoretic hypotheses) unlearnable by construction of cryptograph- 
icaJly secure subclasses. Thus for example [Kearns and Valiant, 1988] show that 
the class of feedforward networks of threshold gates of some constant depth, or of 
Boolean gates of logarithmic depth, is not learnable by construction of a crypto- 
graphically secure subclass. The relevance of such results to learning in the natural 
world is unclear to us. For example, these results do not rule out a learning al- 
gorithm that would learn almost any log depth net. We would thus prefer a less 
restrictive definition of learnability, so that if a class were proved unlearnable, it 
would provide a meaningful limit on pragmatic learning. 
Third, the results of [Blumer et al, 1987] imply that we can only expect to learn 
a class of functions F if F has finite V-C dimension. Thus we are in the position 
of assuming an enormous amount of information about the class of functions to be 
learned- namely that it be some specific class of finite V-C dimension, but nothing 
whatever about the distribution of examples. In the real world, by contrast, we 
are likely to know at least as much about the distribution D as we know about the 
class of functions F. If we relax the distribution independence criterion, then it can 
be shown that classes of infinite Vapnik-Chervonenkis dimension are learnable. For 
example, for the uniform distribution, the class of convex sets and a class of nested 
differences of convex sets ( both of which trivially have infinite V-C dimension) are 
shown to be learnable in Appendix A. 
�3: The Perceptron Algorithm and Uniform Distributions 
The Perceptton algorithm yields, in finite time, a half-space (wt,0t) which 
correctly classifies any given set of linearly separable examples [Posenblatt,1962]. 
That is, given a set of classified examples {z. } such that, for some (w,, 0,), w,. z. > 
0, and w, . a:  _ < 0, for all/, the algorithm converges in finite time to output a 
(wt, 0t) such that wt � a: _> 0t and wt � a:  _ < 0t. We will normalize so that 
,,. u, = 1. Note that Iw,. a:- 0,1 is the Euclidean distance from z to the separating 
hyperplane {y: w, .y = 0,}. 
The algorithm is the following. Start with some initial candidate (wo,0o), 
which we will take to be (l, 0). Cycle through the examples. For each example, test 
whether that example is correctly classified. If so, proceed to the next example. If 
not, modify the candidate by 
(wk+ = wk + 0k+ = 0k 1) 
(1) 
where the sign of the modification is determined by the classification of the miss- 
classified example. 
In this section we will apply the Perceptron algorithm to the problem of learning 
The Perceptron Algorithm Is Fast for Non-Malicious Distributions 679 
in the probabilistic context described in section 2, where however the distribution 
D generating examples is uniform on the unit sphere S n. Rather than have a 
fixed set of examples, we apply the algorithm in a slightly novel way: we call an 
example, perform a Perceptton update step, discard the example, and iterate until 
we converge to accuracy e. I2 If we applied the Perceptton algorithm in the standard 
way, it seemingly would not converge as rapidly. We will return to this point at the 
end 
