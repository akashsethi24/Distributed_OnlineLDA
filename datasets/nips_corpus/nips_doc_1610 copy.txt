Neural Representation of Multi-Dimensional 
Stimuli 
Christian W. Eurich, Stefan D. Wilke and Helmut Schwegler 
Institut ffir Theoretische Physik 
Universitit Bremen, Germany 
(eurich, swilke, schwegler) @physik. uni-bremen.de 
Abstract 
The encoding accuracy of a population of stochastically spiking neurons 
is studied for different distributions of their tuning widths. The situation 
of identical radially symmetric receptive fields for all neurons, which 
is usually considered in the literature, turns out to be disadvantageous 
from an information-theoretic point of view. Both a variability of tun- 
ing widths and a fragmentation of the neural population into specialized 
subpopulations improve the encoding accuracy. 
1 Introduction 
The topic of neuronal tuning properties and their functional significance has focused much 
attention in the last decades. However, neither empirical findings nor theoretical consider- 
ations have yielded a unified picture of optimal neural encoding strategies given a sensory 
or motor task. More specifically, the question as to whether narrow tuning or broad tuning 
is advantageous for the representation of a set of stimulus features is still being discussed. 
Empirically, both situations are encountered: small receptive fields whose diameter is less 
than one degree can, for example, be found in the human retina [7], and large receptive 
fields up to 180 � in diameter occur in the visual system of tongue-projecting salamanders 
[10]. On the theoretical side, arguments have been put forward for small [8] as well as for 
large [5, 1, 9, 3, 13] receptive fields. 
In the last years, several approaches have been made to calculate the encoding accuracy 
of a neural population as a function of receptive field size [5, 1, 9, 3, 13]. It has turned 
out that for a firing rate coding, large receptive fields are advantageous provided that D > 
3 stimulus features are encoded [9, 13]. For binary neurons, large receptive fields are 
advantageous also for D = 2 [5, 3]. 
However, so far only radially symmetric tuning curves have been considered. For neural 
populations which lack this symmetry, the situation may be very different. Here we study 
the encoding accuracy of a population of stochastically spiking neurons. A Fisher infor- 
mation analysis performed on different distributions of tunings widths will indeed reveal a 
much more detailed picture of neural encoding strategies. 
116 C. W. Eurich, S. D. Willre and H. Schwegler 
2 Model 
Consider a D-dimensional stimulus space, X. A stimulus is characterized by a position 
x = (zi,..., ZD)  X, where the value of feature i, zi (i = 1,..., D), is measured 
relative to the total range of values in the i-th dimension such that it is dimensionless. 
Information about the stimulus is encoded by a population of N stochastically spiking 
neurons. They are assumed to have independent spike generation mechanisms such that the 
joint probability distribution for observing n = (nO),..., n(k),..., n(iV)) spikes within a 
time interval r, Ps (n; x), can be written in the form 
N 
?s(n;x) = II ??)(.(*);x), 
k=l 
where P?) (n �; x) is the single-neuron probability distribution of the number of observed 
spikes given the stimulus at position x. Note that (1) does not exclude a correlation of the 
neural firing rates, i.e., the neurons may have common input or even share the same tuning 
function. 
The firing rates depend on the stimulus via the local values of the tuning functions, such that 
P?) (n(k); x) can be written in the form P?)(n(); x) = S (n � , f()(x), r), where the 
tuning function of neuron k, f(k) (x), gives its mean firing rate in response to the stimulus 
at position x. We assume here a form of the tuning function that is not necessarily radially 
symmetric, 
where c � = (c?),... ,c?) is the center of the tuning curve of neuron k, a? ) is its 
F()2 (tO,2. ()2 
tuning width in the i-th dimension, - :- (zi for i - 1, D, and 
-ci ) IO'i ..., 
(t)2 := ?)2 + ... + )2. F > 0 denotes the maximal firing rate of the neurons, which 
requires that maxz>0 b(z) = 1. 
We assume that the tuning widths o'? ) a ) of each neuron k are drawn from a distri- 
bution P, (Crl,..., CrD). For a population of tuning functions with centers c(1),..., c( �, a 
N 
density r/(x) is introduced according to r/(x) := Ek--1 (x - c �). 
The encoding accuracy can be quantified by the Fisher information matrix, J, which is 
defined as 
Jij(x):=E[(xilnP(n;x)) (lnP(n;x))], (3) 
where E[...] denotes the expectation value over the probability distribution P(n; x) [2]. 
The Fisher information yields a lower bound on the expected error of an unbiased estimator 
that retrieves the stimulus x from the noisy neural activity (Cram6r-Rao inequality) [2]. The 
minimal estimation error for the i-th feature xi, i,min, is given by e. 2 � : (j-1)ii which 
,mln 
reduces to e.2 . = 1/Jii(x) if J is diagonal. 
,mln 
We shall now derive a general expression for the population Fisher information. In the 
next chapter, several cases and their consequences for neural encoding strategies will be 
discussed. 
For model neuron (k), the Fisher information (3) reduces to 
(4) 
Neural Representation of Multi-Dimensional Stimuli 117 
where the dependence on the tuning widths is indicated by the list of arguments. The 
function AO depends on the shape of the tuning function and is given in [13]. The in- 
dependence assumption (1) implies that the population Fisher information is the sum of 
the contributions of the individual neurons, -N j)(x; a? ) , , a (k)) We now define 
k=l _ ' '.' .D ' . . 
a population Fisher information which is averaged over the distribution of tuning widths 
P,,(a,... 
Introducing the density of tuning curves, r/(x), into (5) and assuming a constant distri- 
bution, r/(x)   _-- const., one obtains the result that the population Fisher information 
becomes independent of x and that the off-diagonal elements of J vanish [ 13]. The average 
population Fisher information then becomes 
(Jij) =DK(F,r,D) ( Htl at ) 
2 
60, (6) 
where K depends on the geometry of the tuning curves and is defined in [ 13]. 
3 Results 
In this section, we consider different distributions of tuning widths in (6) and discuss ad- 
vantageous and disadvantageous strategies for obtaining a high representational accuracy 
in the neural population. 
Radially symmetric tuning curves. For radially symmetric tuning curves of width , 
the tuning-width distribution reads 
D 
,0�) = II - 
i=1 
see Fig. 1 a for a schematic visualization of the arrangement of the tuning widths for the 
case D = 2. The average population Fisher information (6) for i = j becomes 
(Jii) = riDK�(F, r, D)D-2, (7) 
a result already obtained by Zhang and Sejnowski [ 13]. Equation (7) basically shows that 
the minimal estimation error increases with  for D - 1, that it does not depend on  for 
D - 2, and that it decreases as  increases for D _> 3. We shall discuss the relevance of 
this case below. 
Identical tuning curves without radial symmetry. Next we discuss tuning curves which 
are identical but not radially symmetric; the tuning-width distribution for this case is 
D 
= II - 
i=1 
where  denotes the fixed width in dimension i. For i = j, the average population Fisher 
information (6) reduces to [ 11, 4] 
-2 (8) 
o' i 
118 C. W. Eurich, S. D. Vqlke and H. Schwegler 
(c) 
(a) 
,/ 
b 1 
(b) 
(d) 
IJ 1 IJ 1 
Figure 1: Visualization of different distributions of 
tuning widths for D - 2. (a) Radially symmetric tun- 
ing curves. The dot indicates a fixed , while the diag- 
onal line symbolizes a variation in  discussed in [13]. 
(b) Identical tuning curves which are not radially sym- 
metric. (c) Tuning widths uniformly distributed within 
a small rectangle. (d) Two subpopulations each of 
which is narrowly tuned in one dimension and broadly 
tuned in the other direction. 
Equation (8) contains (7) as a special case. From (8) it becomes immediately clear that the 
expected minimal square encoding error for the i-th stimulus feature, 2 
i,min- 1/(Jii(x))a, 
depends on i, i.e., the population specializes in certain features. The error obtained in 
dimension i thereby depends on the tuning widths in all dimensions. 
Which encoding strategy is optimal for a population whose task it is to encode a single 
feature, say feature i, with high accuracy while not caring about the other dimensions? In 
order to answer this question, we re-write (8) in terms of receptive field overlap. 
For the tuning functions f (k) (x) encountered empirically, large values of the single-neuron 
Fisher information (4) are typically restricted to a region around the center of the tuning 
function, c �. The fraction p(fl) of the Fisher information that falls into a region ED ' 
V/(k) 2 _< fl around c � is given by 
f dZx E/= JJ/)(x) 
p() := Er> 
f dVx =t J/)(x) 
x 
f d D+I Acp(2, F T) 
0 
f d t>+t A(2, F, r) 
0 
(9) 
where the index (k) was dropped because the tuning curves are assumed to have iden- 
tical shapes. Equation (9) allows the definition of an effective receptive field, RFr ), 
inside of which neuron k conveys a major fraction P0 of Fisher information, RF? := 
{xl V/- ()2 _< 0 }, where 0 is chosen such that P(fio): Po. The Fisher information a 
n,() 
neuron k carries is small unless x E *- eft � This has the consequence that a fixed stimulus 
x is actually encoded only by a subpopulation of neurons. The point x in stimulus space is 
covered by 
D 
2rt>/2(�)t> H J (10) 
Ncode :=. Dr(D/2) 
receptive fields. With the help of (10), the average population Fisher information (8) can 
be re-written as 
D2r( D /2) %ode 
(Jii)a = 27rD/2(o) D Kq(F,r,D) -2 (11) 
ai 
Equation (11) can be interpreted as follows: We assume that the population of neurons 
encodes stimulus dimension i accurately, while all other dimensions are of secondary im- 
portance. The average population Fisher information for dimension
