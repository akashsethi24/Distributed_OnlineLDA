Limitations of self-organizing maps for 
vector quantization and multidimensional 
scaling 
Arthur Flexer 
The Austrian Research Institute for Artificial Intelligence 
Schottengasse 3, A-1010 Vienna, Austria 
and 
Department of Psychology, University of Vienna 
Liebiggasse 5, A-1010 Vienna, Austria 
arthur@ai .univie. ac. at 
Abstract 
The limitations of using self-organizing maps (SOM) for either 
clustering/vector quantization (VQ) or multidimensional scaling 
(MDS) are being discussed by reviewing recent empirical findings 
and the relevant theory. SOM's remaining ability of doing both VQ 
and MDS at the same time is challenged by a new combined tech- 
nique of online K-means clustering plus Sammon mapping of the 
cluster centroids. SOM are shown to perform significantly worse in 
terms of quantization error, in recovering the structure of the clus- 
ters and in preserving the topology in a comprehensive empirical 
study using a series of multivariate normal clustering problems. 
1 Introduction 
Self-organizing maps (SOM) introduced by [Kohonen 84] are a very popular tool 
used for visualization of high dimensional data spaces. SOM can be said to do 
clustering/vector quantization (VQ) and at the same time to preserve the spatial 
ordering of the input data reflected by an ordering of the code book vectors (cluster 
centroids) in a one or two dimensional output space, where the latter property is 
closely related to multidimensional scaling (MDS) in statistics. Although the level 
of activity and research around the SOM algorithm is quite large (a recent overview 
by [Kohonen 95] contains more than 1000 citations), only little comparison among 
the numerous existing variants of the basic approach and also to more traditional 
statistical techniques of the larger frameworks of VQ and MDS is available. Ad- 
ditionally, there is only little advice in the literature about how to properly use 
446 A. Flexer 
SOM in order to get optimal results in terms of either vector quantization (VQ) or 
multidimensional scaling or maybe even both of them. To make the notion of SOM 
being a tool for data visualization more precise, the following question has to be 
answered: Should SOM be used for doing VQ, MDS, both at the same time or none 
of them? 
Two recent comprehensive studies comparing SOM either to traditional VQ or MDS 
techniques separately seem to indicate that SOM is not competitive when used for 
either VQ or MDS: [Balakrishnan et al. 94] compare SOM to K-means clustering 
on 108 multivariate normal clustering problems with known clustering solutions and 
show that SOM performs significantly worse in terms of data points misclassified 1, 
especially with higher numbers of clusters in the data sets. [Bezdek &: Nikhil 95] 
compare SOM to principal component analysis and the MDS-technique Sammon 
mapping on seven artificial data sets with different numbers of points and dimen- 
sionality and different shapes of input distributions. The degree of preservation of 
the spatial ordering of the input data is measured via a Spearman rank correla- 
tion between the distances of points in the input space and the distances of their 
projections in the two dimensional output space. The traditional MDS-techniques 
preserve the distances much more effectively than SOM, the performance of which 
decreases rapidly with increasing dimensionality of the input data. 
Despite these strong empirical findings that speak against the use of SOM for either 
VQ or MDS there remains the appealing ability of SOM to do both VQ and MDS at 
the same time. It is the aim of this work to find out, whether a combined technique 
of traditional vector quantization (clustering) plus MDS on the code book vectors 
(cluster centroids) can perform better than Kohonen's SOM on a series of multi- 
variate normal clustering problems in terms of quantization error (mean squared 
error), recovering the cluster structure (Rand index) and preserving the topology 
(Pearson correlation). All the experiments were done in a rigoruos statistical design 
using multiple analysis of variance for evaluation of the results. 
2 SOM and vector quantization/clustering 
A vector quantizer (VQ) is a mapping, q, that assigns to each input vector x a 
reproduction (code book) vector k: q(x) drawn from a finite reproduction alphabet 
fi = {i, i = 1,..., N}. The quantizer q is completely described by the reproduction 
alphabet (or codebook)  together with the partition q: {qi, i: 1, ...,N}, of the 
input vector space into the sets Si = {x: q(x) - i} of input vectors mapping into 
the i th reproduction vector (or code word) [Linde et al. 80]. To be compareable to 
SOM, our VQ assigns to each of the input vectors x = (x �, xl,..., x k-l) a socalled 
code book vector : (0, kl,..., k-1) of the same dimensionality k. For reasons of 
data compression, the number of code book vectors N << n, where n is the number 
of input vectors. 
Demanded is a VQ that produces a mapping q for which the expected distortion 
caused by reproducing the input vectors x by code book vectors q(x) is at least 
locally minimal. The expected distortion is usually esimated by using the aver- 
age distortion D, where the most common distortion measure is the squared-error 
Although SOM is an unsupervised technique not built for classification, the number 
of points missclassified to a wrong cluster center is an appropriate and commonly used 
performance measure for cluster procedures if the true cluster structure is known. 
Limitations of Self-organizing Maps 447 
distortion d: 
1 n-1 k-1 
D:-d(xi, q(xi)) (1) d(x,k)=lxi-&il 2 (2) 
i=0 i=0 
The classical vector quantization technique to achieve such a mapping is the LBG- 
algorithm [Linde et al. 80], where a given quantizer is iteratively improved. Al- 
ready [Linde et al. 80] noted that their proposed algorithm is almost similar to 
the k-means approach developed in the cluster analysis literature starting from 
[MacQueen 67]. Closely related to SOM is online K-means clustering (oKMC) con- 
sisting of the following steps: 
1. Initialization: Given N = number of code book vectors, k - dimensionality 
of the vectors, n = number of input vectors, a training sequence {xj;j = 
0,..., n - 1}, an initial set 0 of N code book vectors & and a discrete-time 
coordinate t = 0..., n 
2. Given At = {&i;i = 
?(At) : {Xi;i- 1, 
d(x, <_ for 
3. Update the code book vector with the minimum distortion 
= + (3) 
where c is a learning parameter to be defined by the user. Define +1 - 
(P()), replace t by t + 1, if t: n - 1, halt. Else go to step 2. 
-1. 
1,...,N}, find the minimum distortion partition 
...,N}. Compute d(x,ki) for i - 1,...,N. If 
all l, then x  Si. 
The main difference between the SOM-algorithm and oKMC is the fact that the 
code book vectors are ordered either on a line or on a planar grid (i.e. in a one or 
two dimensional output space). The iterative procedure is the same as with oKMC 
where formula (3) is replaced by 
;(t)(Si)---- ;(t_l)(S/)q-h[lg(t)- ;(t-1)(Si)] (4) 
and this update is not only computed for the i that gives minimum distortion, but 
also for all the code book vectors which are in the neighbourhood of this i on the 
line or planar grid. The degree of neighbourhood and amount of code book vectors 
which are updated together with the ff:i that gives minimum distortion is expressed 
by h, a function that decreases both with distance on the line or planar grid and 
with time and that also includes an additional learning parameter a. If the degree 
of neighbourhood is decreased to zero, the SOM-algorithm becomes equal to the 
oKMC-algorithm. 
Whereas local convergence is guaranteed for oKMC (at least for decreasing a, 
[Bottou & Bengio 95]), no general proof for the convergence of SOM with nonzero 
neighbourhood is known. [Kohonen 95, p.128] notes that the last steps of the SOM 
algorithm should be computed with zero neighbourhood in order to guarantee the 
most accurate density approximation of the input samples. 
3 SO M and multidimensional scaling 
Formally, a topology preserving algorithm is a transformation �: R k - R p, that 
either preserves similarities or just similarity orderings of the points in the input 
space R k when they are mapped into the outputspace R p. For most algorithms it is 
the case that both the number of input vectors I x  R  ] and the number of output 
448 A. Flexer 
vectors l i  R r I are equal to n. A transformation : i = (x), that preserves 
similarities poses the strongest possible constraint since d(xi, xj) = d(i, j) for all 
x, xj  R k, all i, ij  R r, i, j = 1,..., n - 1 and d (d) being a measure of distance 
in R k (Rr). Such a transformation is said to produce an isometric image. 
Techniques for finding such transformations (I) are, among others, various forms of 
multidimensional scaling 2 (MDS) like metric MDS [Torgerson 52], nonmetric MDS 
[Shepard 62] .or Sammon mapping [Sammon 69], but also principal component anal- 
ysis (PCA) (see e.g. [Jolliffe 86]) or SOM. Sammon mapping is doing MDS by 
minimizing the following via steepest descent: 
n-1 
n-1 
(5) 
Since the SOM has been designed heuristically and not to find an extremum for a 
certain cost or energy function s and the theoretical connection to the other MDS 
algorithms remains unclear. It should be noted that for SOM the number of output 
vectors ]/:  R p ] is limited to N, the number of cluster centroids : and that the & 
are further restricted to lie on a planar grid. This restriction entails a discretization 
of the outputspace R p. 
4 
Online K-means clustering plus Sammon mapping of the 
cluster centroids 
Our new combined approach consists of simply finding the set of  = {i,i = 
1,..., N} code book vectors that give the minimum distortion partition P() = 
{Si; i = 1,..., N} via the oKMC algorithm and then using the/:i as input vectors 
to Sammon mapping and thereby obtaining a two dimensional repres
