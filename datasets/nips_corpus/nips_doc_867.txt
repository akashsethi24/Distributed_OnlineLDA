Analysis of Unstandardized Contributions 
in Cross Connected Networks 
Thomas R. Shultz 
shultz@psych.mcgill.ca 
Yuriko Oshima-Takane 
yurikopsych .mcgill .ca 
Yoshio Takane 
takanepsych.mcgill.ca 
Department of Psychology 
McGill University 
Montrtal, Qutbec, Canada H3A lB1 
Abstract 
Understanding knowledge representations in neural nets has been a 
difficult problem. Principal components analysis (PCA) of 
contributions (products of sending activations and connection weights) 
has yielded valuable insights into knowledge representations, but much 
of this work has focused on the correlation matrix of contributions. The 
present work shows that analyzing the variance-covariance matrix of 
contributions yields more valid insights by taking account of weights. 
1 INTRODUCTION 
The knowledge representations learned by neural networks are usually difficult to 
understand because of the non-linear properties of these nets and the fact that knowledge is 
often distributed across many units. Standard network analysis techniques, based on a 
network% connection weights or on its hidden unit activations, have been limited. Weight 
diagrams are typically complex and weights vary across multiple networks trained on the 
same problem. Analysis of activation patterns on hidden units is limited to nets with a 
single layer of hidden units without cross connections. 
Cross connections are direct connections that bypass intervening hidden unit layers. They 
increase learning speed in static networks by focusing .on linear relations (Lang & 
Witbrock, 1988) and are a standard feature of generative algorithms such as cascade- 
correlation (Fahlman & Lebiere, 1990). Because such cross connections do so much of 
the work, analyses that are restricted to hidden unit activations furnish only a partial 
picture of the network's knowledge. 
Contribution analysis has been shown to be a useful technique for multi-layer, cross 
connected nets. Sanger (1989) defined a contribution as the product of an output weight, 
the activation of a sending unit, and the sign of the output target for that input. Such 
contributions are potentially more informative than either weights alone or hidden unit 
activations alone since they take account of both weight and sending activation. Shultz 
and Elman (1994) used PCA to reduce the dimensionality of such contributions in several 
different types of cascade-correlation nets. Shultz and Oshima-Takane (1994) demonstrated 
that PCA of unscaled contributions produced even better insights into cascade-correlation 
solutions than did comparable analyses of contributions scaled by the sign of output 
targets. Sanger (1989) had recommended scaling contributions by the signs of output 
targets in order to determine whether the contributions helped or hindered the network's 
solution. But since the signs of output targets are only available to networks during error 
602 Thomas R. Shultz, Yuriko Oshima-Takane, Yoshio Takane 
correction learning, it is more natural to use unscaled contributions in analyzing 
knowledge representations. 
There is an issue in PCA about whether to use the correlation matrix or the variance- 
covariance matrix. The correlation matrix contains ls in the diagonal and Pearson 
correlation coefficients between contributions off the diagonal. This has the effect of 
standardizing the variables (contributions) so that each has a mean of 0 and standard 
deviation of 1. Effectively, this ensures that the PCA of a correlation matrix exploits 
variation in input activation patterns but ignores variation in connection weights (because 
variation in connection weights is eliminated as the contributions are standardized). 
Here, we report on work that investigates whether more useful insights into network 
knowledge structures can be revealed by PCA of unstandardized contributions. To do this, 
we apply PCA to the variance-covariance matrix of contributions. The variance-covariance 
matrix has contribution variances along the diagonal and covariances between 
contributions off the diagonal. Taking explicit account of the variation in connection 
weights ha this way may produce a more valid picture of the network's knowledge. 
We use some of the same networks and problems employed in our earlier work (Shultz & 
Elman, 1994; Shultz & Oshima-Takane, 1994) to facilitate comparison of results. The 
problems include continuous XOR, arithmetic comparisons involving addition and 
multiplication, and distinguishing between two interlocking spirals. All of the nets were 
generated with the cascade-correlation algorithm (Fahlman & Lebiere, 1990). 
Cascade-correlation begins as a perceptron and recruits hidden units into the network as it 
needs them in order to reduce error. The recruited hidden unit is the one whose activations 
correlate best with the network's current error. Recruited units are installed in a cascade, 
each on a separate layer and receiving input from the input units and from any previously 
existing hidden units. We used the default values for all cascade-correlation parameters. 
The goal of understanding knowledge representations learned by networks ought to be 
useful in a variety of contexts. One such context is cognitive modeling, where the ability 
of nets to merely simulate psychological phenomena is not sufficient (McCloskey, 
1991). In addition, it is important to determine whether the network representations bear 
any systematic relation to the representations employed by human subjects. 
2 PCA OF CONTRIBUTIONS 
Sanger's (1989) original contribution analysis began with a three-dimensional array of 
contributions (output unit x hidden unit x input pattern). In contrast, we start with a two- 
dimensional output weight x input pattern array of contributions. This is more efficient 
than the slicing technique used by Sanger to focus on particular output or hidden units and 
still allows identification of the roles of specific contributions (Shultz & Elman, 1994; 
Shultz & Oshima-Takane, 1994). 
We subject the variance-covariance matrix of contributions to PCA in order to identify the 
main dimensions of variation in the contributions (Jolliffe, 1986). A component is a line 
of best fit to a set of data points in multi-dimensional space. The goal of PCA is to 
summarize a multivariate data set with a relatively small number of components by 
capitalizing on covariance among the variables (in this case, contributions). 
We use the scree test (Cattell, 1966) to determine how many components are useful to 
include in the analysis. Varimax rotation is applied to improve the interpretability of the 
solution. Component scores are plotted to identify the function of each component. 
3 APPLICATION TO CONTINUOUS XOR 
The classical binary XOR problem does not have enough training patterns to make 
contribution analysis worthwhile. However, we constructed a continuous version of the 
XOR problem by dividing the input space into four quadrants. Starting from 0.1, input 
values were incremented in steps of 0.1, producing 100 x, y input pairs that can be 
partitioned into four quadrants of the input space. Quadrant a had values of x less than 
Analysis of Unstandardized Contributions in Cross Connected Nem'orks 603 
0.55 combined with values ofy above 0.55. Quadrant b had values ofx and y greater than 
0.55. Quadrant c had values ofx andy less than 0.55. Quadrant d had values ofx greater 
than 0.55 combined with values of y below 0.55. Similar to binary XOR, problems from 
quadrants a and d had a positive output target (0.5) for the net, whereas problems from 
quadrants b and c had a negative output target (-0.5). There was a single output unit with 
a sigmoid activation. 
Three cascade-correlation nets were trained on continuous XOR. Each of these nets 
generated a unique solution, recruiting five or six hidden units and taking from 541 to 765 
epochs to learn to correctly classify all of the input patterns. Generalization to test 
patterns not in the training set was excellent. PCA of unscaled, unstandardized 
contributions yielded three components. A plot of rotated component scores for the 100 
training patterns of net 1 is shown in Figure 1. The component scores are labeled 
according to their respective quadrant in the input space. Three components are required to 
account for 96.0% of the variance in the contributions. 
Figure 1 shows that component 1, with 44.3% of the variance in contributions, has the 
role of distinguishing those quadrants with a positive output target (a and d) from those 
with a negative output target (b and c). This is indicated by the fact that the black shapes 
are at the top of the component space cube in Figure 1 and the white shapes are at the 
bottom. Components 2 and 3 represent variation along the x and y input dimensions, 
respectively. Component 2 accounted for 26.1% of the variance in contributions, and 
component 3 accounted for 25.6% of the variance in contributions. Input pairs from 
quadrants b and d (square shapes) are concentrated on the negative end of component 2, 
whereas input pairs from quadrants a and c (circle shapes) are concentrated on the positive 
end of component 2. Similarly, input pairs from quadrants a and b cluster on the negative 
end of component 3, and input pairs from quadrants c and d cluster on the positive end of 
component 3. Although the network was not explicitly trained to represent the x and y 
input dimensions, it did so as an incidental feature of its learning the distinction between 
quadrants a and d vs. quadrants b and c. Similar results were obtained from the other two 
nets learning the continuous XOR problem. 
In contrast, PCA of the correlation matrix from these nets had yielded a somewhat less 
clear picture with the third component separating quadrants a and d from quadrants b and c, 
and the first two components representing variation along the x and y input dimensions 
(Shultz & Oshima-Takane, 1994). PCA
