258 Seibert and Waxman 
Learning 
Aspect Graph Representations 
from View Sequences 
Michael Seibert and Allen M. Waxman 
Lincoln Laboratory, Massachusetts Institute of Techuology 
Lexington, MA 02173-9108 
ABSTRACT 
In our effort to develop a modular neural system for invariant learn- 
ing and recognition of 3D objects, we introduce here a new module 
architecture called an aspect network constructed around adaptive 
axo-axo-dendritic synapses. This builds upon our existing system 
(Seibert & Waxman, 1989) which processes 2D shapes and classifies 
them into view categories (i.e., aspects) invariant to illumination, 
position, orientation, scale, and projective deformations. From a 
sequence'of views, the aspect network learns the transitions be- 
tween these aspects, crystallizing a graph-like structure from an 
initially amorphous network. Object recognition emerges by ac- 
cumulating evidence over multiple views vhich activate competing 
object hypotheses. 
1 INTRODUCTION 
One can learn a three-dimensional object by exploring it and noticing how its 
appearance changes. When moving from one view to another, intermediate views 
are presented. The imagery is continuous, unless some feature of the object appears 
or disappears at the object's horizon (called the occluding contour). Such visual 
events can be used to partition continuously varying input imagery into a discrete 
sequence of aspects. The sequence of aspects (and the transitions between them) can 
be coded and organized into a representation of the 3D object under consideration. 
This is the form of 3D object representation that is learned by our aspect network. 
We call it an aspect network because it was inspired by the aspect graph concept 
of Koenderink and van Doom (1979). This paper introduces this new network 
Learning Aspect Graph Representations from View Sequences 259 
which learus and recognizes sequences of aspects, and leaves most of the discussion 
of the visnal preprocessing to earlier papers (Seibert & Waxman, 1989; Waxman, 
Seibert, Cunningham, & Wu, 1989). Pr(-sentcd in this way, we hope that our ideas 
of sequence learning, representation, and recognition are also useful to investigators 
concerned with speech, finite-state machines, planning, and control. 
1.1 2D VISION BEFORE 3D VISION 
Tile aspect network is one module of a more complete vision system (Figure 1) 
introduced by us (Scibert & Waxman, 1989). The carly stages of the complete 
system learn and recognize 2D views of objects, invariant to the scene illumina- 
Fntz  
2D-Invmlant 
ViW Leamlng 
and Recognilion 
Overlappin! 
3D-Object 
Recogibon ........ 
 N&t'wMk 
Ol'lMltaUO arid Trim)lent Detec'llr . . ;?' 
Cou13ing 
AttenloM Cue 
Feature 
Extraction Tr&nlent Dett .... � Eye 
Position 
Contrast BIl&y&r AttntimM Cu Invarlanc$ 
Enhancement 
Input Natwerk 
Fignre 1: Neural system architecture for 3D objec! learning and recognition. The 
aspect network is part, of the upper-right module. 
tion and an object's orientation, size, and position in the visual field. Additionally, 
projective deformations such as foreshortening and perspective effects are removed 
from the learned 2D representations. These processing steps make use of Diffusion- 
Enhancement Bilayers (DEBs) 1 to generate attentional cues and featural groupings. 
The point of our neural preprocessing is to generate a sequence of views (i.e., as- 
pects) which depends on the object's orientation in 3-space, but which does not 
depend on how the 2D images happen to fall on the retina. If no preprocessing 
were done, then tile 3D representation would have to account for every possible 
2D appearance in addition to the 3D information which relates the views to each 
other. Compressing tile views into aspects avoids such combinatorial problems, but 
may result iu an ambignous representation, in that some aspects may be common 
to a number of objects. Such ambiguity is overcome by learning and recognizing a 
This architecture was previously called the NADEL (Neural Analog Diffusion-Enhancement 
Layer), but has been renamed to avoid causing any problems or confusion, since there is an active 
researcher in the field with this name. 
260 Seibert and Waxman 
sequence of aspects (i.e., a trajectory through the aspect graph). The partitioning 
and sequence recognition is analogous to building a symbol alphabet and learning 
syntactic structures within the alphabet. Each symbol represents an aspect. and is 
encoded in our system as a separate category by an Adaptive Resonance Network 
architecture (Carpenter & Grossberg, 1987). This unsupcrvised learning is compet- 
itive and may proceed on-line with recognition; no separate training is required. 
1.2 ASPECT GRAPHS AND OBJECT REPRESENTATIONS 
Figure 2 shows a sinplified aspect graph for a prismatic object.' Each node of 
Figure 2: Aspect Graph. A 3D object can be represented as a graph of the char- 
acteristic view-nodes with adjacent views encoded by arcs between the nodes. 
the graph represents a characteristic viev, while the allowable transitions among 
views are represented by the arcs between the nodes. In this depiction, symmetries 
have been considered to simplify the graph. Although Koenderink and van Doom 
suggested assigning aspects based on topological equivalences, we instead allow the 
ART 2 portion of our 2D system to decide when an invariant 2D view is sufficiently 
different from previously experienced views to allocate a new view category (aspect). 
Transitions between adjacent aspects provide the key to the aspect network rep- 
resentation and recognition processes. Storing the transitions in a self-organizing 
synaptic weight array becomes the learned view-based representation of a 3D object. 
Transitions are exploited again during recognition to distinguish among objects with 
similar views. Whereas most investigators are interested in the computational com- 
plexity of generating aspect graphs from CAD libraries (Bowyet, Eggeft, Stewman, 
2Neither the apect graph concept nor our aspect network implementation is limited to simple 
polyhedral objects, nor must the objects even be convex, i.e., they may be self-occluding. 
Learning Aspect Graph Representations from View Sequences 261 
& St. ark, 1989), we are luterested in designing it as a self-organizing representat ion, 
learned from visual experience and usefid for object recognition. 
2 ASPECT-NETWORK LEARNING 
The view-category nodes of ART 2 excit.e the aspect nodes (wlich we also call the x- 
nodes) of the aspect network (Figure 3). The aspect nodes fan-out to the dendritic 
Object 
Competition Layer 
Evidence Accumulation 
Nodee 
Synaptic Arraye of 
Learned View 
Traneitione 
3, Vj = 0 View Traneition 
� I, Vj = 1 Aspect Nodal 
Input View Categorlea 
Figure 3: Aspect Network. The learned graph representations of 3D objects are re- 
alized as weights in the synaptic arrays. Evidence for experienced view-trajectories 
is simultaneously accumulated for all competing objec.ts. 
trees of object neurons. An object neuron consists of an adaptive synaptic array 
and an evidence accumulating y-node. Each object is learned by a single object 
neuron. A view sequence leads to accumulating activity in the y-nodes, which 
compete to determine the recognized object (i.e., maximally active z-node) in 
the object competition layer. Gating signals from these nodes then modulate 
learning in the corresponding synaptic array, as in competitive learning paradigms. 
The system is designed so that the learning pha.se is integral with recognition. 
Learning (and forgetting) is always possible so that existing representations can 
always be elaborated with new information as it becomes available. 
Differential equations govern the dynamics and architecture of the aspect network. 
These shunting equations model cell membrane and synapse dynamics as pioneered 
by Grossberg (1973, 1989). Input activities to the network are given by equation 
(1), the learned aspect transitions by equation (2), and the objects recognized from 
the experienced view sequences by equation (3). 
262 Seibert and Waxman 
2.1 ASPECT NODE DYNAMICS 
The aspect node activities are governed by equation (1): 
dxi 
d! 
(1) 
where A is a passive decay rate, and h = 1 during tile presentation of aspect 
i and zero otherxvise as determined by the output of tile ART 2 module in the 
complete system (Figure 1). This equation assures that the activities of the aspect 
nodes build and decay in nonzero time (see the timetraces for the input/-nodes and 
aspect x-nodes in Figure 3). Whenever an aspect transition occurs, the activity of 
the previous aspect decays (with rate X) and the activity of the new aspect builds 
(again with rate  in this case, which is convenient but not necessary). During the 
transient time when both activities are nonzero, only the synapses between these 
nodes have both pre- and post-synaptic activities which are significant (i.e., above 
the threshold) and Hebbian learning can be supported. The overlap of the pre- and 
post-synaptic activities is transient, and the extent of the transient is controlled by 
the selection of . This is the fundamental parameter for the dynamical behavior 
of the entire network, since it defines the response time of the aspect nodes to their 
inputs. As such, nearly every other parameter of the network depends on it. 
2.2 VIEW TRANSITION ENCODING BY ADAPTIVE SYNAPSES 
The aspect transitions that represent objects are realized by synaptic weights on 
the dendritc trees of object neurons. Equation (2) defines how the (initially small 
and random) weight relating aspect i, aspect j, and object k changes: 
dt - 
Here, w governs the rate of evolution of the weights relative to the x-node dynamics, 
and Aw is the decay rate of the weights. Note that a small background level of 
activity e is added to each x-node activity. This will be discussed in connectiou 
with (3) b
