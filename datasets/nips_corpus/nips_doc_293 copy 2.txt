An Attractor Neural Network Model of Recall 
and Recognition 
Eytan Ruppin 
Department of Computer Science 
School of Mathematical Sciences 
Sacklet Faculty of Exact Sciences 
Tel Aviv University 
69978, Tel Aviv, Israel 
Yechezkel Yeshurun 
Department of Computer Science 
School of Mathematical Sciences 
Sacklet Faculty of Exact Sciences 
Tel Aviv University 
69978, Tel Aviv, Israel 
Abstract 
This work presents an Attractor Neural Network (ANN) model of Re- 
call and Recognition. It is shown that an ANN model can qualitatively 
account for a wide range of experimental psychological data pertaining 
to the these two main aspects of memory access. Certain psychological 
phenomena are accounted for, including the effects of list-length, word- 
frequency, presentation time, context shift, and aging. Thereafter, the 
probabilities of successful Recall and Recognition are estimated, in order 
to possibly enable further quantitative examination of the model. 
1 Motivation 
The goal of this paper is to demonstrate that a Hopfield-based [Hop82] ANN model 
can qualitatively account for a wide range of experimental psychological data per- 
taining to the two main aspects of memory access, Recall and Recognition. Recall 
is defined as the ability to retrieve an item from a list of items (words) originally 
presented during a previous learning phase, given an appropriate cue (cued Recall), 
or spontaneously (free Recall). Recognition is defined as the ability to successfully 
acknowledge that a certain item has or has not appeared in the tutorial list learned 
before. 
The main prospects of ANN modeling is that some parameter values, that in former, 
'classical' models of memory retrieval (see e.g. [GS84]) had to be explicitly assigned, 
can now be shown to be emergent properties of the model. 
642 
An Attractor Neural Network Model of Recall and Recognition 643 
2 The Model 
The model consists of a Hopfield ANN, in which distributed patterns representing 
the learned items are stored during the learning phase, and are later presented as 
inputs during the test phase. In this framework, successful Recall and Recognition 
is defined. Some additional components are added to the basic Hopfield model to 
enable the modeling of the relevant psychological phenomena. 
2.1 The Hopfield Model 
The Hopfield model's dynamics are composed of a non-linear, iterative, asyn- 
chronous transformation of the network state [Hop82]. The process may include 
a stochastic noise which is analogous to the 'temperature' T in statistical mechan- 
ics. Formally, the Hopfield model is described as follows: Let neuron's i state be a 
binary variable $i, taking the values +1 denoting a firing or a resting state, corre- 
spondingly. Let the network's state be a vector $ specifying the binary values of 
all its neurons. Let Jij be the synaptic strength between neurons i and j. Then, 
hi, the input 'field' of neuron i is given by hi = --qi JijSj. The neuron's dynamic 
behavior is described by 
Si(t + 1)= { 1, with probability �(1 + tgh()) 
-1, with probability �(1 - tgh(-)) 
Storing a new memory pattern f t' in the network is performed by modifying every 
ij element of the synaptic connection matrix according to J'} = j]a + 7ij. 
A Hopfield network will always converge to a stable state, and every stored memory 
is an attractor having an area surrounding it termed its basin of attraction [Hop82]. 
In addition to the stored memories, also other, non-memory states exist as stable 
states (local minima) of the network [AGS85]. The maximal number m of (randomly 
generated) memory patterns which can be stored in the basic Hopfield network of 
n neurons is m = ac .n, ac  0.14 [AGS85]. 
2.2 Recall and Recognition in the model's framework 
2.2.1 Recall 
Recall is considered successful when upon starting from an initial cue the network 
converges to a stable state which corresponds to the learned memory nearest to 
the input pattern. Inter-pattern distance is measured by the Hamming distance 
between the input and the learned item encodings. If the network converges to a 
1 
non-memory stable state, its output will stand for a 'failure of recall' response. 
The question of How do such non-memory states bear the meaning of 'recall failure' ? 
is out of the scope of this work. However, a possible explanation is that during the learning 
phase 'meaning' is assigned to the stored patterns via connections formed with external 
patterns, and since non-memory states lack such associations with external patterns, they 
are 'meaningless', yielding the 'recall failure' response. Another possible mechanism is that 
every output pattern generated in the recall process passes also a recognition phase so that 
non-memory states are rejected, (see the following paragraph describing recognition in our 
model). 
644 Ruppin and Yeshurun 
2.2.2 Recognition 
Recognition is considered successful when the network arrives at a stable state dur- 
ing a time interval A, beginning from input presentation. In general, the shorter 
the distance between an input and its nearest memory, the faster is its convergence 
[AM88, KP88, RY90]. Since non-memory (non-learned) stable states have higher 
energy levels and much shallower basins of attraction than memorized stable states 
[AGS85, LN89], convergence to such states takes significantly longer timer. There- 
fore, there exists a range of possible values of A that enable successful recognition 
only of inputs similar to one of the stored memories. 
2.3 
Other features of the model 
The context of the psychological experiments is represented as a substring of 
the input's encoding. In order to minimize inter-pattern correlation, the size of 
the context encoding relative to the total size of the memory encoding is kept 
small. 
The total associational linkage of a learned item, is modeled as an external 
field vector E. When a learned memory pattern ' is presented to the network, 
the value of the external field vector generated is Ei = h � ', where h is an 
'orientation' coefficient, expressing the association strength. 
Additional features, including a modified storage equation accounting for learning 
taking place at the test phase, and a storage decay parameter, are described in 
[RY90]. 
3 The Modeling of experimental data. 
Regarding every phenomenon discussed, a brief description of the psychological 
findings is followed by an account of its modeling. We rely on the known results 
pertaining to Hop field models to show that qualitatively, the psychological phenom- 
ena reviewed are emergent properties of the model. When such analytical evidence 
is lacking, simulations were performed in order to account for the experimental 
data. For a review of the psychological literature supporting the findings modeled 
see [GS84]. 
The List-Length Effect: It is known that the probability of successful Recall or 
Recognition of a particular item decreases as the length of list of learned items 
increases. 
List length is expressed in memory load. Since It has been shown that the 
width of the memories basins of attraction monotonically decreases following 
an approximately inverse parabolic curve [Wei85], Recall performance should 
decrease as memory load is increased. We have examined the convergence time 
of the same set of input patterns at different values of memory load. As demon- 
strated in Fig. 1, it was found that, as the memory load is increased, successful 
convergence has occurred (on the average) only after an increasingly growing 
number of asynchronous iterations. Hence, convergence takes more time and 
can result in Recognition failure, although memories' stability is maintained 
till the critical capacity ac is reached. 
An Attractor Neural Network Model of Recall and Recognition 645 
4000.0 
I � I I 
3000.0 
2000.0 
1000.0 
0.0 , I , I , I , I 
10.0 20.0 30.0 40.0 50.0 60.0 
Memory Iood 
Figure 1' Itccog.itiol Sl,ccd (No. of asy,chro,ous ite,'atio,ls) a., a fu,ction of memory 
load (No. of slored memories). The netwo,'k's parameters are n = ,500, T = 0.28 
The word-fi'equency effect: The more frequent a word is in language, the prob- 
ability of recalling it increases, while the probability of recognizing it decreases. 
A word's frequency in the language is assumed to effect its retrieval through 
the stored word's semantic relations and associations [Kat85, NCBK87]. It 
is assumed, that relative to low frequency words, high frequency words have 
more semantic relations and therefore more connections between the patterns 
representing them and other' patterns stored in the memory (i.e., in other net- 
works). This one-to-many relationship is assumed to be reciprocal, i.e., each f 
the externally stored patterns has also connections projected to several of 
stored patterns in the allocated network. 
The process leading to the formation of the external field E (acting upon the 
allocated network), generated by an input pattern nearest to some stored mem- 
ory pattern � is assumed to be characterized as follows: 
1. There is a threshold degree of ove. rlap 0,,,i,, such that E > 0 only when 
the allocated network's state overlap H u is higher than 
2. At overlap values H  which are only moderately larger than 0in, h t' is 
monotonically increasing, but as H t' continues to rise, a certain 'optimal' 
point is reached, beyond which h  is monotonically decreasing. 
3. High-frequency words have lower O,i, values than low-frequency words. 
Recognition tests are characterized by a high initial value of overlap H u, to 
some memory �. The value of b  and E ' generated is post-optimal and 
therefore smaller than in the case of low-frequency words which have higher 
Omi, values. 
In Recall tests the initial situation is characterized by low values of overlap H ' 
to some nearest memory '. only the overlap value of high-frequency words is 
sufficient for activating associated items, i.e. H   O,i,. 
646 Ruppin
