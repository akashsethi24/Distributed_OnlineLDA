Human Reading and the Curse of 
Dimensionality 
Gale L. Martin 
MCC Austin, TX 78613 galem@mcc.com 
Abstract 
Whereas optical character recognition (OCR) systems learn to clas- 
sify single characters; people learn to classify long character strings 
in parallel, within a single fixation. This difference is surprising 
because high dimensionality is associated with poor classification 
learning. This paper suggests that the human reading system 
avoids these problems because the number of to-be-classified im- 
ages is reduced by consistent and optimal eye fixation positions, 
and by character sequence regularities. 
An interesting difference exists between human reading and optical character recog- 
nition (OCR) systems. The input/output dimensionality of character classification 
in human reading is much greater than that for OCR systems (see Figure 1). OCR 
systems classify one character at time; while the human reading system classifies 
as many as 8-13 characters per eye fixation (Rayner, 1979) and within a fixation, 
character category and sequence information is extracted in parallel (Blanchard, 
McConkie, Zola, and Wolverton, 1984; Reicher, 1969). 
OCR (Low Dimensionality) 
[Dorothy lived in the .... ] 
Human Reading (High Dimensionality) 
[ Dorothy lived in the raidst of the ..... ] 
Dot[ oyl ................................................................. .- DOROTHY LI 
[livecl in the ] ................................ l- LIVED IN THE 
[midst of the ] . MIDST OF THE 
Figure 1: Character classification versus character sequence classification. 
This is an interesting difference because high dimensionality is associated with poor 
classification learning-the so-called curse of dimensionality (Denker, et al; 1987; 
Geman, Bienenstock, & Doursat, 1992). OCR systems are designed to classify 
single characters to minimize such problems. The fact that most people learn to read 
quite well even with the high dimensional inputs and outputs, implies that variance 
18 G.L. MARTIN 
is somehow lowered in this domain, thereby making accurate classification learning 
possible. The present paper reports on simulations of parallel character classification 
which suggest that variance is lowered through regularities in eye fixation positions 
and in character sequences making up valid words. 
1 Training and Testing Materials 
Training and testing materials were drawn from the story The Wonderful Wizard 
of Oz by L. Frank Baum. Images of text lines were created from 120 pages of text 
(about 160,000 characters, 33,000 total words, or 2,600 different words), which were 
divided into 6 different font and case conditions of 20 pages each. Three different 
fonts (variable and constant-width fonts), and two different cases (all upper-case or 
mixed-case characters) were used. Text line images were normalized with respect 
to height, but not width. All training and test sets contained an equal mix of the 
six font/case conditions. Two generalization sets were used, for test and cross- 
validation, and each consisted of about 14,000 characters. 
Dorothy lived in the midst of the great Kansas Prairies. 
DOROTHY LIVED IN  MIDST OF TI-IE GREAT KANSAS PRAIRIF_.S. 
Dorothy lived in the midst of the great Kansas Prairies. 
DOROTHY LIVED IN THE MIDST OF THE GREAT KANSAS PRAIRIES. 
Dorothy lived in the midst of the great Kansas Prairies. 
DOROTHY LIVED IN THE MIDST OF THE GREAT KANSAS PRAIRIES. 
Figure 2: Samples of the type font and case conditions used in the simulations 
2 Network Architectures 
The simulations used backpropagation networks (Rumelhart, Hinton & Williams, 
1986) that extended the local receptive field, shared-weight architecture used in 
many character-based OCR. neural networks (LeCun, et al, 1989; Martin & Pittman, 
1991). In the previous single character-based approach, the input to the net is an 
image of a single character. The output is a vector representing the category of the 
character. Hidden nodes have local receptive fields that receive input from a spa- 
tially local region, (e.g., a 6x6 area) in the preceding layer. Groups of hidden nodes 
share their weights. Corresponding weights in each receptive field are initialized to 
the same value and updated by the same value. Different hidden nodes within a 
group learn to detect the same feature at different locations. A group is depicted 
as hidden nodes within a single plane of a cube that corresponds to a hidden layer. 
Different groups occupy different planes in the cube, and learn to detect different 
features. This architecture biases learning by reducing the number of free parame- 
ters available for representing a function. The fact that these nets usually train and 
generalize well in this domain, and that the local feature detectors that emerge are 
similar to the oriented-edge and -line detectors found in mammalian visual cortex 
(Hubel & Wiesel, 1979), suggests that the bias is at least roughly appropriate. 
The extension of this character network to a character-sequence network is illus- 
trated in Figure 3, where n (number of to-be-classified characters) is equal to 4. 
Each output node represents a character category (e.g., D) in one of the nth or- 
dinal positions (e.g., First character on the left). The size of the input window 
is expanded horizontally to cover at least the n widest characters (WWWW). 
When the character string is made up of relatively narrow characters, more than n 
characters will appear in the input window and the network must learn to ignore 
Human Reading and the Curse of Dimensionality 19 
them. Increasing input/output dimensionality is accomplished by expanding the 
number of hidden nodes horizontally. Network capacity is described by the depth 
of each hidden layer (the number of different features detected), as well as by the 
width of each hidden layer (the spatial coverage of the network). 
The network is potentially sensitive to both local and global visual information. 
Local receptive fields build in a sensitivity to letter features. Shared weights make 
learning transfer possible across representations of the same character at different 
positions. Output nodes are globally connected to all the nodes in the second hidden 
layer, but not with one another or with any word-level representations. Networks 
were trained until the training set accuracy failed to improve by at least .1% over 5 
epochs, or overfitting became evident from periodic testing with the generalization 
test set. 
-C-'E FO H IJKL M N O P Q RlgTU VWXYZ 
AB C DEFG H IJKL M NP Q RSTU VWXYZ_ 1 
ABCDEFG HIJKLMNOPQ[STU VWXYZ 
AB C D EFO H [JKL M N[IP Q RSTU VWXYZ_ 
 Local. shced-weight recep've fields 
Figure 3: Net architecture for parallel character sequence classification, n=4 chars. 
3 
Effects of Dimensionality on Training Difficulty and 
Generalization 
Experiment 1 provides a baseline measure of the impact of dimensionality. Increases 
in dimensionality result in exponential increases in the number of input and output 
patterns and the number of mapping functions. As a result, training problems arise 
due to limitations in network capacity or search scope. Generalization problems 
arise because it becomes impractical to use training sets large enough to obtain a 
good estimate of the underlying function. Four different levels of dimensionality 
were used (see Figure 4), from an input window of 20x20 pixels, with 1 to-be- 
classified character to an 80x20 window, with 4 to-be-classified characters ). Input 
patterns were generated by starting the window at the left edge of the text line such 
that the first character was centered 10 pixels from the left of the window, and then 
successively scanning across the text line at each character position. Five training 
set sizes were used (about 700 samples to 50,000). Two relative network capacities 
were used (15 and 18 different feature detectors per hidden layer). Forty different 
20x20 - 1 Character 
40x20 - 2 Characters 60x20 - 3 Characters 80x20 - 4 Characters 
Fl  DO D[-- .--. DOll. D['- 4 DORO 
 ---- OR o[-r[- --.. ORO or/---. OROT 
.................................................... Dirnensionality ........................................................................ ... Higt 
Figure 4: Four levels of input/output dimensionality used in the experiment. 
2 0 G.L. MARTIN 
networks were trained, one for each combination of dimensionality, training set size 
and relative network capacity (4x5x2). Training difficulty is described by asymptotic 
accuracy achieved on the training set and by amount of training required to reach 
the asymptote. Generalization is reported for both the test set (used to check for 
overfitting) and the cross-validation set. The results (see Figure 5) are consistent 
Lower Capacity Nets 
AQ'mptotie Aeeml on Trainl- E Set 
3 Ch  
4 Ch 
Trig Set Size Trig Set Size 
At of inin E R  R  
HiEher Capacity Nets 
. - 4 Ch 
I 
Ch 3 Ch 
Ch 2 Ch 
1 Ch mo ï¿½ ' 
1 Ch 
Generalizan A on Test Set 
100 - ; _- f) 100 
75 75 
I I I 
I I I I I 
Figure 5: Impact of dimensionality on training and generalization. 
with expectations. Increasing dimensionality results in increased training difficulty 
and lower generalization. Since the problems associated with high dimensionality 
occur in both training and test sets, and seem to be alleviated somewhat in the 
high capacity nets, they are presumably due to both capacity/search limitations 
and insufficient sample size. 
4 Regularities in Window Positioning 
One way human reading might reduce the problems associated with high dimen- 
sionality is to constrain eye fixation positions during reading; thereby reducing the 
number of different input images the system must learn to classify. Eye movement 
Human Reading and the Curse of Dimensionality 21 
studies suggest that, although fixation positions within words do vary, there are 
consistencies Rayner, 1979). Moreover, th
