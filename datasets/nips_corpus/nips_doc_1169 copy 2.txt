Self-Organizing and Adaptive Algorithms for 
Generalized Eigen-Decomposition 
Chanchal Chatterjee 
Newport Corporation 
1791 Deere Avenue, Irvine, CA 92606 
Vwani P. Roychowdhury 
Electrical Engineering Department 
UCLA, Los Angeles, CA 90095 
ABSTRACT 
The paper is developed in two parts where we discuss a new approach 
to self-organization in a single-layer linear feed-forward network. First, 
two novel algorithms for self-organization are derived from a two-layer 
linear hetero-associative network performing a one-of-m classification, 
and trained with the constrained least-mean-squared classification error 
criterion. Second, two adaptive algorithms are derived from these self- 
organizing procedures to compute the principal generalized 
eigenvectors of two correlation matrices from two sequences of 
random vectors. These novel adaptive algorithms can be implemented 
in a single-layer linear feed-forward network. We give a rigorous 
convergence analysis of the adaptive algorithms by using stochastic 
approximation theory. As an example, we consider a problem of online 
signal detection in digital mobile communications. 
1. INTRODUCTION 
We study the problems of hetero-associative training, linear discriminant analysis, 
generalized eigen-decomposition and their theoretical connections. The paper is divided 
into two parts. In the first part, we study the relations between hetero-associative training 
with a linear feed-forward network, and feature extraction by the linear discriminant 
analysis (LDA) criterion. Here we derive two novel algorithms that unify the two 
problems. In the second part, we generalize the self-organizing algorithm for LDA to 
obtain adaptive algorithms for generalized eigen-decomposition, for which we provide a 
rigorous proof of convergence by using stochastic approximation theory. 
1.1 HETERO-ASSOCIATION AND LINEAR DISCRIMINANT ANALYSIS 
In this discussion, we consider a special case of hetero-association that deals with the 
classification problems. Here the inputs belong to a finite m-set of pattern classes, and the 
Self-Organizing and Adaptive Generalized Eigen-Decomposition 397 
outputs indicate the classes to which the inputs belong. Usually, the i th standard basis 
vector e i is chosen to indicate that a particular input vector x belongs to class i. 
The LDA problem, on the other hand, aims at projecting a multi-class data in a lower 
dimensional subspace such that it is grouped into well-separated clusters for the m 
classes. The method is based upon a set of scatter matrices commonly known as the 
mixture scatter S m and between class scatter S o (Fukunaga, 1990). These matrices are 
used to formulate criteria such as tr(Sm-lSb) and det(Sb)/ det(Sm) which yield a linear 
transform � that satisfy the generalized eigenvector problem Sb=SmA, where A is the 
generalized eigenvalue matrix. If S m is positive definite, we obtain a � such that TSm 
=I and Tsb=A. Furthermore, the significance of each eigenvector (for class 
separability) is determined by the corresponding generalized eigenvalue. 
A relation between hetero-association and LDA was demonstrated by Gallinari et al. 
(1991). Their work made explicit that for a linear multi-layer perceptron performing a 
one-from-m classification that minimized the total mean square error (MSE) at the 
network output, also maximized a criterion det(So)/det(Sm) for LDA at the final hidden 
layer. This study was generalized by Webb and Lowe (1990) by using a nonlinear 
transform from the input data to the final hidden units, and a linear transform in the final 
layer. This has been further generalized by Chatterjee and Roychowdhury (1996) by 
including the Bayes cost for misclassification into the criteria tr(Sm-lSo). 
Although the above studies offer useful insights into the relations between hetero- 
association and LDA, they do not suggest an algorithm to extract the optimal LDA 
transform . Since the criteria for class separability are insensitive to multiplication by 
nonsingular matrices, the above studies suggest that any training procedure that 
minimizes the MSE at the network output will yield a nonsingular transformation of ; 
i.e., we obtain Q where Q is a nonsingular matrix. Since Q does not satisfy the 
generalized eigenvector problem Sb(I)=Sm(I)A for any arbitrary nonsingular matrix Q, we 
need to determine an algorithm that will yield Q=I. 
In order to obtain the optimum linear transform , we constrain the training of a two- 
layer linear feed-forward network, such that at convergence, the weights for the first 
layer simultaneously diagonalizes S m and S b. Thus, the hetero-associative network is 
trained by minimizing a constrained MSE at the network output. This training procedure 
yields two novel algorithms for LDA. 
1.2 LDA AND GENERALIZED EIGEN-DECOMPOSITION 
Since the LDA problem is a generalized eigen-decomposition problem for the 
symmetric-definite case, the self-organizing algorithms derived from the hetero- 
associative networks lead us to construct adaptive algorithms for generalized eigen- 
decomposition. Such adaptive algorithms are required in several applications of image 
and signal processing. As an example, we consider the problem of online interference 
cancellation in digital mobile communications. 
Similar to the LDA problem Sb=SmA, the generalized eigen-decomposition problem 
A=BA involves the matrix pencil (A,B), where A and B are assumed to be real, 
symmetric and positive definite. Although a solution to the problem can be obtained by a 
conventional method, there are several applications in image and signal processing where 
an online solution of generalized eigen-decomposition is desired. In these real-time 
situations, the matrices A and B are themselves unknown. Instead, there are available two 
398 C. Chatterjee and V. P. Roychowdhury 
sequences of random vectors {x/} and {Yk} with limk_�E[xlxq =A and lim_� 
E[yy)=B, where xk and y represent the online observations of the application. For 
every sample (x/rye), we need to obtain the current estimates cI) and A of cI) and A 
respectively, such that (I)/ and A/ converge strongly to their true values. 
The conventional approach for evaluating cI) and A requires the computation of (A,B) 
after collecting all of the samples, and then the application of a numerical procedure; i.e., 
the approach works in a batch fashion. There are two problems with this approach. 
Firstly, the dimension of the samples may be large so that even if all of the samples are 
available, performing the generalized eigen-decomposition may take prohibitively large 
amount of computational time. Secondly, the conventional schemes can not adapt to slow 
or small changes in the data. So the approach is not suitable for real-time applications 
where the samples come in an online fashion. 
Although the adaptive generalized eigen-decomposition algorithms are natural 
generalizations of the self-organizing algorithms for LDA, their derivations do not 
constitute a proof of convergence. We, therefore, give a rigorous proof of convergence 
by stochastic approximation theory, that shows that the estimates obtained from our 
adaptive algorithms converge with probability one to the generalized eigenvectors. 
In summary, the study offers the following contributions: (1) we present two novel 
algorithms that unify the problems of hetero-associative training and LDA feature 
extraction; and (2) we discuss two single-stage adaptive algorithms for generalized eigen- 
decomposition from two sequences of random vectors. 
In our experiments, we consider an example of online interference cancellation in digital 
mobile communications. In this problem, the signal from a desired user at a far distance 
from the receiver is corrupted by another user very near to the base. The optimum linear 
transform w for weighting the signal is the first principal generalized eigenvector of the 
signal correlation matrix with respect to the interference correlation matrix. Experiments 
with our algorithm suggest a rapid convergence within four bits of transmitted signal, and 
provides a significant advantage over many current methods. 
2. HETERO-ASSOCIATIVE TRAINING AND LDA 
We consider a two-layer linear network performing a one-from-m classification. Let x 
9t n be an input to the network to be classified into one out ofm classes co l,',com' If xco i 
then the desired output d=ei (i th std. basis vector). Without loss of generality, we assume 
the inputs to be a zero-mean stationary process with a nonsingular covariance matrix. 
2.1 EXTRACTING THE PRINCIPAL LDA COMPONENTS 
In the two-layer linear hetero-associative network, let there be p neurons in the hidden 
layer, and m output units. The aim is to develop an algorithm so that individual weight 
vectors for the first layer converge to the first p_<m generalized eigenvectors 
corresponding to the p significant generalized eigenvalues arranged in decreasing order. 
Let WiEF{n (i=l,...,n) be the weight vectors for the input layer, and viOtm (i=l,...,m) be 
the weight vectors for the output layer. 
The neurons are trained sequentially; i.e., the training of the jth neuron is started only 
after the weight vector of the (/-1) th neuron has converged. Assume that all the j-1 
previous neurons have already been trained and their weights have converged to the 
Self-Organizing and Adaptive Generalized Eigen-Decomposition 399 
optimal weight vectors w i for i [1 /-1]. To extract the j th generalized eigenvector in the 
output of the jth neuron, the updating model for this neuron should be constructed by 
subtracting the results from all previously computed j-1 generalized eigenvectors from 
the desired output dj as below 
dj =d/- Z viw;x. (1) 
i=1 
This process is equivalent to the deflation of the desired output. 
The scatter matrices Srn and St can be obtained from x and d as Sm=E[xx r] and St= 
MM r, where M=E[xd. We nee
