SIMPLIFYING NEURAL NETS BY 
DISCOVERING FLAT MINIMA 
Sepp Hochreiter* Jiirgen Schmidhuber t 
Fakultt flit Informatik, H2 
Technische Universitt Mfinchen 
80290 Mfinchen, Germany 
Abstract 
We present a new algorithm for finding low complexity networks 
with high generalization capability. The algorithm searches for 
large connected regions of so-called fiat minima of the error func- 
tion. In the weight-space environment of a fiat minimum, the 
error remains approximately constant. Using an MDL-based ar- 
gument, fiat minima can be shown to correspond to low expected 
overfitting. Although our algorithm requires the computation of 
second order derivatives, it has backprop's order of complexity. 
Experiments with feedforward and recurrent nets are described. In 
an application to stock market prediction, the method outperforms 
conventional backprop, weight decay, and optimal brain surgeon. 
I INTRODUCTION 
Previous algorithms for finding low complexity networks with high generalization 
capability are based on significant prior assumptions. They can be broadly classified 
as follows: (1) Assumptions about the prior weight distribution. Hinton and van 
Camp [3] and Williams [17] assume that pushing the posterior distribution (after 
learning) close to the prior leads to good generalization. Weight decay can be 
derived e.g. from Gaussian priors. Nowlan and Hinton [10] assume that networks 
with many similar weights generated by Gaussian mixtures are better a priori. 
MacKay's priors [6] are implicit in additional penalty terms, which embody the 
*hochreit @informatik.tu-muenchen.de 
t schmidhu@informatik.tu-muenchen.de 
530 Sepp Hochreiter, Jiirgen Schmidhuber 
assumptions made. (�) Prior assumptions about how theoretical results on early 
stopping and network complexity carry over to praciical applications. Examples are 
methods based on validation sets (see [8]), Vapnik's structural risk minimization 
[1] [14], and the methods of Holden [5] and Wang et al. [15]. Our approach requires 
less prior assumptions than most other approaches (see appendix A.1). 
Basic idea of flat minima search. Our algorithm finds a large region in weight 
space with the property that each weight vector from that region has similar small 
error. Such regions are called '`fiat minima. To get an intuitive feeling for why 
fiat minima are interesting, consider this (see also Wolpert [18]): a sharp mini- 
mum corresponds to weights which have to be specified with high precision. A fiat 
minimum corresponds to weights many of which can be given with low precision. In 
the terminology of the theory of minimum description length (MDL), fewer bits of 
information are required to pick a fiat minimum (corresponding to a simple or 
low complexity-network). The MDL principle suggests that low network complex- 
ity corresponds to high generalization performance (see e.g. [4, 13]). Unlike Hinton 
and van Camp's method [3] (see appendix A.3), our approach does not depend on 
explicitly choosing a good prior. 
Our algorithm finds fiat minima by searching for weights that minimize both 
training error and weight precision. This requires the computation of the Hessian. 
However, by using Pearlmutter's and Mller's efficient second order method [11, 7], 
we obtain the same order of complexity as with conventional backprop. Aulomat- 
ically, the melhod effectively reduces numbers of units, weiglhs, and inpul lines, 
as well as the sensitivity of outputs with respect to remaining weights and units. 
Excellent experimental generalization results will be reported in section 4. 
2 TASK / ARCHITECTURE / BOXES 
Generalization task. The task is to approximate an unknown relation D C X x Z 
between a set of inputs X C R N and a set of outputs Z C R K. b is taken to be 
a function. A relation D is obtained from ) by adding noise to the outputs. All 
training information is given by a finite relation Do C D. Do is called the training 
set. The pth element of Do is denoted by an input/target pair (xr, dr). 
Architecture. For simplicity, we will focus on a standard feedforward net (but in 
the experiments, we will use recurrent nets as well). The net has N input units, 
K output units, W weights, and differenttable activation functions. It maps input 
vectors x r 6 R N to output vectors o r 6 R K. The weight from unit j to i is denoted 
by wij. The W-dimensional weight vector is denoted by w. 
�raining error. Mean squared error Eq(w, Do) '- 
� - w-;i II d,,-o,, II a is 
used, where II � II denotes the Euclidian norm, and I.I denotes the cardinality of a set. 
To define regions in weight space with the property that each weight vector from 
that region has similar small error, we introduce the tolerable error Etol, a positive 
constant. Small error is defined as being smaller than Etol. Eq(w, Do) > Eto 
implies under fitting. 
Boxes. Each weight w satisfying Eq(w, Do) _< Eto defines an acceptable mini- 
mum. We are interested in large regions of connected acceptable rainitoh. 
Simplifying Neural Nets by Discovering Flat Minima 531 
Such regions are called flat minima. They are associated with low ex- 
pected generalization error (see [4]). To simplify the algorithm for finding large 
connected regions (see below), we do not consider maximal connected regions but 
focus on so-called boa:eswithin regions: for each acceptable minimum w, its boa: 
M in weight space is a W-dimensional hypercuboid with center w. For simplicity, 
each edge of the box is taken to be parallel to one weight axis. Half the length of the 
box edge in direction of the axis corresponding to weight wij is denoted by Awij, 
which is the maximal (positive) value such that for all i, j, all positive ij _< Awi 
can be added to or subtracted from the corresponding component of w simultane- 
ously without violating Eq(.,Do) _< Eo (Awij gives the precision of wo). M's 
boa: volume is defined by Aw := 2 W rli,j Awij. 
3 THE ALGORITHM 
The algorithm is designed to find a w defining a box Mo with maximal box vol- 
ume Aw. This is equivalent to finding a box Mo with minimal /(w, Do) := 
- log(Aw/2 w) = i. -- log Awi. Note the relationship to MDL (/ is the number 
of bits required to describe the weights). In appendix A.2, we derive the following 
algorithm. It minimizes E(w, Do)= Eq(w, Do) + AB(w, Do), where 
(1) 
Here o h is the activation of the kth output unit, e is a constant, and ) is a positive 
variable ensuring either Eq(w, Do) _< Etol, or ensuring an expected decrease of 
Eq(.,Do) during learning (see [16] for adjusting 
E(w, Do) is minimized by gradient descent. To minimize B(w, Do), we compute 
It can be shown (see [4]) that by using Pearlmutter's and Meller's efficient second 
order method [11, 7], the gradient of B(w, Do) can be computed in O(W) time (see 
details in [4]). Therefore, our algorithm has the same order of complexity 
as standard backprop. 
4 EXPERIMENTAL RESULTS (se [4] for details) 
EXPERIMENT 1 - noisy classification. The first experiment is taken from 
Pearlmutter and losenfeld [12]. The task is to decide whether the x-coordinate of 
a point in 2-dimensional space exceeds zero (class 1) or does not (class 2). Noisy 
training examples are generated as follows: data points are obtained from a Gaus- 
sian with zero mean and stdev 1.0, bounded in the interval [-3.0, 3.0]. The data 
points are misclassified with a probability of 0.05. Final input data is obtained by 
adding a zero mean Gaussian with stdev 0.15 to the data points. In a test with 
2,000,000 data points, it was found that the procedure above leads to 9.27 per cent 
532 Sepp Hochreiter, Jargen Schmidhuber 
Backprop New approach Backprop New approach 
MSE dto MSE dto MSE dto MSE dto 
1 0.220 1.35 0.193 0.00 6 0.219 1.24 0.187 0.04 
2 0.223 1.16 0.189 0.09 7 0.215 1.14 0.187 0.07 
3 0.222 1.37 0.186 0.13 8 0.214 1.10 0.185 0.01 
4 0.213 1.18 0.181 0.01 9 0.218 1.21 0.190 0.09 
5 0.222 1.24 0.195 0.25 10 0.214 1.21 0.188 0.07 
Table 1:10 comparisons of conventional backprop (BP) and our new method (FMS). 
The second row (labeled MSE) shows mean squared error on the test set. The third 
row (dto) shows the difference between the fraction (in per cent) of misclassifica- 
tions and the optimal fraction (9.�7). The remaining rows provide the analoguous 
information for the new approach, which clearly outperforms backprop. 
misclassified data. No method will misclassify less than 9.27 per cent, due to the 
inherent noise in the data. The training set is based on 200 fixed data points. The 
test set is based on 120,000 data points. 
Results. 10 conventional backprop (BP) nets were tested against 10 equally ini- 
tialized networks based on our new method (fiat minima search, FMS). After 
1,000 epochs, the weights of our nets essentially stopped changing (automatic early 
stopping), while backprop kept changing weights to learn the outliers in the data 
set and overfit. In the end, our approach left a single hidden unit h with a maximal 
weight of 30.0 or -30.0 from the x-axis input. Unlike with backprop, the other 
hidden units were effectively pruned away (outputs near zero). So was the y-axis 
input (zero weight to h). It can be shown that this corresponds to an optimal 
net with minimal numbers of units and weights. Table i illustrates the superior 
performance of our approach. 
EXPERIMENT 2 - recurrent nets. The method works for continually running 
fully recurrent nets as well. At every time step, a recurrent net with sigmoid 
activations in [0, 1] sees an input vector from a stream of randomly chosen input 
vectors from the set ((0,0),(0, 1),(1,0),(1, 1)). The task is to switch on the first 
output unit whenever an input (1, 0) had occurred two time steps ago, and to switch 
on the second output unit without delay in response to any input (0, 1). The task 
can be solved by a single hidden unit. 
Results. With conventional recurrent net algorithms, after tr
