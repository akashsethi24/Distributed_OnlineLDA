Using Voice Transformations to Create 
Additional Training Talkers for Word Spotting 
Eric I. Chang and Richard P. Lippmann 
MIT Lincoln Laboratory 
Lexington, MA 02173-0073, USA 
eichang @ sst.ll.mit.edu and rpl @ sst.ll.mit.edu 
Abstract 
Speech recognizers provide good performance for most users but the 
error rate often increases dramatically for a small percentage of talkers 
who are different from those talkers used for training. One expensive 
solution to this problem is to gather more training data in an attempt to 
sample these outlier users. A second solution, explored in this paper, is 
to artificially enlarge the number of training talkers by transforming the 
speech of existing training talkers. This approach is similar to enlarging 
the training set for OCR digit recognition by warping the training digit 
images, but is more difficult because continuous speech has a much 
larger number of dimensions (e.g. linguistic, phonetic, style, temporal, 
spectral) that differ across talkers. We explored the use of simple linear 
spectral warping to enlarge a 48-talker training data base used for word 
spotting. The average detection rate overall was increased by 2.9 
percentage points (from 68.3% to 71.2%) for male speakers and 2.5 
percentage points (from 64.8% to 67.3%) for female speakers. This 
increase is small but similar to that obtained by doubling the amount of 
training data. 
1 INTRODUCTION 
Speech recognizers, optical character recognizers, and other types of pattern classifiers 
used for human interface applications often provide good performance for most users. Per- 
formance is often, however, low and unacceptable for a small percentage of outlier users 
who are presumably not represented in the training data. One expensive solution to this 
problem is to obtain more training data in the hope of including users from these outlier 
876 Eric I. Chang, Richard P. Lippmann 
classes. Other approaches already used for speech recognition are to use input features and 
distance metrics that are relatively invariant to linguistically unimportant differences be- 
tween talkers and to adapt a recognizer for individual talkers. Talker adaptation is difficult 
for word spotting and with poor outlier users because the recognition error rate is high and 
talkers often can not be prompted to recite standard phrases that can be used for adaptation. 
An alternative approach, that has not been fully explored for speech recognition, is to arti- 
ficially expand the number of training talkers using voice transformations. 
Transforming the speech of one talker to make it sound like that of another is difficult be- 
cause speech varies across many difficult-to-measure dimensions including linguistic, pho- 
netic, duration, spectra, style, and accent. The transformation task is thus more difficult than 
in optical character recognition where a small set of warping functions can be successfully 
applied to character images to enlarge the number of training images (Drucker, 1993). This 
paper demonstrates how a transformation accomplished by warping the spectra of training 
talkers to create more training data can improve the performance of a whole-word word 
spotter on a large spontaneous-speech data base. 
2 BASELINE WORD SPOTTER 
A hybrid radial basis function (RBF) - hidden Markov model (HMM) keyword spotter has 
been developed over the past few years that provides state-of-the-art performance for a 
whole-word word spotter on the large spontaneous-speech credit-card speech corpus. This 
system spots 20 target keywords, includes one general filler class, and uses a Viterbi decod- 
ing backtrace as described in (Chang, 1994) to backpropagate errors over a sequence of in- 
put speech frames. This neural network word spotter is trained on target and background 
classes, normalizes target outputs using the background output, and thresholds the resulting 
score to generate putative hits, as shown in Figure 1. Putative hits in this figure are input 
patterns which generate normalized scores above a threshold. The performance of this, and 
other spotting systems, is analyzed by plotting a detection versus false alarm rate curve. 
This curve is generated by adjusting the classifier output threshold to allow few or many 
putative hits. The figure of merit (FOM) is defined as the average keyword detection rate 
when the false alarm rate ranges from 1 to 10 false alarms per keyword per hour. The pre- 
vious best FOM for this word spotter is 67.8% when trained using 24 male talkers and test- 
ed on 11 male talkers, and 65.9% when trained using 24 female talkers and tested on 11 
female talkers. The overall FOM for all talkers is 66.3%. 
CONTINUOUS I 
SPEECH INPUT 
NEURAL NETWORK 
WORDSPOTTER 
PUTATIVE HITS 
Figure 1: Block diagram of neural network word spotten 
Using Voice Transformations to Create Additional Training Talkers for Word Spotting 877 
3 TALKER VARIABILITY 
FOM scores of test talkers vary over a wide range. When training on 48 talkers and then 
performing testing on 22 talkers from the 70 conversations in the NIST Switchboard credit 
card database, the FOM of the test talkers varies from 16.7% to 100%. Most talkers perform 
well above 50%, but there are two female talkers with FOM's of 16.7% and 21.4%. The low 
FOM for individual speakers indicates a lack of training data with voice qualities that are 
similar to these test speakers. 
4 CREATING MORE TRAINING DATA USING VOICE 
TRANSFORMATIONS 
Talker adaptation is difficult for word spotting because error rates are high and talkers often 
can not be prompted to verify adaptation phrases. Our approach to increasing performance 
across talkers uses voice transformation techniques to generate more varied training exam- 
ples of keywords as shown in Figure 2. Other researchers have used talker transformation 
techniques to produce more natural synthesized speech (Iwahashi, 1994, Mizuno, 1994), 
but using talker transformation techniques to generate more training data is novel. 
We have implemented a new voice transformation technique which utilizes the Sinusoidal 
Transform Analysis/Synthesis System (STS) described in (Quatieri, 1992). This technique 
attempts to transform one talker's speech pattern to that of a different talker. The STS gen- 
erates a 512 point spectral envelope of the input speech 100 times a second and also sepa- 
rates pitch and voicing information. Separation of vocal tract characteristic and pitch 
information has allowed the implementation of pitch and time transformations in previous 
work (Quatieri, 1992). The system has been modified to generate and accept a spectral en- 
ORIGINAL 
SPEECH 
VOICE 
TRANSFORMATION 
SYSTEM 
TRANSFORMED 
SPEECH 
Figure 2: 
Generating more training data by artificially transforming original speech 
training data. 
878 Eric L Chang, Richard P. Lippmann 
velope file from an input speech sample. We informally explored different techniques to 
transform the spectral envelope to generate more varied training examples by listening to 
transformed speech. This resulted in the following algorithm that transforms a talker's voice 
by scaling the spectral envelope of training talkers. 
1. Training conversations are upsampled from 8000 Hz to 10,000 Hz to be 
compatible with existing STS coding software. 
2. The STS system processes the upsampled files and generates a 512 point 
spectral envelope of the input speech waveform at a frame rate of 100 frames a 
second and with a window length of approximately 2.5 times the length of each 
pitch period. 
3. A new spectral envelope is generated by linearly expanding or compressing 
the spectral axis. Each spectral point is identified by its index, ranging from 0 to 
511. To transform a spectral profile by 2, the new spectral value at frequencyf is 
generated by averaging the spectral values around the original spectral profile at 
frequency of 0.5 f The transformation process is illustrated in Figure 3. In this 
figure, an original spectral envelope is being expanded by two. The spectral value 
at index 150 is thus transformed to spectral index 300 in the new envelope and the 
original spectral information at high frequencies is lost. 
4. The transformed spectral value is used to resynthesize a speech waveform 
using the vocal tract excitation information extracted from the original file. 
Voice transformation with the STS coder allows listening to transformed speech but re- 
quires long computation. We simplified our approach to one of modifying the spectral scale 
in the spectral domain directly within a mel-scale filterbank analysis program. The incom- 
ing speech sample is processed with an FFT to calculate spectral magnitudes. Then spectral 
magnitudes are linearly transformed. Lastly mel-scale filtering is performed with 10 linear- 
ly spaced filters up to 1000 Hz and logarithmically spaced filters from 1000 Hz up. A cosine 
transform is then used to generate mel-scaled cepstral values that are used by the wordspot- 
ter. Much faster processing can be achieved by applying the spectral transformation as part 
of the filterbank analysis. For example, while performing spectral transformation using the 
STS algorithm takes up to approximately 10 times real time, spectral transformation within 
the mel-scale filterbank program can be accomplished within 1/10 real time on a Sparc 10 
workstation. The rapid processing rate allows on-line spectral transformation. 
5 WORD SPOTTING EXPERIMENTS 
Linear warping in the spectral domain, which is used in the above algorithm, is correct 
when the vocal tract is modelled as a series of lossless acoustic tubes and the excitation 
source is at one end of the vocal tract (Wakita, 1977). Wakita showed that if the vocal tract 
is modelled as a series of equal length, lossless, and concatenated acoustic tubes, then the 
ratio of the areas between the tubes determines the relative resonant frequencies of the vocal 
tract, while the overall 
