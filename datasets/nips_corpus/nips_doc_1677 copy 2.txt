Understanding stepwise generalization of 
Support Vector Machines: a toy model 
Sebastian Risau-Gusman and Mirta B. Gordon 
DRFMC/SPSMS CEA Grenoble, 17 av. des Martyrs 
38054 Grenoble cedex 09, France 
Abstract 
In this article we study the effects of introducing structure in the 
input distribution of the data to be learnt by a simple perceptron. 
We determine the learning curves within the framework of Statis- 
tical Mechanics. Stepwise generalization occurs as a function of 
the number of examples when the distribution of patterns is highly 
anisotropic. Although extremely simple, the model seems to cap- 
ture the relevant features of a class of Support Vector Machines 
which was recently shown to present this behavior. 
1 Introduction 
A new approach to learning has recently been proposed as an alternative to feedfor- 
ward neural networks: the Support Vector Machines (SVM) [1]. Instead of trying to 
learn a non linear mapping between the input patterns and internal representations, 
like in multilayered perceptrons, the SVMs choose a priori a non-linear kernel that 
transforms the input space into a high dimensional feature space. In binary classi- 
fication tasks like those considered in the present paper, the SVMs look for linear 
separation with optimal margin in feature space. The main advantage of SVMs 
is that learning becomes a convex optimization problem. The difficulties of having 
many local minima that hinder the process of training multilayered neural networks 
is thus avoided. One of the questions raised by this approach is why SVMs do not 
overfit the data in spite of the extremely large dimensions of the feature spaces 
considered. 
Two recent theoretical papers [2, 3] studied a family of SVMs with the tools of 
Statistical Mechanics, predicting typical properties in the limit of large dimensional 
spaces. Both papers considered mappings generated by polynomial kernels, and 
more specifically quadratic ones. In these, the input vectors x  R S are trans- 
formed to N(N-b 1)/2-dimensional feature vectors ((x). More precisely, the map- 
ping ((x) - (X, XlX, X2X,.--,xkx) has been studied in [3] as a function of k, 
the number of quadratic features, and (2 (x) -- (x, XlX/N, xx/N,..., XNX/N) has 
been considered in [2], leading to different results. These mappings are particu- 
lar cases of quadratic kernels. In particular, in the case of learning quadratically 
separable tasks with mapping (, the generalization error decreases up to a lower 
bound for a number of examples proportional to N, followed by a further decrease 
if the number of examples increases proportionally to the dimension of the feature 
322 S. Risau-Gusman and M. B. Gordon 
space, i.e. to N 2. In fact, this behavior is not specific of the SVMs. It also arises 
in the typical case of Gibbs learning (defined below) in quadratic feature spaces [4]: 
on increasing the training set size, the quadratic components of the discriminating 
surface are learnt after the linear ones. In the case of learning linearly separable 
tasks in quadratic feature spaces, the effect of overfitting is harmless, as it only 
slows down the decrease of the generalization error with the training set size. In 
the case of mapping (1, overfitting is dramatic, as the generalization error at any 
given training set size increases with the number k of features. 
The aim of the present paper is to understand the influence of the mapping scaling- 
factor on the generalization performance of the SVMs. To this end, it is worth to 
remark that features 2 may be obtained by compressing the quadratic subspace 
of  by a fixed factor. In order to mimic this contraction, we consider a linearly 
separable task in which the input patterns have a highly anisotropic distribution, so 
that the variance in one subspace is much smaller than in the orthogonal directions. 
We show that in this simple toy model, the generalization error as a function of 
the training set size exhibits a cross-over between two different behaviors: a rapid 
decrease corresponding to learning the components in the uncompressed space, fol- 
lowed by a slow improvement in which mainly the components in the compressed 
space are learnt. The latter would correspond, in this highly stylized model, to 
learning the scaled quadratic features in the SVM with mapping 2. 
The paper is organized as follows: after a short presentation of the model, we de- 
scribe the main steps of the Statistical Mechanics calculation. The order parameters 
caracterizing the properties of the learning process are defined, and their evolution 
as a function of the training set size is analyzed. The two regimes of the generaliza- 
tion error are described, and we determine the training set size per input dimension 
at the crossover, as a function of the pertinent parameters. Finally we discuss our 
results, and their relevance to the understanding of the generalization properties of 
SVMs. 
2 The model 
We consider the problem of learning a binary classification task from examples. 
The training data set a contains P = aN N-dimensional patterns (,r ) 
(p - 1,--.,P) where r - sign(  w*) is given by a teacher of weights 
w* - (w, w, .... , Wn). Without any loss of generality we consider normalized teach- 
ers: w* � w* - N. We assume that the components i, (i - 1,-.., N) of the input 
patterns  are independent, identically distributed random variables drawn from 
a zero-mean gaussian distribution, with variance a along Nc directions and unit 
variance in the Nu remaining ones (Nc + Nu - N): 
P(')- H v/2ra2 exp ff-a H x/' exp - ' (1) 
i N. i N, 
We take a < 1 without any loss of generality, as the case a > 1 may be deduced 
from the former through a straightforward rescaling of Nc and Nu. Hereafter, the 
subspace of dimension Nc and variance a will be called compressed subspace. The 
corresponding orthogonal subspace, of dimension Nu = N - Nc, will be called 
uncompressed subspace. 
We study the typical generalization error of a student perceptron learning the clas- 
sification task, using the tools of Statistical Mechanics. The pertinent cost function 
Understanding Stepwise Generalization of SVM's: a Toy Model 323 
is the number of misclassified patterns: 
P 
(2) 
The weight vectors in version space correspond to a vanishing cost (2). Choosing a 
w at random from the a posteriori distribution 
- z -1 PO(W) exp 
(3) 
in the limit of  -  is called Gibbs' learning. In eq. (3),  is equivalent to an 
inverse temperature in the Statistical Mechanics formulation, the cost (2) being the 
energy function. We assume that P0, the a priori distribution of the weights, is 
uniform on the hypersphere of radius v/: 
Po(w) -- (27r�) -N/2 ((w � w -- X). 
(4) 
The normalization constant (2e) N/2 is the leading order term of the hypersphere's 
surface in N-dimensional space. Z is the partition function ensuring the correct 
normalization of P(wla )' 
Z(;T)a) = f dw Po(w) exp (-E(w;T)a)). 
(5) 
In general, the properties of the student are related to those of the free energy 
F(/; a) - -lnZ(; a)/. In the limit N -  with the training set size per 
input dimension a -- PIN constant, the properties of the student weights become 
independant of the particular training set a. They are deduced from the averaged 
free energy per degree of freedom, calculated using the replica trick: 
f()_ __1 lnZ(;Va )=_ l___lim lnZn(;Va) (6) 
N3 N3 n-0 n 
where the overline represents the average over 7), composed of patterns selected 
according to (1). In the case of Gibbs learning, the typical behavior of any intensive 
quantity is obtained in the zero temperature limit/ - oc. In this limit, only error- 
free solutions, with vanishing cost, have non-vanishing posterior probability (3). 
Thus, Gibbs learning corresponds to picking at random a student in version space, 
i.e. a vector w that classifies correctly the training set 7)a, with a probability 
proportional to P0 (w). 
In the case of an isotropic pattern distribution, which corresponds to a - I in 
(1), the properties of cost function (2) have been extensively studied [5]. The case 
of patterns drawn from two gaussian clusters in which the symmetry axis of the 
clusters is the same [6] and different [7] from the teacher's axis, have recently been 
addressed. Here we consider the problem where, instead of having a single direction 
along which the patterns' distribution is contracted (or expanded), there is a finite 
fraction of compressed dimensions. In this case, all the properties of the student's 
perceptron may be expressed in terms of the following order parameters, that have 
to satisfy corresponding extremum conditions of the free energy: 
qb 1 
= (7) 
i6 N. 
ua 1 
= / N w,aw,) (s) 
i N 
324 S. Risau-Gusman and M. B. Gordon 
1 
= waw;) (9) 
iNc 
1 
k = < E waw) (10) 
i N 
1 
Q = ( E (wi)2) (11) 
iN 
where (---) indicates the average over the posterior (3); a, b are replica indices, 
and the subcripts c and u stand for compressed and uncompressed respectively. 
Notice that we do not impose that Q, the typical squared norm of the student's 
components in the compressed subspace, be equal to the corresponding teacher's 
norm Q* = ieN.(W)2/N. 
3 Order parameters and learning curves 
Assuming that the order parameters are invariant under permutation of replicas, 
we can drop the replica indices in equations (7) to (11). We expect that this 
hypothesis of replica symmetry is consistent, like it is in other cases of perceptrons 
learning realizable tasks. The problem is thus reduced to the determination of 
five order parameters. Their meaning becomes clearer if we consider the following 
combinations: 
qc 
qu -- 
Rc -- 
Ru -- 
, (12) 
1 -Q' (13) 
kc 
v/Qx/Q  , (14) 
x/i-Z-x/1 _ Q * , (15) 
1 
q = < E (wi)2)' (16) 
N. 
qc and qu are the typical overlaps between the components of two student vectors in 
the compressed and the uncompress
