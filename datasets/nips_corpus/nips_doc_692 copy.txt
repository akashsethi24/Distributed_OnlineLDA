Extended Regularization Methods for 
Nonconvergent Model Selection 
W. Finnoff, F. Hergert and H.G. Zimmermann 
Siemens AG, Corporate Research and Development 
Otto-Hahn-Ring 6 
8000 Munich 83, Fed. Rep. Germany 
Abstract 
Many techniques for model selection in the field of neural networks 
correspond to well established statistical methods. The method 
of 'stopped training', on the other hand, in which an oversized 
network is trained until the error on a further validation set of ex- 
amples deteriorates, then training is stopped, is a true innovation, 
since model selection doesn't require convergence of the training 
process. 
In this paper we show that this performance can be significantly 
enhanced by extending the 'nonconvergent model selection method' 
of stopped training to include dynamic topology modifications 
(dynamic weight pruning) and modified complexity penalty term 
methods in which the weighting of the penalty term is adjusted 
during the training process. 
1 INTRODUCTION 
One of the central topics in the field of neural networks is that of model selection. 
Both the theoretical and practical side of this have been intensively investigated and 
a vast array of methods have been suggested to perform this task. A widely used 
class of techniques starts by choosing an 'oversized' network architecture then either 
removing redundant elements based on some measure of saliency (pruning), adding a 
further term to the cost function penalizing complexity (penalty terms), and finally, 
observing the error on a further validation set of examples, then stopping training 
as soon as this performance begins to deteriorate (stopped training). The first 
two methods can be viewed as variations of long established statistical techniques 
228 
Extended Regularization Methods for Nonconvergent Model Selection 229 
corresponding in the case of pruning to specification searches, and with respect to 
penalty terms as regularization or biased regression. 
The method of stopped training, on the other hand, seems to be one of the true 
innovations to come out of neural network research. Here, the model chosen doesn't 
require the training process to converge, rather, the training process is used to per- 
form a directed search of weight space to find a model with superior generalization 
performance. Recent theoretical ([B,C,91], IF,91], IF,Z,91]) and empirical results 
([H,F,Z,92], [W,R,H,90]) have provided strong evidence for the efficiency of stopped 
training. In this paper we will show that generalization performance can be fur- 
ther enhanced by expanding the 'nonconvergent method' of stopped training to 
include dynamic topology modifications (dynamic pruning) and modified complex- 
ity penalty term methods in which the weighting of the penalty term is adjusted 
during the training process. Here, the empirical results are based on an extensive se- 
quence of simulation examples designed to reduce the effects of domain dependence 
on the performance comparisons. 
2 CLASSICAL MODEL SELECTION 
Classical model selection methods are generally divided into a number of steps 
that are performed independently. The first step consists of choosing a network 
architecture, then either an objective function (possibly including a penalty term) 
is chosen directly, or in a Bayesian setting, prior distributions on the elements of 
the data generating process (noise, weights in the model, regularizers, etc.) are 
specified from which an objective function is derived. Next, using the specified 
objective function, the training process is started and continued until a convergence 
criterion is fulfilled. The resulting parametrization of the given architecture is then 
placed in a 'pool' from which a final model will be selected. 
The next step can consist of a modification of the network architecture (for exam- 
ple by pruning weights/hidden-neurons/input-neurons), or of the penalty term (for 
example by changing its weighting in' the objective function) or of the Bayesian 
prior distributions. The last two modifications then result in a modification of 
the objective function. This establishes a new framework for the training process 
which is then restatted and continued until convergence, producing another model 
for the pool. This process is iterated until the model builder is satisfied that the 
pool contains a reasonable diversity of candidate models, which are then compared 
with one another using some estimator of generalization ability, (for example, the 
performance on a validation set). 
Stopped training, on the other hand, has a fundamentally different character. Al- 
though the choice of framework remains the same, the essential innovation consists 
of considering every parametrization of a given architecture as a potential model. 
This contrasts with classical methods in which only those parametrizations corre- 
sponding to minima of the objective function are taken into consideration for the 
model pool. 
Under the weight of accumulated empirical evidence (see [W,R,H,90], [H,F,Z,92]) 
theorists have begun to investigate the properties of this technique and have been 
able to show that stopped training has the same sort of regularization effect (i.e. 
reduction of model variance at the cost of bias) that penalty terms provide (see 
230 Finnoff, Hergert, and Zimmermann 
[B,C,91], [F,91]). Since the basic effect of pruning procedures is also to reduce 
network complexity (and consequent model variance) one sees that there is a close 
relationship in the instrumental effects of stopped training, pruning and regular- 
ization. The question remains whether (or under what circumstances) any one or 
combination of these methods produces superior results. 
THE METHODS TESTED 
In our expirements a single hidden layer feedforward network with tanh activation 
functions and ten hidden units was used to fit data sets generated in such a manner 
that network complexity had to be reduced or constrained to prevent overfitting. A 
variety of both classical and nonconvergent methods were tested for this purpose. 
The first we will discuss used weight pruning. To characterize the relevance of a 
weight in a given network, three different test variables were used. The first simply 
measures weight size under the assumption that the training process naturally forces 
nonrelevant weights into a region around zero. The second test variable is that used 
in the Optimal Brain Damage (OBD) pruning procedure of Le Cun et al. (see 
[L,D,S,90]). The final test variables considered are those proposed by Finnoff and 
Zimmermann in [F,Z,91], based on significance tests for deviations from zero in the 
weight update process. 
Two pruning algorithms were used in the experiments, both of which attempt to 
emulate successful interactive methods. In the first algorithm, one removes a cer- 
tain fixed percentage of weights in the network after a stopping criterion is reached. 
The reduced network is then trained further until the stopping criterion is once 
again fulfilled. This process is then repeated until performance breaks down com- 
pletely. This method will be referred to in the following as auto-pruning and was 
implemented using all three types of test variables to determine the weights to be 
removed. The only difference lay in the stopping criterion used. In the case of 
the OBD test variables, training was stopped after the training process converged. 
In the case of the statistical and small weight test variables, training was stopped 
whenever overtraining (defined by a repeated increase in the error on a validation 
set) was observed. A final (restart) variant of auto-pruning using the statistical 
test variables was also tested. This version of auto-pruning only differs in that the 
weights are reinitialized (on the reduced topology) after every pruning step. In 
the tables of results presented in the appendix, the results for auto-pruning using 
the statistical (resp. small weight, resp. OBD) test variables will be denoted by P* 
(resp. G*, resp. O*). The version of auto-pruning using restarts will be denoted by 
p*. 
The second method uses the statistical test variables to both remove and reactivate 
weights. As in auto-pruning the network is trained until overfitting is observed after 
a fixed number of epochs, then test values are calculated for all active and inactive 
weights. Here a fixed number e > 0 is given, corresponding to some quantile value 
of a probability distribution. If the test variable for an active weight falls below 
e the weight is pruned (deactivated). For weights that have already been set to 
zero, the value of the test variables are compared with e, and if larger, the weight is 
reactivated with a small random value. Furthermore, the value of e is increased by 
some Ae > 0 after each pruning step until some value erna is reached. This method 
is referred to as epsi-pruning. Epsi-pruning was tested in versions both with (e*) 
Extended Regularization Methods for Nonconvergent Model Selection 231 
and without restarts (E*). 
Two complexity penalty terms were considered. These consist of a further term 
C,(w) added to the error function which forces the network to achieve a compro- 
mise between fit and network complexity during the training process; here, the 
parameter A  [0, oo) controls the strength of the complexity penalty. The first is 
the quadratic term, the first derivative of which leads to the so-called weight decay 
term in the weight updates (see [H,P,89]). The second is the Weigend/Rumelhart 
penalty term (see [W,R,H,91]). The weight decay penalty term was tested using 
two techniques. {n the first of these, (D*), A was held constant throughout the 
training process. In the second, (d*), A was set to zero until overtraining was ob- 
served, then turned on and held constant for the remainder of the training process. 
The Weigend/Rumelhart penalty term was also tested using these two methods 
