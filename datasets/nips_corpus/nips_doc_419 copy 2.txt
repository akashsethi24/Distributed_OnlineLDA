ADAPTIVE SPLINE NETWORKS 
Jerome H. Friedman 
Department of Statistics and 
Stanford Linear Accelerator Center 
Stanford University 
Stanford, CA 94305 
Abstract 
A network based on splines is described. It automatically adapts the num- 
ber of units, unit parameters, and the architecture of the network for each 
application. 
I INTRODUCTION 
In supervised learning one has a system under study that responds to a set of 
simultaneous input signals x...xn). The response is characterized by a set of 
output signals y, y2,', Ym). The goal is to learn the relationship between the 
inputs and the outputs. This exercise generally has two purposes: prediction and 
understanding. With prediction one is given a set of input values and wishes to 
predict or forecast likely values of the corresponding outputs without having to 
actually run the system. Sometimes prediction is the only purpose. Often, however, 
one wishes to use the derived relationship to gain understanding of how the system 
works. Such knowledge is often useful in its own right, for example in science, or it 
may be used to help improve the characteristics of the system, as in industrial or 
engineering applications. 
The learning is accomplished by taking training data. One observes the outputs 
produced by the system in response to varying sets of input values 
(Yli '''Ymi [ Xli''' Xni]l N. (1) 
These data (1) are then used to train an artificial system (usually a computer 
program) to learn the input/output relationship. The underlying framework or 
model is usually taken to be 
y- f(x...xn)+e, k= l,m (2) 
675 
676 Friedman 
with ave(ek I Xl''' Xn) -- 0. Here (2) yk is the kth responding output signal, f is 
a single valued deterministic function of an n-dimensional argument (inputs) and 
� is a random (stochastic) component that reflects the fact that (if nonzero) y 
is not completely specified by the observed inputs, but is also responding to other 
quantities that are neither controlled nor observed. In this framework the learning 
goal is to use the training data to derive a function f(xl ... Xn) that can serve as a 
reasonable approximation (estimate) of the true underlying (target) function f 
(2). The supervised learning problem can in this way be viewed as one of function 
or surface approximation, usually in high dimensions (n >> 2). 
2 SPLINES 
There is an extensive literature on the theory of function approximation (see Cheney 
[1986] and Chui [1988], and references therein). From this literature spline methods 
have emerged as being among the most successful (see deBoor [1978] for a nice in- 
troduction to spline methods). Loosely speaking, spline functions have the property 
that they are the smoothest for a given flexibility and vice versa. This is impor- 
tant if one wishes to operate under the least restrictive assumptions concerning 
fk(xl... Xn) (2), namely, that it is relatively smooth compared to the noise � but 
is otherwise arbitrary. A spline approximation is characterized by its order q [q = 1 
(linear), q = 2 (quadratic), and q = 3 (cubic) are the most popular orders]. The 
procedure is to first partition the input variable space into a set of disjoint regions. 
The approximation f(xl ... Xn) is taken to be a separate n-dimensional polynomial 
in each region with maximum degree q in any one variable, constrained so that f 
and all of its derivatives to order q - 1 are continuous across all region boundaries. 
Thus, a particular spline approximation is determined by a choice for q, which tends 
not to be very important, and the particular set of chosen regions, which tends to 
be crucial. The central problem associated with spline approximations is how to 
choose a good set of associated regions for the problem at hand. 
2.1 TENSOR-PRODUCT SPLINES 
The most popular method for partitioning the input variable space is by the tensor 
or outer product of interval sets on each of the n axes. Each input axis is partitioned 
into K + i intervals delineated by K points (knots). The regions in the n- 
dimensional space are taken to be the (K + 1) n intersections of all such intervals. 
Figure I illustrates this procedure for K -- 4 knots on each of two axes producing 
25 regions in the corresponding two-dimensional space. 
Owing to the regularity of tensor-product representations, the corresponding spline 
approximation can be represented in a simple form as a basis function expansion. 
Let x- (x...xn). Then 
f(x) =  wt Bt(x) (3) 
where {wt} are the coefficients (weights) for each respective basis function Bt(x), 
and the basis function set {Be(x)} is obtained by taking the tensor product of the 
set of functions 
' ),=0, {(xj - (4) 
Adaptive Spline Networks 677 
over all of the axes, j = 1, n. That is, each of the K + q + 1 functions on each axis j 
(j = 1, n) is multiplied by all of the functions (4) corresponding to all of the other 
axes k (k = 1,n; k  j). As a result the total number of basis functions (3) defining 
the tensor-product spline approximation is 
(K + q + 1) n . (5) 
The functions comprising the second set in (4) are known as the truncated power 
functions: 
(xj -tkj) = { 0 < (6) 
(xj -- tj) q xj > tj 
and there is one for each knot location tj (k = 1,K) on each input axis j (j = 1, n). 
Although conceptually quite simple, tensor-product splines have severe limitations 
that preclude their use in high dimensional settings (n >> 2). These limitations 
stem from the exponentially large number of basis functions that are required (5). 
For cubic splines (q = 3) with five inputs (n = 5) and only five knots per axis 
(K = 5) 59049 basis functions are required. For n = 6 that number is 531441, and 
for n = 10 it is approximately 3.5 x 109. This poses severe statistical problems 
in fitting the corresponding number of weights unless the training sample is large 
compared to these numbers, and computational problems in any case since the 
computation grows as the number of weights (basis functions) cubed. These are 
typical manifestations of the so-called curse-of-dimensionality (Bellman [1961]) 
that afflicts nearly all high-dimensional problems. 
3 ADAPTIVE SPLINES 
This section gives a very brief overview of an adaptive strategy that attempts 
to overcome the limitations of the straightforward application of tensor-product 
splines, making practical their use in high-dimensional settings. This method, called 
MARS (multivariate adaptive regression splines), is described in detail in Friedman 
[1991] along with many examples of its use involving both real and artificially gen- 
erated data. (A FORTRAN program implementing the method is available from 
the author.) 
The method (conceptually) begins by generating a tensor-product partition of the 
input variable space using a large number of knots, K < N, on each axis. Here 
N (1) is the training sample size. This induces a very large (K + 1) number 
of regions. The procedure then uses the training data to select particular unions 
of these (initially large number of) regions to define a relatively small number of 
(larger) regions most suitable for the problem at hand. 
This strategy is implemented through the basis function representation of spline 
approximations (3). The idea is to select a relatively small subset of basis functions 
{B(x))0 M C ,,{Be(x)) uge (7) 
small 
from the very large set (3) (4) (5) i,,duced by the initial tensor-product partition. 
The particular subset for a problem at hand is obtained through standard statistical 
variable subset selection, treating the basis functions as the variables. At the 
678 Friedman 
first step the best single basis function is chosen. The second step chooses the basis 
function that works best in conjunction with the first. At the rath step, the one 
that works best with the ra- 1 already selected, is chosen, and so on. The process 
stops when including additional basis functions fails to improve the approximation. 
3.1 ADAPTIVE SPLINE NETWORKS 
This section describes a network implementation that approximates the adaptive 
spline strategy described in the previous section. The goal is to synthesize a good 
set of spline basis functions (7) to approximate a particular system's input/output 
relationship, using the training data. For the moment, consider only one output y; 
this is generalized later. The basic observation leading to this implementation is 
that the approximation takes the form of sums of products of very simple functions, 
namely the truncated power functions (6), each involving a single input variable, 
and 
= 
k=l 
(8) 
M 
/(x) = wmB(x). (9) 
Here 1 _< j(k) _< n is an input variable and 1 <_ K,, _ n is the number of factors in 
the product (interaction level). 
The network is comprised of an ordered set of interconnected units. Figure 2 shows 
a diagram of the interconnections for a (small) network. Figure 3 shows a schematic 
diagram of each individual unit. Each unit has as its inputs all of the system inputs 
x � � � z, and all of the outputs from the previous units in the network B0 � -. BM. It 
is also characterized by three parameters' j, �, t. The triangles in Figure 3 represent 
selectors. The upper triangle selects one of the system inputs, zj; the left triangle 
selects one of the previous unit outputs, Be. These serve as inputs, along with the 
parameter t, to two internal units that each produce an output. The first output 
is Be-(zj - t)_ and the second is Be. (t - zd).. The whole unit thereby produces 
two outputs BM+t and BM+2, that are available to serve as inputs to future units. 
In addition to units of this nature, there is an initial unit (B0) that produces the 
constant output B0 = 1, that is also available to be selected as an input to all units. 
The output of the entire network, , is a weighted sum (9) of all of the unit outputs 
(including B0 = 1). This is represented by the bottom trapezoid in Figure 2. 
The parameters associated with the
