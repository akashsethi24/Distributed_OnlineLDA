Memory Capacity of Linear vs. Nonlinear 
Models of Dendritic Integration 
Panayiota Poirazi* 
Biomedical Engineering Department 
University of Southern California 
Los Angeles, CA 90089 
poiraziscf . usc. edu 
Bartlett W. Mel* 
Biomedical Engineering Department 
University of Southern California 
Los Angeles, CA 90089 
mel @lnc. us c. edu 
Abstract 
Previous biophysical modeling work showed that nonlinear interac- 
tions among nearby synapses located on active dendritic trees can 
provide a large boost in the memory capacity of a cell (Mel, 1992a, 
1992b). The aim of our present work is to quantify this boost by 
estimating the capacity of (1) a neuron model with passive den- 
dritic integration where inputs are combined linearly across the 
entire cell followed by a single global threshold, and (2) an active 
dendrite model in which a threshold is applied separately to the 
output of each branch, and the branch subtotals are combined lin- 
early. We focus here on the limiting case of binary-valued synaptic 
weights, and derive expressions which measure model capacity by 
estimating the number of distinct input-output functions available 
to both neuron types. We show that (1) the application of a fixed 
nonlinearity to each dendritic compartment substantially increases 
the model's flexibility, (2) for a neuron of realistic size, the capacity 
of the nonlinear cell can exceed that of the same-sized linear cell by 
more than an order of magnitude, and (3) the largest capacity boost 
occurs for cells with a relatively large number of dendritic subunits 
of relatively small size. We validated the analysis by empirically 
measuring memory capacity with randomized two-class classifica- 
tion problems, where a stochastic delta rule was used to train both 
linear and nonlinear models. We found that large capacity boosts 
predicted for the nonlinear dendritic model were readily achieved 
in practice. 
*http://lnc.usc.edu 
158 P Poirazi and B. 144. Mel 
Introduction 
Both physiological evidence and connectionist theory support the notion that in 
the brain, memories are stored in the pattern of learned synaptic weight values. 
Experiments in a variety of neuronal preparations however, ind, icate that the ef- 
ficacy of synaptic transmission can undergo substantial fluctuations up or down, 
or both, during brief trains of synaptic stimuli. Large fluctuations in synaptic ef- 
ficacy on short time scales seem inconsistent with the conventional connectionist 
assumption of stable, high-resolution synaptic weight values. Furthermore, a recent 
experimental study suggests that excitatory synapses in the hippocampus--a region 
implicated in certain forms of explicit memory--may exist in only a few long-term 
stable states, where the continuous grading of synaptic strength seen in standard 
measures of long-term potentiation (LTP) may exist only in the average over a large 
population of two-state synapses with randomly staggered thresholds for learning 
(Petersen, Malenka, Nicoll, & Hopfield, 1998). According to conventional connec- 
tionist notions, the possibility that individual synapses hold only one or two bits of 
long-term state information would seem to have serious implications for the storage 
capacity of neural tissue. Exploration of this question is one of the main themes of 
this paper. 
In a related vein, we have found in previous biophysical modeling studies that 
nonlinear interactions between synapses co-activated on the same branch of an ac- 
tive dendritic tree could provide an alternative form of long-term storage capacity. 
This capacity, which is largely orthogonal to that tied up in conventional synaptic 
weights, is contained instead in the spatial permutation of synaptic connections 
onto the dendritic tree which could in principle be modified in the course of learn- 
ing or development (Mel, 1992a, 1992b). In a more abstract setting, we recently 
showed that a large repository of model flexibility lies in the choice as to which of 
a large number of possible interaction terms available in high dimension is actually 
included in a learning machine's discriminant function, and that the excess capac- 
ity contained in this choice flexibility can be quantified using straightforward 
counting arguments (Poirazi & Mel, 1999). 
2 Two Alternative Models of Dendritic Integration 
In this paper, we use a similar function-counting approach to address the more 
biologically relevant case of a neuron with multiple quasi-independent dendritic 
compartments (fig. 1). Our primary objective has been to compare the memory 
capacity of a cell assuming two different modes of dendritic integration. According 
to the linear model, the neuron's activation level a�(x) prior to thresholding is 
given by a weighted sum of of its inputs over the cell as a whole. According to the 
nonlinear model, the k synaptic inputs to each branch are first combined linearly, 
a static (e.g. sigmoidal) nonlinearity is applied to each of the rn branch subtotals, 
and the resulting branch outputs are summed to produce the cell's overall activity 
aN(x).. 
aN(x) Ei= k 
---- m g(Ej=l X �) (1) 
The expressions for a� and aN were written in similar form to emphasize that the 
models have an identical number of synaptic weights, differing only in the presence 
or absence of a fixed nonlinear function g applied to the branch subtotals. Though 
individual synaptic weights in both models are constrained to have a value of 1, 
any of the d input lines may form multiple connections on the same or different 
Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration 159 
m 1 
k 
Figure 1: A cell is modeled as a set of m identical branches connected to a soma, 
where each branch contains k synaptic contacts driven by one of d distinct input 
lines. 
branches as a means of representing graded synaptic strengths. Similarly, an input 
line which forms no connection has an implicit weight of 0. In light of this restriction 
to positive (or zero) weight values, both the linear and nonlinear models are split 
into two opponent channels a + and a- dedicated to positive vs. negative coefficients, 
respectively. This leads to a final output for each model: 
yL(x) = sgn 
yN(x): sgn - (2) 
where the sgn operator maps the total activation level into a class label of {-1, 1}. 
In the following, we derive expressions for the number of distinct parameter states 
available to the linear rs. nonlinear models, a measure which we have found to be 
a reliable predictor of storage capacity under certain restrictions (Poirazi & Mel 
1999). Based on these expressions, we compute the capacity boost provided by 
the branch nonlinearity as a function of the number of branches m, synaptic sites 
per branch k, and input space dimensionality d. Finally, we test the predictions of 
the analytical model by training both linear and nonlinear models on randomized 
classification problems using a stochastic delta rule, and empirically measure and 
compare the storage capacities of the two models. 
3 Results 
3.1 Counting Parameter States: Linear vs. Nonlinear Model 
We derived expressions for B� and BN, which estimate the total number of param- 
eter bits available to the linear vs. nonlinear models, respectively: 
((k+d-1)) 
BN = 2 log 2 k + m - 1 B: 2 log 2 s + d - 1 
8 
m 
These expressions estimate the number of non-redundant states in each neuron 
type, i.e., those assignments of input lines to dendritic sites which yield distinct 
160 P Poirazi and B. W. Mel 
input-output functions YL or YN. 
These formulae are plotted in figure 2A with d = 100, where each curve represents 
a cell with a fixed number of branches (indicated by m). In each case, the capac- 
ity increases steadily as the number of synapses per branch, k, is increased. The 
logarithmic growth in the capacity of the linear model (evident in an asymptotic 
analysis of the expression for B�) is shown at the bottom of the graph (circles), 
from which it may be seen that the boost in capacity provided by the dendritic 
branch nonlinearity increases steadily with the number of synaptic sites. For a cell 
with 100 branches containing 100 synaptic sites each, the capacity boost relative to 
the linear model exceeds a factor of 20. 
Figure 2B shows that for a given total number of synaptic sites, in this case s = 
m. k = 10,000, the capacity of the nonlinear cell is maximized for a specific choice 
of m and k. The peak of each of the three curves (computed for different values 
of d) occurs for a cell containing 1,250 branches with 8 synapses each. However, 
the capacity is only moderately sensitive to the branch count: the capacity of a cell 
with 100 branches of 100 synapses each, for example, lies within a factor of two of 
the optimal configuration. The linear cell capacities can be found at the far right 
edge of the plot (m = 10,000), since a nonlinear model with one synapse per branch 
has a number of trainable states identical to that of a linear model. 
3.2 Validating the Analytical Model 
To test the predictions of the analytical model, we trained both linear and non- 
linear cells on randomized two-class classification problems. Training samples were 
drawn from a 40-dimensional spherical Gaussian distribution and were randomly 
assigned positive or negative labels--in some runs, training patterns were evenly 
divided between positive and negative labels, with similar results. Each of the 40 
original input dimensions was recoded using a set of 10 1-dimensional binary, non- 
overlapping receptive fields with centers spaced along each dimension such that all 
receptive fields would be activated equally often. This manipulation mapped the 
original 40-dimensional learning problem into 400 dimensions, thereby increasing 
the discriminability of the training samples. The relative memory capacity of linear 
vs. nonlinear cells was then determined empirically by comparing the number of 
training pattern
