Statistical Mechanics of Temporal Association 
in Neural Networks with Delayed Interactions 
Andreas V.M. Herz 
Division of Chemistry 
Caltech 139-74 
Pasadena, CA 91125 
Zhaoping Li 
School of Natural Sciences 
Institute for Advanced Study 
Princeton, NJ 08540 
J. Leo van Hemmen 
Physik-Department 
der TU Miinchen 
D-8046 Garching, FRG 
Abstract 
We study the representation of static patterns and temporal associa- 
tions in neural networks with a broad distribution of signal delays. 
For a certain class of such systems, a simple intuitive understanding 
of the spatio-temporal computation becomes possible with the help 
of a novel Lyapunov functional. It allows a quantitative study of 
the asymptotic network behavior through a statistical mechanical 
analysis. We present analytic calculations of both retrieval quality 
and storage capacity and compare them with simulation results. 
I INTRODUCTION 
Basic computational functions of associative neural structures may be analytically 
studied within the framework of attractor neural networks where static patterns are 
stored as stable fixed-points for the system's dynamics. If the interactions between 
single neurons are instantaneous and mediated by symmetric couplings, there is a 
Lyapunov function for the retrieval dynamics (Hopfield 1982). The global compu- 
tation corresponds in that case to a downhill motion in an energy landscape created 
by the stored information. Methods of equilibrium statistical mechanics may be ap- 
plied and permit a quantitative analysis of the asymptotic network behavior (Amit 
et al. 1985, 1987). The existence of a Lyapunov function is thus of great con- 
ceptual as well as technical importance. Nevertheless, one should be aware that 
environmental inputs to a neural net always provide information in both space and 
time. It is therefore desirable to extend the original Hopfield scheme and to explore 
possibilities for a joint representation of static patterns and temporal associations. 
176 
Statistical Mechanics of Temporal Association in Neural Networks 177 
Signal delays are omnipresent in the brain and play an important role in biolog- 
ical information processing. Their incorporation into theoretical models seems to 
be rather convincing, especially if one includes the distribution of the delay times 
involved. Kleinreid (1986) and Sompolinsky and Kanter (1986) proposed models 
for temporal associations, but they only used a single delay line between two neu- 
rons. Tank and Hopfield (1987) presented a feedforward architecture for sequence 
recognition based on multiple delays, but they just considered information relative 
to the very end of a given sequence. Besides these deficiences, both approaches lack 
the quality to acquire knowledge through a true learning mechanism: Synaptic ef- 
ficacies have to be calculated by hand which is certainly not satisfactory both from 
a neurobiological point of view and also for applications in artificial intelligence. 
This drawback has been overcome by a careful interpretation of the Hebb principle 
(1949) for neural networks with a broad distribution of transmission delays (Herz 
et al. 1988, 1989). After the system has been taught stationary patterns and 
temporal sequences -- by the same principle ! -- it reproduces them with high 
precission when triggered suitably. In the present contribution, we focus on a special 
class of such delay networks and introduce a Lyapunov (energy) functional for the 
deterministic retrieval dynamics (Li and Herz 1990). We thus generalize Hopfield's 
approach to the domain of temporal associations. Through an extension of the usual 
formalism of equilibrium statistical mechanics to time-dependent phenomena, we 
analyze the network performance under a stochastic (noisy) dynamics. We derive 
quantitative results on both the retrieval quality and storage capacity, and close 
with some remarks on possible generalizations of this approach. 
2 DYNAMICS OF THE NEURONS 
Throughout what follows, we describe a neural network as a collection of N two- 
state neurons with activities Si = i for a firing cell and Si = -1 for a quiescent 
one. The cells are connected by synapses with modifiable efficacies Jij(r). Here r 
denotes the delay for the information transport from j to i. We focus on a soliton- 
like propagation of neural signals, characteristic for the (axonal) transmission of 
action potentials, and consider a model where each pair of neurons is linked by 
several axons with delays 0 _< r < truax. Other architectures with only a single 
link have been considered elsewhere (Coolen and Gielen 1988; Herz et al. 1988, 
1989; Kerszberg and Zippelius 1990). External stimuli are fed into the system via 
receptors tri--4-1 with input sensitivity 7. The postsynaptic potentials are given by 
N Tm,x 
h,(t) = (1 - J,j()sj(t - + � (1) 
j=l r=0 
We concentrate on synchronous dynamics (Little 1974) with basic time step At- 
1. Consequently, signal delays take nonnegative integer values. Synaptic noise is 
described by a stochastic Glauber dynamics with noise level 13= T-  (Peretto 1984), 
1 
Prob[Si(t + 1)= 4-1] = 3{1 4- tanh[13hi(t)]}, (2) 
where Prob denotes probability. For 13cx>, we arrive at a deterministic dynamics, 
1, if hi(t) > 0 
Si(t + 1)=sgn[hi(t)] _= -1, if hi(t) < 0 (3) 
178 Herz, Li, and van Hemmen 
3 HEBBIAN LEARNING 
During a learning session the synaptic strengths may change according to the Hebb 
principle (1949). We focus on a connection with delay r between neurons i and j. 
According to Hebb, the corresponding efficacy Jij (r) will be increased if cell j takes 
part in firing cell i. In its physiological context, this rule was originaly formulated 
for excitatory synapses only, but for simplicity, we apply it to all synapses. 
Due to the delay r in (1) and the parallel dynamics (2), it takes r+ ltime steps until 
neuron j actually influences the siaie of neuron i. Jij(v) thus changes by an amount 
proportional to the product of $j(t-r) and $i(t+l). Starting with Jij(r)=O, we 
obtain after P learning sessions, labeled by It and each of duration Dr, 
p D. 
Jij(T) = E(T)N-1 Z Z 
+ - ,9 -- � 
(4) 
The parameters e(r), normalized by x-* e(r) = 1 take morphological character- 
Z_r:O , 
istics of the delay lines into account; N-  is a scaling factor useful for the theoretical 
analysis. By (4), synapses act as microscopic feature detectors during the learning 
sessions and store correlations of the taught sequences in both space (i, j) and time 
(r). In general, they will be asymmetric in the sense that 
During learning, we set T = 0 and 7 = 1 to achieve a clamped learning scenario 
where the system evolves strictly according to the external stimuli, $i(t)= ai(t,-1). 
We study the case where all input sequences 0'i(i) are cyclic with equal periods 
D t, = D, i.e., a(t) = a(t q-D) for all It. In passing we note that one should offer 
the sequences already rmax time steps before allowing synaptic plasticity & la (4) so 
that both Si and S1 are in well defined states during the actual learning sessions. 
We define patterns 0 by 0 =_ ai(t = a) for 0 _< a < D and get 
P D--1 
� (5) 
/=1 a=0 
Our learning scheme is thus a generalization of outer-product rules to spatio- 
temporal patterns. As in the following, temporal arguments of the sequence pattern 
states  and the synaptic couplings should always be understood modulo D. 
4 LYAPUNOV FUNCTIONAL 
Using formulae (1)-(5), one may derive equations of motion for macroscopic order 
parameters (Herz et al. 1988, 1989) but this kind of analysis only applies to the case 
P << logN. However, note that from (4) and (5), we get Jii(r): Jii(D - (2 + r)). 
For all networks whose a priori weights e(r) obey e(r) = e(D - (2 + r)) we have 
thus found an extended synaptic symmetry (Li and Herz 1990), 
%�) = h(o- (2 + 
(6) 
generalizing Hopfield's symmetry assumption Jij - Jji in a natural way to the 
temporal domain. To establish a Lyapunov functional for the noiseless retrieval 
Statistical Mechanics of Temporal Association in Neural Networks 179 
dynamics (3), we take 7=0 in (1) and define 
N D-1 
1 
H(t) --  , , Jij(r)Si(t - a)Sj(t -(a + r + 1)%D) , (7) 
i '= a =0 
where a%b = a mod b. The functional H depends on allstates between t+i-D and 
t so that solutions with constan! H, like D-periodic cycles, need not be static fixed 
points of the dynamics. By (1), (5) and (6), the difference /XH(t)-_-H(t)-H(t-1) 
is 
N 
AH(t) = -y[Si(t)-Si(t-D)]h,(t-1) 
i=1 
e(D- 1) t� D-1 
2N 
/=1 a=0 i=1 
(8) 
The dynamics (3) implies that the first term is nonpositive. Since e(r) > 0, the same 
holds true for the second one. For finite N, H is bounded and AH has to vanish 
as t--*e�. The system therefore settles into a state with Si(t)=Si(t-D) for all i. 
We have thus exposed two important facts: (a) the retrieval dynamics is governed 
by a Lyapunov functional, md (b) the system relaxes to a static state or a limit 
cycle with Si(t)=Si(t -D) --oscillatory solutions with the same period as that of 
the taught cycles or a period which is equal to an integer fraction of D. 
Stepping back for an overview, we notice that H is a Lyapunov functional for all 
networks which exhibit an extended synaptic symmetry (6) and for which the 
matrix J(D - 1) is positive semi-definite. The ttebbian synapses (4) constitute an 
important special case and will be the main subject of our further discussion. 
5 STATISTICAL MECHANICS 
We now prove that a limit cycle of the retrieval dynamics indeed resembles a stored 
sequence. ;Ve proceed in two steps. First, we demonstrate that our task concerning 
cyclic temporal associations can be mapped onto a symmetric network without 
delays. Second, we apply equilibrium statistical mechanics to study such %quivalen! 
systems and derive analytic results for the retrieval quality and storage capacity. 
D-periodic oscillatory solutions of the retrieval dynamics can be interpreted as static 
stat
