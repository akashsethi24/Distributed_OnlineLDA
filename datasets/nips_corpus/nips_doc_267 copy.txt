232 Sejnowski, Yuhas, Goldstein and Jenkins 
Combining Visual and 
with a Neural Network 
Acoustic Speech Signals 
Improves Intelligibility 
T.J. Sejnowski 
The Salk Institute 
and 
Department of Biology 
The University of 
California at San Diego 
San Diego, CA 92037 
B.P. Yuhas 
M.H. Goldstein, Jr. 
Department of Electrical 
and Computer 
Engineering 
The Johns Hopkins 
University 
Baltimore, MD 21218 
R.E. Jenkins 
The Applied Physics 
Laboratory 
The Johns Hopkins 
University 
Laurel, MD 20707 
ABSTRACT 
Acoustic speech recognition degrades in the presence of noise. Com- 
pensatory information is available from the visual speech signals 
around the speaker's mouth. Previous attempts at using these 
visual speech signals to improve automatic speech recognition sys- 
tems have combined the acoustic and visual speech information at a 
symbolic level using heuristic rules. In this paper, we demonstrate 
an alternative approach to fusing the visual and acoustic speech 
information by training feedforward neural networks to map the 
visual signal onto the corresponding short-term spectral amplitude 
envelope (STSAE) of the acoustic signal. This information can 
be directly combined with the degraded acoustic STSAE. Signif- 
icant improvements are demonstrated in vowel recognition from 
noise-degraded acoustic signals. These results are compared to the 
performance of humans, as well as other pattern matching and es- 
timation algorithms. 
I INTRODUCTION 
Current automatic speech recognition systems rely almost exclusively on the acous- 
tic speech signal, and as a consequence, these systems often perform poorly in noisy 
Combining Visual and Acoustic Speech Signals 233 
environments. To compensate for noise-degradation of the acoustic signal, one can 
either attempt to remove the noise from the acoustic signal or supplement the acous- 
tic signal with other sources of speech information. One such source is the visible 
movements of the mouth. For humans, visual speech signals can improve speech 
perception when the acoustic signal is degraded by noise (Sumby and Pollack, 1954) 
and can serve as a source of speech information when the acoustic signal is com- 
pletely absent through lipreading. How can these visual speech signals be used to 
improve the automatic recognition of speech? 
One speech recognition system that has extensively used the visual speech signals 
was developed by Eric Petajan (1987). For a limited vocabulary, Petajan demon- 
strated that the visual speech signals can be used to significantly improve automatic 
speech recognition compared to the acoustic recognition alone. The system relied 
upon a codebook of images that were used to translate incoming images into corre- 
sponding symbols. These symbol strings were then compared to stored sequences 
representing different words in the vocabulary. This categorical treatment of speech 
signals is required because of the computational limitations of currently available 
digital serial hardware. 
This paper proposes an alternative method for processing visual speech signals 
based on analog computation in a distributed network architecture. By using many 
interconnected processors working in parallel large amounts of data can be handled 
concurrently. In addition to speeding up the computation, this approach does not 
require segmentation in the early stages of processing; rather, analog signals from 
the visual and auditory pathways flow through networks in real time and can be 
combined directly. 
Results are presented from a series of experiments that use neural networks to pro- 
cess the visual speech signals of two talkers. In these preliminary experiments, 
the results are limited to static images of vowels. We demonstrate that these net- 
works are able to extract speech information from the visual images, and that this 
information can be used to improve automatic vowel recognition. 
2 VISUAL AND ACOUSTIC SPEECH SIGNALS 
The acoustic speech signal can be modeled as the response of the vocal tract filter to 
a sound source (Fant, 1960). The resonances of the vocal tract are called formants. 
They often appear as peaks in the short-term power spectrum, and are sufficient to 
identify the individual vowels (Peterson and Barney, 1953). The overall shape of 
the short-time spectra is important for general speech perception (Cole, 1980). 
The configuration of the articulators define the shape of the vocal tract and the 
corresponding resonance characteristics of the filter. While some of the articulators 
are visible on the face of the speaker (e.g., the lips, teeth and sometimes the tip 
of the tongue), others are not. The contribution of the visible articulators to the 
acoustic signal results in speech sounds that are much more susceptible to acoustic 
noise distortion than are the contributions from the hidden articulators (Petajan, 
1987), and therefore, the visual speech signal tends to complement the acoustic 
234 Sejnowski, Yuhas, Goldstein and Jenkins 
signal. For example, the visibly distinct speech sounds [b[ and [k[ are among 
the first pairs to be confused when presented acoustically in the presence of noise. 
Because of this complementary structure, the perception of speech in noise is greatly 
improved when both speech signals are present. How and at what level are these 
two speech signals being combined? 
In previous attempts at using the visual speech signals, the information from the 
visual signal was incorporated into the recognition system after the signals were 
categorized (Petajan, 1987). In the approach taken here, visual signals will be used 
to resolve ambiguities in the acoustic signal before either is categorized. By com- 
bining these two sources of information at an early stage of processing, it is possible 
to reduce the number of erroneous decisions made and increase the amount of in- 
formation passed to later stages of processing (Summerfield, 1987). The additional 
information provided by the visual signal can serve to constrain the possible inter- 
pretations of an ambiguous acoustic signal, or it can serve as an alternative source 
of speech information when the acoustical signal is heavily noise-corrupted. In ei- 
ther case, a massive amount of computation must be performed on the raw data. 
New massively-parallel architectures based on neural networks and new training 
procedures have made this approach feasible. 
3 INTERPRETING THE VISUAL SIGNALS 
In our approach, the visual signal was mapped directly into an acoustic representa- 
tion closely related to the vocal tract's transfer function (Summerfield, 1987). This 
representation allowed the visual signal to be fused with the acoustic signal prior 
to any symbolic encoding. 
The visual signals provide only a partial description of the vocal tract transfer 
function and that description is usually ambiguous. For a given visual signal there 
are many possible configurations of the full vocal tract, and consequently many 
possible corresponding acoustic signals. The goal was to define a good estimate of 
that acoustic signal from the visual signal and then use that estimate in conjunction 
with any residual acoustic information. 
The speech signals used in these experiments were obtained from a male speaker 
who was video taped while seated facing the camera, under well-lit conditions. The 
visual and acoustic signals were then transferred and stored on laser disc (Bernstein 
and Eberhardt, 1986), which allowed the access of individual video frames and the 
corresponding sound track. The NTSC video standard is based upon 30 frames per 
second and words are preserved as a series of frames on the laser disc. A data set 
was constructed of 12 examples of 9 different vowels (Yuhas el a!., 1989). 
A reduced area-of-interest in the image was automatically defined and centered 
around the mouth. The resulting sub-image was sampled to produce a topograph- 
ically accurate image of 20 x 25 pixels that would serve to represent the visual 
speech signal. While not the most efficient encoding one could use, it is faithful 
to the parallel approach to computation advocated here and represents what one 
might observe through an array of sensors. 
Combining Visual and Acoustic Speech Signals 235 
Along with each video frame on the laser disc there is 33 ms of acoustic speech. The 
representation chosen for the acoustic output structure was the short-time spectral 
amplitude envelope (STSAE) of the acoustic signal, because it is essential to speech 
recognition and also closely related to the vocal tract's transfer function. It can be 
calculated from the short-term power spectrum of the acoustic signal. The speech 
signal was sampled and cepstral analysis was used to produced a smooth envelope 
of the original power spectrum that could be sampled at 32 frequencies. 
Figure 1: Typical lip images presented to the network. 
Three-layered feedforward networks with non-linear units were used to perform the 
mapping. A lip image was presented across 500 input units, and an estimated 
STSAE was produced across 32 output units. Networks with five hidden units 
were found to provide the necessary bandwidth while minimizing the effects of 
over-learning. The standard backpropagation technique was used to compute the 
error gradients for training the network. However, instead of using a fixed-step 
steepest-descent algorithm for updating the weights, the error gradient was used 
in a conjugate-gradient algorithm. The weights were changed only after all of the 
training patterns were presented. 
4 
INTEGRATING THE VISUAL AND ACOUSTIC SPEECH 
SIGNALS 
To evaluate the spectral estimates, a feedforward network was trained to recognize 
vowels from their STSAE's. With no noise present, the trained network could 
correctly categorized 100% of the 54 STSAE's in its training set: thus serving as a 
perfect recognizer for this data. The vowel recognizer was then presented with speech 
inf
