380 Giles, Sun, Chen, Lee and Chen 
HIGHER ORDER RECURRENT NETWORKS 
& GRAMMATICAL INFERENCE 
C. L. Giles*, G. Z. Sun, H. H. Chen, Y. C. Lee, D. Chen 
Department of Physics and Astronomy 
and 
Institute for Advanced Computer Studies 
A higher 
simulate 
University of Maryland, College Park, MD 20742 
* NEC Research Institute 
4 Independence Way, Princeton, N.J. 08540 
ABSTRACT 
order single layer recursive network easily learns to 
a deterministic finite state machine and recognize regular 
grammars. When an enhanced version of this neural net state machine 
is connected through a common error term to an external analog stack 
memory, the combination can be interpreted as a neural net pushdown 
automata. The neural net finite state machine is given the primitives, 
push and pop, and is able to read the top of the stack. Through a 
gradient descent learning rule derived from the common error 
function, the hybrid network learns to effectively use the stack 
actions to manipulate the stack memory and to learn simple context- 
free grammars. 
INTRODUCTION 
Biological networks readily and easily process temporal information; artificial neural 
networks should do the same. Recurrent neural network models permit the encoding 
and learning of temporal sequences. There are many recurrent neural net models, for ex- 
ample see [Jordan 1986, Pineda 1987, Williams & Zipser 1988]. Nearly all encode the 
current state representation of the models in the activity of the neuron and the next 
state is determined by the current state and input. From an automata perspective, this 
dynamical structure is a state machine. One formal model of sequences and machines 
that generate and recognize them are formal grammars and their respective automata. 
These models formalize some of the foundations of computer science. In the Chomsky 
hierarchy of formal grammars [Hopcroft & Ullman 1979] the simplest level of com- 
plexity is deftned by the finite state machine and its regular grammars. {All machines 
Higher Order Recurrent Networks and Grammatical Inference 381 
and grammars described here are deterministic.} The next level of complexity is de- 
scribed by pushdown automata and their associated context-free grammars. The push- 
down automaton is a finite state machine with the added power to use a stack memory. 
Neural networks should be able to perform the same type of computation and thus 
solve such learning problems as grammatical inference [Fu 1982]. 
Simple grammatical inference is defined as the problem of finding 0earning) a grammar 
from a finite set of strings, often called the teaching sample. Recall that a grammar 
{phrase-structured} is defined as a 4-tuple ( N, V, P, S) where N and V are a nontermi- 
nal and terminal vocabularies, P is a finite set of production rules and S is the start sym- 
bol. Here grammatical inference is also defined as the learning of the machine that 
recognizes the teaching and testing samples. Potential applications of grammatical in- 
ference include such various areas as pattern recognition, information retrieval, pro- 
gramming language design, translation and compiling and graphics languages [Fu 1982]. 
There has been a great deal of interest in teaching neural nets to recognize grammars and 
simulate automata [Allen 1989, Jordan 1986, Pollack 1989, Servant-Schreiber et. al. 
1989,Williams & Zipset 1988]. Some important extensions of that work are discussed 
here. In particular we construct recurrent higher order neural net state machines which 
have no hidden layers and seem to be at least as powerful as any neural net multilayer 
state machine discussed so far. For example, the learning time and training sample size 
are significantly reduced. In addition, we integrate this neural net finite state machine 
with an external stack memory and inform the network through a common objective 
function that it has at its disposal the symbol at the top of the stack and the operation 
primitives of push and pop. By devising a common error function which integrates the 
stack and the neural net state machine, this hybrid structure learns to effectively use the 
stack to recognize context-free grammars. In the interesting work of [Williams & 
Zipset 1988] a recurrent net learns only the state machine part of a Turing Machine, 
since the associated move, read, write operations for each input string are known and are 
given as part of the training set. However, the model we present learns how to manipu- 
late the push, pop, and read primitives of an external stack memory plus learns the ad- 
ditional necessary state operations and structure. 
HIGHER ORDER RECURRENT NETWORK 
The recurrent neural network utilized can be considered as a higher order modification 
of the network model developed by [Williams & Zipset 1988]. Recall that in a recur- 
rent net the activation state S of the neurons at time (t+l) is defined as in a state ma- 
chine automata: 
S(t+l) = F { S(t), I(0; W } , (1) 
where F maps the state S and the input I at time t to the next state. The weight matrix 
W forms the mapping and is usually learned. We use a higher order form for this map- 
ping: 
Si(t+l) = g{ ; Wij k Sj(0 Ik(0 } , (2) 
382 Giles, Sun, Chen, Lee and Chen 
where the range of i, j is the number of state neurons and k the number of input neurons; 
g is defined as g(x)=l/(l+exp(-x)). In order to use the net for grammatical inference, a 
learning rule must be devised. To learn the mapping F and the weight matrix W, given 
a sample set of P strings of the grammar, we construct the following error function E: 
E = 2 = ( Tr- S 0%) ) 2 (3) 
where the sum is over P samples. The error function is evaluated at the end of a present- 
ed sequence of length tp and S o is the activity of the output neuron. For a recurrent net, 
the output neuron is a designated member of the state neurons. The target value of any 
pattern is 1 for a legal string and 0 for an illegal one. Using a gradient descent proce- 
dure, we minimize the error E function for only the rth pattern. The weight update rule 
becomes 
AWij k = -T I 7wE = T I E r { )So(tp) / )Wij k } , (4) 
where T I is the learning rate. Using eq. (2), )So(tp) / )Wij k is easily calculated using 
the recursion relationship and the choice of an initial value for Si(t = 0)/Wij k, 
Sl(t+l)/Wijk = hi (Sl(t+l)) { /Jli Sj(t) Ik(0 + lg Wlm n In( 0 Sm(t)fOWij k } (5) 
where h(x) = dg/dx. Note that this requires )Si(0 / )Wij k be updated as each element 
of each string is presented and to have a known initj_a_! value. Given an adequate network 
topology, the above neural net state machine should be capable of learning any regular 
grammar of arbitrary string length or a more complex grammar of finite length. 
FINITE STATE MACHINE SIMULATION 
In order to see how such a net performs, we trained the net on a regular grammar, the du- 
al parity grammar. An arbitrary length string of O's and l's has dual parity if the 
string contains an even number of O's and an even number of l's. The network architec- 
ture was 3 input neurons and either 3, 4, or 5 state neurons with fully connected second 
order interconnection weights. The string vocabulary 0,1,e (end symbol) used a unary 
representation. The init_ia! training set consisted of 30 positive and negative strings of 
increasing sting length up to length 4. After including in the training all strings up to 
length 10 which resulted in misclassification(about 30 strings), the neural net state ma- 
chine perfectly recognized on all strings up to length 20. Total training time was usual- 
ly 500 epochs or less. 
By looking closely at the dynamics of learning, it was discovered that for different in- 
puts the states of the network tended to cluster around three values plus the initial 
state. These four states can be considered as possible states of an actual finite state ma- 
chine and the movement between these states as a function of input can be interpreted as 
the state transitions of a state machine. Constructing a state machine yields a perfect 
four state machine which will recognize any dual parity grammar. Using minimization 
procedures [Fu 1982], the extraneous state transitions can be reduced to the minimal 4- 
Higher Order Recurrent Networks and Grammatical Inference 383 
state machine. The extracted state machine is shown in Fig. 1. However, for more com- 
plicated grammars and different initial conditions, it might be difficult to extract the 
finite state machine. When different initial weights were chosen, different extraneous 
transition diagrams with more states resulted. What is interesting is that the neural 
net finite state machine learned this simple grammar perfectly. A first order net can al- 
so learn this problem; the higher order net learns it much faster. It is easy to prove that 
there are finite sate machines that cannot be represented by fffst order, single layer re- 
current nets [Minsky 1967]. For further discussion of higher order state machines, see 
[Liu, et. al. 1990]. 
0 
FIGURE 1: A learned four state machine; state 1 is both the start 
and the final state. 
NEURAL NET PUSHDOWN AUTOMATA 
In order to easily learn more complex deterministic grammars, the neural net must 
somehow develop and/or learn to use some type of memory, the simplest being a stack 
memory. Two approaches easily come to mind. Teach the additional weight structure in 
a multilayer neural network to serve as memory [Pollack 1989] or teach the neural net 
to use an external memory source. The latter is appealing because it is well known 
from formal language theory that a finite stack machine requires significantly fewer re- 
sources than a finite state machine for bounded problems such as recognizing a f'mite 
length context-free grammar. To teach a neural net to use a stack memory poses at least 
three problems: 1) how to construct the stack memory, 2) how to couple the stack mem- 
ory to the neural net state machine, and 3) how to
