Self-Organizing Rules for Robust 
Principal Component Analysis 
Lei Xu,2*and Alan Yuille  
1. Division of Applied Sciences, Harvard University, Cambridge, MA 02138 
2. Dept. of Mathematics, Peking University, Beijing, P.R.China 
Abstract 
In the presence of outliers, the existing self-organizing rules for 
Principal Component Analysis (PCA) perform poorly. Using sta- 
tistical physics techniques including the Gibbs distribution, binary 
decision fields and effective energies, we propose self-organizing 
PCA rules which are capable of resisting outliers while fulfilling 
various PCA-related tasks such as obtaining the first principal com- 
ponent vector, the first k principal component vectors, and directly 
finding the subspace spanned by the first k vector principal com- 
ponent vectors without solving for each vector individually. Com- 
parative experiments have shown that the proposed robust rules 
improve the performances of the existing PCA algorithms signifi- 
cantly when outliers are present. 
1 INTRODUCTION 
Principal Component Analysis (PCA) is an essential technique for data compression 
and feature extraction, and has been widely used in statistical data analysis, com- 
munication theory, pattern recognition and image processing. In the neural network 
literature, a lot of studies have been made on learning rules for implementing PCA 
or on networks closely related to PCA (see Xu & Yuille, 1993 for a detailed reference 
list which contains more than 30 papers related to these issues). The existing rules 
can fulfil various PCA-type tasks for a number of application purposes. 
*Present address: Dept. of Brain and Cognitive Sciences, E10-243, Massachusetts 
Institute of Technology, Cambridge, MA 02139. 
467 
468 Xu and Yuille 
However, almost all the previously mentioned PCA algorithms are based on the 
assumption that the data has not been spoiled by outliers (except Xu, Oja&Suen 
1992, where outliers can be resisted to some extent.). In practice, real data often 
contains some outliers and usually they are not easy to separate from the data set. 
As shown by the experiments described in this paper, these outliers will significantly 
worsen the performances of the existing PCA learning algorithms. Currently, little 
attention has been paid to this problem in the neural network literature, although 
the problem is very important for real applications. 
Recently, there have been some success in applying the statistical physics approach 
to a variety of computer vision problems (Yuille, 1990; Yuille, Yang&Geiger 1990; 
Yuille, Geiger&Bulthoff, 1991). In particular, it has also been shown that some 
techniques developed in robust statistics (e.g., redescending M-estimators, least- 
trimmed squares estimators) appear naturally within the Bayesian formulation by 
the use of the statistical physics approach. In this paper we adapt this approach 
to tackle the problem of robust PCA. Robust rules are proposed for various PCA- 
related tasks such as obtaining the first principal component vector, the first k 
principal component vectors, and principal subspaces. Comparative experiments 
have been made and the results show that our robust rules improve the performances 
of the existing PCA algorithms significantly when outliers are present. 
2 PCA LEARNING AND ENERGY MINIMIZATION 
There exist a number of self-organizing rules for finding the first principal compo- 
nent. Three of them are listed as follows (Oja 1982, 85; Xu, 1991, 93): 
(t + 1) = ffi(t) + aa(t)(Zy - r(t)y2), (1) 
_2, 
(t + 1) = r(t) + aa(t)(Yy - r(t)(t) y J' (2) 
r(t + 1) = (t) + rta(t)[y(- if) + (y - yt)x-q. (3) 
where y = (t) T�, ff = yr(t), yt = ffi(t)T,7 and aa(t) _> 0 is the learning rate which 
decreases to zero as t  c while satisfying certain conditions, e.g., 
c, -, aa(t) q < c for some q > 1. 
Each of the three rules will converge to the principal component vector  almost 
surely under some mild conditions which are studied in detail in by Oja (1982&85) 
and Xu (1991&93). Regarding ffi as the weight vector of a linear neuron with output 
y -- r*, all the three rules can be considered as modifications of the well known 
Hebbian rule r(t + 1) = ff(t) + aa(t)y through introducing additional terms for 
preventing Ilr(t)ll from going to 
The performances of these rules deteriorate considerably when data contains out- 
liers. Although some outlier-resisting versions of eq.(1) and eq.(2) have also been 
recently proposed (Xu, Oja &Suen, 1992), they work well only for data which is not 
severely spoiled by outliers. In this paper, we adopt a totally different approach--we 
generalize eq.(1),eq.(2) and eq.(3) into more robust versions by using the statistical 
physics approach. 
To do so, first we need to connect these rules to energy functions. It follows from Xu 
(1991&93) and Xu & Yuille(1993) that the rules eq.(2) and eq.(3) are respectively 
Self-Organizing Rules for Robust Principal Component Analysis 469 
on-line gradient descent rules for minimizing Jl(r), J2(r) respectivelyl: 
N 
i:1 
It has also been provel that the rule given by eq.(1) satisfies (Xu, 1991, 93): 
(a) fzfza _> O,E(ft)T�(fz) _> O, with fz = �y- ry , fza = ay-y2; (b) 
E(fzl)TE(ft3) > 0, with ft3 = y(-g)+(y-y')�; (c) Both J1 and Ja have only one 
local (also global) minimum tr(Z)- rZ(, and all the other critical points (i.e., 
the points satisfy 0s,(,a)-O,i= 1,2)are saddle points. Here Z = E{t}, and  
is the eigenvector of E corresponding to the largest eigenvalue. 
That is, the rule eq.(1) is a downhill algorithm for minimizing Ja in both the on 
line sense and the average sense, and for minimizing Ja in the average sense. 
3 GENERALIZED ENERGY AND ROBUST PCA 
We further regard Jx(), J2(r) as special cases of the following general energy: 
N 
1 
() =  z(i,), z(i,) > 0. (6) 
._. 
where z(i, ) is the portion of energy contributed by the sample i, and 
T, 
= ) 
,11 & (7) 
Following (Yuille, 1990 a& b), we now generalize energy eq.(6) into 
= + (8) 
where 1 = {�,i = 1,...,N} is a binary field {} with each  being a random 
variable taking value either 0 or 1.  acts as a decision indicator for deciding 
whether i is an outlier or a sample. When  = 1, the portion of energy contributed 
by the sample i is taken into consideration; otherwise, it is equivalent to discarding 
  an outlier. Epo() is the a priori portion of energy contributed by the a 
priori distribution of {  }. A natural choice is 
N 
Epio(�) = .(1 - ) (9) 
=1 
This choice of priori has a natural interpretation: for fixed  it is energetically 
favourable to set  = 1 (i.e., not regarding  as an outlier) if z(, ) <  (i.e., 
We have J() > 0, since Ta?--  = i111 = sin  0,n >_ 0. 
470 Xu and Yuille 
the portion of energy contributed by Zi is smaller than a prespecified threshold) 
and to set it to 0 otherwise. 
Based on E(17, ), we define a Gibbs distribution (Parisi 1988): 
(10) 
where Z is the partition function which ensures y. 'ra P[I, if,] = 1. 
1 
compute 
Then we 
1 1 
= H Y e-{V'z()+nO-v')}= e -z*--(r) (11) 
i v,={0,} Z 
Z = Ze  E,jj() = 1 1og{1 + e-{z(2)-}. (12) 
i 
E,j is called the effective energy. Each term in the sum for E, is approximately 
z(i, ) for small values of z but becomes constant as z(i, )  . In this way 
outliens, which are more likely to yield large values of z(i, ), are treated differently 
from samples, and thus the estimation  obtained by minimizing Eejj () will be 
robust and able to resist outliens. 
EeL () is usually not a convex function and may have many local minima. The 
statistical physics framework suggests using deterministic annealing to minimize 
Eelf (nil). That is, by the following gradient descent rule eq.(13), to minimize 
Eefj(r) for small fi and then track the minimum as fi increases to infinity (the 
zero temperature limit): 
1 
rfi(t + 1) = r(t) - ab(t) y. 1 + e(z(,,'(t)) - 
i 
04 
(13) 
More specifically, with z's chosen to correspond to the energies Jx and J respec- 
tively, we have the following batch-way learning rules for robust PCA: 
1 (t) 
r(t + l) = r(t) + ab(t) Y l + e(Z(.,.(O)_.) (iyi - r(t).r(t) y) , (14) 
i 
1  
(t + 1) = r(t) + a(t) E 1 + e(z(e,,m(t)) -0) [yi(i - gi) + (Yi - Yi)�i]. (15) 
i 
For data that comes incrementally or in the ondine way, we correspondingly have 
the following adaptive or stochastic approximation versions 
1 r(t) 
r(t + 1) = ff(t) + a,(t) 1 + e((,,ra(t))-) (iYi - r(t)tr(t ) 
ff(t + 1) = r(t) + ra(t) 
yT), (16) 
1 
1 (17) 
Self-Organizing Rules for Robust Principal Component Analysis 471 
It can be observed that the difference between eq.(2) and eq.(16) or eq.(3) and 
eq.(17) is that the learning rate eta(t) has been modified by a multiplicative factor 
1 
am(t) = 1 + e(Z(,,'(0)-n) ' (18) 
which adaptively modifies the learning rate to suit the current input i. This 
modifying factor has a similar function as that used in Xu, Oja&Suen(1992) for 
robust line fitting. But the modifying factor eq.(18) is more sophisticated and 
performs better. 
Based on the connecticn between the rule eq.(1) and J or J2, given in see.2, we 
can also formally use t,ae modifying factor et,(t) to turn the rule eq.(1) into the 
following robust version: 
1 
+ = + 1 + - 
4 ROBUST RULES FOR k PRINCIPAL COMPONENTS 
In a similar way to SGA (Oja, 1992) and GHA (Sanger, 1989) we can generalize the 
robust rules eq.(19), eq.(16) and eq.(17) into the following general form of robust 
rules for finding the first k principal components: 
1 
rj(t -]- 1) = rj(t) -l- et.(t) l + e(4,(j),.%(O)_n Arj(xi(j), rj(t)), (20) 
j--1 
i(O) = i, gi(j + 1) = gi(j) - EY,(r)rfi(t), Yi(j) = (t)i(j), (21) 
where Arfij(i(3) fij(t)), z(xi(3), fftj(t)) have four possibilities (Xu & Yuille, 1993). 
As an example, one of them is given here 
Arfij('i(j), rfij(t)) = ('i(j)yi(j) -- rfij(t)yi(j)2), 
3, ' 
