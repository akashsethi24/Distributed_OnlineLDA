An Improved Policy Iteration Algorithm 
for Partially Observable MDPs 
Eric A. Hansen 
Computer Science Department 
University of Massachusetts 
Amherst, MA 01003 
hansen(cs.umass.edu 
Abstract 
A new policy iteration algorithm for partially observable Markov 
decision processes is presented that is simpler and more efficient than 
an earlier policy iteration algorithm of Sondik (1971,1978). The key 
simplification is representation of a policy as a finite-state controller. 
This representation makes policy evaluation straightforward. The pa- 
per's contribution is to show that the dynamic-programming update 
used in the policy improvement step can be interpreted as the trans- 
formation of a finite-state controller into an improved finite-state con- 
troller. The new algorithm consistently outperforms value iteration 
as an approach to solving infinite-horizon problems. 
I Introduction 
A partially observable Markov decision process (POMDP) is a generalization of the 
standard completely observable Markov decision process that allows imperfect infor- 
mation about the state of the system. First studied as a model of decision-making in 
operations research, it has recently been used as a framework for decision-theoretic 
planning and reinforcement learning with hidden state (Monahan, 1982; Cassandra, 
Kaelbling, 2 Littman, 1994; Jaakkola, Singh, 2 Jordan, 1995). 
Value iteration and policy iteration algorithms for POMDPs were first developed by 
Sondik and rely on a piecewise linear and convex representation of the value function 
(Sondik, 1971; Smallwood & Sondik,1973; Sondik, 1978). Sondik's policy iteration 
algorithm has proved to be impractical, however, because its policy evaluation step is 
extremely complicated and difficult to implement. As a result, almost all subsequent 
work on dynamic programming for POMDPs has used value iteration. In this paper, 
we describe an improved policy iteration algorithm for POMDPs that avoids the dif- 
ficulties of Sondik's algorithm. We show that these difficulties hinge on the choice of 
a policy representation and can be avoided by representing a policy as a finite-state 
1016 E. A. Hansen 
controller. This representation makes the policy evaluation step easy to implement 
and efficient. We show that the policy improvement step can be interpreted in a nat- 
ural way as the transformation of a finite-state controller into an improved finite-state 
controller. Although it is not always possible to represent an optimal policy for an 
infinite-horizon POMDP as a finite-state controller, it is always possible to do so when 
the optimal value function is piecewise linear and convex. Therefore representation of 
a policy as a finite-state controller is no more limiting than representation of the value 
function as piecewise linear and convex. In fact, it is the close relationship between 
representation of a policy as a finite-state controller and representation of a value 
function as piecewise linear and convex that the new algorithm successfully exploits. 
The paper is organized as follows. Section 2 briefly reviews the POMDP model and 
Sondik's policy iteration algorithm. Section 3 describes an improved policy iteration 
algorithm. Section 4 illustrates the algorithm with a simple example and reports a 
comparison of its performance to value iteration. The paper concludes with a discus- 
sion of the significance of this work. 
2 Background 
Consider a discrete-time POMDP with a finite set of states $, a finite set of actions 
A, and a finite set of observations O. Each time period, the system is in some state 
i  $, an agent chooses an action a  A for which it receives a reward with expected 
value r, the system makes a transition to state j  $ with probability Pi', and the 
agent observes 0  O with probability qj. We assume the performance objective is 
to maximize expected total discounted reward over an infinite horizon. 
Although the state of the system cannot be directly observed, the probability that it is 
in a given state can be calculated. Let r denote a vector of state probabilities, called 
an information state, where ri denotes the probability that the system is in state i. If 
action a is taken in information state r and 0 is observed, the successor information 
state is determined by revising each state probability using Bayes' theorem: rj - 
-ie$ 'iPiqja/-i,je$ 'iPiqja ' Geometrically, each information state r is a point in 
the ([$[ - 1)-dimensional unit simplex, denoted II. 
It is well-known that an information state r is a sufficient statistic that summarizes 
all information about the history of a POMDP necessary for optimal action selection. 
Therefore a POMDP can be recast as a completely observable MDP with a continuous 
state space II and it can be theoretically solved using dynamic programming. The key 
to practical implementation of a dynamic-programming algorithm is a piecewise-linear 
and convex representation of the value function. Smallwood and Sondik (1973) show 
that the dynamic-programming update for POMDPs preserves the piecewise linearity 
and convexity of the value function. They also show that an optimal value function four 
a finite-horizon POMDP is always piecewise linear and convex. For infinite-horizon 
POMDPs, Sondik (1978) shows that an optimal value function is sometimes piecewise 
linear and convex and can be aproximated arbitrarily closely by a piecewise linear and 
convex function otherwise. 
A piecewise linear and convex value function V can be represented by a finite set 
of ISI-dimensional vectors, r = such that V(r) = maxk --ies riai ' A 
dynamic-programming update transforms a value function V represented-by a set F 
of a-vectors into an improved value function V' represented by a set F' of a-vectors. 
Each possible a-vector in F' corresponds to choice of an action, and for each possible 
observation, choice of a successor vector in F. Given the combinatorial number of 
choices that can be made, the maximum nu, mber of vectors in F' is IAI IF[ Iï¿½1. However 
most of these potential vectors are not needed to define the updated value function 
and can be pruned. Thus the dynamic-programming update problem is to find a 
An Improved Policy Iteration Algorithm for Partially Observable MDPs 1017 
minimal set of vectors F  that represents V , given a set of vectors F that represents 
V. Several algorithms for performing this dynamic-programming update have been 
developed but describing them is beyond the scope of this paper. Any algorithm for 
performing the dynamic-programming update can be used in the policy improvement 
step of policy iteration. The algorithm that is presently the fastest is described by 
(Cassandra, Littman, & Zhang, 1997). 
For value iteration, it is sufficient to have a representation of the value function because 
a policy is defined implicitly by the value function, as follows, 
5(r) = a(arg mx Z ricti)' (1) 
iS 
where a(k) denotes the action associated with vector a k. But for policy iteration, 
a policy must be represented independently of the value function because the policy 
evaluation step computes the value function of a given policy. Sondik's choice of 
a policy representation is influenced by Blackwell's proof that for a continuous-space 
infinite-horizon MDP, there is a stationary, deterministic Markov policy that is optimal 
(Blackwell, 1965). Based on this result, Sondik restricts policy space to stationary and 
deterministic Markov policies that map the continuum of information space II into 
action space A. Because it is important for a policy to have a finite representation, 
Sondik defines an admissible policy as a mapping from a finite number of polyhedral 
regions of II to A. Each region is represented by a set of linear inequalities, where 
each linear inequality corresponds to a boundary of the region. 
This is Sondik's canonical representation of a policy, but his policy iteration algorithm 
makes use of two other representations. In the policy evaluation step, he converts a 
policy from this representation to an equivalent, or approximately equivalent, finite- 
state controller. Although no method is known for computing the value function of 
a policy represented as a mapping from II to A, the value function of a finite-state 
controller can be computed in a straightforward way. In the policy improvement 
step, Sondik converts a policy represented implicitly by the updated value function 
and equation (1) back to his canonical representation. The complexity of translating 
between these different policy representations - especially in the policy evaluation step 
- makes Sondik's policy iteration algorithm difficult to implement and explains why 
it is not used in practice. 
3 Algorithm 
We now show that policy iteration for POMDPs can be simplified - both conceptually 
and computationally - by using a single representation of a policy as a finite-state 
controller. 
3.1 Policy evaluation 
As Sondik recognized, policy evaluation is straightforward when a policy is represented 
as a finite-state controller. An a-vector representation of the value function of a finite- 
state controller is computed by solving the system of linear equations, 
 = r() x-', a()qa(&)as(,o) (2) 
ai -{-  2. Pij jo j , 
j,O 
where k is an index of a state of the finite-state controller, a(k) is the action associated 
with machine state k, and s(k, O) is the index of the successor machine state if 0 is 
observed. This value function is convex as well as piecewise linear because the expected 
value of an information state is determined by assuming the controller is started in 
the machine state that optimizes it. 
1018 E. A. Hansen 
1. Specify an initial finite-state controller, 5, and select e for detecting conver- 
gence to an e-optimal policy. 
Policy evaluation: Calculate a set F of a-vectors that represents the value 
function for (5 by solving the system of 
