Neural Network Definitions of Highly 
Predictable Protein Secondary Structure 
Classes 
Alan Lapedes 
Complex Systems Group (T13) 
LANL, MS B213 Los Alamos N.M. 87545 
and The Santa Fe Institute, Santa Fe, New Mexico 
Evan Steeg 
Department of Computer Science 
University of Toronto, Toronto, Canada 
Robert Farbet 
Complex Systems Group (T13) 
LANL, MS B213 Los Alamos N.M. 87545 
Abstract 
We use two co-evolving neural networks to determine new classes 
of protein secondary structure which are significantly more pre- 
dictable from local amino sequence than the conventional secondary 
structure classification. Accurate prediction of the conventional 
secondary structure classes: alpha helix, beta strand, and coil, from 
primary sequence has long been an important problem in compu- 
tational molecular biology. Neural networks have been a popular 
method to attempt to predict these conventional secondary struc- 
ture classes. Accuracy has been disappointingly low. The algo- 
rithm presented here uses neural networks to similtaneously exam- 
ine both sequence and structure data, and to evolve new classes 
of secondary structure that can be predicted from sequence with 
significantly higher accuracy than the conventional classes. These 
new classes have both similarities to, and differences with the con- 
ventional alpha helix, beta strand and coil. 
809 
810 Lapedes, Steeg, and Farber 
The conventional classes of protein secondary structure, alpha helix and beta 
sheet, were first introduced in 1951 by Linus Pauling and Robert Corey 
[Pauling, 1951] on the basis of molecular modeling. Prediction of secondary 
structure from the amino acid sequence has long been an important problem 
in computational molecular biology. There have been numerous attempts to 
predict locally defined secondary structure classes using only a local window 
of sequence information. The prediction methodology ranges from a combina- 
tion of statistical and rule-based methods [Chou, 1978] to neural net methods 
[Qian, 1988], [Maclin, 1992], [Kneller, 1990], [Stolorz, 1992]. Despite a variety of 
intense efforts, the accuracy of prediction of conventional secondary structure is 
still distressingly low. 
In this paper we will use neural networks to generalize the notion of protein sec- 
ondary structure and to find new classes of structure that are significantly more 
predictable. We define protein secondary structure to be any classification of 
protein structure that can be defined using only local windows of structural in- 
formation about the protein. Such structural information could be, e.g., the classic 
�  angles [Schulz, 1979] that describe the relative orientation of peptide units along 
the protein backbone, or any other representation of local backbone structure. A 
classification of local structure into secondary structure classes, is defined to be 
the result of any algorithm that uses a representation of local structure as Input, 
and which produces discrete classification labels as Output. This is a very general 
definition of local secondary structure that subsumes all previous definitions. 
We develop classifications that are more predictable than the standard classifica- 
tions [Pauling, 1951] [Kabsch, 1983] which were used in previous machine learning 
projects, as well as in other analyses of protein shape. We show that these new, 
predictable classes of secondary structure bear some relation to the conventional 
category of helix, but also display significant differences. 
We consider the definition, and prediction from sequence, of just two classes of 
structure. The extension to multiple classes is not difficult, but will not be made 
explicit here for reasons of clarity. We won't discuss details concerning construction 
of a representative training set, or details of conventional neural network train- 
ing algorithms, such as backpropagation. These are well studied subjects that 
are addressed in e.g., [Stolorz, 1992] in the context of protein secondary struc- 
ture prediction. We note in passing that one can employ complicated network 
architectures containing many output neurons (e.g. three output neurons for pre- 
dicting alpha helix, beta chain, random coil), or many hidden units etc. (c.f. 
[Stolorz, 1992], [Qian, 1988], [Kneller, 1990]). However, explanatory figures pre- 
sented in the next section employ only one output unit per net, and no hidden 
units, for clarity. 
Neural Network Definitions of Highly Predictable Protein Secondary Structure Classes 811 
A widely adopted definition of protein secondary structure classes is due to Kabsch 
and Sander [Kabsch, 1983]. It has become conventional to use the Kabsch and 
Sander definition to define, via local structural information, three classes of sec- 
ondary structure: alpha helix, beta strand, and a default class called random coil. 
The Kabsch and Sander alpha helix and beta strand classification captures in large 
part the classification first introduced by Pauling and Corey [Pauling, 1951]. Soft- 
ware implementing the Kabsch and Sander definitions, which take a local window 
of structural information as Input, and produce the Kabsch and Sander secondary 
structure classification of the window as Output, is widely available. 
The key ideas of this paper are contained in Fig. (1). 
E: E,Correlation(()[('),O 
Left Net 
Maps AA sequence to 
structure. 
secondary 
Ri2ht Net 
Maps (l).q  to secondary 
structure. 
In this figure the Kabsch and Sander rules are represented by a second neural net- 
work. The Kabsch and Sander rules are just an Input/Output mapping (from a 
local window of structure to a classification of that structure) and may in princi- 
ple be replaced with an equivalent neural net representing the same Input/Output 
mapping. We explicitly demonstrated that a simple neural net is capable of repre- 
senting rules of the complexity of the Kabsch and Sander rules by training a network 
to perform the same structure classification as the Kabsch and Sander rules, and 
obtained high accuracy. 
The representation of the structure data in the right-hand network uses q)gt angles. 
The right-hand net sees a window of q)gt angles corresponding to the window of 
amino acids in the left-hand network. Problems due to the angular periodicity of 
the gt angles (i.e., 360 degrees and 0 degrees are different numbers, but represent 
the same angle) are eliminated by utilizing both the sin and cos of each angle. 
812 Lapedes, Steeg, and Farber 
The representation of the amino acids in the left-hand network is the usual unary 
representation employing twenty bits per amino acid. Results quoted in this paper 
do not use a special twenty-first bit to represent positions in a window extending 
past the ends of a protein. 
Note that the right-hand neural network could implement extremely general def- 
initions of secondary structure by changing the weights. We next show how to 
change the weights in a fashion so that new classifications of secondary structure 
are derived under the important restriction that they be predictable from amino 
acid sequence. In other words, we require that the synaptic weights be chosen so 
that the output of the left-hand network and the output of the right-hand network 
agree for each sequence-structure pair that is input to the two networks. 
To achieve this, both networks are trained simultaneously, starting from random 
initial weights in each net, under the sole constraint that the outputs of the two 
networks agree for each pattern in the training set. The mathematical implemen- 
tation of this constraint is described in various versions below. This procedure 
is a general, effective method of evolving predictable secondary structure classifi- 
cations of experimental data. The goal of this research is to use two mutually 
self-supervised networks to define new classes of protein secondary structure which 
are more predictable from sequence than the standard classes of alpha helix, beta 
sheet or coil. 
3 CONSTRAINING THE TWO NETS TO AGREE 
One way to impose agreement between the outputs of the two networks is to require 
that they covary when viewed as a stream of real numbers. Note that it is not 
sufficient to merely require that the outputs of the left-hand and right-hand nets 
agree by, e.g., minimizing the following objective function 
E = y.(LeftO � - RightO(r)) 2 (1) 
p 
Here, LeftO (r) and RightO � represent the outputs of the left-hand and right- 
hand networks, respectively, for the ph pair of input windows: (sequence window 
-left net) and (structure window -right net). It is necessary to avoid the trivial 
minimum of E obtained where the weights and thresholds are set so that each net 
presents a constant Output regardless of the input data. This is easily accomplished 
in Eqn (1) by merely setting all the weights and thresholds to 0.0. 
Demanding that the outputs vary, or more explicitly co-vary, is a viable solution 
to avoiding trivial local minima. Therefore, one can maximize the correlation, p, 
between the left-hand and right-hand network outputs. The standard correlation 
measure between two objects, LeftO (r) and RightO � is: 
p = (Le/to(p)- �e/to)(nigatO(p)- nigatO) 
(3) 
where LeftO denotes the mean of the left net's outputs over the training set, and 
respectively for the right net. p is zero if there is no variation, and is maximized 
Neural Network Definitions of Highly Predictable Protein Secondary Structure Classes 813 
if there is simultaneously both individual variation and joint agreement. In our 
situation it is equally desirable to have the networks maximally anti-correlated 
as it is for them to be correlated. (Whether the networks choose correlation, or 
anti-correlation, is evident from the behavior on the training set). Hence the min- 
imization of E = _p2 would ensure that the outputs are maximally correlated 
(or anti-correlated). While this work was in progress we received a 
