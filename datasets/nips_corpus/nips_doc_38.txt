515 
MICROELECTRONIC IMPLEMENTATIONS OF CONNECTIONIST 
NEURAL NETWORKS 
Stuart Mackie, Hans P. Graf, Daniel B. Schwartz, and John S. Denker 
AT&T Bell Labs, Holmdel, NJ 07733 
Abstract 
In this paper we discuss why special purpose chips are needed for useful 
implementations of connectionist neural networks in such applications as pattern 
recognition and classification. Three chip designs are described: a hybrid 
digital/analog programmable connection matrix, an analog connection matrix with 
adjustable connection strengths, and a digital pipelined best-match chip. The common 
feature of the designs is the distribution of arithmetic processing power amongst the 
data storage to minimize data movement. 
10 9 
106 
'103 
,,,/AMs 
',,, Distributed 
'-computation 
chip, s 
I �.jj/ '-, Conventional 
,, CPUs 
10 3 10 6 10 9 
Node Complexity 
(No. of Transistors) 
Figure 1. A schematic graph of addressable node complexity and size for conventional 
computer chips. Memories can contain millions of very simple nodes each 
with a very few transistors but with no processing power. CPU chips are 
essentially one very complex node. Neural network chips are in the 
distributed computation region where chips contain many simple fixed 
instruction processors local to data storage. (After Reece and Treleaven 1 ) 
� American Institute of Physics 1988 
516 
Introduction 
It is clear that conventional computers lag far behind organic computers when it 
comes to dealing with very large data rates in problems such as computer vision and 
speech recognition. Why is this? The reason is that the brain performs a huge number 
of operations in parallel whereas in a conventional computer there is a very fast 
processor that can perform a variety of instructions very quickly, but operates on only 
two pieces of data at a time. 
The rest of the many megabytes of RAM is idle during any instruction cycle. The 
duty cycle of the processor is close to 100%, but that of the stored data is very close to 
zero. If we wish to make better use of the data, we have to distribute processing 
power amongst the stored data, in a similar fashion to the brain. Figure 1 illustrates 
where distributed computation chips lie in comparison to conventional computer chips 
as regard number and complexity of addressable nodes per chip. 
In order for a distributed strategy to work, each processing element must be small 
in order to accommodate many on a chip, and communication must be local and hard- 
wired. Whereas the processing element in a conventional computer may be able to 
execute many hundred different operations, in our scheme the processor is hard-wired 
to perform just one. This operation should be tailored to some particular application. 
In neural network and pattern recognition algorithms, the dot products of an input 
vector with a series of stored vectors (referred to as features or memories) is often 
required. The general calculation is: 
Sum of Products 
V. F(i)= v. f.. 
j j Ij 
where V is the input vector and F(i) is one of the stored feature vectors. Two 
variations of this are of particular interest. In feature extraction, we wish to find all the 
features for which the dot product with the input vector is greater than some threshold 
T, in which case we say that such features are present in the input vector. 
Feature Extraction 
V. F(i) = v. f.. 
j J IJ 
In pattern classification we wish to find the stored vector that has the largest dot 
product with the input vector, and we say that the the input is a member of the class 
represented by that feature, or simply that that stored vector is closest to input vector. 
Classification 
max(V. F(i) = v. f.. 
j j Ij 
The chips described here are each designed to perform one or more of the above 
functions with an input vector and a number of feature vectors in parallel. The overall 
strategy may be summed up as follows: we recognize that in typical pattern recognition 
applications, the feature vectors need to be changed infrequenfiy compared to the input 
517 
vectors, and the calculation that is performed is fixed and low-precision, we therefore 
distribute simple fixed-instruction processors throughout the data storage area, thus 
minimizing the data movement and optimizing the use of silicon. Our ideal is to have 
every transistor on the chip doing something useful during every instruction cycle. 
Analog Sum-of-Products 
Using an idea slightly reminiscent of synapses and neurons from the brain, in two 
of the chips we store elements of features as connections from input wires on which the 
elements of the input vectors appear as voltages to summing wires where a sum-of- 
products is performed. The voltage resulting from the current summing is applied to 
the input of an amplifier whose output is then read to determine the result of the 
calculation. A schematic arrangement is shown in Figure 2 with the vertical inputs 
connected to the horizontal summing wires through resistors chosen such that the 
conductance is proportional to the magnitude of the feature element. When both 
positive and negative values are required, inverted input lines are also necessary. 
Resistor matrices have been fabricated using amorphous silicon connections and metal 
linewidths. These were programmed during fabrication by electron beam lithography 
to store names using the distributed feedback method described by Hopfield 2,3. This 
work is described more fully elsewhere. n,s Hard-wired resistor matrices are very 
compact, but also very inflexible. In many applications it is deskable to be able to 
reprogram the matrix without having to fabricate a new chip. For this reason, a series 
of programmable chips has been designed. 
Feature 4 
Feature 3 
Feature 2 
Feature 1 
Input lines 
? 
/ ,? 
,/ ? 
- ? 
> 
> 
> 
Figure 2. 
A schematic arrangement for calculating parallel sum-of-products with a 
resistor matrix. Features are stored as connections along summing wires and 
the input elements are applied as voltages on the input wires. The voltage 
generated by the current summing is thresholded by the amplifer whose 
output is read out at the end of the calculation. Feedback connections may be 
518 
made to give mutual inhibition and allow only one feature amplifier to tum 
on, or allow the matrix to be used as a distributed feedbick memory. 
Programmable Connection Matrix 
Figure 3 is a schematic diagram of a programmable connection using the contents of 
two RAM cells to control current sinking or sourcing into the summing wire. The 
switches are pass transistors and the 'resistors' are transistors with gates connected to 
their drains. Current is sourced or sunk if the appropriate RAM cell contains a '1' and 
the input Vi is high thus closing both switches in the path. Feature elements can 
therefore take on values (a,0,-b) where the values of a and b are determined by the 
conductivities of the n- and p-transistors obtained during processing. A matrix with 
2916 such connections allowing full interconnection of the inputs and outputs of 54 
amplifiers was designed and fabricated in 2.5gm CMOS (Figure 4). Each connection 
is about 100x100gm, the chip is 7x7mm and contains about 75,000 transistors. When 
loaded with 49 49-bit features (7x7 kernel), and presented with a 49-bit input vector, 
the chip performs 49 dot products in parallel in under 1 gs. This is equivalent to 2.4 
billion bit operations/sec. The flexibility of the design allows the chip to be operated in 
several modes. The chip was programmed as a distributed feedback memory 
(associative memory), but this did not work well because the current sinking capability 
of the n-type transistors was 6 times that of the p-types. An associative memory was 
implemented by using a 'grandmother cell' representation, where the memories were 
stored along the input lines of amplifiers, as for feature extraction, but mutually 
inhibitory connections were also made that allowed only one output to turn on. With 
10 stored vectors each 40 bits long, the best match was found in 50-600ns, depending 
on the data. The circuit can also be programmed to recognize sequences of vectors and 
to do error correction when vectors were omitted or wrong vectors were inserted into 
the sequences. The details of operation of the chip are described more fully 
elsewhere 6. This chip has been interfaced to a UNIX minicomputer and is in everyday 
use as an accelerator for feature extraction in optical character recognition of hand- 
written numerals. The chip speeds up this time consuming calculation by a factor of 
more than 1000. The use of the chip enables experiments to be done which would be 
too time consuming to simulate. 
Experience with this device has led to the design of four new chips, which are 
currently being tested. These have no feedback capability and are intended exclusively 
for feature extraction. The designs each incorporate new features which are being 
tested separately, but all are based on a connection matrix which stores 46 vectors each 
96 bits long. The chip will perform a full parallel calculation in lOOns. 
519 
VDD 
Output(i) 
vj 
Excitatory 
Inhibitory 
VSS 
Figure 3. 
Schematic diagram of a programmable connection. A current sourcing or 
sinking connection is made if a RAM cell contains a '1' and the input Vi is 
high. The currents are summed on the input wire of the amplifier. 
111 
I I I I,t,1 I t t. 
Pads 
[:=] Row Decoders 
'_IL- Connections 
ITITI Amplifiers 
Figure 4. Programmable connection matrix chip. The chip contains 75,000 transistors 
in 7x7mm, and was fabricated using 2.5gm design rules. 
52O 
Adaptive Connection Matrix 
Many problems require analog depth in the connection strengths, and this is 
especially important if the chip is to be used for learning, where small adjustments are 
required during training. Typical approaches which use transistors sized in powers of 
two to give conductance variability take up an area equivalent
