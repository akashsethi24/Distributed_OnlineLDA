Spherical Units as Dynamic Consequential Regions: 
Implications for Attention, Competition and Categorization 
Stephen Jos6 Hanson* 
Learning and Knowledge 
Acquisition Group 
Siemens Corporate Research 
Princeton, NJ 08540 
Mark A. G!uck 
Center for Molecular & 
Behavioral Neuroscience 
Rutgers University 
Newark, NJ 07102 
Abstract 
Spherical Units can be used to construct dynamic reconfigurable 
consequential regions, the geometric bases for Shepard's (1987) theory of 
stimulus generalization in animals and humans. We derive from Shepard's 
(1987) generalization theory a particular multi-layer network with dynamic 
(centers and radii) spherical regions which possesses a specific mass function 
(Cauchy). This learning model generalizes the configural-cue network model 
(Gluck & Bower 1988): (1) configural cues can be learned and do not require 
pre-wiring the power-set of cues, (2) Consequential regions are continuous 
rather than discrete and (3) Competition amoungst receptive fields is shown 
to be increased by the global extent of a particular mass function (Cauchy). 
We compare other common mass functions (Gaussian; used in models of 
Moody & Darken; 1989, Krushke, 1990) or just standard backpropogation 
networks with hyperplane/logistic hidden units showing that neither fare as 
well as models of human generalization and learning. 
1 The Generalization Problem 
Given a favorable or unfavorable consequence, what should an organism assume about 
the contingent stimuli? If a moving shadow overhead appears prior to a hawk attack 
what should an organism assume about other moving shadows, their shapes and 
positions? If a dense food patch is occasioned by a particular density of certain kinds of 
shrubbery what should the organism assume about other shurbbery, vegetation or its 
spatial density? In an pattern recognition context, given a character of a certain shape, 
orientation, noise level etc.. has been recognized correctly what should the system 
assume about other shapes, orientations, noise levels it has yet to encounter? 
Also a member of Cognitive Science Laboratory, Princeton University, Princeton, NJ 08544 
656 
Spherical Units as Dynamic Consequential Regions 657 
Many generalization theories assume stimulus similarity represents a failure to 
discriminate, rather than a cognitive decision about what to assume is consequential 
about the stimulus event. In this paper we implement a generalization theory with 
multilayer architecture and localized kemel functions (cf. Cooper, 1962; Albus 1975; 
Kanerva, 1984; Hanson & Burr, 1987,1990; Niranjan & Fallside, 1988; Moody & 
Darken, 1989; Nowlan, 1990; Krushke, 1990) in which the learning system constructs 
hypotheses about novel stimulus events. 
2 Shepard's (1987) Generalization Theory 
Considerable empirical evidence indicates that when stimuli are represented within an 
multi-dimensional psychological space, similarity, as measured by stimulus 
generalization, drops off in an approximate exponential decay fashion with psychological 
distance (Shepard, 1957, 1987). In comparison to a linear function, a similarity-distance 
relationship with upwards concave curvature, such as an exponential-decay curve, 
exaggerates the similarity of items which are nearby in psychological space and 
minimizes the impact of items which are further away. 
Recently, Roger Shepard (1987) has proposed a Universal Law of Generalization for 
stimulus generalization which derives this exponential decay similarity-distance function 
as a rational strategy given minimal information about the stimulus domain (see also 
Shepard & Kannappan, this volume). To derive the exponential-decay similarity- 
distance rule, Shepard (1987) begins by assuming that stimuli can be placed within a 
psychological space such that the response learned to any one stimulus will generalize to 
another according to an invariant monotonic function of the distance between them. If a 
stimulus, O, is known to have an important consequence, what is the probability that a 
novel test stimulus, X, will lead to the same consequence? Shepard shows, through 
arguments based on probabilistic reasoning that regardless of the a priori expectations 
for regions of different sizes, this expectation will almost always yield an approximate 
exponentially decaying gradient away from a central memory point. In particular, very 
simple geometric constraints can lead to the exponential generalization gradient. 
Shepard (1987) assumes (1) that the consequential region overlaps the consequential 
stimulus event. and (2) bounded center symmetric consequential regions of unknown 
shape and size In the 1-dimensional case it can be shown that g(x) is robust over a wide 
variety of assumptions for the distribution of p(s); although for p(s) exactly the Erlangian 
or discrete Gamma, g(x) is exactly Exponential. 
We now investigate possible ways to implement a model which can learn consequential 
regions and appropriate generalization behavior (cf. Shepard, 1990). 
3 G!uck & Bower's Configural-cue Network Model 
The first point of contact is to discrete model due to Gluck and Bower: The configural- 
cue network model (Gluck & Bower, 1988) The network model adapts its weights 
(associations) according to Rescorla and Wagner's (1972) model of classical 
conditioning which is a special case of Widrow & Hoff's (1961) Least-Mean-Squares 
(LMS) algorithm for training one-layer networks. Presentation of a stimulus pattern is 
658 Hanson and Gluck 
represented by activating nodes on the input layer which correspond to the pattem's 
elementary features and pair-wise conjunctions of features. 
The configural-cue network model implicitly embodies an exponential generalization 
(similarity) gradient (Gluck, 1991) as an emergent property of it's stimulus 
representation scheme. This equivalence can be seen by computing how the number of 
overlapping active input nodes (similarity) changes as a function of the number of 
overlapping component cues (distance). If a stimulus pattem is associated with some 
outcome, the configural-cue model will generalize this association to other stimulus 
pattems in proportion to the number of common input nodes they both activate. 
Although the configural cue model has been successful with various categorization data, 
there are several limitations of the configural cue model: (1) it is discrete and can not deal 
adequately with continuous stimuli (2) it possesses a non-adaptable internal 
representation (3) it can involve the pre-wiring the power set of possible cues 
Nonetheless, there are several properties that make the Configural Cue model successful 
that are important to retain for generalizations of this model: (a) the competitive stimulus 
properties deriving from the delta rule (b) the exponential stimulus generalization 
property deriving from the successive combinations of higher-order features encoded by 
hidden units. 
4 A Continuous Version of Shepard's Theory 
We derive in this section a new model which generalizes the configural cue model and 
derives directly from Shepard's generalization theory. In Figure 1, is shown a one 
dimensional depiction of the present theory. Similar to Shepard we assume there is a 
consequential 
o less Ilkelg 
/ 
o' o o' o'x o' 
Figure 1: Hypothesis Distributions based on Consequential Region 
region associated with a significant stimulus event, O. Also similar to Shepard we 
assume the learning system knows that the significant stimulus event is contained in the 
consequential region, but does not know the size or location of the consquential region. 
In absence of this information the learning system constructs hypothesis distributions 
Spherical Units as Dynamic Consequential Regions 659 
(O') which may or maynot be contained in the consequential region but at least overlap 
the significant stimulus event with some finite probablity measure. In some hypothesis 
distributions the significant stimulus event is typical in the consequential region, in 
other hypothesis distributions the significant stimulus event is rare. Consequently, the 
present model differs from Shepard's approach in that the learning system uses the 
consequential region to project into a continuous hypothesis space in order to construct 
the conditional probability of the novel stimulus, X, given the significant stimulus event 
O. 
Given no further information on the location and size of the consequential region the 
learning system averages over all possible locations (equally weighted) and all possible 
(equally weighted) variances over the known stimulus dimension: 
g (x) = (s) (c)H (x ,s ,c ) dc ds 
In order to derive particular gradients we must assume particular forms for the hypothesis 
distribution, H(x,s,c). Although we have investigated many different hypothesis 
distributions and wieghting functions (p(c), p(s)), we only have space here to report on 
two bounding cases, one with very light tails, the Gaussian, and one with very heavy 
mils, the Cauchy (see Figure 2). These two distributions are extremes and provide a 
test of the robustness of the generalization gradient. At the same time they represent 
different commitments to the amount of overlap of hidden unit receptive fields and the 
consequent amount of stimulus competition during leaming. 
-4 - 0  4 
Figure 2: Gaussian compared to the Cauchy: Note heavier Cauchy tail 
Equation 2 was numerically integrated (using mathematica), over a large range of 
variances and a large range of locations using uniform densities representing the 
weighting functions and both Gaussian and Cauchy distributions representing the 
hypothesis distributions. Shown in Figure 3 are the results of the integrations for both the 
Cauchy and Gaussian distributions. The resultant gradients are shown by open circles 
(Cauchy) or stars (Gaussian) while the solid lines show the best fitting exponential 
gradient. We note that they app
