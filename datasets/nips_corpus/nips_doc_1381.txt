Statistical Models of Conditioning 
Peter Dayan* 
Brain & Cognitive Sciences 
E25-210 MIT 
Cambridge, MA 02139 
Theresa Long 
123 Hunting Cove 
Williamsburg, VA 23185 
Abstract 
Conditioning experiments probe the ways that animals make pre- 
dictions about rewards and punishments and use those predic- 
tions to control their behavior. One standard model of condition- 
ing paradigms which involve many conditioned stimuli suggests 
that individual predictions should be added together. Various key 
results show that this model fails in some circumstances, and mo- 
tivate an alternative model, in which there is attentional selection 
between different available stimuli. The new model is a form of 
mixture of experts, has a close relationship with some other exist- 
ing psychological suggestions, and is statistically well-founded. 
1 Introduction 
Classical and instrumental conditioning experiments study the way that animals 
learn about the causal texture of the world (Dickinson, 1980) and use this informa- 
tion to their advantage. Although it reached a high level of behavioral sophistica- 
tion, conditioning has long since gone out of fashion as a paradigm for studying 
learning in animals, partly because of the philosophical stance of many practition- 
ers, that the neurobiological implementation of learning is essentially irrelevant. 
However, more recently it has become possible to study how conditioning phe- 
nomena are affected by particular lesions or pharmacological treatments to the 
brain (eg Gallagher & Holland, 1994), and how particular systems, during simple 
learning tasks, report information that is consistent with models of conditioning 
(Gluck & Thompson, 1987; Gabriel & Moore, 1989). 
In particular, we have studied the involvement of the dopamine (DA) system in 
the ventral tegmental area of vertebrates in reward based learning (Montague et 
al, 1996; Schultz et al, 1997). The activity of these cells is consistent with a model 
in which they report a temporal difference (TD) based prediction error for reward 
* This work was funded by the Surdna Foundation. 
118 P Dayan and T. Long 
(Sutton & Barto, 1981; 1989). This prediction error signal can be used to learn correct 
predictions and also to learn appropriate actions (Barto, Sutton & Anderson, 1983). 
The DA system is important since it is crucially involved in normal reward learning, 
and also in the effects of drugs of addiction, self stimulation, and various neural 
diseases. 
The TD model is consistent with a whole body of experiments, and has even cor- 
rectly anticipated new experimental findings. However, like the Rescorla-Wagner 
(RW; 1972) or delta rule, it embodies a particular additive model for the net pre- 
diction made when there are multiple stimuli. Various sophisticated conditioning 
experiments have challenged this model and found it wanting. The results support 
competitive rather than additive models. Although ad hoc suggestions have been 
made to repair the model, none has a sound basis in appropriate prediction. There 
is a well established statistical theory for competitive models, and it is this that we 
adopt. 
In this paper we review existing evidence and theories, show what constraints 
a new theory must satisfy, and suggest and demonstrate a credible candidate. 
Although it is based on behavioral data, it also has direct implications for our 
neural theory. 
2 Data and Existing Models 
Table 1 describes some of the key paradigms in conditioning (Dickinson, 1980; 
Mackintosh, 1983). Although the collection of experiments may seem rather arcane 
(the standard notation is even more so), in fact it shows exactly the basis behind 
the key capacity of animals in the world to predict events of consequence. We will 
extract further biological constraints implied by these and other experiments in the 
discussion. 
In the table, l (light) and s (tone) are potential predictors (called conditioned stimuli or 
CSs), of a consequence, r, such as the delivery of a reward (called an unconditioned 
stimulus or US). Even though we use TD rules in practice, we discuss some of the 
abstract learning rules without much reference to the detailed time course of trials. 
The same considerations apply to TD. 
In Pavlovian conditioning, the light acquires a positive association with the reward 
in a way that can be reasonably well modeled by: 
aWl(t) -- Ctl(t)(r(t) -- wl(t))l(t), (1) 
where l(t) E (0, 1) represents the presence of the light in trial t (s(t) will similarly 
represent the presence of a tone), w (t) (we will often drop the index t) represents 
the strength of the expectation about the delivery of reward r(t) in trial t if the 
light is also delivered, and at (t) is the learning rate. This is just the delta rule. It 
also captures well the probabilistic contingent nature of conditioning - for binary 
r(t)  {0, 1}, animals seem to assess 51 -- P[r(t)ll(t)= 1] - P[r(t)il(t)=O], and then 
only expect reward following the light (in the model, have Wl > 0) if et > 0. 
Pavlovian conditioning is easy to explain under a whole wealth of rules. The trouble 
comes in extending equation I to the case of multiple predictors (in this paper we 
consider just two). The other paradigms in table 1 probe different aspects of this. 
The one that is most puzzling is (perversely) called downwards unblocking (Holland, 
1988). In a first set of trials, an association is established between the light and two 
presentations of reward separated by a few (u) seconds. In a second set, a tone is 
included with the light, but the second reward is dropped. The animal amasses 
less reward in conjunction with the tone. However, when presented with the tone 
Statistical Models of Conditioning 119 
Name Set 1 Set 2 Test 
Pavlovian 1  r 1 o r 
Overshadowing 1 + s 
sq r 
{1--r} 
Inhibitory l + s 
Blocking lr l+sr sq-- 
Upwards unblocking 1 - r 1 + s - rA,r s q r 
Downwards unblocking 1 
Table 1: Paradigms. Sets 1 and 2 are separate sets of learning trials, which are continued 
until convergence. Symbols l and s indicate presentation of lights and tones as potential 
predictors. The q- in the test set indicates that the associations of the predictors are tested, 
producing the listed results. In overshadowing, association with the reward can be divided 
between the light and the sound, indicated by r �. In some cases overshadowing favours 
one stimulus at the complete expense of the other; and at the end of very prolonged training, 
all effects of overshadowing can disappear. In blocking, the tone makes no prediction of r. 
In set 2 of inhibitory conditioning, the two types of trials are interleaved and the outcome 
is that the tone predicts the absence of reward. In upwards and downwards unblocking, 
the A, indicates that the delivery of two rewards is separated by time u. For downwards 
unblocking, if u is small, then s is associated with the absence of r; if u is large, then s is 
associated with the presence of r. 
alone, the animal expects the presence rather than the absence of reward. On the face 
of it, this seems an insurmountable challenge to prediction-based theories. First 
we describe the existing theories, then we formalise some potential replacements. 
One theory (called a US-processing theory) is due to Rescorla & Wagner (RW; 1972), 
and, as pointed out by Sutton & Barto (1981), is just the delta rule. For RW, the 
animal constructs a net prediction: 
V(t) = wl(t)l(t) + ws(t)s(t) 
(2) 
for r(t), and then changes Awl(t) -- al(t)(r(t) -- V(t))l(t) (and similarly for ws(t)) 
using the prediction error r(t) - V(t). Its foundation in the delta rule makes it 
computationally appropriate (Marr, 1982) as a method of making predictions. TD 
uses the same additive model in equation 2, but uses r(t) + V(t + 1) - V(t) as the 
prediction error. 
RW explains overshadowing, inhibitory conditioning, blocking, and upwards un- 
blocking, but not downwards unblocking. In overshadowing, the terminal asso- 
ciation between l and r is weaker if 1 and s are simultaneously trained - this is 
expected under RW since learning stops when V(t) = r(t), and wt and ws will 
share the prediction. In inhibitory conditioning, the sound comes to predict the ab- 
sence of r. The explanation of inhibitory conditioning is actually quite complicated 
(Konorski, 1967; Mackintosh, 1983); however RW provides the simple account that 
w --- r for the 1  r trials, forcing w - -r for the 1 + s - � trials. In blocking, 
the prior association between 1 and r means that wt = r in the second set of trials, 
leading to no learning for the tone (since V(t) - r(t) = 0). In upwards unblocking, 
wt = r at the start of set 2. Therefore, r(t) - wt -- r > 0, allowing ws to share in the 
prediction. 
As described above, downwards unblocking is the key thorn in the side of RW. 
Since the TD rule combines the predictions from different stimuli in a similar way, 
120 P. Dayan and T. Long 
it also fails to account properly for downwards unblocking. This is one reason why 
it is incorrect as a model of reward learning. 
The class of theories (called CS-processing theories) that is alternative to RW does 
not construct a net prediction V (t), but instead uses equation 1 for all the stimuli, 
only changing the learning rates ct(t) and cs(t) as a function of the conditioning 
history of the stimuli (eg Mackintosh, 1975; Pearce & Hall, 1980; Grossberg, 1982). 
A standard notion is that there is a competition between different stimuli for a 
limited capacity learning processor (Broadbent, 1958; Mackintosh, 1975; Pearce & 
Hall, 1980), translating into competition between the learning rates. In blocking, 
nothing unexpected happens in the second set of trials and equally, the ton does 
not predict anything novel. In either case cs is set to ,-0 0 and so no learning 
happens. In these models, downwards unblocking now makes qualitative sense: 
the surprising consequences in set 2 can be enough t
