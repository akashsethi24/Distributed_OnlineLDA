Mixture Density Estimation 
Jonathan Q. Li 
Department of Statistics 
Yale University 
P.O. Box 208290 
New Haven, CT 06520 
Qiang. Li@aya. yale. edu 
Andrew R. Barron 
Department of Statistics 
Yale University 
P.O. Box 208290 
New Haven, CT 06520 
Andrew. Barron@yale. edu 
Abstract 
Gaussian mixtures (or so-called radial basis function networks) for 
density estimation provide a natural counterpart to sigmoidal neu- 
ral networks for function fitting and approximation. In both cases, 
it is possible to give simple expressions for the iterative improve- 
ment of performance as components of the network are introduced 
one at a time. In particular, for mixture density estimation we show 
that a k-component mixture estimated by maximum likelihood (or 
by an iterative likelihood improvement that we introduce) achieves 
log-likelihood within order 1/k of the log-likelihood achievable by 
any convex combination. Consequences for approximation and es- 
timation using Kullback-Leibler risk are also given. A Minimum 
Description Length principle selects the optimal number of compo- 
nents k that minimizes the risk bound. 
I Introduction 
In density estimation, Gaussian mixtures provide flexible-basis representations for 
densities that can be used to model heterogeneous data in high dimensions. We 
introduce ar index of regularity cy of density functions f with respect to mixtures 
of densities from a given family. Mixture models with k components are shown to 
achieve Kullback-Leibler approximation error bounded by c/k for every k. Thus 
in a manner analogous to the treatment of sinusoidal and sigrnoidal networks in 
Barron [1],[2], we find classes of density functions f such that reasonable size net- 
works (not exponentially large as function of the input dimension) achieve suitable 
approximation and estimation error. 
Consider a parametric family G = (b0(x), x E 3J C R d' :0 E O C R d) of probability 
density functions parameterized by 0 & O. Then consider the class C = CONV(G') 
of density functions for which there is a mixture representation of the form 
f(x) = fo 0�)P(dO) (1) 
where �0 () are density functions from G and P is a probability measure on �. 
The main theme of the paper is to give approximation and estimation bounds of 
arbitrary densities by finite mixture densities. We focus our attention on densities 
280 J. Q. Li and A. R. Barron 
inside C first and give an approximation error bound by finite mixtures for arbi- 
trary f E C. The approximation error is measured by Kullback-Leibler divergence 
between two densities, defined as 
D(fllg ) = f f(x) log[f(x)/g(x)]dx. (2) 
In density estimation, D is more natural to use than the L 2 distance often seen in 
the function fitting literature. Indeed, D is invariant under scale transformations 
(and other 1-1 transformation of the variables) and it has an intrinsic connection 
with Maximum Likelihood, one of the most useful methods in the mixture density 
estimation. The following result quantifies the approximation error. 
THEOREM 1 Let G = {qb0(x) � 0 e O} and C= CONV(G). Let f(x) = 
f c)o(x)P(dO)  �. There exists fk, a k-component mixture of trio, such that 
(3) 
O(fllfk)-< k 
In the bound, we have 
f f 
f o(x)p(do) 
and ' = 411og(3v) + a], where 
dx, (4) 
log (x) 
a= sup (5) 
01,02,X 
Here, a characterizes an upper bound of the log ratio of the densities in G, when 
the parameters are restricted to O and the variable to 
Note that the rate of convergence, l/k, is not related to the dimensions of 
The behavior of the constants, though, depends on the choices of G and the target 
f. 
For example we may take G to be the Gaussian location family, which we restrict 
to a set r which is a cube of side-length A. Likewise we restrict the parameters to 
be in the same cube. Then, 
dA : 
a < (6) 
In this case, a is linear in dimension. 
The value of c depends on the target density f. Suppose f is a finite mixture with 
M components, then 
< (7) 
with equality if and only if those M components are disjoint. Indeed, suppose 
f(x) M 
-- i=lPiqbOi(X), then piqbo,(X)/iM=l piqboi(X) _ 1 and hence 
M 
c}: f '4M--(P'bO' (x))bO' (x)dx < f E(1)cfio,(x)dx: M. (8) 
M -- 
--i=1PiO0, (X) i=1 
Genovese and Wasserman [3] deal with a similar setting. A Kullback-Leibler ap- 
proximation bound of order 1/Vr for one-dimensional mixtures of Gaussians is 
given by them. 
In the more general case that f is not necessarily in �, we have a competitive 
optimality result. Our density approximation is nearly at least as good as any gp 
in �. 
Mixture Density Estimation 281 
THEOREM 2 For every gp(x) - f 
�2 
O(fllf) 5 O(fllgP) q- 
(9) 
Here, 
2 f fck(x)P(dO) f(x)dx. (10) 
In particular, we can take infimum over all gr E C, and still obtain a bound. 
Let D(flIC) = infgec D(fllg). A theory of information projection shows that if 
there exists a sequence of fk such that D(fllfk ) -+ D(fll�), then f converges to 
a function f*, which achieves D(fll� ). Note that f* is not necessarily an element 
in �. This is developed in Li[4] building on the work of Bell and Cover[5]. As a 
consequence of Theorem 2 we have 
�2 
O(fllf) 5 O(fllf*) q- 
(11) 
where c s is the smallest limit of , for sequences of P achieving D(fllg) that 
f,* 
approaches the infimum D(fllC ). 
We prove Theorem 1 by induction in the following section. An appealing feature 
of such an approach is that it provides an iterative estimation procedure which 
allows us to estimate one component at a time. This greedy procedure is shown 
to perform almost as well as the full-mixture procedures, while the computational 
task of estimating one component is considerably easier than estimating the full 
mixtures. 
Section 2 gives the iterative construction of a suitable approximation, while Section 
3 shows how such mixtures may be estimated from data. Risk bounds are stated in 
Section 4. 
2 An iterative construction of the approximation 
We provide an iterative construction of f's in the following fashion. Suppose 
during our discussion of approximation that f is given. We seek a k-component 
mixture f close to f. Initialize fx by choosing a single component from G to 
minimize D(fllfx ) = D(fllq5o). Now suppose we have fk-l(X). Then let f(x) = 
(1 - a)f-l(X) + acfo(x) where a and 0 are chosen to minimize D(fllf ). More 
generally let f be any sequence of k-component mixtures, for k = 1, 2, ... such 
that D(fllfk) <_ mina,0 D(fll(1 - a)fk-x q- aO). We prove that such sequences fk 
achieve the error bounds in Theorem 1 and Theorem 2. 
Those familiar with the iterative Hilbert space approximation results of Jones[6], 
Barron[I], and Lee, Bartlett and Williamson[7], will see that we follow a similar 
strategy. The use of L2 distance measures for density approximation involves L2 
norms of component densities that are exponentially large with dimension. Naive 
Taylor expansion of the Kullback-Leibler divergence leads to an L2 norm approxi- 
mation (weighted by the reciprocal of the density) for which the difficulty remains 
(Zeevi gc Meir[8], Li[9]). The challenge for us was to adapt iterative approximation 
to the use of Kullback-Leibler divergence in a manner that permits the constant 
a in the bound to involve the logarithm of the density ratio (rather than the ratio 
itself) to allow more manageable constants. 
282 J. Q. Li and A. R. Barron 
The proof establishes the inductive relationship 
Dk _< (1 -o)Dk-1 + o?B, 
where B is bounded and D = D(fllf). By choosing a = 1,a2 
thereafter a = 2/k, it's easy to see by induction that D _< 4B/k. 
(12) 
= 1/2 and 
To get (12) we establish a quadratic upper bound for -log/x - 
' f -- 
-log ((1-a)f_+a�0) Three key analytic inequalities regarding to the logarithm 
will be handy for us, 
forr_>ro >0, 
and 
- log(to) + ro - 1]( r _ 1) 2 
-log(r) _< -(r- 1) +[ (to - 1) 2 
(13) 
2[-log(r) +r- 1] < logr, (14) 
r-1 - 
- log(r) + r - 1 
(r- 1) 2 
_< 1/2 + log-(r) 
(15) 
Note that in our application, ro is a ratio of densities in �. Thus we obtain an upper 
bound for log-(ro) involving a. Indeed we find that (1/2 + log-(ro)) _< 7/4 where 
7 is as defined in the theorem. 
In the case that f is in �, we take g = f. Then taking the expectation with respect 
to f of both sides of (16), we acquire a quadratic upper bound for Dk, noting that 
r = L Also note that D is a function of 0. The greedy algorithm chooses 0 to 
minimize D(O). Therefore 
Plugging the upper bound (16) for Dk(O) into (17), we have 
_ __ + .2 (7/4) + - log(ro)]f(x)dxP(dO). (18) 
g g 
(16) 
Now apply (14) and (15) respectively. We get 
log(r) < log(to) aqb a2 (1/2 log- 
.... + + (to)) + - log(ro). 
- g -/ 
where log-(.) is the negative part of the logarithm. The proof of of inequality (13) 
-- 1og(r)+r--1 
is done by verifying that (r_1)2 is monotone decreasing in r. Inequalities (14) 
and (15) are shown by separately considering the cases that r < I and r > i (as 
well as the limit as r -+ 1). To get the inequalities one multiplies through by (r- 1) 
or (r - 1) 2, respectively, and then takes derivatives to obtain suitable monotonicity 
in r as one moves away from r = 1. 
(x-a)f_+a�o and ro = (1-a)f/_t where 
Now apply the inequality (13) with r = /7 /7 , 
g is an arbitrary density in � with g = f coP(dO). Note that r >_ ro in this case 
because ao > 0. Plug in r = ro + a  at the right side of (13) and expand the 
square. Then we get 
aqb - log(to) + ro aqb)] 2 
-log(r) < -(to+---1)+[ -!][(ro-1)+( 
- g (ro - 1) 2 g 
log(to) + a 2 -[- log(to) + ro- 1] log(to) + ro- 1] 
g (to 1) 2 + 2a-[- ' 
- g ro- 1 
Mxture Density Estimation 283 
where ro = (1- a)fk-x (x)/g(x) and P is chosen to satisfy f0 c)o(x)P(dO) = g(x). 
Thus 
z>k < (1 - f 
- 
f(x)dx(7/4) + a log(1 - a) - a - log(1 - a). 
(19) 
It can be shown that a log(1 - a) - a - log(1 - a) _< O. Thus we have the desired 
inductive relationship, 
D < (1- a)Dk-1 + a2c,e7/4. (20) 
Therefore, D _< k' 
In the case that f does not have a mixture repre
