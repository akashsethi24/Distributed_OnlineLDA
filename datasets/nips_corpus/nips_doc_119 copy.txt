748 
Performance of a Stochastic Learning Microchip 
Joshua Alspector, Bhusan Gupta, and Robert B. Allen 
Bellcore, Morristown, NJ 07960 
ABSTRACT
We have fabricated a test chip in 2 micron CMOS that can perform supervised 
learning in a manner similar to the Boltzmann machine. Patterns can be 
presented to it at 100,000 per second. The chip learns to solve the XOR 
problem in a few milliseconds. We also have demonstrated the capability to 
do unsupervised competitive learning with it. The functions of the chip 
components are examined and the performance is assessed. 
1. INTRODUCTION 
In previous workil I t2] we have pointed out the importance of a local learning rule, 
feedback connections, and stochastic elements t31 for making learning models that are 
electronically implementable. We have fabricated a test chip in 2 micron CMOS 
technology that embodies these ideas and we report our evaluation of the microchip and 
our plans for improvements. 
Knowledge is encoded in the test chip by presenting digital patterns to it that are 
examples of a desired input-output Boolean mapping. This knowledge is learned and 
stored entirely on chip in a digitally controlled synapse-like element in the form of 
connection strengths between neuron-like elements. The only portion of this leaming 
system which is off chip is the VLSI test equipment used to present the patterns. 
This learning system uses a modified Boltzmann machine algorithm 31 which, if 
simulated on a serial digital computer, takes enormous amounts of computer time. Our 
physical implementation is about 100,000 times faster. The test chip, if expanded to a 
board-level system of thousands of neurons, would be an appropriate architecture for 
solving artificial intelligence problems whose solutions are hard to specify using a 
conventional role-based approach. Examples include speech and pattern recognition and 
encoding some types of expert knowledge. 
2. CHIP COMPONENTS 
Fig. 1 is a photograph of the silicon chip. It contains various test structures, the largest of 
which, in the lower left, is a neural-style learning network composed of 6 neurons, each 
with its own noise amplifier, and 15 bidirectional synapses which potentially allow the 
network to be fully connected. In order to study these components separately, there is a 
also a noise amplifier in the upper left comer of the chip, a neuron in the upper right, and 
2 synapses in the lower right. 
* Permanent address: University of Califomia, Berkeley; EE Dep't, Cory Hall; Berkeley, CA 94720 
Performance of a Stochastic Learning Microchip 749 
Figure 1. Photograph of Test Chip Containing a Learning Network in Lower Left. 
2.1 Neuron 
The electronic neuron performs the physical computation: 
activation =f (Ew/j sj +noise )=f (gain*neti ) 
where f is a monotonic non-linear function such as tanh. In some of our computer 
simulations this is a step function corresponding to a high value of gain. The signal from 
other neurons to neuron i is the sum of neural states si giving input weighted by the 
connection strengths wij, while the noise simulates a temperature in a physical 
thermodynamic system. Their sum is the effective net input neti. 
The model neuron is a double differential amplifier as shown in Fig. 2. Noise and signal 
have separate differential inputs and are summed at low gain. The differential outputs of 
this summing stage are converted to a single output by a high gain stage before being fed 
into a switching arrangement. This selects either the net input or an external clamping 
signal which forces the neuron into a desired state. The output of the switch is then 
750 Alspector, Gupta and Allen 
Vdd 
V 
source 
S- S + 
-' Vii 
desired 
Figure 2. Circuitry of Electronic Analog Neuron. 
further amplified before driving the network. The final output approximates a two-state 
binary neuron. 
2.2 Noise amplifier 
annl 
Figure 3. Block Diagram of Noise Amplifier. 
Performance of a Stochastic Learning Microchip 751 
Fig. 3 is a block diagram of the noise amplifier. The original idea was to amplify the 
thermal noise in the channel of a transistor with a gain of nearly a million but to stabilize 
the dc output using low pass negative feedback in 3 stages. By controlling the feedback, 
one could control both the bandpass of the noise signal as well as the gain to provide for 
annealin the temperature (amount of noise) as required by the Boltzmann machine 
g 
algorithm. t31 Unfortunately this amplifier proved unstable at high gain values leading to 
oscillations of a few MHz which were highly correlated among all the noise amplifiers in 
the network. In spite of this undesirable correlation in the noise signals, the network was 
still able to learn (see section 3). Rather than a slow annealing, we used a rapid 
heating and flash freezing of the network to randomize ,it. This was done by 
momentarily-opening a noise on switch during the time allotted for annealing. 
Learning was also demonstrated by clamping the free running neurons momentarily to a 
pseudo-random state and then releasing them to allow the network to settle. 
2.3 Synapse 
Fig. 4 is a block diagram of the digitally controlled electronic synapse. The weights are 
stored as a sign and four bits of magnitude in five flip-flops arranged as an up-down 
counter. The correlation logic tests whether the two neurons that the synapse connects 
have the same binary state (correlated) or not at the end of the anneal cycle. If the 
neurons are correlated in the teacher phase (when the teacher is clamping the output 
neurons in the correct state) and not in the student phase (when the output neurons are 
running free), then a signal to the counter increments the weight by one. If the reverse is 
true, the counter is decremented. If the teacher and student phase have the same 
correlation, no change is made. 
correlation 
8.4.2.1 = W/L _  
Figure 4. Block Diagram of Synapse. 
752 Alspector, Gupta and Allen 
The digital weight is converted to an analog conductance by a set of pass transistors with 
graduated binary conductance ratios. Measurements confirmed that the synapse 
conductance increased monotonically from a value of -15 though +15 as the counter was 
incremented. The -0 value, when loaded into the synapse, disconnected that link. We 
usually initialized all the weights to +0 before learning. 
3. PERFORMANCE EVALUATION OF NETWORK 
3.1 XOR tests 
The most difficult test for our 6 neuron network was to have it learn the exclusive-OR 
function. The network was arranged with 2 input neurons, 2 hidden neurons, and 1 
output neuron as shown in Fig. 5. There is also a so-called 'true' neuron which is always 
clamped on. The negative of the weights from that neuron provide the threshold for the 
other neurons. The exclusive-OR function is of historical interest because the neural 
models of the 1960's could not learn it. [4] [5] This is because those learning algorithms did 
not work when there was a layer of hidden neurons. Networks with only a single layer of 
modifiable weights could learn the logical OR function but not the exclusive-OR (XOR). 
The truth table in Fig. 5 shows that the XOR is 1 (on or true) when either one of the two 
inputs is 1, but not when both are 1. However, recent algorithms such as the Boltzmann 
machine are able to learn with a hidden layer and hence can solve the XOR. 
hidden 
in 
nine' XOR 
1 
in hidden: 
00 0 
01 ? 1 
10 1 
11 / 0 
Learn 'rules' to solve problem 
I 2 
Figure 5. 2-2-1 Network to Learn XOR. 
To teach a network to be an XOR, we start with a blank slate where all the weights are 
ï¿½ 
zero and then present the patterns of l's and O's in the figure wth the teacher alternately 
clamping the output to the correct state and letting it run free. On each presentation, the 
network is jittered by noise and correlations are counted by each synapse. At the end of 
each teacher-student cycle, weights are adjusted. 
Performance of a Stochastic Learning Microchip 753 
Tests of the chip were conducted using an HP 8180A data generator to present digital 
patterns to the chip, an HP 8182 data analyzer to capture the chip's digital outputs, and 
an HP 54112A digitizing oscilloscope to capture waveforms. Analog waveforms were 
generated using an HP 8770A arbitrary waveform synthesizer feeding a Comlinear E201I 
amplifier. These instruments were controlled by an HP 9836 computer running UNIX 
with test programs written in C. 
A pattern presentation phase consisted of five subphases and hence five clock cycles of 
the data generator. The input and/or output pattern to be presented to the clamped 
neurons is present during all five cycles. The first cycle presents noise or an annealing 
waveform to the network. The second cycle sends a signal to each synapse to count 
correlations. The fourth cycle can be used to send a signal to each synapse to adjust 
weights. This is usually done only after two 5 cycle phases, one for the teacher phase 
and one for the student phase. Thus, during learning, ten digital words were used in the 
data generator for each pattern presentation. 
In addition to presenting patterns, digital weights can also be read into the chip with a 
similar 5 cycle phase. This uses the flip-flop storage arranged as a shift register for 
weight storage and readout. Because the memory of the data generator was only 1024 
bits deep, we would present only 66 patterns (660 words) each time the data generator 
was loaded by the control computer. The remaining memory was used to initialize the 
network to its previous value after the destructive readout of weights. In this way, 
performance of the network was monitored after sets of 66 pseudo-randomly selected 
patterns. 100 test patterns could also be presented, without learning, to see what 
performance the network achieved at that point. 
For the XOR, we organized the connectivity as in Fig. 5. For example, the connections 
between input
