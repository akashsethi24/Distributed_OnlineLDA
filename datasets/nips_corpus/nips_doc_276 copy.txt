Generalized Hop field Networks and Nonlinear Optimization 355 
Generalized Hopfield Networks 
and 
Nonlinear Optimization 
Gintaras V. Reklaitis 
Dept. of Chemical Eng. 
Purdue University 
W. Lafayette, IN. 47907 
Athanasios G. Tsirukis 1 
Dept. of Chemical Eng. 
Purdue University 
W. Lafayette, IN. 47907 
Manoel F. Tenorio 
Dept. of Electrical Eng. 
Purdue University 
W. Lafayette, IN. 47907 
ABSTRACT 
A nonlinear neural framework, called the Generalized Hopfield 
network, is proposed, which is able to solve in a parallel distributed 
manner systems of nonlinear equations. The method is applied to the 
general nonlinear optimization problem. We demonstrate GHNs 
implementing the three most important optimization algorithms, 
namely the Augmented Lagrangian, Generalized Reduced Gradient and 
Successive Quadratic Programming methods. The study results in a 
dynamic view of the optimization problem and offers a straightforward 
model for the parallelization of the optimization computations, thus 
significantly extending the practical limits of problems that can be 
formulated as an optimization problem and which can gain from the 
introduction of nonlinearities in their structure (eg. pattern recognition, 
supervised learning, design of content-addressable memories). 
! To whom correspondence should be addressed. 
356 Reklaitis, Tsirukis and Tenorio 
1 RELATED WORK 
The ability of networks of highly interconnected simple nonlinear analog processors 
(neurons) to solve complicated optimization problems was demonstrated in a series of 
papers by Hopfield and Tank (Hopfield, 1984), (Tank, 1986). 
The Hopfield computational model is almost exclusively applied to the solution of 
combinatorially complex linear decision problems (eg. Traveling Salesman Problem). 
Unfortunately such problems can not be solved with guaranteed quality, (Bruck, 1987), 
getting trapped in locally optimal solutions. 
Jeffrey and Rossnet, (Jeffrey, 1986), extended Hopfield's technique to the nonlinear 
unconstrained optimization problem, using Cauchy dynamics. Kennedy and Chua, 
(Kennedy, 1988), presented an analog implementation of a network solving a nonlinear 
optimization problem. The underlying optimization algorithm is a simple transformation 
method, (Reklaitis, 1983), which is known to be relatively inefficient for large nonlinear 
optimization problems. 
2 LINEAR HOPFIELD NETWORK (LHN) 
The computation in a Hopfield network is done by a collection of highly interconnected 
simple neurons. Each processing element, i, is characterized by the activation level, ui, 
which is a function of the input received from the external environment, Ii, and the state 
of the other neurons. The activation level of i is transmitted to the other processors, after 
passing through a filter that converts ui to a 0-1 binary value, Vi. 
The time behavior of the system is described by the following model: 
du i ui 
Ci('-) = E TijVj + Ii 
j R 
where Tij are the interconnection strengths. The network is characterized as linear, 
because the neuron inputs appear linearly in the neuron's constitutive equation. The 
steady-state of a Hopfield network corresponds to a local minimum of the corresponding 
quadratic Lyapunov function: 
E .,_. 
If the matrix [ Tij ] is symmetric, the steady-state values of Vi are binary These 
observations turn the Hopfield network to a very useful discrete optimization tool. 
Nonetheless, the linear structure poses two major limitations: The Lyapunov (objective) 
function can only take a quadratic form, whereas the feasible region can only have a 
hypercube geometry (-1 < Vi < 1). Therefore, the Linear Hopfield Network is limited 
to solve optimization problems with quadratic objective function and linear constraints. 
The general nonlinear optimization problem requires arbitrarily nonlinear neural 
interactions. 
Generalized Hop field Networks and Nonlinear Optimization 357 
3 THE NONLINEAR OPTIMIZATION PROBLEM 
The general nonlinear optimization problem consists of a search for the values of the 
independent variables xi, optimizing a multivariable objective function so that some 
conditions (equality, hi, and inequality, g j, constraints) are satisfied at the optimum. 
optimize f (x, x2, ..., x,) 
subject to 
hi (x,x2, ...,xn) = 0 
aj < gj(x,x2,...,xn) < bj 
_< _< 
i = 1,2,...,K, 
j = 1,2,...,M 
k = 1,2,...,N 
K<N 
The influence of the constraint geometry on the shape of the objective function is 
described in a unified manner by the Lagrangian Function: 
L = f-vrh 
The vi variables , also known as Lagrange multipliers, are unknown weighting 
parameters to be specified. In the optimum, the following conditions are satisfied: 
VL = 0 (N equations) (1) 
VvL = 0 (K equations) (2) 
From (1) and (2) it is clear that the optimization problem is transformed into a nonlinear 
equation solving problem. In a Generalized Hopfield Network each neuron represents an 
independent variable. The nonlinear connectivity among them is determined by the 
specific problem at hand and the implemented optimization algorithm. The network is 
designed to relax from an initial state to a steady-state that conesponds to a locally 
optimal solution of the problem. 
Therefore, the optimization algorithms must be transformed into a dynamic model - 
system of differential equations - that will dictate the nonlinear neural interactions. 
4 OPTIMIZATION METHODS 
Cauchy and Newton dynamics are the two most important unconstrained optimization 
(equation solving) methods, adopted by the majority of the existing algorithms. 
4.1 CAUCHY'S METHOD 
This is the famous steepest descent algorithm, which tracks the direction of the largest 
change in the value of the objective function, f. The equation of motion for a Cauchy 
dynamic system is: 
358 Reklaitis, Tsirukis and Tenorio 
= - vf ; x(O) = Xo 
dt 
4.2 NEWTON'S METHOD 
If second-order information is available, a more rapid convergence is produced using 
Newton's approximation: 
- _+ (v2f)-vf ; x(O) = Xo 
dt 
The steepest descent dynamics are very efficient initially, producing large objective- 
value changes, but close to the optimum they become very small, significantly increasing 
the convergence time. In contrast, Newton's method has a fast convergence close to the 
optimum, but the optimization direction is uncontrollable. The Levenberg - Marquardt 
heuristic, (Reklaitis, 1983), solves the problem by adopting Cauchy dynamics initially 
and switch to Newton dynamics near the optimum. Figure 1 shows the optimization 
trajectory of a Cauchy network. The algorithm converges to locally optimal solutions. 
minimize f(x) = (x + x2 - 11) 2 + (x + x - 11) 2 
Figure 1: Convergence to  Optima 
Generalized Hop field Networks and Nonlinear Optimization 359 
5 CONSTRAINED OPTIMIZATION 
The constrained optimization algorithms attempt to conveniently manipulate the equality 
and inequality constraints so that the problem is finally reduced to an unconstrained 
optimization, which is solved using Cauchy's or Newton's methods. Three are the most 
important constrained optimization algorithms: The Augmented Lagrangian, the 
Generalized Reduced Gradient (GRG) and the Successive Quadratic Programming 
(SQP). Corresponding Generalized Hopfield Networks will be developed for all of them. 
5.1 TRANSFORMATION METHODS - AUGMENTED LAGRANGIAN 
According to the transformation methods, a measure of the distance from the feasibility 
region is attached to the objective function and the problem is solved as an unconstrained 
optimization one. A transformation method was employed by Hopfield. These 
algorithms are proved inefficient because of numerical difficulties implicitly embedded in 
their structure, (Reklaitis, 1983). The Augmented Lagrangian is specifically designed to 
avoid these problems. The transformed unconstrained objective function becomes: 
P (x,o,:) 
2 
= f(x) + R[<gi(x) + cj> 2 - cj} 
J 
+ R  { [hi(x) + ,i]2 _ ,/2 } 
i 
where R is a predetermined weighting factor, and cj, 'i the corresponding inequality - 
equality Lagrange multipliers. The operator <a> returns a for a < 0. Otherwise it 
returns 0. 
The design of an Augmented Lagrangian GHN requires (N+K) neurons, where N is the 
number of variables and K is the number of constraints. The neuron connectivity of a 
GHN with Cauchy performance is described by the following model: 
d.x 
dt 
= -V f- 2R<g + >rVg - 2R[h +x] rVh 
- +VoP = 2R <g + o'> - 2Ro 
dt 
- +VxP = 2R h 
dt 
where Vg and Vh are matrices, eg. Vh = [Vh, ..., Vh,]. 
5.2 GENERALIZED REDUCED GRADIENT 
According to the GRG method, K variables (basics, x) are determined by solving the K 
nonlinear constraint equations, as functions of the rest (N-K) variables (non-basics, x--). 
Subsequently the problem is solved as a reduced-dimension unconstrained optimization 
problem. Equations (1) and (2) are transformed to: 
360 Reklaitis, Tsirukis and Tenorio 
v/- v/-v? (v/;)-' v;- o 
n(x) = o 
The constraint equations are solved using Newton's method. Note that the Lagrange 
multipliers are explicitly eliminated. The design of a GRG GHN requires N neurons, 
each one representing an independent variable. The neuron connectivity using Cauchy 
dynamics for the unconstrained optimization is given by: 
az _ _ v? - - v/+ vf ( w;)-' v- () 
dt 
h(x) = 0 (---> - h (Vh ) ) (4) 
dt 
x(O) = Xo 
System (3)-(4) is a differential - algebraic system, with an inherent sequential character: 
for each small step towards lower objective values, produced by (3), the system of 
nonlinear constraints should be solved, by relaxing equations (4) to a steady-state. The 
procedure is repeated until both equations (3) and (4) reach a steady state. 
5.3 SUCCESSIVE QUADRATIC PROGRAMMING 
In the SQP algorithm equations (1) and (2) are simultaneously solved as a nonlinear 
system of equations with both the independent variables, x, and the Lagrange multipliers, 
v, as unknowns. The solution is determined using Newton's method. 

