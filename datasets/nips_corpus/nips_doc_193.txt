758 Satyanarayana, Tsividis and Graf 
A Reconfigurable 
Analog VLSI Neural 
Chip 
Network 
Srinagesh Satyanarayana and Yannis Tsiviclls 
Department of Electrical Engineering 
and 
Center for Telecommunications Research 
Columbia University, New York, NY 10027, USA 
Hans Peter Graf 
AT&T 
Bell Laboratories 
Holmdel, NJ 07733 
USA 
ABSTRACT 
1024 distributed-neuron synapses have been integrated in an active 
area of 6.1mm x 3.3mm using a 0.9ttm, double-metal, single-poly, 
n-well CMOS technology. The distributed-neuron synapses are ar- 
ranged in blocks of 16, which we call '4 x 4 tiles ' Switch matrices 
are interleaved between each of these tiles to provide programma- 
bility of interconnections. With a small area overhead (15 %), the 
1024 units of the network can be rearranged in various configura- 
tions. Some of the possible configurations are, a 12-32-12 network, 
a 16-12-12-16 network, two 12-32 networks etc. (the numbers sep- 
arated by dashes indicate the number of units per layer, including 
the input layer). Weights are stored in analog form on MOS ca- 
pacitors. The synaptic weights are usable to a resolution of 1% of 
their full scale value. The limitation arises due to charge injection 
from the access switch and charge leakage. Other parameters like 
gain and shape of nonlinearity are also programmable. 
Introduction 
A wide variety of problems can be solved by using the neural network framework 
[1]. However each of these problems requires a different topology and weight set. 
At a much lower system level, the performance of the network can be improved 
by selecting suitable neuron gains and saturation levels. Hardware realizations of 
A Reconfigurable Analog VLSI Neural Network Chip 759 
r $ output 
o1$ 
uts 
Figure 1: Reconfigurability 
neural networks provide a fast means of solving the problem. We have chosen 
analog circuits to implement neural networks because they provide high synapse 
density and high computational speed. In order to provide a general purpose hard- 
ware for solving a wide variety of problems that can be mapped into the neural 
network framework, it is necessary to make the topology, weights and other neuro- 
synaptic parameters programmable. Weight programmability has been extensively 
dealt in several implementations [2 - 9]. However features like programmable topol- 
ogy, neuron gains and saturation levels have not been addressed extensively. We 
have designed, fabricated and tested an analog VLSI neural network in which the 
topology, weights and neuron gains and saturations levels are all programmable. 
Since the process of design, fabrication and testing is time-consuming and expensive, 
redesigning the hardware for each application is inefficient. Since the field of neural 
networks is still in its infancy, new solutions to problems are being searched for 
everyday. These involve modifying the topology [10] and finding the best weight 
set. In such an environment, a computational tool that is fully programmable is 
very desirable. 
The Concept of Reconfigurability 
We define reconfigurability as the ability to alter the topology (the number of layers, 
number of neurons per layer, interconnections from layer to layer and interconnec- 
tions within a layer) of the network. The topology of a network does not describe 
the value of each synaptic weight. It only specifies the presence or absence of a 
synapse between two neurons (However in the special case of binary weight (0,1), 
defining the topology specifies the weight). The ability to alter the synaptic weight 
can be defined as weight programmability. Figure i illustrates reconfigurability, 
whereas Figure 2 shows how the weight value is realized in our implementation. 
The Voltage Vw across the capacitor represents the synaptic weight. Altering this 
voltage makes weight programmability possible. 
Why is On-Chip Reconfigurability Important ? 
Synapses, neurons and interconnections occupy real estate on a chip. Chip sizes 
are limited due to various factors like yield and cost. Hence only a limited number 
760 Satyanarayana, Tsividis and Graf 
Figure 2: Weight programmability 
of synapses can be integrated in a given chip area. Currently the most compact 
realizations (considering more than 6 bits of synaptic accuracy) permit us to inte- 
grate only a few thousand synapses per cm 2. In such a situation every zero-valued 
(inactive) synapse represents wasted area, and decreases the computational ability 
per unit area of the chip. If a fixed topology network is used for different problems, 
it will be underutilized as long as some synapses are set to zero value. On the 
other hand, if the network is reconfigurable, the limited resources on-chip can be 
reallocated to build networks with different topologies more efficiently. For example 
the network with topology-2 of Figure I requires 30 synapses. If the network was 
reconfigurable, we could utilize these synapses to build a two-layer network with 15 
synapses in the first layer and 15 in the second layer. In a similar fashion we could 
also build the network with topology-3 which is a network with localized receptive 
fields. 
The Distributed-Neuron Concept 
In order to provide reconfigurability on-chip, we have developed a new cell called the 
distributed-neuron synapse [11]. In addition to making reconfiguration easy, it has 
other advantages like being modular hence making design easy, provides automatic 
gain scaling , avoids large current build-up at any point and makes possible a fault 
tolerant system. 
Figure 3 shows a lumped neuron with N synaptic inputs. We call it 'lumped' 
because, the circuit that provides the nonlinear function is lumped into one block. 
Lumped 
Neuron 
Figure 3: A lumped neuron with N synaptic inputs 
A Reconfigurable Analog VLSI Neural Network Chip 761 
The synapses are assumed to be voltage-to-current (transconductor) cells, and the 
neuron is assumed to be a current-to-voltage cell. Summation is achieved through 
addition of the synapse output currents in the parallel connection. 
Figure 4 shows the equivalent distributed-neuron with N synaptic inputs. It is 
called 'distributed' because the circuit that functions as the neuron, is split into 'N' 
parts. One of these parts is integrated with each synapse. This new block ( that 
contains a a synapse and a fraction of the neuron ) is called the 'distributed-neuron 
synapse'. Details of the distributed-neuron concept are described in [11]. It has to 
be noted that the splitting of the neuron to form the distributed-neuron synapse 
is done at the summation point where the computation is linear. Hence the two 
realizations of the neuron are computationally equivalent. However, the distributed- 
neuron implementation offers a number of advantages, as is now explained. 
Neuron 
X 1 :x 2 x Distributed-neuron x 
Sl nape N 
Figure 4: A distributed-neuron with N synaptic inputs 
Modularity of the design 
As is obvious from Figure 4, the task of building a complete network involves de- 
signing one single distributed-neuron synapse module and interconnecting several of 
them to form the whole system. Though at a circuit level, a fraction of the neuron 
has to be integrated with each synapse, the system level design is simplified due to 
the modularity. 
Automatic gain normalization 
In the distributed-neuron, each unit of the neuron serves as a load to the output 
of a synapse. As the number of synapses at the input of a neuron increases, the 
number of neuron elements also increases by the same number. The neuron output 
is given by: 
N 
yi = f{-E wiixi - Oj} (1) 
i=1 
Where YJ is the output of the jtn neuron, wij is the weight from the i th synaptic 
input xi and Oj is the threshold, implemented by connecting in parallel an appro- 
priate number of distributed-neuron synapses with fixed inputs. Assume for the 
762 Satyanarayana, Tsividis and Graf 
Figure 5: Switches used for reconfiguration in the distributed-neuron implemen- 
tation. 
moment that all the inputs zi are at a maximum possible value. Then it is easily 
seen that YJ is independent of N. This is the manifestation of the automatic gain 
normalization that is inherent to the idea of distributed-neuron synapses. 
Ease of reconfiguration 
In a distributed-neuron implementation, reconfiguration involves interconnecting a 
set of distributed-neuron synapse modules (Figure 5). A neuron of the right size 
gets formed when the outputs of the required number of synapses are connected. 
In a lumped neuron implementation, reconfiguration involves interconnecting a set 
of synapses with a set of neurons. This involves more wiring, switches and logic 
control blocks. 
Avoiding large current build-up in the neuron 
In our implementation the synaptic outputs are currents. These currents are summed 
by Kirchoffs current law and sent to the neuron. Since the neuron is distributed, the 
total current is divided into N equal parts, where N is the number of distributed- 
neuron synapses. One of these part flows through each unit of the distributed neuron 
as illustrated in Figure 4. This obviates the need for large current summation wires 
and avoids other problems associated with large currents at any single point. 
Fault tolerance 
On a VLSI chip defects are commonly seen. Some of these defects can short wires, 
hence corrupting the signals that are carried on them. Defects can also render some 
synapses and neurons defective. In our implementation, we have integrated switches 
in-between groups of distributed-neuron synapses (which we call 'tiles') to make the 
chip reconfigurable (Figure 6). This makes each tile of the chip externally testable. 
The defective sections of the chip can be isolated and the remaining synapses can 
thus be reconfigured into another topology as shown in Figure 6. 
Circuit Description of the Distributed-Neuron Synapse 
Figure 7 shows a distributed-neuron synapse constructed around a differential-input, 
di
