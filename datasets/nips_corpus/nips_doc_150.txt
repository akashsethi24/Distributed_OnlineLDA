124 
ADAPTIVE NEURAL NET PREPROCESSING 
FOR SIGNAL DETECTION 
IN NON-GAUSSIAN NOISE 
Richard P. Lippmann and Paul Beckman 
MIT Lincoln Laboratory 
Lexington, MA 02173 
ABSTRACT 
A nonlinearity is required before matched filtering in minimum error 
receivers when additive noise is present which is impulsive and highly 
non-Gaussian. Experiments were performed to determine whether the 
correct clipping nonlinearity could be provided by a single-input single- 
output multi-layer perceptron trained with back propagation. It was 
found that a multi-layer perceptron with one input and output node, 20 
nodes in the first hidden layer, and 5 nodes in the second hidden layer 
could be trained to provide a clipping nonlinearity with fewer than 5,000 
presentations of noiseless and corrupted waveform samples. A network 
trained at a relatively high signal-to-noise (S/N) ratio and then used 
as a front end for a linear matched filter detector greatly reduced the 
probability of error. The clipping nonlinearity formed by this network 
was similar to that used in current receivers designed for impulsive noise 
and provided similar substantial improvements in performance. 
INTRODUCTION 
The most widely used neural net, the adaptive hnear combiner (ALC), is a single- 
layer perceptron with linear input and output nodes. It is typically trained using the 
LMS algorithm and forms one of the most common components of adaptive filters. 
ALCs are used in high-speed modems to construct equalization filters, in telephone 
links as echo cancelers, and in many other signal processing applications where linear 
filtering is required [9]. The purpose of this study was to determine whether multi- 
layer perceptrons with linear input and output nodes but with sigmoidal hidden 
nodes could be as effective for adaptive nonlinear filtering as ALCs are for linear 
filtering. 
 This work was sponsored by the Defense Advanced Research Projects Agency and the Depart- 
ment of the Air Force. The views expressed are those of the authors and do not reflect the policy 
or position of the U.S. Government. 
Adaptive Neural Net Preprocessing for Signal Detection 125 
The task explored in this paper is signal detection with impulsive noise where an 
adaptive nonlinearity is required for optimal performance. Impulsive noise occurs 
in underwater acoustics and in extremely low frequency communications channels 
where impulses caused by lightning strikes propagate many thousands of miles [2]. 
This task was selected because a nonlinearity is required in the optimal receiver, the 
structure of the optimal receiver is known, and the resulting signal detection error 
rate provides an objective measure of performance. The only other previous studies 
of the use of multi-layer perceptrons for adaptive nonlinear filtering that we are 
aware of [6,8] appear promising but provide no objective performance comparisons. 
In the following we first present examples which illustrate that multi-layer percep- 
trons trained with back-propagation can rapidly form clipping and other nonhnear- 
ities useful for signal processing with deterministic training. The signal detection 
task is then described and theory is presented which illustrates the need for non- 
hnear processing with non-Gaussian noise. Nonhnearities formed when the input 
to a net is a corrupted signal and the desired output is the uncorrupted signal are 
then presented for no noise, impulsive noise, and Gaussian noise. Finally, signal 
detection performance results are presented that demonstrate large improvements 
in performance with an adaptive nonlinearity and impulsive noise. 
FORMING DETERMINISTIC NONLINEARITIES 
A theorem proven by Kohnogorov and described in [5] demonstrates that single- 
input single-output continuous nonlinearities can be formed by a multi-layer percep- 
tron with two layers of hidden nodes. This proof, however, requires complex nonlin- 
ear functions in the hidden nodes that are very sensitive to the desired input/output 
function and may be difficult to realize. More recently, Lapedes [4] presented an 
intuitive description of how multi-layer perceptrons with sigmoidal nonlinearities 
could produce continuous nonlinear mappings. A careful mathematical proof was 
recently developed by Cybenko [1] which demonstrated that continuous nonlinear 
mappings can be formed using sigmoidal nonlinearities and a multi-layer perceptron 
with one layer of hidden nodes. This proof, however, is not constructive and does 
not indicate how many nodes are required in the hidden layer. The purpose of our 
study was to determine whether multi-layer perceptrons with sigmoidal nonlineari- 
ties and trained using back-propagation could adaptively and rapidly form clipping 
nonlinearities. 
Initial experiments were performed to determine the difficulty of learning complex 
mappings using multi-layer perceptrons trained using back-propagation. Networks 
with 1 and 2 hidden layers and from 1 to 50 hidden nodes per layer were evalu- 
ated. Input and output nodes were linear and all other nodes included sigmoidal 
nonlinearities. Best overall performance was provided by the three-layer perceptron 
shown in Fig. 1. It has 20 nodes in the first and 5 nodes in the second hidden layer. 
This network could form a wide variety of mappings and required only slightly more 
training than other networks. It was used in all experiments. 
126 Lippmann and Beckman 
y - OUTPUT 
(Linear Sum) 
� � � � � � � 
(5 Nodes) 
(20 Nodes) 
x - INPUT 
Figure 1: The multi-layer perceptron with linear input and output nodes titat was 
used in all experiments. 
The three-layer network shown in Fig. 1 was used to form chpping and other 
deterministic nonhnearities. Results in Fig. 2 demonstrate that a clipping nonhnear- 
ity could be formed with fewer than 1,000 input samples. Input/output point pairs 
were determined by selecting the input at random over the range plotted and using 
the deterministic clipping function shown as a sohd hue in Fig. 2. Back-propagation 
training [7] was used with the gain term (71) equal to 0.1 and the momentum term 
(c) equal to 0.5. These values provide good convergence rates for the chpping func- 
tion and all other functions tested. Initial connection weights were set to small 
random values. 
The multi-layer perceptron from Fig. 1 was also used to form the four nonhnear 
functions shown in Fig. 3. The Hole Punch is useful in nonhnear signal process- 
ing. It performs much the same function as the chpper but completely ehminates 
amphtudes above a certain threshold level. Accurate approximation of titis function 
required more than 50,000 input smnples. The Step has one sharp edge and could 
be roughly approximated after 2,000 input samples. The Double Pulse requires 
approximation of two close pulses and is the nonhnear function analogy of the 
disjoint region problem studied in [3]. In titis example, back-propagation training 
approximated the rightmost pulse first after 5,000 input samples. Both pulses were 
then approximated fairly well after 50,000 input samples. The Gaussian Pulse 
is a smooth curve that could be approximated well after only 2,000 input samples. 
These results demonstrate that back-propagation training witIt sigmoidal nonlin- 
earities can form many different nonlinear functions. Quahtative results on training 
timcs are similar to those reported in [3]. In titis previous study it was demon- 
Adaptive Neural Net Preprocessing for Signal Detection 127 
BEFORE TRAINING 
'2-2 -1 0 I 2 
0.2 
40 TRIALS 1000 TRIALS 
-2 -1 0 I 
-2 -I 0 I 
INPUT (x) 
RMS ERROR 
200 400 600 800 
TRIALS 
1000 
Figure 2: Clipping nonlinearides formed using back-propagation training and the 
multi-layer perceptron front Fig. 1 (top) and the rms error produced by these non- 
linearities versus training time (bottom). 
strated that simple half-plane decision regions could be formed for classification 
problems with little training while complex disjoint decision regions required long 
training times. These new results suggest that complex nonlinearities with many 
sharp discontinuities require much more training time than simple smooth curves. 
THE SIGNAL DETECTION TASK 
The signal detection task was to discriminate between two equally likely input sig- 
nals as sitown in Fig. 4. Oue signal (so(t)) corresponds to no input and the other 
signal (s(t)) was a sinewave pulse witIt fixed duration and known amplitude, fre- 
quency, and phase. Noise was added to these inputs, the resultant signal was passed 
through a memoryless nonhnearity, and a matched filter was then used to select hy- 
pothesis H0 corresponding to no input or H corresponding to the sinewave pulse. 
The matched filter multiplied the output of the nonlincarity by the known time- 
ahgued signal waveform, integrated this product over time, and decided H if the 
result was greater than a threshold and H0 otherwise. The threshold was selected 
to provide a minimum overall error rate. The optimum nonlinearity used in the de- 
tector depends on the noise distribution. If the signal levels are small relative to the 
d 
noise levels, then the optimum nonlinearity is approximated by f(x) = ,T7. ln(p, (x)), 
where p,(x)is the instantaneous probability density function of the noise [2]. This 
function is linear for Gaussian noise but has a clipping sitape for impulsive noise. 
128 Lippmann and Beckman 
-1 
-2 
-2 
HOLE PUNCH 
I I I 
 ...'.... ..... 
N - 5,000 
N = 50.000 
-1 0 1 2 
-1 
-2 
-2 
DOUBLEPULSE 
I I 
-1 0 1 
2 
INPUT (x) 
STEP 
I I I 
N = 2,000 
I I I 
-2 -1 0 1 2 
GAUSSIAN PULSE 
I I I 
N = 500 
N = 2,000 
� i --T'----' 
-2 -1 0 1 2 
Figure 3: Four deterministic nonlinearities formed using the multi-layer perceptron 
from Fig. 1. Desired functions are plotted as sohd hues while functions forlned 
using back-propagation with different numbers of input samples are plotted using 
dots and dashes. 
Examples of the signal, impulsiv
