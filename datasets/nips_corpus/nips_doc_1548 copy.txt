A Neuromorphic Monaural Sound 
Localizer 
John CI. Harris, Chiang-Jung Pu, and Jose C. Principe 
Department of Electrical & Computer Engineering 
University of Florida 
Gainesville, FL 32611 
Abstract 
We describe the first single microphone sound localization system 
and its inspiration from theories of human monaural sound localiza- 
tion. Reflections and diffractions caused by the external ear (pinna) 
allow humans to estimate sound source elevations using only one 
ear. Our single microphone localization model relies on a specially 
shaped reflecting structure that serves the role of the pinna. Spe- 
cially designed analog VLSI circuitry uses echo-time processing to 
localize the sound. A CMOS integrated circuit has been designed, 
fabricated, and successfully demonstrated on actual sounds. 
1 Introduction 
The principal cues for human sound localization arise from time and intensity dif- 
ferences between the signals received at the two ears. For low-frequency components 
of sounds (below 1500Hz for humans), the phase-derived interaural time difference 
(ITD) can be used to localize the sound source. For these frequencies, the sound 
wavelength is at least several times larger than the head and the amount of shadow- 
ing (which depends on the wavelength of the sound compared with the dimensions of 
the head) is negligible. ITD localization is a well-studied system in biology (see e.g., 
[5]) and has even been mapped to neuromorphic analog VLSI circuits with limited 
success on actual sound signals [6] [2]. Above 3000Hz, interaural phase differences 
become ambiguous by mtltiples of 360 ï¿½ and are no longer viable localization cues. 
For these high frequencies, the wavelength of the sound is small enough that the 
sound amplitude is attenuated by the head. The intensity difference of the log mag- 
nitudes at the ears provides a unique interaural intensity difference (IID) that can 
be used to localize. 
Many studies have shown that when one ear is completely blocked, humans can 
still localize sounds in space, albeit at a worse resolution in the horizontal direc- 
A Neurornorphic Monaural Sound Localizer 693 
Sound Signal 
Neuromorphic 
Microphone 
', Cochlea 
Model 
Detecting 
Onset 
(a) 
Reflector 
S2 
Reflector 
Generating 
Pulse 
t 
Adaptive 
Threshold 
Computing 
Delay 
Figure 1: (a) Proposed localization model is inspired from the biological model (b) 
Special reflection surface to serve the role of the pinna 
tion. Monaural localization requires that information is somehow extracted from 
the direction-dependent effects of the reflections and diffractions of sound off of the 
external ear (pinna), head, shoulder, and torso. The so-called Head Related Trans- 
fer Function (HRTF) is the effective direction-dependent transfer function that is 
applied to the incoming sound to produce the sound in the middle ear. Section 
2 of this paper introduces our monaural sound localization model and Section 3 
discusses the simulation and measurement results. 
2 Monaural Sound Localization Model 
Batteau [1] was one of the first to emphasize that the external ear, specifically the 
pinna, could be a source of spatial cues that account for vertical localization. He 
concluded that the physical structure of the external ear introduced two significant 
echoes in addition to the original sound. One echo varies with the azimuthal position 
of the sound source, having a latency in the 0 to 80/s range, while the other varies 
with elevation in the 100/s to 300/s range. The output y(t) at the inner ear is 
related to the original sound source x(t) as 
y(t) = x(t) + alx(t -- rs) + a2x(t -- rv) (1) 
where rs, rv refer to azimuth and elevation echoes respectively; al and a2 are two 
reflection constants. Other researchers subsequently verified these results [11] [4]. 
Our localizer system (shown in Figure l(a)) is composed of a special reflection 
surface that encodes the sound source's direction, a silicon cochlea that functions 
as a band-pass filter bank, onset detecting circuitry that detects and amplifies the 
energy change at each frequency tap, pulse generating circuitry that transfers analog 
sound signals into pulse signals based on adaptively thresholding the onset signal, 
and delay time computation circuitry that computes the echo's time delay then 
decodes the sound source's direction. 
Since our recorded signal is composed of a direct sound and an echo, the sound is a 
simplified version of actual HRTF recordings that are composed of the direct sound 
694 d. G. Harris, C.-J. Pu and d. C. Principe 
Vt Va'l__bl  Von out 
Vol_b2 I 
Vneu_ou 
Figure 2: (a) Sound signal's onset is detected by taking the difference of two low-pass 
filters with different time constants. (b) Pulse generating circuit. 
and its reflections from the external ear, head, shoulder, and torso. To achieve 
localization in a 1D plane, we may use any shape of reflection surface as long as the 
reflection echo caused by the surface provides a one-to-one mapping between the 
echo's delay time and the source's direction. Thus, we propose two fiat surfaces to 
compose the reflection structure in our proposed model depicted in Figure l(b). A 
microphone is placed at distances al and w2 from two fiat surfaces (S1 and S2), d is 
the distance between the microphone and the sound source moving line (the dotted 
line in Figure l(b). As shown in Figure l(b), a sound source is at Zb position. 
If the source is far enough from the reflection surface, the ray diagram is valid to 
analyze the sound's behavior. We skip the complete derivation but the echo's delay 
time can be expressed as 
rl -{- r2 -- dl 
= (2) 
c 
where dl is the length of the direct path, rl + r2 is reflected path length, and c 
is the speed of sound. The path distance are easily solved in terms of the source 
direction and the geometry of the setup (see [9] for complete details). 
The echo's delay time - decreases as the source position b moves from 0 to 90 
degrees. A similar analysis can be made if the source moves in the opposite direction, 
and the reflection is caused by the other reflection surface S. Since the reflection 
path is longer for reflection surface S than for reflection surface S1, the echo's delay 
time can be segmented into two ranges. Therefore, the echo's delay time encodes 
the source's directions in a one-to-one mapping relation. 
In the setup, an Earthworks M30 microphone and Lab1 amplifier were used to record 
and amplify the sound signals [3]. For this preliminary study of monaural localiza- 
tion, we have chosen to localize simple impulse sounds generated through speakers 
and therefore can drop the silicon cochlea from our model. In the future, more 
complicated signals, such as speech, will require a silicon cochlea implementation. 
Inspired by ideas from visual processing, onset detection is used'to segment sounds 
[10]. The detection of an onset is produced by first taking the difference of two 
first-order, low-pass filters given by [10] 
O(t,k,r) = fz(t- x,k)8(x)az - fz(t- x,k/r)8()az 
(3) 
where r>l, k is a time constant, s(x) is the input sound signal, and fz(x, k) = 
k exp(-kx). 
A hardware implementation of the above equation is depicted in Figure 2a. In our 
model, sound signals from the special reflection surface microphone are fed into 
two low-pass filters which have different time constants determined by two bias 
A Neuromorphic Monaural Sound Localizer 695 
Vthresh 
Figure 3: Adaptive threshold circuit used to remove unwanted reflections. 
Figure 4: Neural signal processing model 
voltages Von, and Vo,,:. The bias voltage Vo,8 determines the amplification of the 
difference. The output of the onset detecting circuit is Vono,. The onset detection 
circuit determines significant increases in the signal energy and therefore segments 
sound events. By computing the delay time between two sound events (direct 
sound and its echo caused by the reflection surface), the system is able to decode 
the source's direction. Each sound event is then transformed into a fixed-width 
pulse so that the delay time can be computed with binary autocorrelators. 
The fixed-width pulse generating circuit is depicted in Figure 2b. The pulse generat- 
ing circuit includes a self-resetting neuron circuit [8] that controls the pulse duration 
based on the bias voltage Vneu8. As discussed above, an appropriate threshold is 
required to discriminate sound events from noise. One input of the pulse generating 
circuit is the output of the onset detecting signal, Vono,. Vthresh is set properly in 
the pulse generating circuit in order to generate a fixed width pulse when Vono, 
exceeds Vthresh- Unfortunately the system may be confused by unwanted sound 
events due to extraneous reflections from the desks and walls. However, since we 
know the expect range of echo delays, we can inhibit many of the environmental 
echoes that fall outside this range using an adaptive threshold circuit. 
In order to cancel unwanted signals, we need to design an inhibition mechanism 
which suppresses signals arriving to our system outside of the expected time range. 
This inhibition is implemented in Figure 3. As the pulse generating circuit detects 
the first sound event (which is the direct sound signal), the threshold becomes high 
in a certain period of time to suppress the detection of the unwanted reflections (not 
from our reflection surfaces). The input of the adaptive threshold circuit is Vneuo, 
which is the output of the pulse generating circuit. The output of the threshold 
circuit is Vthresh which is the input of the pulse generating circuit. When the pulse 
generating circuit detects a sound event, Vneuo becomes high, which increases 
[/thresh from Vre.f 2 to Vre.f 1 as shown in Figure 3. The higher l/thresh suppresses the 
detection. The suppression time is determined by the other self-resetting neuron 
circuit. 
696 ,L. G. Harris, C.-J. Pu and 2 C. Principe 
INP
