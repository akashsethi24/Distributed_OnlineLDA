Shrinking the Tube: 
A New Support Vector Regression Algorithm 
Bernhard Schiilkopl '*, Peter Bartlett*, Alex Smola�,*, Robert Williamson* 
� GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany 
* FEIT/RSISE, Australian National University, Canberra 0200, Australia 
bs, smola @ first.gmd.de, Peter. Bartlett, Bob. Williamson @anu.edu.au 
Abstract 
A new algorithm for Support Vector regression is described. For a priori 
chosen , it automatically adjusts a flexible tube of minimal radius to the 
data such that at most a fraction  of the data points lie outside. More- 
over, it is shown how to use parametric tube shapes with non-constant 
radius. The algorithm is analysed theoretically and experimentally. 
1 INTRODUCTION 
Support Vector (SV) machines comprise a new class of learning algorithms, motivated by 
results of statistical learning theory (Vapnik, 1995). Originally developed for pattern recog- 
nition, they represent the decision boundary in terms of a typically small subset (Sch61kopf 
et al., 1995) of all training examples, called the Support Vectors. In order for this property 
to carry over to the case of SV Regression, Vapnik devised the so-called s-insensitive loss 
function I!t - f(x)[ = max{O, lY - f(x)[ - }, which does not penalize errors below 
some e > O, chosen a priori. His algorithm, which we will henceforth call e-SVR, seeks to 
estimate functions 
f(x) = (w-x) + b, w,x  11N,b  11, (1) 
based on data 
(x, y),..., (x, y)  N x , (2) 
by minimizing the regularized risk functional 
IIw[I 2/2 + c. R 
,top, (3) 
where U is a constant determining the trade-off between niinimizing training errors and 
minimizing the model complexity term Ilwll 2, and R 
ernp .- 
The parameter s can be useful if the desired accuracy of the approximation can be specified 
beforehand. In some cases, however, we just want the estimate to be as accurate as possible, 
without having to commit ourselves to a certain level of accuracy. 
We present a modification of the s-SVR algorithm which automatically minimizes s, thus 
adjusting the accuracy level to the data at hand. 
Shrinking the Tube.' A New Support Vector Regression Algorithm 331 
2 v-SV REGRESSION AND c-SV REGRESSION 
To estimate functions (1) from empirical data (2) we proceed as follows (Sch61kopf et al., 
1998a). At each point xi, we allow an error of e. Everything above e is captured in 
slack variables ,c(*) ((.) being a shorthand implying both the variables with and without 
asterisks), which are penalized in the objective function via a regularization constant (7, 
chosen a priori (Vapnik, 1995). The tube size s is traded off against model complexity and 
slack variables via a constant t,, > 0: 
minimize r(w,(*),) = I1w112/2 
subject to ((w � xi) +b)-Yi 
- ((w. + 
(*) > 0, e 
I )) 
< e + i i= 
<_ e + 
> O. 
(4) 
(5) 
(6) 
(7) 
Here and below, it is understood that i = 1,..., �, and that bold face greek letters denote 
�-dimensional vectors of the corresponding variables. Introducing a Lagrangian with mul- 
tipliers - (*) -(*) 
cr i , 'tli ,/3 >_ 0, we obtain the the Wolfe dual problem. Moreover, as Boser et al. 
(1992), we substitute a kernel k for the dot product, corresponding to a dot product in some 
feature space related to input space via a nonlinear map , 
k(x,y) = ((x)- (y)). (8) 
This leads to the v-SVR Optimization Problem: for v >_ 0, C > 0, 
maximize W(c (*)) = (ct? -- cti)Yi 2 
subject to 
 C 
(10) 0 < -(*)<- 
The regression estimate can be shown to take the form 
/(x): (ct? - cti)k(xi,x) + b, 
i=1 
i,j=l 
(9) 
(13) 
where b (and ) can be computed by taking into account that (5) and (6) (substitution of 
j (ctj - ctd)k(xd, x) for (w-x) is understood) become equalities with I*) = 0 for points 
 (*) C/E, respectively, due to the Karush-Kuhn-Tucker conditions (cf. Vapnik, 
with 0 < c i < 
1995). The latter moreover imply that in the kernel expansion (13), only those - (*) will 
cr i 
be nonzero that correspond to a constraint (5)/(6) which is precisely met. The respective 
patterns xi are referred to as Support Vectors. 
Before we give theoretical results explaining the significance of the parameter t,,, the fol- 
lowing observation concerning  is helpful. If t., > 1, then  - 0, since it does not pay to 
increase s (cf. (4)). If t,, < 1, it can still happen that e = 0, e.g. if the data are noise-free 
and can perfectly be interpolated with a low capacity model. The case s = 0, however, is 
not what we are interested in; it corresponds to plain Lt loss regression. Below, we will use 
the term errors to refer to training points lying outside of the tube, and the term fraction 
of errors/SVs to denote the relative numbers of errors/SVs, i.e. divided by �. 
Proposition 1 Assume  > O. The following statements hold: 
(i) t,, is an upper bound on the fraction of errors. 
(ii) t,, is a lower bound on the fraction of SVs. 
332 B. Sch6lkopf, P. L. Bartlett, A.d. Smola and R. Williamson 
(iii) Suppose the data (2) were generated iid from a distribution P(x,j) = 
P(x)P(ylx) with/v(/[x) continuous. With probability 1, asymptotically, t., equals 
both the fraction of SVs and the fraction of errors. 
The first two statements of this proposition can be proven from the structure of the dual op- 
timization problem, with (12) playing a crucial role. Presently, we instead give a graphical 
proof based on the primal problem (Fig. 1). 
To understand the third statement, note that all errors are also SVs, but there can be SVs 
which are not errors: namely, if they lie exactly at the edge of the tube. Asymptotically, 
however, these SVs form a negligible fraction of the whole SV set, and the set of errors and 
the one of SVs essentially coincide. This is due to the fact that for a class of functions with 
well-behaved capacity (such as SV regression functions), and for a distribution satisfying 
the above continuity condition, the number of points that the tube edges f 4- s can pass 
through cannot asymptotically increase linearly with the sample size. Interestingly, the 
proof (Sch61kopf et al., 1998a) uses a uniform convergence argument similar in spirit to 
those used in statistical learning theory. 
Due to this proposition, 0 _< , < 1 can be used to control the number of errors (note that 
for t., _> 1, (11) implies (12), since ci � ct. = 0 for all i (Vapnik, 1995)). Moreover, since 
the constraint (10) implies that (12) is equivalent to -i - � < C///2, we conclude that 
Proposition 1 actually holds for the upper and the lower edge of the tube separately, with 
t.,/2 each. As an aside, note that by the same argument, the number of SVs at the two edges 
of the standard s-SVR tube asymptotically agree. 
Moreover, note that this bears on the robustness of t.,-SVR. At first glance, SVR seems all 
but robust: using the s-insensitive loss function, only the patterns outside of the s-tube con- 
tribute to the empirical risk term, whereas the patterns closest to the estimated regression 
have zero loss. This, however, does not mean that it is only the outliers that determine the 
regression. In fact, the contrary is the case: one can show that local movements of target 
values ti of points xi outside the tube do not influence the regression (Sch61kopf et al., 
1998c). Hence, t.,-SVR is a generalization of an estimator for the mean of a random vari- 
able which throws away the largest and smallest examples (a fraction of at most ,/2 of 
either category), and estimates the mean by taking the average of the two extremal ones of 
the remaining examples. This is close in spirit to robust estimators like the trimmed mean. 
Let us briefly discuss how the new algorithm relates to s-SVR (Vapnik, 1995). By rewriting 
(3) as a constrained optimization problem, and deriving a dual much like we did for t.,-SVR, 
Figure 1' Graphical depiction of the t.,-trick. Imag- 
ine increasing s, starting from 0. The first term in 
1 � � 
t.,s+ ? Y-'i= (i +i ) (cf. (4)) will increase propor- 
tionally to t.,, while the second term will decrease 
proportionally to the fraction of points outside of 
the tube. Hence, s will grow as long as the latter 
fraction is larger than t.,. At the optimum, it there- 
fore must be _< t., (Proposition 1, (i)). Next, imag- 
ine decreasing s, starting from some large value. 
Again, the change in the first term is proportional 
to t.,, but this time, the change in the second term 
is proportional to the fraction of SVs (even points 
on the edge of the tube will contribute). Hence, s 
will shrink as long as the fraction of SVs is smaller 
than t.,, eventually leading to Proposition 1, (ii). 
Shrinla'ng the Tube.' ,4 New &tpport Vector Regession ,4lgorithm 333 
one arrives at the following quadratic program: maximize 
1 
W(c, cU) = -s E(ct;+cti)+E(ctT-cti)yi-  E (ct'-cti)(ctJ -ctj)k(xi'xd) (14) 
/=1 /=1 i,j=l 
subject to (10) and (11). Compared to (9), we have an additional term -c 
which makes it plausible that the constraint (12) is not needed. 
In the following sense, y-SVR includes c-SVR. Note that in the general case, using kernels, 
' is a vector in feature space. 
Proposition 2 If u-SVR leads to the solution_ g, ,, D, then c-SVR with c set a priori to , 
and the same value of C', has the solution ,, b. 
Proof If we minimize (4), then fix c and minimize only over the remaining variables, the 
solution does not change. I 
3 PARAMETRIC INSENSITIVITY MODELS 
We generalized c-SVR by considering the tube as not given but instead estimated it as a 
model parameter. What we have so far retained is the assumption that the c-insensitive zone 
has a tube (or slab) shape. We now go one step further and use parametric models of arbi- 
trary shape. Let {(*)} (here and below, q = 1, ..., p is understood) be a set of 2p positive 
functions on I/v . Consider the following quadratic program: for given v' *) t,� (*) > 0, 
minimize 
(*)) -IIwll2/2 + c- (%% + + + (15) 
i:1 
subjectto ((w.xi)+b)-yi 5 qsq(q(xi)+i (16) 

