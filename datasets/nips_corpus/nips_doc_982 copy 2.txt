On the Computational Utility of 
Consciousness 
Donald W. Mathis and Michael C. Mozer 
mathis@cs.colorado.edu, mozer@cs.colorado.edu 
Department of Computer Science and Institute of Cognitive Science 
University of Colorado, Boulder 
Boulder, CO 80309-0430 
Abstract 
We propose a computational framework for understanding and 
modeling human consciousness. This framework integrates many 
existing theoretical perspectives, yet is sufficiently concrete to allow 
simulation experiments. We do not attempt to explain qualia (sub- 
jective experience), but instead ask what differences exist within 
the cognitive information processing system when a person is con- 
scious of mentally-represented information versus when that infor- 
mation is unconscious. The central idea we explore is that the con- 
tents of consciousness correspond to temporally persistent states 
in a network of computational modules. Three simulations are de- 
scribed illustrating that the behavior of persistent states in the 
models corresponds roughly to the behavior of conscious states 
people experience when performing similar tasks. Our simulations 
show that periodic settling to persistent (i.e., conscious) states im- 
proves performance by cleaning up inaccuracies and noise, forcing 
decisions, and helping keep the system on track toward a solution. 
1 INTRODUCTION 
We propose a computational framework for understanding and modeling conscious- 
ness. Though our ultimate goal is to explain psychological and brain imaging data 
with our theory, and to make testable predictions, here we simply present the frame- 
work in the context of previous experimental and theoretical work, and argue that 
12 Donald Mathis, Michael C. Mozer 
it is sensible from a computational perspective. We do not attempt to explain 
qualia--subjective experience and feelings. It is not clear that qualia are amenable 
to scientific investigation. Rather, our aim is to understand the mechanisms under- 
lying awareness, and their role in cognition. We address three key questions: 
What are the preconditions for a mental representation to reach consciousness? 
� What are the computational consequences of a representation reaching conscious- 
ness? Does a conscious state affect processing differently than an unconscious state? 
What is the computational utility of consciousness? That is, what is the compu- 
tational role of the mechanism(s) underlying consciousness? 
2 THEORETICAL FRAMEWORK 
Modular Cognitive Architecture. We propose that the human cognitive ar- 
chitecture consists of a set of functionally specialized computational modules (e.g., 
Fodor, 1983). We imagine the modules to be organized at a somewhat coarse level 
and to implement processes such as visual object recognition, visual word-form 
recognition, auditory word and sound recognition, computation of spatial relation- 
ships, activation of semantic representations of words, sentences, and visual objects, 
construction of motor plans, etc. Cognitive behaviors require the coordination of 
many modules. For example, functional brain imaging studies indicate that there 
are several brain areas used for different subtasks during cognitive tasks such as 
word recognition (Posner  Cart, 1992). 
Modules Have Mapping And Cleanup Processes. We propose that mod- 
ules perform an associative memory function in their domain, and operate via a 
two-stage process: a fast, essentially feedforward input-output mapping  followed 
by a slower relaxation search (Figure 1). The computational justification for this 
two stage process is as follows. We assume that, in general, the output space of a 
module can represent a large number of states relative to the number of states that 
are meaningful or well formed--i.e., states that are interpretable by other modules 
or (for output modules) that correspond to sensible motor primitives. If we know 
which representations are well-formed, we can tolerate an inaccurate feedforward 
mapping, and clean up noise in the output by constraining it to be one of the 
well-formed states. This is the purpose of the relaxation step: to clean up the 
output of the feedforward step, resulting in a well-formed state. The cleanup pro- 
cess knows nothing about which output state is the best response to the input; 
it acts solely to enforce well-formedness. Similar architectures have been used re- 
cently to model various neuropsychological data (Hinton  Shallice, 1991; Mozer  
Behrmann, 1990; Plaut  Shallice, 1993). The empirical motivation for identifying 
consciousness with the results of relaxation search comes from studies indicating 
that the contents of consciousness tend to be coherent, or well-formed (e.g., Baars, 
1988; Crick, 1994; Damasio, 1989). 
Persistent States Enter Consciousness. In our model, module outputs enter 
consciousness if they persist for a sufficiently long time. What counts as long enough 
We do not propose that this process is feedforward at the neural level. Rather, we 
mean that any iterative refinement of the output over time is unimportant and irrelevant, 
On the Computational Utili of Consciousness 13 
relaxation search 
feedforward mapping 
Figure 1: Modules consist of two components. 
is not yet determined, but in order to model specific psychological data, we will be 
required to make this issue precise. At that time a specific commitment will need 
to be made, and this commitment must be maintained when modeling further data. 
An important property of our model is that there is no hierarchy of modules with 
respect to awareness, in contrast to several existing theories that propose that access 
to some particular module (or neural processing area) is required for consciousness 
(e.g., Baars, 1988). Rather, information in any module reaches awareness simply by 
persisting long enough. The persistence hypothesis is consistent with the theoretical 
perspectives of Smolensky (1988), Rumelhart et al (1986), Damado (1989), Crick 
and Koch (1990), and others. 
2.1 WHEN ARE MENTAL STATES CONSCIOUS? 
In our framework, the output of any module will enter consciousness if it persists in 
time. The persistence of an output state of a module is assured if: (1) it is a point 
attractor of the relaxation search (i.e., a well-formed state), and (2) the inputs to 
the module are relatively constant, i.e., they continue to be mapped into the same 
attractor basin. 
While our framework appears to make strong claims about the necessary and suf- 
ficient conditions for consciousness, without an exact specification of the modules 
forming the cognitive architecture, it is lacking as a rigorous, testable theory. A 
complete theory will require not only a specification of the modules, but will also 
have to avoid arbitrariness in claiming that certain cognitive operations or brain 
regions are modules while other are not. Ultimately, one must identify the neu- 
rophysiological and neuroanatomical properties of the brain that determine the 
module boundaries (see Crick, 1994, for a promising step in this regard). 
3 COMPUTATIONAL UTILITY OF CONSCIOUSNESS 
For the moment, suppose that our framework provides a sensible account of aware- 
ness phenomena (demonstrating this is the goal of ongoing work.) If one accepts 
this, and hence the notion that a cleanup process and the resulting persistent states 
are required for awareness, questions about the role of cleanup in the model be- 
come quite interesting because they are equivalent to questions about the role of 
the mechanism underlying awareness in cognition. One question one might ask is 
whether there is computational utility to achieving conscious states. That is, does a 
system that achieves persistent states perform better than a system that does not? 
14 Donald Mathis, Michael C. Mozer 
Does a system that encourages settling to well-formed states perform better than 
system that does not? We now show that the answer to this question is yes. 
3.1 ADDITION SIMULATION 
To examine the utility of cleanup, we trained a module to perform a simple multistep 
cognitive task: adding a pair of two-digit numbers in three steps? We tested the 
system with and without cleanup and compared the generalization performance. 
The network architecture (Figure 2) consists of a single module. The inputs consist 
of the problem statement and the current partial solution-state. The output is 
an updated solution-state. The module's output feeds back into its input. The 
problem statement is represented by four pools of units, one for each digit of each 
operand, where each pool uses a local encoding of digits. Partial solution states are 
represented by five pools, one for each of the three result digits and one for each of 
the two carry digits. 
Projection 
Input-o.utput 
mapp,ng 
I hidden units 
I op II op2 II o3 II �41 re[--' rel-Y[''lcarryllcarry21 
Figure 2: Network architecture for the addition task 
Each addition problem was decomposed into three steps (Figure 3), each describing 
a transformation from one partial solution state to the next, and the mapping net 
was trained perform each transformation individually. 
?? 71 11 11 
48 step1 48 step2 48 step3 48 
+ 62 + 62 + 62 + 62 
??? ?70 710 110 
Figure 3: The sequence of steps in an example addition problem 
Step 1 Given the problem statement, activate the rightmost result digit and right- 
most carry digit (comprising the first partial solution). 
' Of course, we don't believe that there is a brain module dedicated to addition problems. 
This choice was made because addition is an intuitive example of a multistep task. 
On the Computational Utility of Consciousness 15 
Step 2 Given the first partial solution, activate the next result and carry digits 
(second partial solution). 
Step 3 Given the second partial solution, activate the leftmost result digit (final 
solution). 
The set of well-formed states in this domain consists of all possible combinations of 
digits a
