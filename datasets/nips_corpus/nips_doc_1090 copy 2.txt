Absence of Cycles in Symmetric Neural 
Networks 
Xin Wang 
Computer Science Dept 
UCLA 
Los Angeles, CA 90024 
xwang@cs.ucla.edu 
Arun Jagota, Fernanda Botelho, Max (3arzon 
Dept of Mathematical Sciences 
University of Memphis 
Memphis, TN 38152 
jagota, botelhof, garzonm@hermes.msci.memst.edu 
Abstract 
For a given recurrent neural network, a discrete-time model may 
have asymptotic dynamics different from the one of a related 
continuous-time model. In this paper, we consider a discrete-time 
model that discretizes the continuous-time leaky integrator model 
and study its parallel and sequential dynamics for symmetric net- 
works. We provide sufficient (and necessary in many cases) con- 
ditions for the discretized model to have the same cycle-free dy- 
namics of the corresponding continuous-time model in symmetric 
networks. 
I INTRODUCTION 
For an n-neuron recurrent ntwork, a much-studied and widely-used continuous- 
time (CT) model is the leaky integrator model (Hertz, e! al., 1991; Hopfield, 1984), 
given by a system of nonlinear differential equations: 
ri dt - -xi + ai wiixi + Ii), t _ O, i = 1, ..., n, (1) 
j=l 
and a related discrete-time (DT) version is the sigmoidal model (Hopfield, 1982; 
Marcus &; Westervelt, 1989), specified by a system of nonlinear difference equations: 
xi(t + 1)=ai(Zwijxj(t)+ Ii), t - 0, 1,..., i= 1,...,n, (2) 
j----1 
where xi(t), taking values in a compact interval [a, b], represents the state of neuron 
i at time t, ri is the time constant, W = [Wij ] is the real-valued weight matrix, 
ai '  - [a, b] is the activation function which often takes a sigmoidal form and Ii 
Absence of Cycles in Symmetric Neural Networks 3 73 
is the constant external input to neuron i. When the network is symmetric (i.e., 
I/V is symmetric), the dynamics of both models have been well understood: the CT 
model (1) is always convergent, namely, every initial state will approach a fixed 
point asymptotically (Hirsch, 1989; Hertz, et al., 1991; Hopfield, 1984), and the 
DT model (2) is either convergent or approaches a periodic orbit of period 2 (i.e., 
a 2-cycle) (Goles, et al., 1985; Marcus &; Westervelt, 1989; Koiran, 1994). For 
results and analyses of fixed points and cycles in networks that are not necessarily 
symmetric, see (Brown, 1992; Bruck, 1990; Goles, 1986). 
For a given symmetric network (n, l/V, ai, Ii), the existence of possible 2-cycles in 
its discrete-time operation is sometimes trouble-some and undesirable, especially in 
associative memory and neural optimization applications where only fixed points are 
used to represent memory patterns (Hopfield, 1982) or to encode feasible solutions 
(Hertz, et al., 1991). Originally in (Hopfield, 1982) a type of sequential dynamics 
(in which only one randomly chosen neuron updates its state at any time) had 
to be employed in order to ensure the convergent dynamics of (2). A great deal 
of work on asymptotic behavior of (2) has focused on constraining the symmetric 
matrix I/V so that the model exhibits only convergent dynamics. It was shown in 
(Goles, et al., 1985) that, for ai equal to the -1/+1 signum function, if I/V is positive 
definite on the set {-1, 0, 1} n, then the model (2) is convergent only to fixed points. 
In (Marcus &; Westervelt, 1989), a similar condition on W and neuron gains was 
derived for networks with differentiable ai's (see also (Marcus, et al., 1990; Waugh 
&; Westervelt, 1993)). Nevertheless, the fact remains as that not all symmetric 
networks that are convergent in (1) show the same convergent dynamics in (2). 
Such implausibility of the DT model (2) in fully inheriting the dynamics of the CT 
model (1) leads to study of another DT model in this paper, which generalizes (2) 
with some new parameters. For symmetric networks, this model has the same types 
of parallel and sequential dynamics of (2). But, under some conditions on the new 
parameters (rather than on the weight matrix itself), this model has the same global 
convergent parallel dynamics and the same local stability around fixed points of (1). 
Moreover, with these new parameters as bifurcation parameters, the existence of 
possible 2-cycles can be understood in this model as resulting from the existence of 
possible period-doubling bifurcation when the parameters are varied. Finally, it is 
this model, rather than (2), that is used more often in practice as a discrete-time 
approximation of (1). Based on all of the above, the DT model studied here is a 
more appropriate discrete-time model of neural networks for purposes of theoretical 
investigation, numerical simulation and practical application. 
2 A DISCRETE-TIME MODEL 
The DT model that is studied in this paper is 
xi(t + 1) = (1 - ai)xi(t) + aiai( wijxj(t ) + Ii), 
j=l 
t=0,1,..., i= 1,...,n, (3) 
where ci's are newly introduced parameters, taking values in (0, 1]. This model is 
based on the Euler discretization of the CT model (1) with all ri = I 1, with xi(s) 
and (xi(s + 1)- xi(s))/ai approximating xi(t) and dxi(t)/dt in (1), respectively, 
at t: s.ci. It takes the model (2) as its special case of all ci = 1. The new 
neuron state xi(t + 1) is now a linear combination of the activation function value 
When (1) is globally convergent to fixed points, neglecting all r does not change its 
dynamics. 
3 74 X. WANG, A. JAGOTA, F. BOTELHO, M. GARZON 
ai(Y?= wlixj(t) + Ii) and the old state xi(t). Because of oq  (0, 11, the model (3) 
is well defined, in that the iterative maps resulting from the model, 
Fi(z) = (1 - ai)zi + aiai( wijz + Ii), (4) 
j=l 
preserve neuron states in the compact interval [a, b]. 
For the purposes of this paper, neuron activation functions al are sumed to satisfy 
the following constraints: 
(i) ai have continuous first-order derivatives a(U) for all U 6 R; 
(ii) al are monotone increing with a(U) > 0; 
(iii) a(U)  0  y  ; and 
(iv) a(U) take mimal values , which are usually referred to  neuron gains. 
Such functions are fairly general, including often-used [-1, 1]- and [0, 1]-sigmoids, 
such  tanh(iU), 2/tan-(iU/2), and 1/(1 + e-,). The constraints on ai's 
are sufficient for the functions defined by 
ei(xi) = ?l(y)dy. (S) 
to have the following properties that will be used subsequently in proofs of several 
propositions of this paper: 
(i) Gi(y) = a(y), and particularly G(xi(t)/ai + xi(t)) : 1 wijxi(t) + Ii; 
(ii) Gi(y)- Gi(z)  G'(y)(y- z)- 1/(2yi)(y- z)   G'(y)(y- z); 
(iii) Gi'(y)= 1/(Fl(y)); and 
(iv) Gi(yo + y) - Gi(yo) > (rain, 
-- i [ ))Yl  Yl/i, 
where Axi(t) = xi(t + 1)- xi(t). 
3 PARALLEL AND SEQUENTIAL DYNAMICS 
In parallel dynamics, also called synchronous dynamics, all neurons update their 
states in each time step. In sequential dynamics, a single neuron updates its state 
in each time step in such a way that each neuron updates its state infinitely-many 
times, over all time steps t. The most widely studied special case of sequential 
dynamics is called asynchronous dynamics (Hopfield, 1982), in which the neuron 
whose state is updated is chosen at random. This models asynchronous evolution 
of a neural network circuit composed of autonomous neurons. 
It is easy to see that the discretized DT model (3) shares the same set of fixed 
points with the CT model (1); that is, a point x* is a fixed point of (3) (i.e., 
x? = Fi(x*) with Fi given in (4)), if and only if it is a fixed point of (1) (i.e., 
+ i(Ej + = 0). 
However, as the result of discretization, fixed points may have different asymptotic 
stability (Wang & Blum, 1992) and periodic points that are not fixed points may 
occur (Blum & Wang, 1992; Marcus & Westervelt, 1989) in the DT model, especially 
when all ai = 1. Nevertheless, the discretized DT model retains the same type of the 
global parallel dynamics and sequential dynamics of (2), as stated in the following 
two propositions. These results extend the results for all cq = 1 in (Marcus & 
Westervelt, 1989) to cq  (0, 1]. 
Absence of Cycles in Symmetric Neural Networks 3 75 
Proposition I If W is symmetric, any trajectory in parallel dynamics of (3) tends 
to either a fized point or a ï¿½-cycle. 
Proof. Consider the following function: 
E(t) = - _ wijxi(t)xj(t - 1) -  Ii[xi(t) + xi(t - 1)] 
i,j i 
+ (2 - ci)Gi(xi(t - 1)) +  ciGi(Axi(t - 1)/ai + xi(t - 1)). 
i 
When cq = 1, this function is the one used in (Goles, et al., 1985; Marcus & 
Westervelt, 1989). It can be shown (the details is omitted due to space limitation) 
that the one-step change of E(t), AE(t) = E(t+ 1)-E(t), is always less than or equal 
to 0 and AE(t) = 0 implies that the two-step change Av.z(t) = z(t + 1)- z(t- 1) is 
necessarily equal to zero. As E(t) is bounded from below, the network is therefore 
convergent to either a fixed point or a 2-cycle. [] 
Proposition 2 If I/V is symmetric with all wii  -(2 - i)/(ili), the DT model 
(3) has the sequential dynamics convergent to ficed points for any ci 6 (0, 1]. 
Proof. Consider the function used in (Hopfield, 1984; Marcus & Westervelt, 1989), 
1 
L(t) = -7 w,jx,(t)xj(t) - + (6) 
i,j i 
If at time t only neuron i is chosen to update its state and all the others remain 
unchanged, then L(t) is not increasing, and it is strictly decreasing when the one- 
step change in xi(t), Axi(t), is not 0. (The derivation is omitted due to space 
limitation.) Hence, any sequential trajectory tends to some fixed point. 13 
4 GLOBAL CONVERGENCE 
Call a model of a neural network cycle-free if it is globally convergent to fixed points 
only. The following proposition provides a condition that eliminates the possible 
spurious periodic dynamic behaviors of the discretized DT model (3). 
Proposition 3 If I/V is symmetric, a suJficient condition for (3) to be cycle-free in 
parallel dynamics is 
the matrix W + (2I - A)A-1M -1 is positive definite, (7) 
where A - diag(ci) and M - diag(tzi ) are the diagonal matrices formed by the 
parameters (i and the neuron gains tzi. 
Proof. U
