Multi-State Time Delay Neural Networks 
for Continuous Speech Recognition 
Patrick Haffner 
CNET Lannion A TSS/RCP 
22301 LANNION, FRANCE 
haffnerlannion.cnet. fr 
Alex Waibel 
Carnegie Mellon University 
Pittsburgh, PA 15213 
ahw@cs.cmu.edu 
Abstract 
We present the Multi-State Time Delay Neural Network (MS-TDNN) as an 
extension of the TDNN to robust word recognition. Unlike most other hybrid 
methods, the MS-TDNN embeds an alignment search procedure into the con- 
nectionist architecture, and allows for word level supervision. The resulting 
system has the ability to manage the sequential order of subword units, while 
optimizing for the recognizer performance. In this paper we present extensive 
new evaluations of this approach over speaker-dependent and speaker-indepen- 
dent connected alphabet. 
1 INTRODUCTION 
Classification based Neural Networks (NN) have been successfully applied to phoneme 
recognition tasks. Extending those classification capabilities to word recognition is an 
important research direction in speech recognition. However, connectionist architectures 
do not model time alignment properly, and they have to be combined with a Dynamic Pro- 
gramming (DP) alignment procedure to be applied to word recognition. Most of these 
hybrid systems (Bourlard, 1989) take advantage of the powerful and well tried probaN- 
Ilstic formalism provided by Hidden Markov Models (HMM) to make use o� a reliable 
alignment procedure. However, the use of this HMM formalism strongly limits one's 
choice of word models and classification procedures. 
MS-TDNNs, which do not use this HMM formalism, suggest new ways to design speech 
recognition systems in a connectionist framework. Unlike most hybrid systems where 
connectionist procedures replace some parts of a pre-existing system, MS-TDNNs are 
designed from scratch as a global Neural Network that performs word recognition. No 
bootstrapping is required from an HMM, and we can apply learning procedures that cor- 
rect the recognizer's errors explicitly. These networks have been successfully tested on 
135 
136 Haffner and Waibel 
difficult word recognition tasks, such as speaker-dependent connected alphabet recogni- 
tion (Haffner et al, 1991a) and speaker-independent telephone digit recognition (Haffner 
and Waibel, 1991b). Section 2 presents an overview of hybrid Connectionist/HMM archi- 
tectures and training procedures. Section 3 describes the MS-TDNN architecture. Section 
4 presents our novel training procedure. In section 5, MS-TDNNs are tested on speaker- 
dependent and speaker-independent continuous alphabet recognition. 
2 HYBRID SYSTEMS 
HMMs are currently the most efficient and commonly used approach for large speech rec- 
ognition tasks: their modeling capacity, however limited, fits many speech recognition 
problems fairly well (Lee, 1988). The main limit to the modelling capacity of HMMs is 
the fact that trainable parameters must be interpretable in a probabilistic framework to be 
reestimated using the Baum-Welch algorithm with the Maximal Likelihood Estimation 
training criterion (MLE). 
Connecfionist learning techniques used in NNs (generally error back-propagation) allow 
for a much wider variety of architectures and parameterization possibilities. Unlike 
HMMs, NNs model discrimination surfaces between classes rather than the complete 
input/output distributions (as in HMMs): their parameters are only trained to minimize 
some error criterion. This gain in data modeling capacity, associated with a more discrim- 
inant training procedure, has permitted improved performance on a number of speech 
tasks, especially those in which modeling sequential information is not necessary. For 
instance, Time Delay Neural Networks have been applied, with high performance, to pho- 
neme classification (Waibel et al, 1989). To extend this performance to word recognition, 
one has to combine a front-end NN with a procedure performing time alignment, usually 
based on DP. A variety of alignment procedures and training methods have been proposed 
for those hybrid systems. 
2.1 TIME ALIGNMENT 
To take into account the time distortions that may appear within its boundaries, a word is 
generally modeled by a sequence of states (I ..... s .... dV) that can have variable durations. 
The score of a wor. d in the vocabulary accumulates frame-level scores which are a func- 
tion of the output Y(t) = (Yl(t), .... Yl(t)) of the front end NN 
0 = Max{r ' ..... r,+O Score('(t)) (1) 
s= 1 t>T s 
The DP algorithm finds the optimal alignment {Tp ..., Tv + } which maximizes this word 
score. A variety of Score functions have been proposed for Eq.(1). They are most often 
treated as likelihoods, to apply the probabilistic Viterbi alignment algorithm. 
2.1.1 NN outputs probabilities 
Outputs of classification based NNs have been shown to approximate Bayes probabilities, 
provided that they are trained with a proper objective function (Bourlard, 1989). For 
instance, we can train our front-end NN to output, at each time frame, state probabilities 
that can be used by a Viterbi alignment procedure (to each state s there corresponds a NN 
Multi-State Time Delay Neural Networks for Continuous Speech Recognition 137 
output i(s)). Eq.(1) gives the resulting word log (likelihood) as a sum of frame-level log(- 
likelihoods) which are writtenl: 
Scor%(Y(t)) = log (Yi()(t)) (2) 
2.1.2 Comparing NN output to a reference vector 
The front end NN can be interpreted as a system remapping the input to a single density 
continuous HMM (Bengio, 1991). In the case of identity covariance matrices, Eq.(1) gives 
the log(likelihood) for the k-th word (after Viterbi alignment) as a sum of distances 
between the NN frame-level output and a reference vector associated with the current 
state 2. 
Score a ('(t)) II '(t) = 
= - (3) 
Here, the reference vectors (Y, .... /, ..., ) correspond to the means of gaussian PDFs, 
and can be estimated with the Baum-Welch algorithm. 
2.2 TRAINING 
The first hybrid models that were proposed (Bourlard, 1989; Franzini, 1991) optimized the 
state-level NN (with gradient descent) and the word-level HMM (with Baum-Welch) sep- 
arately. Even though each level of the system may have reached a local optimum of its 
cost function, training is potentially suboptimal for the given complete system. Global 
optimization of hybrid connectionist/I-IMM systems requires a unified training algorithm, 
which makes use of global gradient descent (Bridle, 1990). 
3 THE MS-TDNN ARCHITECTURE 
MS-TDNNs have been designed to extend TDNNs classification performance to the word 
level, within the simplest possible connectionist framework. Unlike the hybrid methods 
presented in the previous section, the HMM formalism is not taken as the underlying 
framework here, but many of the models developed within this formalism are applicable 
to MS-TDNNs. 
3.1 FRAME-LEVEL TDNN ARCHITECTURE 
All the MS-TDNNs architectures described in this paper use the front-end TDNN archi- 
tecture (Waibel et al, 1989), shown in Fig. l, at the state level. Each unit of the first hidden 
layer receives input from a 3-frame window of input coefficients. Similarly, each unit in 
the second hidden layer receives input from a 5-frame window of outputs of the first hid- 
den layer. At this level of the system (2nd hidden layer), the network produces, at each 
time frame, the scores for the desired phonetic features. Phoneme recognition TDNNs are 
trained in a time-shift invariant way by integrating over time the output of a single state. 
3.2 BASELINE MS-TDNN 
With MS-TDNNs, we have extended the formalism of TDNNs to incorporate time align- 
ment. The front-end TDNN architecture has I output units, whose activations (ranging 
1. State prior probabilities would add a constant term to Eq.(2) 
2. State transition probabilities add an offset to Eq.(3) 
138 Haffner and Waibel 
Input 
Figure 1' Frame-Level TDNN 
State scores are 
n'copied from the 
2nd hidden layer 
of the TDNN 
Figure 2: MS-TDNN 
from 0 to 1) represent the IYame-level scores. To each state s corresponds a TDNN output 
i(s). Different states may share the same output (for instance with phone models). The DP 
procedure, as described in Eq.(1), determines the sequence of states producing the maxi- 
mum sum of activations3: 
Sc�res((t)) = Yi(s) (4) 
The frame-level score used in the MS-TDNN combines the advantages of being simple 
with that of having a formal description as an extension of the TDNN accumulation pro- 
cess to multiple states. It becomes possible to model the accumulation process as a con- 
nectionist word unit that sums the activations from the best sequence of incoming state 
units, as shown in Fig.2. This is mostly useful during the back-propagation phase: at each 
time frame, we imagine a virtual connection between the active state unit and the word 
unit, which is used to backpropagate the error at the word level down to the state level. 4 
3.3 EXTENDING MS-TDNNs 
In the previous section, we presented the baseline MS-TDNN architecture. We now 
present extensions to the word-level architecture, which provide additional trainable 
parameters. Eq.(4) is extended as: 
Score s('(t)) = Weighti. Yi(s) +Bias i (5) 
3. This equation is not very different from Eq.(2) presented in the previous section, however, all 
attempts to use log(Y,[t)) instead of Y,[t) have resulted in unstable learning runs, that have never con- 
verged properly. During the test phase, the two approaches may be functionally not very different. 
Outputs that affect the error rate in a critical way are mostly those of the correct word and the best 
incorrect word, especially when they are close. We have observed that frame level scores which play 
a key role in discrimination are close to 1.0: the two scores become asymptotically equivalent (less 
1): log(Y,(O) ~ Y,[O - 1. 
4. The alignment path found by the DP routine during the forward phase is frozen, so that
