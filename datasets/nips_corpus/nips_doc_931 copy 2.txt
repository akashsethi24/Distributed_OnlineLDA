Reinforcement Learning with Soft State 
Aggregation 
Satinder P. Singh Tommi Jaakkola Michael I. Jordan 
singh@psyche.mit.edu tommi@psyche.mit.edu jordan@psyche.mit.edu 
Dept. of Brain & Cognitive Sciences (E-10) 
M.I.T. 
Cambridge, MA 02139 
Abstract 
It is widely accepted that the use of more compact representations 
than lookup tables is crucial to scaling reinforcement learning (RL) 
algorithms to real-world problems. Unfortunately almost all of the 
theory of reinforcement learning assumes lookup table representa- 
tions. In this paper we address the pressing issue of combining 
function approximation and RL, and present 1) a function approx- 
imator based on a simple extension to state aggregation (a com- 
monly used form of compact representation), namely soft state 
aggregation, 2) a theory of convergence for RL with arbitrary, but 
fixed, soft state aggregation, 3) a novel intuitive understanding of 
the effect of state aggregation on online RL, and 4) a new heuristic 
adaptive state aggregation algorithm that finds improved compact 
representations by exploiting the non-discrete nature of soft state 
aggregation. Preliminary empirical results are also presented. 
I INTRODUCTION 
The strong theory of convergence available for reinforcement learning algorithms 
(e.g., Dayan & Sejnowski, 1994; Watkins &: Dayan, 1992; Jaakkola, Jordan & Singh, 
1994; Tsitsiklis, 1994) makes them attractive as a basis for building learning con- 
trol architectures to solve a wide variety of search, planning, and control problems. 
Unfortunately, almost all of the convergence results assume lookup table representa- 
362 Satinder Singh, Tommi Jaakkola, Michael L Jordan 
tions for value functions (see Sutton, 1988; Dayan, 1992; Bradtke, 1993; and Vanroy 
& Tsitsiklis, personal communication; for exceptions). It is widely accepted that 
the use of more compact representations than lookup tables is crucial to scaling RL 
algorithms to real-world problems. 
In this paper we address the pressing issue of combining function approximation and 
RL, and present 1) a function approximator based on a simple extension to state 
aggregation (a commonly used form of compact representation, e.g., Moore, 1991), 
namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but 
fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state 
aggregation on online RL, and 4) a new heuristic adaptive state aggregation algo- 
rithm that finds improved compact representations by exploiting the non-discrete 
nature of soft state aggregations. Preliminary empirical results are also presented. 
Problem Definition and Notation: We consider the problem of solving large 
Markovian decision processes (MDPs) using RL algorithms and compact function 
approximation. We use the following notation: $ for state space, 4 for action space, 
pa(s, s t) for transition probability, Ra(s) for payoff, and ? for discount factor. The 
objective is to maximize the expected, infinite horizon, discounted sum of payoffs. 
1.1 FUNCTION APPROXIMATION: SOFT STATE CLUSTERS 
In this section we describe a new function approximator (FA) for RL. In section 3 
we will analyze it theoretically and present convergence results. The FA maps the 
state space $ into M > 0 aggregates or clusters from cluster space X. Typically, 
M << I$1. We allow soft clustering, where each state s belongs to cluster x with 
probability P(xls), called the clustering probabilities. This allows each state s to 
belong to several clusters. An interesting special case is that of the usual state 
aggregation where each state belongs only to one cluster. The theoretical model is 
that the agent can observe the underlying state but can only update a value function 
for the clusters. The value of a cluster generalizes to all states in proportion to 
the clustering probabilities. Throughout we use the symbols x and y to represent 
individual clusters and the symbols s and s t to represent individual states. 
2 A GENERAL CONVERGENCE THEOREM 
An online RL algorithm essentially sees a sequence of quadruples, < st, at, st+, rt >, 
representing a transition from current state st to next state st+ on current action 
at with an associated payoff ft. We will first prove a general convergence theorem 
for Q-learning (Watkins & Dayan, 1992) applied to a sequence of quadruples that 
may or may not be generated by a Markov process (Bertsekas, 1987). This is 
required because the RL problem at the level of the clusters may be non-Markovian. 
Conceptually, the sequence of quadruples can be thought of as being produced by 
some process that is allowed to modify the sequence of quadruples produced by a 
Markov process, e.g., by mapping states to clusters. In Section 3 we will specialize 
the following theorem to provide specific results for our function approximator. 
Consider any stochastic process that generates a sequence of random quadruples, 
 = {< xi, ai, Yi, ri >}i, where xi, Yi  Y, ai  A, and ri is a bounded real number. 
Note that xi+ does not have to be equal to yi. Let IYI and IAI be finite, and define 
Reinforcement Learning with Soft State Aggregation 363 
indicator variables 
x(x, a, u) = { 
and 
1 when 9i =< z, a,.,. > (for any y, and any r) 
Xi(x,a)-- 0 otherwise. 
1 when 9i =< z,a,y,. > (for any r) 
0 otherwise, 
Define 
Pia, J( x' y) = E5..i Xi(x, a, y) 
and 
E=i rixi(x, a) 
n?,;�) = Ei= x(x, a) 
Theorem 1: IfYe > 0, 3Me < oo, such that for alli > 0, for allx, y G Y, and 
for all a G A, the following conditions characterize the infinite sequence 9: with 
probability i - e, 
pa , and 
I i,i+M(X, y) -- Pa(x Y)I < 
IR,i+M(x)_ta(x)] < , (1) 
where for all x a, and y, with probability one P,(x, y) P(x, y), and R  
a(x). Then, online Q-learning applied to such a sequence will converge with 
probability one to the solution of the following system of equations: Vx G Y, and 
Va  A, 
Q(x,a) = a(x) + 7  pa(x,Y)Q(Y' at) (2) 
yY 
Prooff Consider the semi-batch version of Q-learning that collects the changes 
to the value function for M steps before making the change. By assumption, for 
any e, making Me large enough will ensure that with probability i - e, the sample 
quantities for the i th batch, Pia. i.M (x, y) and trl,i+M(i)(x) are within � of the 
, -e (,) 
asymptotic quantities. In Appendix A we prove that the semi-batch version of Q- 
learning outlined above converges to the solution of Equation 2 with probability one. 
The semi-batch proof can be extended to online Q-learning by using the analysis 
developed in Theorem 3 of Jaakkola el al. (1994). In brief, it can be shown that 
the difference caused by the online updating vanishes in the limit thereby forcing 
semi-batch Q-learning and online Q-learning to be equal asymptotically. The use 
of the analysis in Theorem 3 from Jaakkola et al. (1994) requires that the learning 
'() -- 1 uniformly w.p.1.; Me(k) is 
rate parameters a are such that maxtEM(i)at(x ) 
the k th batch of size Me. If at(x) is non-increasing in addition to satisfying the 
conventional Q-learning conditions, then it will also meet the above requirement. 
Theorem i provides the most general convergence result available for Q-learning 
(and TD(0)); it shows that for an arbitrary quadruple sequence satisfying the er- 
godicity conditions given in Equations 1, Q-learning will converge to the solution 
of the MDP constructed with the limiting probabilities (P0,oo) and payoffs (R0,oo). 
Theorem i combines and generalizes the results on hard state aggregation and 
value iteration presented in Vanroy & Tsitsiklis (personal communication), and on 
partially observable MDPs in Singh et al. (1994). 
364 Satinder Singh, Tommi Jaakkola, Michael I. Jordan 
3 RL AND SOFT STATE AGGREGATION 
In this section we apply Theorem 1 to provide convergence results for two cases: 1) 
using Q-learning and our FA to solve MDPs, and 2) using Sutton's (1988) TD(0) 
and our FA to determine the value function for a fixed policy. As is usual in online 
RL, we continue to assume that the transition probabilities and the payoff function 
of the MDP are unknown to the learning agent. Furthermore, being online such 
algorithms cannot sample states in arbitrary order. In this section, the clustering 
probabilities P(xls ) are assumed to be fixed. 
Case 1: Q-learning and Fixed Soft State Aggregation 
Because of function approximation, the domain of the learned Q-value function is 
constrained to be 2' x A (2' is cluster space). This section develops a Bellman 
equation (e.g., Bertsekas, 1987) for Q-learning at the level of the cluster space. We 
assume that the agent follows a stationary stochastic policy r that assigns to each 
state a non-zero probability of executing every action in every state. Furthermore, 
we assume that the Markov chain under policy r is ergodic. Such a policy r is a 
persistently exciting policy. Under the above conditions P(slx) = 
where for all s, P(s) is the steady-state probability of being in state s. 
Corollary 1: Q-learning with soft state aggregation applied to an MDP while 
following a persistently exciting policy r will converge with probability one to the 
solution of the following system of equations: �(x, a) 6 (A' x A), 
Q(x,a) = yP(slx) IRa(s) + ? Pa(s,y)rn,axQ(y,a')] (3) 
s y 
and Pa(s, y) = Es, P(s, d)P(yld ). The Q-value function for the state space can 
then be constructed via Q(s,a) - - P(xls)Q(x, a)for all (s,a). 
Proof.' It can be shown that the sequence of quadruples produced by following pol- 
icy r and independently mapping the current state s to a cluster x with probability 
P(xls ) satisfies the conditions of Theorem 1. Also, it can be shown that 
15a(x,y) = y. P(sIx)P(s,y), and la(x)= y. P(slx)Ra(S). 
$ 
Note that the Q-values found by clustering are dependent on the sampling policy 
r, unlike the lookup table case. 
Case 2: TD(0) and Fixed Soft State Aggregation 
We present separate r
