494 
TRAINING A 3-NODE NEURAL NETWORK 
IS NP-COMPLETE 
Avrim Blum* 
MIT Lab. for Computer Science 
Cambridge, Mass. 02139 USA 
Ronald L. Rivestt 
MIT Lab. for Computer Science 
Cambridge, Mass. 02139 USA 
ABSTRACT 
We consider a 2-layer, 3-node, n-input neural network whose nodes 
compute linear threshold functions of their inputs. We show that it 
is NP-complete to decide whether there exist weights and thresholds 
for the three nodes of this network so that it will produce output con- 
sistent with a given set of training examples. We extend the result 
to other simple networks. This result suggests that those looking for 
perfect training algorithms cannot escape inherent computational 
difficulties just by considering only simple or very regular networks. 
It also suggests the importance, given a training problem, of finding 
an appropriate network and input encoding for that problem. It is 
left as an open problem to extend our result to nodes with non-linear 
functions such as sigmoids. 
INTRODUCTION 
One reason for the recent surge in interest in neural networks is the develop- 
ment of the back-propagation algorithm for training neural networks. The 
ability to train large multi-layer neural networks is essential for utilizing neural 
networks in practice, and the back-propagation algorithm promises just that. 
In practice, however, the back-propagation algorithm runs very slowly, and the 
question naturally arises as to whether there are necessarily intrinsic compu- 
tational difficulties associated with training neural networks, or whether better 
training algorithms might exist. This paper provides additional support for the 
position that training neural networks is intrinsically difficult. 
A common method of demonstrating a problem to be intrinsically hard is to 
show the problem to be NP-complete. The theory of NP-complete problems 
is well-understood (Garey and Johnson, 1979), and many infamous problems-- 
such as the traveling salesman problem--are now known to be NP-complete. 
While NP-completeness does not render a problem totally unapproachable in 
*Supported by an NSF graduate fellowship. 
tThis paper was prepared with support from NSF grant DCR-8607494, ARO Grant 
DAAL03-86-K-0171, and the Siemens Corporation. 
Training a 3-Node Neural Network is NP-Complete 495 
practice, it usually implies that only small instances of the problem can be solved 
exactly, and that large instances can at best only be solved approximately, even 
with large amounts of computer time. 
The work in this paper is inspired by Judd (Judd, 1987) who shows the following 
problem to be NP-complete: 
Given a neural network and a set of training examples, does there 
exist a set of edge weights for the network so that the network pro- 
duces the correct output for all the training examples? 
Judd also shows that the problem remains NP-complete even if it is only required 
a network produce the correct output for two-thirds of the training examples, 
which implies that even approximately training a neural network is intrinsically 
difficult in the worst case. Judd produces a class of networks and training ex- 
amples for those networks such that any training algorithm will perform poorly 
on some networks and training examples in that class. The results, however, 
do not specify any particular hard network--that is, any single network hard 
for all algorithms. Also, the networks produced have a number of hidden nodes 
that grows with the number of inputs, as well as a quite irregular connection 
pattern. 
We extend his result by showing that it is NP-complete to train a specific very 
simple network, having only two hidden nodes and a regular interconnection 
pattern. We also present classes of regular 2-layer networks such that for all 
networks in these classes, the training problem is hard in the worst case (in 
that there exists some hard sets of training examples). The NP-completeness 
proof also yields results showing that finding approximation algorithms that 
make only one-sided error or that approximate closely the minimum number 
of hidden-layer nodes needed for a network to be powerful enough to correctly 
classify the training data, is probably hard, in that these problems can be related 
to other difficult (but not known to be NP-complete) approximation problems. 
Our results, like Judd's, are described in terms of batch-style learning algo- 
rithms that are given all the training examples at once. It is worth noting that 
training with an incremental algorithm that sees the examples one at a time 
such as back-propagation is at least as hard. Thus the NP-completeness result 
given here also implies that incremental training algorithms are likely to run 
slowly. 
Our results state that given a network of the classes considered, for any training 
algorithm there will be some types of training problems such that the algorithm 
will perform poorly as the problem size increases. The results leave open the 
possibility that given a training problem that is hard for some network, there 
might exist a different network and encoding of the input that make training 
easy. 
Blum snd Rivest 
1 2 3 4 ... . 
Figure 1: The three node neural network. 
THE NEURAL NETWORK TRAINING PROBLEM 
The multilayer network that we consider has n binary inputs and three nodes: 
N, N2, Ns. All the inputs are connected to nodes N and N2. The outputs 
of hidden nodes N and N2 are connected to output node Ns which gives the 
output of the network. 
Each node Ni computes a linear threshold function fi on its inputs. If Ni has 
input z = (z,... ,x,n), then for some constants ao,..., am, 
+1 if az + a2z2 +... + a,nz,n > ao 
fi(x) = -1 otherwise. 
The a i's (j _> 1) are typically viewed as weights on the incoming edges and a0 
as the threshold. 
The training algorithm for the network is given a set of training examples. Each 
is either a positive example (an input for which the desired network output is +1) 
or a negative example (an input for which the desired output is -1). Consider 
the following problem. Note that we have stated it as a decision (yes or no) 
problem, but that the search problem (finding the weights) is at least equally 
hard. 
TRAINING A 3-NODE NEURAL NETWORK: 
Given: A set of O(n) training examples on n inputs. 
Question: Do there exist linear threshold functions f, f2, fa for nodes N, N2, Na 
Training a 3-Node Neural Network is NP-Complete 497 
such that the network of figure I produces outputs consistent with the training 
set? 
Theorem: Training a 3-node neural network is NP-complete. 
We also show (proofs omitted here due to space requirements) NP-completeness 
results for the following networks: 
o 
The 3-node network described above, even if any or all of the weights for 
one hidden node are required to equal the corresponding weights of the 
other, so possibly only the thresholds differ, and even if any or all of the 
weights are forced to be from {41,-1}. 
Any k-hidden node, for k bounded by some polynomial in n (eg: k -- n2), 
two-layer fully-connected network with linear threshold function nodes 
where the output node is required to compute the AND function of its 
inputs. 
The 2-layer, 3-node n-input network with an XOR output node, if ternary 
features are allowed. 
In addition we show (proof omitted here) that any set of positive and negative 
training examples classifiable by the 3-node network with XOR output node (for 
which training is NP-complete) can be correctly classified by a perceptron with 
O(n 2) inputs which consist of the original n inputs and all products of pairs of 
the original n inputs (for which training can be done in polynomial-time using 
linear programming techniques). 
THE GEOMETRIC POINT OF VIEW 
A training example can be thought of as a point in n-dimensional space, labeled 
'4' or '-' depending on whether it is a positive or negative example. The points 
are vertices of the n-dimensional hypercube. The zeros of the functions fx and 
f2 for the hidden nodes can be thought of as (n- 1)-dimensional hyperplanes 
in this space. The planes P1 and P2 corresponding to the functions fl and 
f2 divide the space into four quadrants according to the four possible pairs of 
outputs for nodes N1 and N2. If the planes are parallel, then one or two of the 
quadrants is degenerate (non-existent). Since the output node receives as input 
only the outputs of the hidden nodes N1 and N2, it can only distinguish between 
points in different quadrants. The output node is also restricted to be a linear 
function. It may not, for example, output +1 when its inputs are (+1,41) 
and (- 1, - 1), and output - 1 when its inputs are (+ 1, - 1) and (- 1, + 1). 
So, we may reduce our question to the following: given O(n) points in {0, 1} 
each point labeled '4' or '-', does there exist either 
498 Blum and Rivest 
1. a single plane that separates the '--' points from the '-' points, or 
2. two planes that partition the points so that either one quadrant contains 
all and only '-' points or one quadrant contains all and only '-' points. 
We first look at the restricted question of whether there exist two planes that 
partition the points such that one quadrant contains all and only the '--' points. 
This corresponds to having an AND function at the output node. We will call 
this problem: 2-Linear Confinement of Positive Boolean Examples. Once we 
have shown this to be NP-complete, we will extend the proof to the full problem 
by adding examples that disallow the other possibilities at the output node. 
Megiddo (Megiddo, 1986) has shown that for O(n) arbitrary '--' and '-' points 
in n-dimensional Euclidean space, the problem of whether there exist two hy- 
perplanes that separate them is NP-complete. His proof breaks down, however, 
when one restricts the coordinate values to C0, 1) as we do here. Our proof 
turns out to be of a quite different style. 
SET SPLITTING 
The follo
