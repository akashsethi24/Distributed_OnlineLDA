Size of multilayer networks for exact 
learning: analytic approach 
Andr Elisseeff 
D?pt Mathdmatiques et Informatique 
Ecole Normale Supdrieure de Lyon 
46 allde d'Italie 
F69364 Lyon cedex 07, FRANCE 
H61[ne Paugam-Moisy 
LIP, URA 1398 CNRS 
lcole Normale Supdrieure de Lyon 
46 allde d'Italie 
F69364 Lyon cedex 07, FRANCE 
Abstract 
This article presents a new result about the size of a multilayer 
neural network computing real outputs for exact learning of a finite 
set of real samples. The architecture of the network is feedforward, 
with one hidden layer and several outputs. Starting from a fixed 
training set, we consider the network as a function of its weights. 
We derive, for a wide family of transfer functions, a lower and an 
upper bound on the number of hidden units for exact learning, 
given the size of the dataset and the dimensions of the input and 
output spaces. 
I RELATED WORKS 
The context of our work is rather similar to the well-known results of Baum et al. [1, 
2, 3, 5, 10], but we consider both real inputs and outputs, instead of the dichotomies 
usually addressed. We are interested in learning exactly all the examples of a fixed 
database, hence our work is different from stating that multilayer networks are 
universal approximators [6, 8, 9]. Since we consider real outputs and not only 
dichotomies, it is not straightforward to compare our results to the recent works 
about the VC-dimension of multilayer networks [11, 12, 13]. Our study is more 
closely related to several works of Sontag [14, 15], but with different hypotheses on 
the transfer functions of the units. Finally, our approach is based on geometrical 
considerations and is close to the model of Coetzee and Stonick [4]. 
First we define the model of network and the notations and second we develop 
our analytic approach and prove the fundamental theorem. In the last section, we 
discuss our point of view and propose some practical consequences of the result. 
Size of Multilayer Networks for Exact Learning: Analytic Approach 163 
2 THE NETWORK AS A FUNCTION OF ITS WEIGHTS 
General concepts on neural networks are presented in matrix and vector notations, 
in a geometrical perspective. All vectors are written in bold and considered as 
column vectors, whereas matrices are denoted with upper-case script. 
2.1 THE NETWORK ARCHITECTURE AND NOTATIONS 
Consider a multilayer network with N input units, Ns hidden units and Ns output 
units. The inputs and outputs are real-valued. The hidden units compute a non- 
linear function f which will be specified later on. The output units are assumed to 
be linear. A learning set of Np examples is given and fixed. For all p  { 1..Np}, the 
pth example is defined by its input vector dp  N and the corresponding desired 
output vector tp  ]Ns. The learning set can be represented as an input matrix, 
with both row and column notations, as follows 
I d d2 ... dlN 
dN,l dN,2 ... dN,N 
= [6,..., 
� ,tv,] , with independent row vectors. 
Similarly, the target matrix is T = [t,.. r r 
2.2 THE NETWORK AS A FUNCTION g OF ITS WEIGHTS 
For all h  {1..N/}, w} = (w,..., Wv,) T  v, is the vector of the weights be- 
tween all the input units and the h th hidden unit. The input weight matrix W 1 is de- 
2 2 2 T N. 
-- ....... WsNs) 
fined as W 1 [w}, ,Wv.] Similarly, a vector w, = (w,, ,  
represents the weights between all the hidden units and the s th output unit, for all 
s e {1..Ns}. Thus the output weight matrix W  is defined as W2= [w,..., wvs]. 
For an input matrix D, the network computes an output matrix 
zl(dl) 
z(v) = � 
... zNs(di) 
z(dl) r 
z(dv.) r I 
-- [i(D),... ,N$(D)] 
where each output vector z(dp) must be equal to the target tp for exact learning. 
The network computation can be detailed as follows, for all s  {1..Ns} 
Ns N 
= 
h=l i=1 
= wsh.f(d; .w}) 
h=l 
Hence, 
(1) 
for the whole learning set, the s th output component is 
- ' 
-- Wsh. 
h=x f(dTN..W}) 
Ns 
w,h.F(D.wn) 
s()) -- E 2 1 
h--1 
164 A. Elisseeff and H. Paugam-Moisy 
In equation (1), F is a vector operator which transforms a n vector v into a n vector 
F(v) according to the relation [F(v)]i -- f([v]i), i  {1..n}. The same notation F 
will be used for the matrix operator. Finally, the expression of the output matrix 
can be deduced from equation (1) as follows 
z(v) : 
(2) z(v) : F(V.W).W 
From equation (2), the network output matrix appears as a simple function of 
the input matrix and the network weights. Unlike Coetzee and Stonick, we will 
consider that the input matrix 7) is not a variable of the problem. Thus we express 
the network output matrix 2(7)) as a function of its weights. Let g be this function 
g : NtxNI-I+NI-IXN$ ) NpxNs 
w = (w w 2) > r(v.w).w 
The # function clearly depends on the input matrix and could have be denoted by 
#v but this index will be dropped for clarity. 
3 FUNDAMENTAL RESULT 
3.1 PROPERTY OF FUNCTION g 
Learning is said to be exact on 7) if and only if there exists a network such that 
its output matrix Z(7)) is equal to the target matrix 7-. If g is a diffeomorphic 
function from R Nt xNH+NH xN$ Onto/Np xNs then the network can learn any target 
in R NP x Ns exactly. We prove that it is sufficient for the network function g to be 
a local diffeomorphism. Suppose there exist a set of weights X, an open subset 
U C 7 NNH+NHNs including X and an open subset V C 7 N'Ns including g(X) 
such that g is diffeomorphic from U to V. Since V is an open neighborhood of 
g(X), there exist a real A and a point y in V such that 7- = A(y-g(X)). Since g is 
diffeomorphic from U to V, there exists a set of weights Y in U such that y = g(Y), 
hence 7- = A(g(Y)- g(X)). The output units of the network compute a linear 
transfer function, hence the linear combination of g(X) and g(Y) can be integrated 
in the output weights and a network with twice NNH + NHNs weights can learn 
(V, 7-) exactly (see Figure 1). 
= (g(Y)-g(X)) 
Nt -'--.2N a 
Z 
Figure 1: A network for exact learning of a target 7- (unique output for clarity) 
For g a local diffeomorphism, it is sufficient to find a set of weights X such that the 
Jacobian of g in X is non-zero and to apply the theorem of local inversion. This 
analysis is developed in next sections and requires some assumptions on the transfer 
function f of the hidden units. A function which verifies such an hypothesis 7/will 
be called a 7/-function and is defined below. 
Size of Multilayer Networks for Exact Learning: Analytic Approach 165 
3.2 DEFINITION AND THEOREM 
Definition 1 Consider a function f: JZ --> JZ which is C  (JZ) (i.e. with continuous 
derivative) and which has finite limits in -cx> and +oe. Such a function is called a 
-function iff it verifies the following property 
() 
f'(ax) 
(VaJZ/lal> 1) lim [ f, 
 (x) 
From this hypothesis on the transfer function of all the hidden units, the funda- 
mental result can be stated as follows 
Theorem 1 Exact learning of a set of Np examples, in general position, from 7 Nt 
to 7 Ns, can be realized by a network with linear output units and a transfer function 
which is a 7i-function, if the size NH of its hidden layer verifies the following bounds 
Lower Bound NH = I NpNs ] hidden units are necessary 
Nt + Ns 
Upper Bound NH--2 [ Np ] 
Nt+Ns Ns hidden units are sufficient 
The proof of the lower bound is straightforward, since a condition for g to be 
diffeomorphic from R NtxNH+NsNs onto R NPxNs is the equality of its input and 
output space dimensions Ni NH + NH Ns - Np Ns. 
3.3 SKETCH OF THE PROOF FOR THE UPPER BOUND 
The g function is an expression of the network as a function of its weights, for a given 
input matrix: g(W , W 2) = F(.W).W  and g can be decomposed according to 
its vectorial components on the learning set (which are themselves vectors of size 
Ns). For all p  { 1..Np} 
gp(W W ) z(d) [.wh r  
, = = f(dp .wh),... 
kh=l 
SNs h 
h=l 
T 
The derivatives of g w.r.t. the input weight matrix W  are, for all i  {1..NI}, for 
all h  { 1..NH } 
_ ' 
Ow, i ..., wNs f (d .)di] r 
For the output weight matrix W , the derivatives ofg are, for all h  {1..NH}, for 
all s � {1..Ns} 
,0,...,0,, 
cgwh --[ $(d }),,0,... ,0,] T 
s- 1 Ns-s 
The Jacobian matrix AAa(g) of g, the size of which is NINH q- NHN$ columns 
and NsNp rows, is thus composed of a block-diagonal part (derivatives w.r.t. W a) 
and several other blocks (derivatives w.r.t. W). Hence the Jacobian J(g) can be 
rewritten J(g) =[ J, Ja,..., JNH I, after permutations of rows and columns, and 
using the Hadamard and Kronecker product notations, each Ja being equal to 
2 
(3) Jh = [F(/).w}) � IN,[F'(I).w})oJ...F'(I).w})oJN,] � [w...WNsa]] 
where INs is for the identity matrix in dimension Ns. 
166 A. Elisseeff and H. Paugam-Moisy 
Our purpose is to prove that there exists a point X = (W , W 2) such that the 
Jacobian J(g) is non-zero at X, i.e. such that the column vectors of the Jacobian 
matrix AA j (g) are linearly independent at X. The proof can be divided in two steps. 
First we address the case of a single output unit. Afterwards, this proof can be used 
to extend the result to several output units. Since the complete development of both 
proofs require a lot of calculations, we only present their sketchs below. More details 
can be found in [7]. 
3.3.1 Case of a single output unit 
The proof is based on a linear arrangement of the projections of the column vectors 
of Ja onto a subspace. This subspace is orthogonal to all the Ji for i < h. We 
build a vector w and a scalar wa such that the projected column vectors are an 
independent family, hence they are independent with the Ji for i < h. Such a con- 
struction is recursively applied until h NH. We derive then vectors w,  
-- ... ,iONH 
and w such that J(g) is non-zero. The assumption on 7/-fonctions is essential for 
proving that the projected column vectors of Ja are independent. 
3.3.2 Case of multiple output units 
