Attractor Neural Networks with Local 
Inhibition: from Statistical Physics to a 
Digital Programmable Integrated Circuit 
E. Pasero 
Dipartimento di Elettronica 
Politechico di Torino 
1-10129 Torino, Italy 
R. Zecchlna 
Dipartimento di Fisica Teorica e INFN 
Universiti di Torino 
1-10125 Torino, Italy 
Abstract 
Networks with local inhibition are shown to have enhanced compu- 
tational performance with respect to the classical Hopfield-like net- 
works. In particular the critical capacity of the network is increased 
as well as its capability to store correlated patterns. Chaotic dy- 
namic behaviour (exponentially long transients) of the devices in- 
dicates the overloading of the associative memory. An implementa- 
tion based on a programmable logic device is here presented. A 16 
neurons circuit is implemented whir a XILINK 4020 device. The 
peculiarity of this solution is the possibility to change parts of the 
project (weights, transfer function or the whole architecture) with 
a simple software download of the configuration into the XILINK 
chip. 
1 INTRODUCTION 
Attractor Neural Networks endowed with local inhibitory feedbacks, have been 
shown to have interesting computational performances[I]. Past effort was con- 
centrated in studying a variety of synaptic structures or learning algorithms, while 
less attention was devoted to study the possible role played by different dynamical 
schemes. The definition of relaxation dynamics is the central problem for the study 
of the associative and computational capabilities in models of attractor neural net- 
works and might be of interest also for hardware implementation in view of the 
805 
806 Pasero and Zecchina 
constraints on the precision of the synaptic weights. 
In this paper, we give a brief discussion concerning the computational and physical 
role played by local inhibitory interactions which lead to an effective non-monotonic 
transfer function for the neurons. In the last few years others models characterized 
by non-monotonic neurons have been proposed[2,3]. 
For Hebbian learning we show, numerically, that the critical capacity increases with 
respect to the Hopfield case and that such result can be interpreted in terms of a 
twofold task realized by the dynamical process. By means of local inhibition, the 
system dynamically selects a subspace (or subnetwork) of minimal static noise with 
respect to the recalled pattern; at the same time, and in the selected subspace, the 
retrieval of the memorized pattern is performed. The dynamic behaviour of the 
network, for deterministic sequential updating, range from fixed points to chaotic 
evolution, with the storage ratio as control parameter, the transition appearing 
in correspondence to the collapse of the associative performance. Resorting to two 
simplified versions of the model, we study the problem of their optimal performance 
by the replica method; in particular the role of non-monotonic functions and of 
subspaces dynamical selection are discussed. 
In a second part of the work, the implementation of the discussed model by means 
of a XILINK programmable gate array is discussed. The circuit implements a 16-32 
neurons network in which the analogical characteristics (such as a capacitive decay) 
are emulated by digital solutions. As expected, the limited resolution of the weghts 
does not represent a limit for the performance of the network. 
2 THE MODEL: theory and performance 
We study an attractor neural network composed of N three state :kl,0 formal 
neurons. The :kl values code for the patterns (the patterns are indeed binary) and 
are thus used during the learning phase, while the 0-state is a don't care state, not 
belonging to the patterns code, which has only a dynamical role. The system is 
assumed to be fully connected and its evolution is governed by sequential or parallel 
updating of the following equations 
sgn(hi) if ]hil _( 7 (1) 
Si - 0 if Ihi[ 'y 
N 
hi(t + 1) -- Ahi(t) + E JijSj(t) i - 1, ...,N (2) 
j----1 
where 7 is a dynamic threshold of the local inhibitory feedback (typically we take 
7(t) -  i Ihi( t - 1)I), the Jij) are the synaptic conductances and A is a capac- 
1 
itive decay factor of the input potential (A - er, where r - RC). 
The performance of the network are described in terms of two parameters which 
have both a dynamical and a computational simple interpretation. In particular we 
define the retrieval activity as the fraction of neurons which are not not in the 
zero state 
Attractor Neural Networks with Local Inhibition 807 
I 
i 
while the parameter that defines the retrieval quality is the scaled overlap 
-- 1 
- zï¿½. (4) 
i 
where the {{ = +1, i = 1, N;/z = 1, P) are the memorized binary patterns. The 
scaled overlap can be thought simply as the overlap computed in the subspace M 
of the active neurons, M -- {i / $i Y 0, i -- 1, N). 
Given a set of P random independent binary patterns {}, the Hebb-Hopfield 
learning rule corresponds to fix the synaptic matrix Jij by the additive relation 
P 
1 
Jis =  Y. { (with Jii = 0). The effect of the dynamical process defined by 
(1) and (2) is the selection of subspaces M of active neurons in which the static 
noise is minimized (such subspaces will be hereafter referred to as orthogonal sub- 
spaces). Before entering in the description of the results, it is worthwhile to remem- 
ber that, in Hopfield-like attractor neural networks, the mean of cross correlation 
fluctuations produce in the local fields of the neurons a static noise, referred to as 
cross-talk of the memories. Together with temporal correlations, the static noise is 
responsible of the phase transition of the neural networks from associative memory 
to spin-glass. More precisely, when the Hopfield model is in a fixed point a which 
belongs to the set of memories, the local fields are given by hi = 1 + R where 
1 
= -- -i -j -i -j is the static noise (gaussian distribution with 0 mean and 
variance V/). 
The preliminary performance study of the model under discussion have revealed 
several new basic features, in particular: (i) the critical capacity, for the Hebb 
learning rule, results increased up to ac  0.33 (instead of 0.1414]); (ii) the mean 
cross correlation fluctuations computed in the selected subspaces is minimized by 
the dynamical process in the region a < ac; ('fii) in correspondence to the associative 
transition the system goes through a dynamic transition from fixed points to chaotic 
trajectories. 
The quantitative results concerning associative performance, are obtained by means 
of extended simulations. A typical simulation takes the memorized patterns as 
initial configurations and lets the system relax until it reaches a stationary point. 
The quantity describing the performance of the network as an associative memory 
is the mean scaled overlap m between the final stationary states and the memorized 
patterns, used as initial states. As the number of memorized configurations grows, 
one observes a threshold at a = acm 0.33 beyond which the stored states become 
unstable. (numerical results were performed for networks of size up to N - 1000). 
We observe that since the recall of the patterns is performed with no errors (up to 
808 Pasero and Zecchina 
cz m 0.31), also the number of stored bits in the synaptic matrix results increased 
with respect to the Hopfield case. 
The typical size of the sub-networks Du, like the network capacity, depends on the 
1 
threshold parameter 7 and on the kind of updating: for 7(t) -   lh(t- 1)1 and 
parallel updating we find DM -- N/2 (czc = 0.33). 
The static noise reduction corresponds to the minimization of the mean fluctuation 
of the cross correlations (cross talk) in the subspaces, defined by 
N 
I I ) I I 
where e[ = 1 if i  M in pattern cr and zero otherwise, as a function of c. Under 
the dynamical process (1) and (2), C does not follow a statistical law but undergoes 
a minimization that qualitatively explains the increase in the storage capacity. For 
c ( cc, once the system has relaxed in a stationary subspace, the model becomes 
equivalent (in the subspace) to a Hopfield network with a static noise term which is 
no longer random. The statistical mechanics of the combinatorial task of minimizing 
the noise-energy term (5) can be studied analytically by the replica method; the 
results are of general interest in that give an upper bound to the performance 
of networks endowed with Hebb-like synaptic matrices and with the possibility 
selecting optimal subnetworks for retrieval dynamics of the patterns[8]. 
As already stated, the behaviour of the neural network as a dynamical system is 
directly related to its performance as an associative memory. The system shows 
an abrupt transition in the dynamics, from fixed points to chaotic exponentially 
long transients, in correspondence to the value of the storage ratio at which the 
memorized configurations become unstable. The only (external) control parameter 
of the model as a dynamical system is the storage ratio c = P/N. Dynamic complex 
behaviour appears as a clear signal of saturation of the attractor neural network 
and does not depend on the symmetry of the couplings. 
As a concluding remark concerning this short description of the network perfor- 
mance, we observe that the dynamic selection of subspaces seems to take advantage 
of finite size effects allowing the storage of correlated patterns also with the simple 
Hebb rule. Analytical and numerical work is in progress on this point, devoted to 
clarify the performance with spatially correlated patterns[5]. 
Finally, we end this theoretical section by addressing the problem of optimal per- 
formance for a different choice of the synaptic weights. In this direction, it is of 
basic interest to understand whether a dynamical scheme which allows for dynamic 
selection of subnetworks provides a neural network model with enhanced optimal 
capacity with
