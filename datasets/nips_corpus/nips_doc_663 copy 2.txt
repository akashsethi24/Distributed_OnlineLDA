Planar Hidden Markov Modeling: 
from Speech to Optical Character Recognition 
Esther Levin and Roberto Pieraccini 
ATr Bell Laboratories 
600 Mountain Ave. 
Murray Hill, NJ 07974 
Abstract 
We propose in this paper a statistical model (planar hidden Markov model - 
PHMM) describing statistical properties of images. The model generalizes 
the single-dimensional HMM, used for speech processing, to the planar case. 
For this model to be useful an efficient segmentation algorithm, similar to the 
Viterbi algorithm for HMM, must exist. We present conditions in terms of 
the PHMM parameters that are sufficient to guarantee that the planar 
segmentation problem can be solved in polynomial time, and describe an 
algorithm for that. This algorithm aligns optimally the image with the model, 
and therefore is insensitive to elastic distortions of images. Using this 
algorithm a joint optimal segmentation and recognition of the image can be 
performed, thus overcoming the wealmess of traditional OCR systems where 
segmentation is performed independently before the recognition leading to 
unrecoverable recognition errors. 
The PHMM approach was evaluated using a set of isolated hand-written 
digits. An overall digit recognition accuracy of 95% was achieved. An 
analysis of the results showed that even in the simple case of recognition of 
isolated characters, the elimination of elastic distortions enhances the 
performance significantly. We expect that the advantage of this approach will 
be even more significant for tasks such as connected writing 
recognition/spotting, for which there is no known high accuracy method of 
recognition. 
1 Introduction 
The performance of traditional OCR systems deteriorate very quickly when documents 
are degraded by noise, blur, and other forms of distortion. The main reason for such 
deterioration is that in addition to the intra-class character variability caused by distortion, 
the segmentation of the text into words and characters becomes a nontrivial task. In most 
of the traditional systems, such segmentation is done before recognition, leading to many 
recognition errors, since recognition algorithms cannot usually recover from errors 
introduced in the segmentation phase. Moreover, in many cases the segmentation is ill- 
defined, since many plausible segmentations might exist, and only grammatical and 
linguistic analysis can find the right one. To address these problems, an algorithm is 
needed that can: 
� be tolerant to distortions leading to intra-class variability 
731 
732 Levin and Pieraccini 
� perform segmentation together with recognition, thus join fly optimizing both 
processes, while incorporating grammatical/linguistic constraints. 
In this paper we describe a planar segmentation algorithm that has the above properties. 
It results from a direct extension of the Viterbi (Forney, 1973) algorithm, widely used in 
automatic speech recognition, to two-dimensional signals. 
In the next section we describe the basic hidden Markov model and define the 
segmentation problem. In section 3 we introduce the planar HMM that extends the HMM 
concept to model images. The planar segmentation problem for PffMM is defined in 
section 4. It was recently shown (Keams and Levin, 1992) that the planar segmentation 
problem is NP-hard, and therefore, in order to obtain an effective planar segmentation 
algorithm, we propose to constrain the parameters of the PHMM. We show sufficient 
conditions in terms of PffMM parameters for such algorithm to exist and describe the 
algorithm. This approach differs from the one taken in references (Chellappa and 
Chatterjee, 1985) and (Derin and Elliot, 1987), where instead of restricting the problem, 
a suboptimal solution to the general problem was found. Since in (Kearns and Levin, 
1992) it was also shown that planar segmentation problem is hard to approximate, such 
suboptimal solution doesn't have any guaranteed bounds. The segmentation algorithm 
can now be used effectively not only for aligning isolated images, but also for joint 
recognition/segmentation, eliminating the need of independent segmentation that usually 
leads to unrecoverable errors in recognition. The same algorithm is used for estimation of 
the parameters of the model given a set of example images. In section 5, results of 
isolated hand-written digit recognition experiments are presented. The results indicate 
that even in the simple case of isolated characters, the elimination of planar distortions 
enhances the performance signilicamfly. Section 6 contains the summary of this work. 
2 Hidden Markov Model 
The HMM is a st__a_fi.qtical model that is used to describe temporal signals 
G= {g(t): l<t<T,g  G oR'} in speech processing applications (Rabiner, 1989; Lee 
et al., 1990; Wilpon et al., 1990; Pieraccini and Levin, 1991). The HMM is a composite 
statistical source comprising a set s= { 1, � ,TR} of TR sources called states. The i-th 
state, i  s, is characterized by its probability distribution Pi(g) over G. At each time t 
only one of the states is active, emitting the observable g(t). We denote by s(t), s(t)  s 
the random variable corresponding to the active state at time t. The joint probability 
distribution (for real-valued g) or discrete probability mass (for g being a discrete 
variable) P(s(t),g(t)) for t > 1 is characterized by the following property: 
P(s(t),g(t)ls(l:t-1),g(l:t-1))=P(s(t)] s(t-1))P(g(t)]s(t))--- (1) 
=P(s(t) ] s(t-1)) P,(t)(g(t) ), 
where s(l:t-1) stands for the sequence {s(1), ..- s(t-1) }, and 
g(l:t-1)={g(1) ..... g(t-1)}. We denote by a 0 the transition probability 
P(s(t)=jls(t-1)=i), and by r, the probability of state i being active at t=l, 
i=P(s(1)=i). The probability of the entire sequence of states S--s(I:T) and 
observations G =g( I :T) can be expressed as 
T 
P (G,S) = (1)P(1)( g (1)) rI a(t-os(t) P(t)(g(t) ). (2) 
t--2 
The interpretation of equations (1) and (2) is that the observable sequence G is generated 
in two stages: first, a sequence S of T states is chosen according to the Markovian 
dis,'ibution parametrized by {ao} and {hi}; then each one of the states s (t), l<r, in S 
generates an observable g(t) according to its own memoryless distribution P,(t), forming 
the observable sequence G. This model is called a hidden Markov model, because the 
state sequence S is not given, and only the observation sequence G is known. A 
particular case of this model, called a left-to-right HMM, where aij =0 for j <i, and 
Planar Hidden Markov Modeling: from Speech to Optical Character Recognition 733 
l = 1, is especially useful for speech recognition. In this case each state of the model 
represents an unspecified acoustic unit, and due to the eft-to-right structure, the whole 
word is modeled as a concatenation of such acoustic mits. The time spent in each of the 
states is not fixed, and therefore the model can take into account the duration variability 
between different utterances of the same word. 
The segm^entafion problem of HMM is that of estimating the most probable state 
sequence S, given the observation G, 
}=argmaxP(S l G)=argmaxP(G,S). (3) 
$ $ 
The problem of finding } through exhaustive search is of exponential complexity, since 
there exist T r possible state sequences, but it can be solved in polynomial time using a 
dynamic programming approach (i.e. Viterbi algorithm). The segmentation plays a 
central role in all HMM-based speech recognizers, since for connected speech it gives the 
segmentation into words or sub-word units, and performs a recognition simultaneously, in 
an optimal way. This is in contrast to sequential systems, in which the connected speech 
is first segmented into words/subwords according to some rules, and than the individual 
segments are recognized by computing the appropriate likelihoods, and where many 
recognition errors are mused by unrecoverable segmentation errors. Higher-level 
syntactical knowledge can be integrated into decoding process through transition 
probabilities between the models. The segmentation is also used for estimating the 
HMMs parameters using a corpus of a training dam. 
3 The Two-Dimensional Case: Planar HMM 
In this section we describe a statistical model for planar image 
G = {g(x,y):(x,y) 6 Lx, r , g  G}. We call this model Planar HMM (PHMM) and 
design it to extend the advantages of conventional HMM to the two-dimensional case. 
The PHMM is a composite source, comprising a set s= {(,), I<.<_i<_X R, I<<YR} of 
N=XeYt� states. Each state in s is a stochastic source characterized by its probability 
density P;,9(g) over the space of observations g  G. It is convenient to think of the 
states of the model as being located on a rectangular lattice where each state corresponds 
to a pixel of the corresponding reference image. Similarly to the conventional HMM, 
only one state is active in the generation of the (x,y)-th image pixel g (x,y). We denote 
by s(x,y)e s the active state of the model that generates g (x,y). The joint distribution 
governing the choice of active states and image values has the following MarkovJan 
property: 
P(g(x,y), s(x,y) I g(l:X, l:y-1), g(l:x-l,y), s(l:X, l:y-1),s(l:x-l,y))= (4) 
=P(g(x,y) ] s(x,y)) P(s(x,y) l s(x-l,y),s(x,y-1))= 
=P3(x,y)(g(x,Y)) P(s(x,y) [ s(x-l,y),s(x,y-1))= 
where g(l:X,y-1) = {g(x,y):(x,y)  Rx,y- }, g(l:x-l,y) -= {g (1,y), � � � ,g(x-l,y)}, and 
s(l:X,l:y-1), s(l:x-l,y) are the active states involved in generating g(l'JC, y-1), 
g(l:x-l,y), respectively, and Rx,y_ is an axis parallel rectangle between the origin and 
the point (X,y-1). Similarly to the one-dimensional case, it is useful to define a left-to- 
right bottom-up PHMM where P(s(x,y)=(m,n) I s(x-l,y)=(i,j),s(x,y-1)=(k,l)):t:O only 
when i_Jn and l_n, that does not allow for fold overs in the state image. The 
Markovjan property (4) allows the left-to-right bottom-up PHMM to model elastic 
distortions among different realization
