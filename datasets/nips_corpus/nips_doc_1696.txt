Statistical Dynamics of Batch Learning 
S. Li and K. Y. Michael Wong 
Department of Physics, Hong Kong University of Science and Technology 
Clear Water Bay, Kowloon, Hong Kong 
phlisong, phkywong) @ust. hk 
Abstract 
An important issue in neural computing concerns the description of 
learning dynamics with macroscopic dynamical variables. Recen- 
t progress on on-line learning only addresses the often unrealistic 
case of an infinite training set. We introduce a new framework to 
model batch learning of restricted sets of examples, widely applica- 
ble to any learning cost function, and fully taking into account the 
temporal correlations introduced by the recycling of the examples. 
For illustration we analyze the effects of weight decay and early 
stopping during the learning of teacher-generated examples. 
I Introduction 
The dynamics of learning in neural computing is a complex multi-variate process. 
The interest on the macroscopic level is thus to describe the process with macro- 
scopic dynamical variables. Recently, much progress has been made on modeling 
the dynamics of on-line learning, in which an independent example is generated for 
each learning step [1, 2]. Since statistical correlations among the examples can be 
ignored, the dynamics can be simply described by instantaneous dynamical vari- 
ables. 
However, most studies on on-line learning focus on the ideal case in which the net- 
work has access to an almost infinite training set, whereas in many applications, 
the collection of training examples may be costly. A restricted set of examples 
introduces extra temporal correlations during learning, and the dynamics is much 
more complicated. Early studies briefly considered the dynamics of Adaline learn- 
ing [3, 4, 5], and has recently been extended to linear percepttons learning nonlinear 
rules [6, 7]. Recent attempts, using the dynamical replica theory, have been made 
to study the learning of restricted sets of examples, but so far exact results are pub- 
lished for simple learning rules such as Hebbian learning, beyond which appropriate 
approximations are needed [8]. 
In this paper, we introduce a new framework to model batch learning of restricted 
sets of examples, widely applicable to any learning rule which minimizes an arbi- 
trary cost function by gradient descent. It fully takes into account the temporal 
correlations during learning, and is therefore exact for large networks. 
Statistical Dynamics of Batch Learning 287 
2 Formulation 
Consider the single layer perceptron with N >> I input nodes { } connecting to a 
single output node by the weights {Jj}. For convenience we assume that the inputs 
j are Gaussian variables with mean 0 and variance 1, and the output state $ is a 
function f(x) of the activation x at the output node, i.e. 
S=f(x); x=f-'. (1) 
The network is assigned to learn p -_- aN examples which map inputs {?} to the 
outputs {S} (p = 1,... ,p). S are the outputs generated by a teacher perceptron 
{Bj }, namely 
S = f(y,); y =/. '. (2) 
Batch learning by gradient descent is achieved by adjusting the weights {Jj} itera- 
tively so that a certain cost function in terms of the student and teacher activations 
{x) and {y) is minimized. Hence we consider a general cost function 
E- -Eg(x,y). (3) 
The precise functional form of g(x, y) depends on the adopted learning algorithm. 
For the case of binary outputs, f(x) -- sgnx. Early studies on the learning dynamics 
considered Adaline learning [3, 4, 5], where g(x,y) -- -(S- x)2/2 with S - sgny. 
For recent studies on Hebbian learning [8], g(x, y) -- xS. 
To ensure that the perceptton is regularized after learning, it is customary to intro- 
duce a weight decay term. Furthermore, to avoid the system being trapped in local 
minima, noise is often added in the dynamics. Hence the gradient descent dynamics 
is given by 
dJj(t) 1 
dt =  Eg'(x(t),y) - AJj(t) + ]j(t), (4) 
where, here and below, g'(x,y) and g(x,y) respectively represent the first and 
second partial derivatives of g(x, y) with respect to x. A is the weight decay strength, 
and ]j (t) is the noise term at temperature T with 
2T 
and = 5kS(t- s). 
(5) 
3 The Cavity Method 
Our theory is the dynamical version of the cavity method [9, 10, 11]. It uses a 
self-consistency argument to consider what happens when a new example is added 
to a training set. The central quantity in this method is the cavity activation, which 
is the activation of a new example for a perceptron trained without that example. 
Since the original network has no information about the new example, the cavity 
activation is stochastic. Specifically, denoting the new example by the label 0, its 
cavity activation at time t is 
ho(t) = J(t). (6) 
For large N and independently generated examples, ho(t) is a Gaussian variable. 
Its covariance is given by the correlation function C(t, s) of the weights at times t 
and s, that is, 
/ho(t)o(s)) = f(t). f(s) -- C(t,s), 
288 S. Li and K. Y.. M. Wong 
where ? and  are assumed to be independent for j  k. The distribution is 
further specified by the teacher-student correlation R(t), given by 
(ho(t)yo} - f(t) . t -- R(t). 
(8) 
Now suppose the perceptron incorporates the new example at the batch-mode learn- 
ing step at time s. Then the activation of this new example at a subsequent time 
t > s will no longer be a random variable. Furthermore, the activations of the 
original p examples at time t will also be adjusted from (x(t)) to (x�,(t)) because 
of the newcomer, which will in turn affect the evolution of the activation of example 
0, giving rise to the so-called Onsager reaction effects. This makes the dynamics 
complex, but fortunately for large p  N, we can assume that the adjustment from 
o (t) is small, and perturbative analysis can be applied. 
x(t) to x, 
Suppose the weights of the original and new perceptron at time t are {Jj(t)} and 
{J(t)} respectively. Then a perturbation of (4) yields 
d 
I , o 
q- A (Jj(t)- Jj(t)) - g (xo(t),yo) 
1 
- (k(t) - A(t)) 
+ N Eg(x(t)'  J . 
k 
(9) 
The first term on the right hand side describes the primary effects of adding example 
0 to the training set, and is the driving term for the difference between the two 
perceptrons. The second term describes the secondary effects due to the changes 
to the original examples caused by the added example, and is referred to as the 
Onsager reaction term. One should note the difference between the cavity and 
generic activations of the added example. The former is denoted by ho(t) and 
corresponds to the activation in the perceptron {Jj (t)}, whereas the latter, denoted 
by xo (t) and corresponding to the activation in the perceptron {Jj (t)}, is the one 
used in calculating the gradient in the driving term of (9). Since their notations 
are sufficiently distinct, we have omitted the superscript 0 in xo(t), which appears 
�(t) 
in the background examples x . 
The equation can be solved by the Green's function technique, yielding 
f (1, O) 
Jj(t)- Sj(t)- E dsGjk(t,s) go(s)k , 
k 
(10) 
where g)(s) = g'(xo (s), yo) and Gin (t, s) is the weight Green's function satisfying 
1 / 
= - - t s), 
Gjn(t,s) G(�)(t s)6 +   dt'G(�)(t '  
(11) 
G (�) (t - s) -- O(t - s)exp(-A(t - s)) is the bare Green's function, and 0 is the 
step function. The weight Green's function describes how the effects of example 0 
propagates from weight Jn at learning time s to weight Jj at a subsequent time t, 
including both primary and secondary effects. Hence all the temporal correlations 
have been taken into account. 
For large N, the equation can be solved by a diagrammatic approach similar to [5]. 
The weight Green's function is self-averaging over the distribution of examples and 
is diagonal, i.e. limN-+oo Gj(t, s) = G(t, s)5j, where 
G(t,s): G'�'(t- s)q-or / dt I / dt2'�'(t- tl)(g(tl)D(tl,t2))(t2,s ). (12) 
Statistical Dynamics of Batch Learning 289 
D(t, s) is the example Green's function given by 
Vu(t,s) = 6(t- s) + f dt'G(t,t')g(t')Du(t',s). (13) 
This allows us to express the generic activations of the examples in terms of their 
cavity counterparts. Multiplying both sides of (10) and summing over j, we get 
xo(t) - no(t) = / dsG(t,s)g(s). (14) 
This equation is interpreted as follows. At time t, the generic activation xo(t) 
deviates from its cavity counterpart because its gradient term g(s) was present 
in the batch learning step at previous times s. This gradient term propagates its 
influence from time s to t via the Green's function G(t, s). Statistically, this equation 
enables us to express the activation distribution in terms of the cavity activation 
distribution, thereby getting a macroscopic description of the dynamics. 
To solve for the Green's functions and the activation distributions, we further need 
the fluctuation-response relation derived by linear response theory, 
C(t,s) a/dt'G(�)(t t' ''tx f 
= - );g. ) .s)) + 2T dt'G(�)(t - t')G(s,t'). (15) 
Finally, the teacher-student correlation is given by 
R(t) a f dt'G(�)(t t') ' t' 
= - (g,()y,). (16) 
4 A Solvable Case 
The cavity method can be applied to the dynamics of learning with an arbitrary cost 
function. When it is applied to the Hebb rule, it yields results identical to the exact 
results in [8]. Here we present the results for the Adaline rule to illustrate features 
of learning dynamics derivable from the study. This is a common learning rule and 
bears resemblance with the more common back-propagation rule. Theoretically, its 
dynamics is particularly convenient for analysis since g(x) = -1, rendering the 
weight Green's function time translation invariant, i.e. G(t, s) = G(t - s). In this 
case, the dynamics can be solved by Laplace transform. 
To monitor the progress of learning, we are interested in three performance mea- 
sures: (a) Training error et, which is the probability of error for the training ex- 
amples. It is given by et = (O(-xsgn
