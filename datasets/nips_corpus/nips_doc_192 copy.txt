474 Mel and Koch 
Sigma-Pi Learning: 
On Radial Basis Functions and 
Associative Learning 
Cortical 
Bartlett W. Mel Christof Koch 
Computation and Neural Systems Program 
Caltech, 216-76 
Pasadena, CA 91125 
ABSTRACT 
The goal in this work has been to identify the neuronal elements 
of the cortical column that are most likely to support the learning 
of nonlinear associative maps. We show that a particular style of 
network learning algorithm based on locally-tuned receptive fields 
maps naturally onto cortical hardware, and gives coherence to a 
variety of features of cortical anatomy, physiology, and biophysics 
whose relations to learning remain poorly understood. 
1 INTRODUCTION 
Synaptic modification is widely believed to be the brain's primary mechanism for 
long-term information storage. The enormous practical and theoretical importance 
of biological synaptic plasticity has stimulated interest among both experimental 
neuroscientists and neural network modelers, and has provided strong incentive for 
the development of computational models that can both explain and predict. 
We present here a model for the synaptic basis of associative learning in cerebral 
cortex. The main hypothesis of this work is that the principal output neurons 
of a cortical association area learn functions of their inputs as locally-generalizing 
lookup tables. As abstractions, locally-generalizing learning methods have a long 
history in statistics and approximation theory (see Atkeson, 1989; Barron & Barron, 
Sigma-Pi Learning 475 
$ 
x$ 
Figure 1: A Neural Lookup Table. A nonlinear function of several variables may 
be decomposed as a weighted sum over a set of localized receptive fields units. 
1988). Radial Basis Function (RBF) methods are essentially similar (see Broomhead 
&; Lowe, 1988) and have recently been discussed by Poggio and Girosi (1989) in 
relation to regularization theory. As is standard for network learning problems, 
locally-generalizing methods involve the learning of a map f(_x) ï¿½ _x -+ y from 
example (_x, y) pairs. Rather than operate directly on the input space, however, 
input vectors are first decoded by a population of receptive field units with 
centers i that each represents a local, often radially-symmetric, region in the input 
space. Thus, an output unit computes its activation level y -- ]i wig(x--i) ' where 
g defines a radial basis function, commonly a Gaussian, and wi is its weight (Fig. 
1). The learning problem can then be characterized as one of finding weights _w 
that minimize the mean squared error over the N element training set. Learning 
schemes of this type lend themselves directly to very simple Hebb-type rules for 
synaptic modification since the initially nonlinear learning problem is transformed 
into a linear one in the unknown parameters _w (see Broomhead &; Lowe, 1988). 
Locally-generalizing learning algorithms as neurobiological models date at least to 
Albus (1971) and Marr (1969, 1970); they have also been explored more recently by 
a number of workers with a more pure computational bent (Broomhead &; Lowe, 
1988; Lapedes &; Farher, 1988; Mel, 1988, 1989; Miller, 1988; Moody, 1989; Poggio 
&; Girosi, 1989). 
476 Mel and Koch 
2 SIGMA-PI LEARNING 
Unlike the classic thresholded linear unit that is the mainstay of many current 
connectionist models, the output of a sigma-pi unit is computed as a sum of contri- 
butions from a set of independent multiplicative clusters of input weights (adapted 
from Rumelhart g McClelland, 1986): y -- a(j wjcj), where cj -- rli vixi is the 
product of weighted inputs to cluster j, w 1 is the weight on cluster j as a whole, 
and a is an optional thresholding nonlinearity applied to the sum of total clus- 
ter activity. During learning, the output may also by clamped by an unconditioned 
teacher input, i.e. such that y -- ti(_x). Units of this general type were first proposed 
by Feldman g Ballard (1982), and have been used occasionally by other connec- 
tionist modelers, most commonly to allow certain inputs to gate others or to allow 
the activation of one unit to control the strength of interconnection between two 
other units (Rumelhart & McClelland, 1986). The use of sigma-pi units as function 
lookup tables was suggested by Feldman & Ballard (1982), who cited a possible 
relevance to local dendritic interactions among synaptic inputs (see also Durbin g 
Rumelhart, 1989). 
In the present work, the specific nonlinear interaction among inputs to a sigma-pi 
cluster is not of primary theoretical importance. The crucial property of a cluster 
is that its output should be AND-like, i.e. selective for the simultaneous activity 
of all of its k input lines  . 
2.1 NETWORK ARCHITECTURE 
We assume an underlying d-dimensional input space ,'  /d over which functions 
are to be learned. Vectors in , are represented by a population X of N units 
whose state is denoted by _x  R N. Within X, each of the d dimensions of ,' is 
individually value-coded, i.e. consists of a set of units with gaussian receptive fields 
distributed in overlapping fashion along the range of allowable parameter values, 
for example, the angle of a joint, or the orientation of a visual stimulus at a specific 
retinal location. (A more biologically realistic case would allow for individual units 
in X to have multi-dimensional gaussian receptive fields, for example a 4-d visual 
receptive field encoding retinal x and y, edge orientation, and binocular disparity.) 
We assume a map t(_x): _x -+ y is to be learned, where the components ofy G /M are 
represented by an output population Y of M units. According to the familiar single- 
layer feedforward network learning paradigm, X projects to Y via an associational 
pathway with modifiable synapses. We consider the task of a single output unit yi 
(hereafter denoted by y), whose job is to estimate the underlying teacher function 
ti(_X): _X - y from examples. Output unit y is assumed to have access to the entire 
input vector _x, and a single unconditioned teacher input ti. We further assume that 
1 A local threshold function can act as an AND in place of a multiplication, and for purposes of 
biological modeling, is a more likely dendritic mechanism than pure multiplication. In continuing 
work, we are exploring the more detailed interactions between Hebb-type learning rules and various 
post-synaptic nonlinearities, specifically the NMDA channel, that could underlie a multiplication 
relation among nearby inputs. 
Sigma-Pi Learning 477 
all possible clusters cj of size 1 through k = kmo pre-exist in y's dendritic field, 
with cluster weights wj initially set to 0, and input weights vi within each cluster set 
equal to 1. Following from our assumption that each of the input lines ai represents 
a 1-dimensional gaussian receptive field in A', a multiplicative cluster of k such 
inputs can yield a k-dimensional receptive field in , that may then be weighted. 
In this way, a sigma-pi unit can directly implement an RBF decomposition over ,. 
Additionally, since a sigma-pi unit is essentially a massively parallel lookup table 
with clusters as stored table entries, it is significant that the sigma-pi function is 
inherently modular, such that groups of sigma-pi units that receive the same teacher 
signal can, by simply adding their outputs, act as a single much larger virtual sigma- 
pi unit with correspondingly increased table capacity 2. A neural architecture that 
allows system storage capacity to be multiplied by a factor of k by growing k neurons 
in the place of one, is one that should be strongly preferred by biological evolution. 
2.2 THE LEARNING RULE 
The cluster weights wj are modified during training according to the following self- 
normalizing Hebb rule: 
bj -- Ot Cjp tp -- [3Wj, 
where c and/3 are small positive constants, and cjp and tp are, respectively, the jth 
cluster response and teacher signal in state p. The steady state of this learning rule 
occurs when wj -  < cjt >, which tries to maximize the correlation s of cluster out- 
put and teacher signal over the training set, while minimizing total synaptic weight 
for all clusters. The inputs weights vi are unmodified during learning, representing 
the degree of cluster membership for each input line. 
We briefly note that because this Hebb-type learning rule is truly local, i.e. depends 
only upon activity levels available directly at a synapse to be modified, it may be 
applied transparently to a group of neurons driven by the same global teacher input 
(see above discussion of sigma-p/modularity). Error-correcting rules that modify 
synapses based on a difference between desired vs. actual neural output do not 
share this property. 
3 TOWARD A BIOLOGICAL MODEL 
In the remainder of this paper we examine the hypothesis that sigma-pi units un- 
derlie associative learning in cerebral cortex. To do so, we identify the six essential 
elements of the sigma-pi learning scheme and discuss the evidence for each: i) a pop- 
ulation of output neurons, ii) a focal teacher input, iii), a diffuse association input, 
iv) Hebb-type synaptic plasticity, v) local dendritic multiplication (or thresholding), 
and vi) a cluster reservoir. 
Following Eccles (1985), we concern ourselves here with the cytoarchitecture of 
generic association cortex, rather than with the more specialized (and more often 
studied) primary sensory and motor areas. We propose the cortical circuit of fig. 
2 This assumes the global thresholding nonlinearity a is weak, i.e. has an extended linear range. 
3Strictly speaking, the average product. 
478 Mel and Koch 
Fibers 
H,m 
IV 
V, VI 
Association Association 
Inputs Specific Inputs 
Afferent 
Figure 2: Elements of the cortical column in a generic association cortex. 
2 to contain all of the basic elements necessary for associative learning, closely 
paralleling the accounts of Marr (1970) and Eccles (1985) at this 
