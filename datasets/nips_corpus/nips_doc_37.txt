693 
Teaching Artificial Neural Systems to Drive: 
Manual Training Techniques for Autonomous Systems 
J. F. Shepanskl and S. A. Macy 
TRW, Inc. 
One Space Park, O2/1779 
Redondo Beach, CA 90278 
ABSTRACT
We have developed a methodology for manually training autonomous control systems 
based on artificial neural systems (ANS). In applications where the rule set governing an expert's 
decisions is difficult to formulate, ANS can be used to extract rules by associating the information 
an expert receives with the actions he. takes. Properly constructed networks imitate rules of 
behavior that permits them to function autonomously when they are trained on the spanning set 
of possible situations. This training can be provided manually, either under the direct supervision 
of a system trainer, or indirectly using a background mode where the network assimilates training 
data as the expert performs his day-to-day tasks. To demonstrate these methods we have trained 
an ANS network to drive a vehicle through simulated freeway traffic. 
Introduction 
Computational systems employing fine grained parallelism are revolutionizing the way we 
approach a number of long standing problems involving pattern recognition and cognitive process- 
ing. The field spans a wide variety of computational networks, from constructs emulating neural 
functions, to more crystalline configurations that resemble systolic arrays. Several titles are used 
to describe this broad area of research, we use the term artificial neural systems (ANS). Our con- 
cern in this work is the use of ANS for manually training certain types of autonomous systems 
where the desired rules of behavior are difficult to formulate. 
Artificial neural systems consist of a number of processing elements interconnected in a 
weighted, user-specified fashion, the interconnection weights acting as memory for the system. 
Each processing element calculate an output value based on the weighted sum of its inputs. In 
addition, the input data is correlated with the output or desired output (specified by an instructive 
agent) in a training rule that is used to adjust the interconnection weights. In this way the net- 
work learns patterns or imitates rules of behavior and decision making. 
The particular ANS architecture we use is a variation of Rummelhart et. al. [1] multi-layer 
perceptton employing the generalized delta rule (GDR). Instead of a single, multi-layer rruc- 
ture, our final network has a a multiple component or block configuration where one block' 
output feeds into another (see Figure 3). The training methodology we have developed is nor 
tied to a particular training rule or architecture and should work well with alternative networks 
like Grossberg's adaptive resonance model[2]. 
� American Institute of Physics 1988 
694 
The equations describing the network are derived and described in detail by Rumelhart et. 
al.[1]. In summary, they are: 
Transfer function: o i - (l+e-Sj) -, S i =  wj,.ol; (1) 
i0 
Weight adaptation rule: Aw. =(1-ot.)rl.Sio i + cr.Au,r'vi�u; (2) 
Error calculation: i ---�i(1- %')  tS,wi, (3) 
where o i is the output of processing element j or a sensor input, % is the interconnection weight 
leading from element i to j, n is the number of inputs to j, Aw is the adjustment of w, q is the 
training constant, a is he training momentum, $i is the calculated error for element j, and m 
is the fanout of a given element. Element zero is a constant input, equal to one, so hat Wio is 
equivalent to the bias threshold of element j. The (1-a) factor in equation (2) differs from stau- 
dard GDR formulation, but. it is useful for keeping track of the relative magnitudes of the two 
terms. For he network's output layer the summation in equation (3) is replaced with the 
difference between he desired and actual output value of element j. 
These networks are usually trained by presenting the system with sets of input/output data 
vectors in cyclic fashion, the entire cycle of database presentation repeated dozens of times. This 
method is effective when he training agent is a computer operating in batch mode, but would be 
intolerable for a human instructor. There are two developments that will help real-time human 
training. The first is a more efficient incorporation of data/response patterns into a network. The 
second, which we are addressing in this paper, is a suitable environment wherein a man and ANS 
network can iteract in training situation with minimum inconvenience or boredom on the 
human's part. The ability to systematically train networks in this fashion is extremely useful for 
developing certain types of expert systems including automatic signal processors, autopilots, 
robots and other autonomous machines. We report a number of techniques aimed at facilitating 
this type of training, and we propose a general method for teaching these networks. 
System Development 
Our work focuses on the utility of ANS for system control. It began as an application of 
Barto and Sutton's associative search network[3]. Although their approach was useful in a 
number of ways, it fell short when we tried to use it for capturing the subtleties of human 
decision-making. In response we shifted our emphasis from constructing goal functions for 
automatic learning, to methods for training networks using direct human instruction. An integral 
part of this is the development of suitable interfaces between humans, networks and he outside 
world or simulator. In this section we will report various approaches to hese ends, and describe a 
general methodology for manually teaching ANS networks. To demonstrate these techniques we 
taught a network to drive a robot vehicle down a simulated highway in traffic. This application 
combines binary decision making and control of continuous parameters. 
Initially we investigated he use of automatic learning based on goal functions[3] for train- 
ing control systems. We trained a network-controlled vehicle to maintain acceptable following 
distances from cats ahead of it. On a graphics workstation, a one lane circular track was 
695 
constructed and occupied by two vehicles: a network-controlled robot car and a pace car that 
varied its speed at random.. Input data to the network consisted of the separation distance and 
the sleed of the robot vehicle. The values of a goal function were translated into desired output 
for GDR training. Output controls consisted of three binary decision elements: 1) accelerate one 
increment of speed, 2) maintain speed, and 3) decelerate one increment of speed. At all times 
the desired output vector had exactly one of these three elements active. The goal function was 
quadratic witIx a minimum corresponding to the optimal following distance. Although it had no 
direct control over the simulation, the goal function positively or negatively reinforced the 
system's behav ior. 
The network was given complete control of the robot vehicle, and the human trainer had 
no influence except the ability to start and terminate training. This proved unsatisfactory because 
the initial system behavior--governed by random interconnection weights--was very unstable. The 
robot tended to run over the car in front of it before significant training occurred. By carefully 
halting and restarting training we achieved stable system behavior. At first the following distance 
maintained by the robot car oscillated as if the vehicle was attached by a spring to the pace car. 
This activity gradually damped. After about one thousand training steps the vehicle maintained 
the optimal following distance and responded quickly to changes in the pace car's speed. 
Constructing composite goal functions to promote more sophisticated abilities proved 
difficult, even ill-defined, because there were many unspecified parameters. To generate goal 
functions for these abilities would be similar to conventional programming--the type of labor we 
want to circumvent using ANS. On the other hand, humans are adept at assessing complex situa- 
tions and making decisions based on qualitative data, but their goal functions are difficult if not 
impossible to capture analytically. One attraction of ANS is that it can imitate behavior based on 
these elusive rules without formally specifying them. At this point we turned our efforts to 
manual training techniques. 
The initially trained network was grafted into a larger system aad augmented with addi- 
tional inputs: distance and speed information on nearby pace cars in a secod traffic lane, and an 
output control signal governing lane changes. The original network's ability to maintain a safe 
following distance wa retained intact. Th grafting procedure is one of two methods we studied 
for adding new abilities to an existins system. (The second, which employs a block structure, is 
described below.) The network remained in direct control of the robot vehicle, but a human 
trainer instructed it when and when not to change lanes. His commands were interpreted as the 
desired output and used in the GDR training algorithm. This technique, which we call coaching, 
proved useful and the network quickly correlated its environmental inputs with the teacher's 
instructions. The network became adept at changing lanes and weaving through traffic. We found 
that the network took on the behavior pattern of its trainer. A conservative teacher produced a 
timid network, while an aggressive trainer produced a network that tended to cut off other auto- 
mobiles and squeeze through tight openings. Despite its success, the coaching method of training 
did not solve the problem of initial network instability. 
The stability problem was solved by giving the trainer direct control over the simulation. 
The system configuration (Figure 1), allows the expert to exert control or release it to the net- 
work. During initial training the expert is in the driver's seat while the 
