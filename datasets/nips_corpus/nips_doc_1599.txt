Agglomerative Information Bottleneck 
Noam $1onim Naftali Tishby* 
Institute of Computer Science and 
Center for Neural Computation 
The Hebrew University 
Jerusalem, 91904 Israel 
emaih {nomm,tishby}cs .huj i. ac. il 
Abstract 
We introduce a novel distributional clustering algorithm that max- 
imizes the mutual information per cluster between data and giv- 
en categories. This algorithm can be considered as a bottom up 
hard version of the recently introduced Information Bottleneck 
Method. The algorithm is compared with the top-down soft ver- 
sion of the information bottleneck method and a relationship be- 
tween the hard and soft results is established. We demonstrate the 
algorithm on the �0 Newsgroups data set. For a subset of two news- 
groups we achieve compression by 3 orders of magnitudes loosing 
only 10% of the original mutual information. 
1 Introduction 
The problem of self-organization of the members of a set X based on the similarity 
of the conditional distributions of the members of another set, Y, {p(y]x)}, was first 
introduced in [8] and was termed distributional clustering. 
This question was recently shown in [9] to be a special case of a much more fun- 
damental problem: What are the features of the variable X that are relevant for 
the prediction of another, relevance, variable Y ? This general problem was shown 
to have a natural information theoretic formulation: Find a compressed represen- 
tation of the variable X, denoted , such that the mutual information between  
and Y, I(;Y), is as high as possible, under a constraint on the mutual infor- 
mation between X and , I(X; ). Surprisingly, this variational problem yields 
an exact self-consistent equations for the conditional distributions p(y]), p(x]), 
and p(). This constrained information optimization problem was called in [9] The 
Information Bottleneck Method. 
The original approach to the solution of the resulting equations, used already in [8], 
was based on an analogy with the deterministic annealing approach to clustering 
(see [7]). This is a top-down hierarchical algorithm that starts from a single cluster 
and undergoes a cascade of cluster splits which are determined stochastically (as 
phase transitions) into a soft (fuzzy) tree of clusters. 
In this paper we propose an alternative approach to the information bottleneck 
618 N. Slonim and N. Tishby 
problem, based on a greedy bottom-up merging. It has several advantages over the 
top-down method. It is fully deterministic, yielding (initially) hard clusters, for 
any desired number of clusters. It gives higher mutual information per-cluster than 
the deterministic annealing algorithm and it can be considered as the hard (zero 
temperature) limit of deterministic annealing, for any prescribed number of clusters. 
Furthermore, using the bottleneck self-consistent equations one can soften the 
resulting hard clusters and recover the deterministic annealing solutions without 
the need to identify the cluster splits, which is rather tricky. The main disadvantage 
of this method is computational, since it starts from the limit of a cluster per each 
member of the set X. 
1.1 The information bottleneck method 
The mutual information between the random variables X and Y is the symmetric 
functional of their joint distribution, 
I(X;Y)= E p(x,y)log( p(x'Y) )= 
xex,e� k,p(x)p(y) 
f P(Yl x) 
E P(x)p(ylx)log k, p-- ) 
xX,yY 
(1) 
The objective of the information bottleneck method is to extract a compact rep- 
resentation of the variable X, denoted here by 5:, with minimal loss of mutual 
information to another, relevance, variable Y. More specifically, we want to find a 
(possibly stochastic) map, p(lx), that minimizes the (lossy) coding length of X via 
5:, I(X; 5:), under a constraint on the mutual information to the relevance variable 
1(5:; �). In other words, we want to find an efficient representation of the variable 
X, 5:, such that the predictions of Y from X through  will be as close as possible 
to the direct prediction of Y from X. 
As shown in [9], by introducing a positive Lagrange multiplier  to enforce the 
mutual information constraint, the problem amounts to minimization of the La~ 
grangian: 
C[p�lx)] = t(X; 5:) - t(5:; Y) , (2) 
with respect to p(lx), subject to the Markov condition 5: -> X -> Y and normal- 
ization. 
This minimization yields directly the following self-consistent equations for the map 
p(5:lx), as well as for p(ylS:) and p(5:): 
{ p(lx) = P() exp(-D::[p(ylx)l]p(yl)]) 
z(,x) 
p(yl) = p(Ylx)P(lx) 
P() -- E P(lx)P(x) 
(3) 
The variational principle, Eq. (2), determines also the shape of the annealing process, 
since by changing  the mutual informations Ix - I(X; 5:) and I� _-- I(Y; 5:) vary 
such that 
6I� _ _ 
5Ix (4) 
where Z(,x) is a normalization function. The functional DcL[Pllq] -= 
y.yp(y) log qP(--) is the Kulback-Liebler divergence [3], which emerges here from the 
variational principle. These equations can be solved by iterations that are proved to 
converge for any finite value of  (see [9]). The Lagrange multiplier ] has the nat- 
ural interpretation of inverse temperature, which suggests deterministic annealing 
[7] to explore the hierarchy of solutions in 5:, an approach taken already in [8]. 
Agglomerative Information Bottleneck 619 
Thus the optimal curve, which is analogous to the rate distortion function in in- 
formation theory [3], follows a strictly concave curve in the (Ix, I�) plane, called 
the information plane. Deterministic annealing, at fixed number of clusters, follows 
such a concave curve as well, but this curve is suboptimal beyond a certain critical 
value of . 
Another interpretation of the bottleneck principle comes from the relation between 
the mutual information and Bayes classification error. This error is bounded above 
and below (see [6]) by an important information theoretic measure of the class con- 
ditional distributions p(xlYi), called the Jensen-Shannon divergence. This measure 
plays an important role in our context. 
The Jensen-Shannon divergence of M class distributions, pi(x), each with a prior 
ri, 1 _< i _< M, is defined as, [6, 4]. 
M M 
JSv,p2, ...,PM] ------ H[y riPi(X)] - y riHvi(x)] , (5) 
i=1 i=1 
where Hip(x)] is Shannon's entropy, Hip(x)] = -Exp(x)logp(x). The convexi- 
ty of the entropy and Jensen inequality guarantees the non-negativity of the JS- 
divergence. 
1.2 The hard clustering limit 
For any finite cardinality of the representation ll -- m the limit  -> o of the 
Eqs.(3) induces a hard partition of X into m disjoint subsets. In this limit each 
member x 6 X belongs only to the subset  6 . for which p(ylS:) has the smallest 
DKLV(ylx)llp(yl)] and the probabilistic map p(l x) obtains the limit values 0 and 
i only. 
In this paper we focus on a bottom up agglomerative algorithm for generating 
good hard partitions of X. We denote an m-partition of X, i.e.  with cardinality 
m, also by Zm = {Zl,Z2, ..., Zm}, in which case p() = p(zi). We say that Zm is an 
optimal m-partition (not necessarily unique) of X if for every other m-partition of 
X, Ztm, I(Zm; Y) _> I(Zm; Y). Starting from the trivial N-partition, with N = IX], 
we seek a sequence of merges into coarser and coarser partitions that are as close 
as possible to optimal. 
It is easy to verify that in the  - o limit Eqs.(3) for the m-partition distributions 
are simplified as follows. Let  -- z = {x,x2,...,xlzl} , xi  X denote a specific 
component (i.e. cluster) of the partition Zm, then 
1 if x  z Vx  X 
p(z[x) = { 0 otherwise 
P(Y] ) = p--(Ty Yi= p(xi, y) Yy e Y 
p(z) ---- Yi= p(xi) 
(6) 
Using these distributions one can easily evaluate the mutual information between 
Zm and Y, I(Zm; Y), and between Zm and X, I(Zm; X), using Eq.(1). 
Once any hard partition, or hard clustering, is obtained one can apply reverse 
annealing and soften the clusters by decreasing  in the self-consistent equations, 
Eqs.(3). Using this procedure we in fact recover the stochastic map, p(lx), from 
the hard partition without the need to identify the cluster splits. We demonstrate 
this reverse deterministic annealing procedure in the last section. 
620 N. Sionin and N. Tishby 
1.3 Relation to other work 
A similar agglomerative procedure, without the information theoretic framework 
and analysis, was recently used in [1] for text categorization on the 20 newsgroup 
corpus. Another approach that stems from the distributional clustering algorith- 
m was given in [5] for clustering dyadic data. An earlier application of mutual 
information for semantic clustering of words was used in [2]. 
2 The agglomerative information bottleneck algorithm 
The algorithm starts with the trivial partition into N = ]X I clusters or components, 
with each component contains exactly one element of X. At each step we merge 
several components of the current partition into a single new component in a way 
that locally minimizes the loss of mutual information I(.; Y) = I(Zm; Y). 
Let Z, be the current m-partition of X and Zm denote the new -partition 
of X after the merge of several components of Zm. Obviously,  < m. Let 
{zi,z2,...,zk} C_ Zm denote the set of components to be merged, and 2k  Zm 
the new component that is generated by the merge, so h = m - k + 1. 
To evaluate the reduction in the mutual information I(Zm; Y) due to this merge one 
needs the distributions that define the new h-partition, which are determined as 
follows. For every z e Zm,z  , its probability distributions (p(z),p(y[z),p(zlx)) 
remains equal to its distributions in Zm. For the new component, 2  Zm, we 
define, 
= 5-.i= p(zi) 
() -i= p(zi,y) Vy e Y (7) 
1 ifxziforsomel<i<k VxX 
p(2lx)- 0 otherwise 
It is easy to verify that Zm is indeed a valid h-partition with proper probability 
distributions. 
Using the same notations, for every merge we define the additional quantities
