Single Transistor Learning Synapses 
Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead 
California Institute of Technology 
Pasadena, CA 91125 
(SlS) 95- 2S12 
paul@hobiecat.pcmp.caltech.edu 
Abstract 
We describe single-transistor silicon synapses that compute, learn, 
and provide non-volatile memory retention. The single transistor 
synapses simultaneously perform long term weight storage, com- 
pute the product of the input and the weight value, and update the 
weight value according to a Hebbian or a backpropagation learning 
rule. Memory is accomplished via charge storage on polysilicon 
floating gates, providing long-term retention xvithout refresh. The 
synapses efficiently use the physics of silicon to perform weight up- 
dates; the weight value is increased using tunneling and the weight 
value decreases using hot electron injection. The small size and 
low power operation of single transistor synapses allows the devel- 
opment of dense synaptic arrays. We describe the design, fabri- 
cation, characterization, and modeling of an array of single tran- 
sistor synapses. When the steady state source current is used as 
the representation of the weight value, both the incrementing and 
decrementing functions are proportional to a power of the source 
current. The synaptic array was fabricated in the standard 2pro 
double - poly, analog process available from MOSIS. 
I INTRODUCTION 
The past few years have produced a number of efforts to design VLSI chips which 
learn from experience. The first step toward this goal is developing a silicon 
analog for a synapse. We have successfully developed such a synapse using only 
818 Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead 
Gate Floating Gate 
Drain  Source ilh oltage . 
 ::::::::::::::::::::: ::::::::::::::::::::::::::: ::::::::::::::::::::::::::::::::::::::::::::::::::: 
Psubstrate ''' '%%'2i::!''J. -''ii!i;Jii'i!}i::'ii:::'.' ':'::':: ''' :' ' ' ':' ' ' ':::'' 
Figure 1: Cross section of the single transistor synapse. Our single transistor 
synapse uses a separate tunneling voltage terminal The pbase implant results in 
a larger threshold voltage, which results in all the electrons reaching the top of the 
$i02 barrier to be swept into the floating gate. 
a single transistor. A synapse has two functional requirements. First, it must 
compute the product of the input multiplied by the strength (the weight) of the 
synapse. Second, the synapse must compute the weight update rule. For a l:Iebbian 
synapse, the change in the weight is the time average of the product of the input and 
output activity. In many supervised algorithms like backpropagation, this weight 
change is the time average of the product of the input and some fed back error 
signal. Both of these computations are similar in function. We have developed 
single transistor synapses which simultaneously perform long term weight storage, 
compute the product of the input and the weight value, and update the weight 
value according to a l:Iebbian or a backpropagation learning rule. The combination 
of functions has not previously been achieved with floating gate synapses. 
There are five requirements for a learning synapse. First, the weight should be 
stored permanently in the absence of learning. Second, the synapse must compute 
as an output the product of the input signal with the synaptic weight. Third, each 
synapse should require minimal area, resulting in the maximum array size for a given 
area. Fourth, each synapse should operate with low power dissipation so that the 
synaptic array is not power constrained. And finally, the array should be capable 
of implementing either l:Iebbian or Backpropagation learning rule for modifying 
the weight on the floating gate. We have designed, fabricated, characterized, and 
modeled an array of single transistor synapses which satisfy these five criteria. We 
believe this is the first instance of a single transistor learning synapse fabricated in 
a standard process. 
2 OVERVIEW 
Figure I shows the cross section for the single transistor synapse. Since the float- 
ing gate is surrounded by Si02, an excellent insulator, charge leakage is negligible 
resulting in nearly permanent storage of the weight value. An advantage of using 
floating gate devices for learning rules is the timescales required to add and remove 
charge from the floating gate are well matched to the learning rates of visual and 
auditory signals. In addition, these learning rates can be electronically controlled. 
Typical resolution of charge on the floating gate after ten years is four bits (l:Ioller 
89). The FETs are in a moderately doped (1 x 107cm -a) substrate, to achieve a 
Single Transistor Learning Synapses 819 
Vgl/ v 
I 
I (1,1) I (1,2) 
Vtunl  
Vdl 
Vtun2 
Vd2 
Vs2 - 
Figure 2: Circuit diagram of the single - transistor synapse array. Each transis- 
tor has a floating gate capacitively coupled to an input column line. A tunneling 
connection (arrow) allows weight increase. Weight decreased is achieved by hot 
electron injection in the transistor itself. Each synapse is capable of simultaneous 
feedforward computations and weight updates. A 2 x 2 section of the array allows 
us to characterize how modifying a single floating gate (such as synapse (1,1)) ef- 
fects the neighboring floating gate values. The synapse currents are a measure of 
the synaptic weights, and are summed along each row by the source (V) or drain 
(Va) lines into some soma circuit. 
high threshold voltage. The moderately doped substrate is formed in the 2/tin MO- 
SIS process by the pbase implant. npn transistor. The implant has the additional 
benefit of increasing the efficiency of the hot electron injection process by increasing 
the electric field in the channel. Each synapse has an additional tunneling junction 
for modifying the charge on the floating gate. The tunneling junction is formed 
with high quality gate oxide separating a well region from the floating gate. 
Each synapse in our synaptic array is a single transistor with its weight stored as 
a charge on a floating silicon gate. Figure 2 shows the circuit diagram of a 2 x 
2 array of synapses. The column 'gate' inputs (Vg) are connected to second level 
polysilicon which capacitively couples to the floating gate. The inputs are shared 
along a column. The source (V), drain (I/a), and tunneling (Vtun) terminals are 
shared along a row. These terminals are involved with computing the output current 
and feeding back 'error' signal voltages. Many other synapses use floating gates to 
store the weight value, as in (l:Ioller 89), but none of the earlier approaches update 
the charge on the floating gate during the multiplication of the input and floating 
gate value. In these previous approaches one must drive the floating gate over large 
a voltage range to tunnel electrons onto the floating gate. Synaptic computation 
must stop for this type of weight update. 
The synapse computes as an output current a product of weight and input signal, 
820 Paul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead 
10-? 
10'* 
10-9 
C3 o-o 
10-n 
10q 
Injeion Operalions Tunneling Operalions 
synapse (1,2) 
synapse (2,1), synapse (2,2) .-' 
 se (1,1) 
tow 2 backgrouad curtera 
ww I background currein 
10-1  
o so 
Step Iteration Number 
350 
Figure 3: Output currents from a 2 x 2 section of the synapse array, showing 
180 injection operations followed by 160 tunneling operations. For the injection 
operations, the drain (Vdl) is pulsed from 2.0 V upto 3.3 V for 0.5s with Vg at 8V 
and Vy2 at 0V. For the tunneling operations, the tunneling line (Vtunl) is pulsed 
from 20 V up to 33.5 V with Vy2 at 0V and Vy at 8V. Because our measurements 
from the 2 x 2 section come from a larger array, we also display the 'background' 
current from all other synapses on the row. This background current is several orders 
of magnitude smaller than the selected synapse current, and therefore negligible. 
and can simultaneously increment or decrement the weight as a function of its input 
and rror voltages. The particular learning algorithm depends on the circuitry at 
the boundaries of the array; in particular the circuitry connected to each of the 
source, drain, and tunneling lines in a row. With charge Q.ry on the floating gate 
and V equal to 0 the subthreshold source current is described by 
Isynapse --Ioe 'o e 
(1) 
The single transistor learning synapses use a combination of electron tunneling and 
hot electron injection to adapt the charge on the floating gate, and thereby the 
where Qo is a device dependent parameter, and U- is the thermal voltage 
The coupling coefficient, 6, of the gate input to the transistor surface potential is 
typically less than 0.1. 17rom (1) We can consider the weight as a current I, defined 
by 
e T '-- T 
I..ap.e-- Ioe'Z-o e mr Ie (2) 
where Vyo is the input voltage bias, and AVy is Vy - Vyo. The synaptic current is 
thus the product of the weight, I, and a weak exponential function of the input 
voltage. 
Single Transistor Learning Synapses 821 
weight of the synapse. Hot electron injection adds electrons to the floating gate, 
thereby decreasing the weight. Injection occurs for large drain voltages; therefore 
the floating gate charge can be reduced during normal feedforward operation by rais- 
ing the drain voltage. Electron tunneling removes electrons from the floating gate, 
thereby increasing the weight. The tunneling line controls the tunneling current; 
thus the floating gate charge can be increased during normal feedforward operation 
by raising the tunneling line voltage. The tunneling rate is modulated by both the 
input voltage and the charge on the floating gate. 
Figure 3 shows an example the nature of the weight update process. The source 
current is used as a measure of the synapse weight. The experiment starts with all 
four synapses set to the
