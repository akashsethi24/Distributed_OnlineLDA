On the Use of Projection Pursuit Constraints for 
Training Neural Networks 
Nathan Intrator* 
Computer Science Department 
Tel-Aviv University 
Ramat-Aviv, 69978 ISRAEL 
and 
Institute for Brain and Neural Systems, 
Browu University 
ninmah. au. ac. il 
Abstract 
XVe present a novel classification and regression method tha. t coin- 
bines exploratory projection pursuit (unSUl)ervised training) with pro- 
jection pursuit regression (supervised training), to yield a new family of 
cost/complexity penalty terms. Some improved generalization properties 
are demonstrated on real world problems. 
1 Introduction 
Parameter estimation becomes difficult in high-dimensional spaces due t.o the in- 
creasing sparseness of the data. Therefore, when a low dimeusional representation 
is embedded in the data, dinensionality reduction methods become useful. One 
such method - projection pursuit regression (Friedman and St. uetzle, 1981) (PPR) 
is capable of performing dimensionality reduction by composition, namely, it con- 
structs an approximation to the desired response function using a coinposition of 
lower dimensional smooth functions. These functions depend ou low dimensional 
projections through the data. 
*Research was supported by the National Science Foundation, the Artny Research Of- 
rice, and the Office of Naval Research. 
3 
4 Intrator 
When the dimensionality of the problem is in the thousands, even projection pur- 
suit methods are almost always over-parametrized, therefore, additional smoothing 
is needed for low variance estimation. Exploratory Projection Pursuit (Friedman 
and Tukey, 1974; Friedman, 1987) (EPP) may be useful for that. It searches in a 
high dimensional space for structure in the form of (semi) linear projections with 
constraints characterized by a projection index. The projection index may be con- 
sidered as a universal prior for a large class of problems, or may be tailored to a 
specific problem based on prior knowledge. 
In this paper, the general form of exploratory projection pursuit is formulated to be 
an additional constraint for projection pursuit regression. In particular, a hybrid 
combination of supervised and unsupervised artificial neural network (ANN) is de- 
scribed as a special case. In addition, a specific projection index that is particularly 
useful for classification (Intrator, 1990; Intrator and Cooper, 1992) is introduced in 
this context. A more detailed discussion appears in Intrator (1993). 
2 Brief Description of Projection Pursuit Reï¿½ression 
Let (X, Y) be a. pair of random variables, X  R a, and Y  t?. The problem is to 
approximate the d dimeusional surface 
h'om n observations ( x  , y ), . . . , ( x, , y,, ). 
PPR tries to approximate a function f by a suni of ridge functions (,functions that 
are constant along lines) 
The fitting procedure alt, ernates between an estimation of a direction fi and an 
estimation of a smooth hmction g, uch that at iteration j, t, he sqnare average of 
the residuals 
is minimized. This process is init. ialized by setting rio = yi. Usually, the initial 
values of aj are taken to be the first few principal components of the data. 
Estimation of the ridge t'unctions can be achieved by various nonparametric smooth- 
ing techniques such a.s locally linear functions (Friedman and Stuetzle, 1981), 
k-nea. rest neighbors (Hall, 1989b), splines or variable degree polynomials. The 
smoothness constraint imposed on g, iml)lies that the actual projection pursuit 
is achieved by minimizing at. it. eratiou j, l.le suni 
+ 
for some smoothness measure C. 
Although PPR converges to the desired response function (Jones, 1987), the use 
of non-pa. rametric function estimation is likely to lead to overfitting. Recent re- 
sults (Hornik, 1991)suggest that a feed forward network architecture with a single 
On the Use of Projection Pursuit Constraints for Training Neural Networks 
hidden layer and a rather general fixed activation function is a universal approxi- 
mator. Therefore, the use of a non-parametric single ridge function estimation can 
be avoided. It is thus appropriate to concentrate on the estimation of good pro- 
jections. In the next section we present. a general h'amework of PPR architecture, 
and in section 4 we restrict it. to a feed-forward architecture xvith sigmoidal hidden 
units. 
3 
Estimating The Projections Using Exploratory 
Projection Pursuit 
Exploratory projection pursuit.is based on seeking interesting projections of high 
dimensional data points (Kruskal, 1969; Switzer, 1970; Kruskal, 1972; Friedman 
and Tukey, 1974; Friedman, 1987; Jones and Sibson, 1987; Hall, 1988; Huber, 1985, 
for review). The notion of interesting projections is motivated by an observation 
that for most high-dimensional data clouds, most low-dimensional projections are 
approximately normal (Diaconis and Froedmau, 1984). This finding suggests that 
the important information in the data is conveyed in those directions whose single 
dimensional projected distribution is far from Gaussiat. Various projection indices 
(measures for the goodness of a. projection) differ ou the assumptions abont the 
nature of deviation fi'om nornmlity, and in their computational efficiency. They can 
be considered as different priors motivated by specific assuniptions on the underlying 
model. 
To partially decouple the search for a projection vector from the search for a non- 
parametric ridge function, we propose to add a penalty term, which is based on 
a projection index, to the energy minimization associated with the estimation of 
the ridge functions and the projections. Specifically, let p(a) be a. projection index 
which is minimized for projections with a certain devia. tion from normality; At the 
j'th iteration, we minimize the sum 
p(a. ). 
i 
When a concurrent minimization over several projections/fuuctions is practical, we 
get a penalty term of the forin 
B(f) = 
J 
Since C and p may not be linear, the more general measure that does not assume a 
stepwise approach, but instead seeks l projections and ridge functions concurrently, 
is given by 
B(f) = C'(g .... ,gt) + p(a,...,at), 
In practice, p depends implicitly on t. he training data, (the empirical density) and 
is therefore replaced by its empirical measure 
3.1 Some Possible Measures 
Some applicable projection indices are discussed in (Huber, 1985; Jones and Sib- 
son, 1987; Friedman, 1987; Hall, 1989a; Intrator, 1990). Probably, all the possible 
6 Intrator 
measures should emphasize some form of deviation from normality bnt the spe- 
cific type may depend on the problem at hand. For example, a measure based 
on the I,;arhunen Leave expansiou (Mougeot et al., 1991) may be useful for image 
compression with autoassociative networks, since in this case one is interested in 
minimizing the L 2 norm of the distance between the reconstructed image and the 
original one, and under mild conditious, the Karhunen Lo/ve expansion gives the 
optimal solution. 
A different type of prior knowledge is required for classification problems. The 
underlying assumption then is that the data is clustered (when projecting in the 
right directions) and tha.[ the classification may be achieved by some (nonlinear) 
mapping of these clusters. Iu such a case, the projection index should emphasize 
multi-modality as a specific deviation from normality. A projection index that em- 
phasizes multimodalities in tile projected distribution (without relytug on the class 
labels) has recently been introduced (Intrator, 1990) and implemented efficiently us- 
ing a variant of a biologically motivated unsupervised uetwork (Intrator and Cooper, 
1992). Its integration into a back-propagation classifier will be discussed below. 
3.2 Adding EPP constraints to back-propagation network 
One xvay of adding sense prior knowledge into tile architecture is by minimizing 
the effective number of parameters using weight sharing, in which a single weight 
is shared among many connections in the network (Waibel et al., 1989; Le Cun 
et al., 1989). An extension of this idea is the soft. weight sharing which favors 
irregularities in the weight distribution in the form of multimodality (Nowlan and 
Hinton, 1992). This penalty improved generalization results obtained by weight 
elimination penalty. Both these methods make an explicit assumption about the 
structure of the weight space, but xvit. h no regard to the structure of the input space. 
As described in the coutext of projection pursuit regression, a penalty term may 
be added to the energy functioual lninimized by error back propagation, for the 
purpose of measuring directly the goodness of the projections songht by the network. 
Since our main interest is in reducing overfitting for high dinensional problems, our 
underlying assumption is that the stirface fuHction to be estimated can be faithfully 
represented using a lmv dimensiom.l composition of sigmoidal functions, namely, 
using a back-propagation network in which the number of hidden units is much 
smaller than the number of input units. Therefore, the penalty term may be added 
only to the hidden layer. The synal)tic modification equations of the hidden units' 
weights become 
Ow 0 _ OS ( u,, .r ) 
at - a,7,;- 
+(Contribution of cost/complexity tem]s)]. 
An approach of this type has been used in image compression, with a penalty 
aimed at minimizing th(' entropy of the projected distribution (Bichsel and Seitz, 
1989). This penalty certainly measures deviation from normality, since entropy is 
maximized for a Gaussian distribut, ion. 
On the Use of Projection Pursuit Constraints for Training Neural Networks 7 
4 
Projection Index for Classification: The Unsupervised 
BCM Neuron 
Intrator (1990) has recently shown that a variant of the Bienenstock, Cooper and 
Munro neuron (Bienenstock et al., 1982) performs exploratory projection pursuit 
using a projection index that measures nmlti-modality. This neuron 
