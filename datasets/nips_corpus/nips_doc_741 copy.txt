Optimal Stopping and Effective Machine 
Complexity in Learning 
Changfeng Wang 
Department of Systems Sci. and Eng. 
University of Pennsylvania 
Philadelphia, PA, U.S.A. 19104 
Santosh S. Venkatesh 
Department of Electrical Engineering 
University of Pennsylvania 
Philadelphia, PA, U.S.A. 19104 
J. Stephen Judd 
Siemens Corporate Research 
755 College Rd. East, 
Princeton, N J, U.S.A. 08540 
Abstract 
We study the problexn of when to stop learning a class of feedforward networks 
- networks with linear outputs neuron and fixed input weights - when they are 
trained with a gradient descent a.lgorithm on a finite number of examples. Under 
general regularity conditions, it is shown that there are in general three distinct 
phases in the generalization performance in the learning process, and in particular, 
the network has better generalization performance when learning is stopped at a 
certain time before the global minimum of the empirical error is reached. A notion 
of effective size of a machine is defined and used to explain the trade-off between 
the complexity of the machine and the training error in the learning process. 
The study leads naturally to a network size selection criterion, which turns out to 
be a generalization of Akaike's Infornmtion Criterion for the learning process. It is 
shown tha. t stopping learning before the global minimum of the empirical error has 
the effect of network size selection. 
1 INTRODUCTION 
The primary goal of learning in neural nets is to find a network that gives valid generalization. In 
achieving this goal, a central issue is the trade-off between the training error and network cmnplexity. 
This usually reduces to a problem of network size selection, which has drawn much research effort in 
recent years. Various principles, theories, and intuitions, including Occam's razor, statistical model 
selection criteria such as Akaike's Information Criterion (AIC) [1] and many others [5, 1, 10, 3, 11] all 
quantitatively support the following PAC prescription: between two machines which have the same 
mnpirical error, the machine with smaller VC-dimension generalizes better. However, it is noted 
that these methods or criteria do not necessarily lead to optimal (or nearly optimal) generalization 
performance. Furthermore, all of these methods are valid only at the global minimum of the enpirical 
error function (e.g, the likelihood timetlon for AIC), and it is not clear by these methods how the 
generalization error is effected by network complexity or, more generally, how a network generalizes 
during the learning process. This paper addresses these issues. 
303 
304 Wang, Venkatesh, and Judd 
Recently, it has often been observed that when a network is 'traiued by a gradient descent 
algorithm, there exists a critical region in the training epochs where the trained network generalizes 
best, and after that region the generalization error will increase (frequently called over-training). Our 
numerical experiments with gradient-type algorithms in training feedforward networks also indicate 
that in this critical region, as long as the network is large enough to learn the examples, the size 
of the network plays little role in the (best) generalization performance of the network. Does this 
mean we must revise Occam's principle? How should one define the complexity of a network and go 
about tuning it to optinize generalization performance? When should one stop learning? Although 
releva.nt learning processes were treated by numerous authors [2, 6, 7, 4], the formal theoretical 
studies of these problexns are abeyant. 
Under rather general regularity conditions (Section 1), we give in Section 2 a theorem which 
relates the generalization error at each epoch of learning to that at the global minimum of the 
training error. Its consequence is that for any linear machine whose VC-dimension is finite but large 
enough to learn the target concept, the number of iterations needed for the best generalization to 
occur is at the order of the logarithm of the sainple size, rather than at the global minimum of 
the training error; it also provides bounds on the improvement expected. Section 3 deals with the 
relation between the size of the machine and generalization error by appealing to the concept of 
effective size. Section 4 concerns the application of these results to the problem of network size 
selection, where the AIC is generalized to cover the time evolution of the learning process. Finally, 
we conclude the paper with comments on practical implementation and further research in this 
direction. 
2 THE LEARNING MACHINE 
The machine we consider accepts input vectors X from an arbitrary input space and produces scalar 
outputs 
d 
Here, a* = (a*,..., a'a)' is a fixed vector of real weights, for each i, b,(X) is a fixed real hmction 
of the inputs, with b(X) = (�(X),..., �(X))' the corresponding vector of fuuctions, and  is a 
random noise term. The machine (1) can be thought of as a feedforward neural network with a fixed 
front end and variable weights at the output. In particular, the functions Pi can represent fixed 
polynomials (higher-order or sigma-pi neural networks), radial basis functions with fixed centers, a 
fixed hidden-layer of sigmoidal neurons, or simply a linear map. In this context, N.J. Nilsson [8] 
has called similar structures -machines. 
We consider the problem of learning from examples a relationship between a random variable Y 
and an n-dimensional random vector X. We assume that this function is given by (1) for some fixed 
integer d, the random vector X and random variable  are defined on the same probability space, 
that E [lX] -- 0, and a2(X) = Var(lX ) = constant < c almost surely. The smallest eigenvalue of 
the matrix b(x)b(x) is assumed to be bounded from below by the inverse of some square integrable 
finction. 
Note that it can be shown that the VC-dinension of the class of -machines with d neurons 
is d under the last assumption. The learning-theoretic properties of the system will be determined 
largely by the eigen structure of . Accordingly, let A _> A2 _> '- � _> A denote the eigenvalues of . 
The goal of the learning is to find the true concept a given independently drawn examples (X, y) 
from (1). Given any hypothesis (vector) w = (w,...,w)' for consideration as an approximation 
to the true concept a, the performance measure we use is the mean-square prediction (or ensemble) 
error 
= E (.v - (2) 
Note that the true concept a* is the mean-square solution 
r* = arg nin �(w) = -'E (l,(X)y), (3) 
Optimal Stopping and Effective Machine Complexity in Learning 305 
and the minionurn prediction error is given by �(c) = minw �(w) = a s. 
Let n be the number of samples of (X, ). We assulne that an independent, identically 
distributed sample (X(),y()), ..., (X(n),y('O), generated according to the joint distribution 
of (X,Y) induced by (1), is provided to the learner. To sinplify notation, define the matrix 
62 _= [b(X ()) ... b(X('))] and the corresponding vector of outputs y - (y(),...,y())'. In 
analogy with (2) define the empirical error on the sample by 
Let & denote the hypothesis vector for ;vhich the empirical error on the sanple is minimized: 
V(&) = 0. Analogously with (3) we can then show that 
5=(kI)-y=-t (y), 
(4) 
where  = o' is the empirical covariance matrix, which is almost surely nonsingular for large n. 
The terms in (4) are the empirical counterparts of the ensemble averages in (3). 
The gradient descent algorithm is given by: 
(5) 
where a = (a,a2,...,a3)', t is the number of iterations, and e is the rate of learning. From this 
we can get 
c, = (I - A(t))& + A(t)c0, (6) 
where A(t) = (I - eta) t, and (0 is the initial weight vector. 
The limit of a is A when t goes to infinity, provided p is positive definite and the learning rate 
 is small enough (i.e., smaller than the smallest eigenvalue of $). This implies that the gradient 
descent algorithm converges to the least squares solution, starting from any point in 7 '. 
3 GENERALIZATION DYNAMICS AND STOPPING TIME 
3.1 MAIN THEOREM OF GENERALIZATION DYNAMICS 
Even if the true concept (i.e., the precise relation between Y and X in the current problem) is in the 
class of models we consider, it is usually hopeless to find it using only a finite number of examples, 
except in some trivial cases. Our goal is hence less ambitious; we seek to find the best approximation 
of the true concept, the approach entailing a minimization of the training or empirical error, and 
then taking the global minimum of the empirical error & as the approximation. As we have seen the 
procedure is unbiased and consistent. Does this then imply that training should always be carried 
out to the limit? Surprisingly, the answer is no. This assertion follows from the next theorem. 
Theorem 3.1 Let M > 0 be an arbitrary real constant (possibly depending on n), and suppose 
assumptions AI to A3 are satisfied; then the generalization dynamics in the training process are 
governed by the followin.q equation: 
71,  n 
uniformly for all initial weight vectors, ao in the d-dimensional ball {(* + 5: 11611 _< M, 6 � 
and for all t > O. [] 
306 Wang, Venkatesh, and Judd 
3.2 THREE PHASES IN GENERALIZATION 
By Theorem 3.1, the mean generalization error at each epoch of the training process is characterized 
by the following function: 
d 
- -;- - . 
The analysis of the evoution of generalization with training is facilitated by treating q(.) as a 
function of a continuous time parame[er t. Ve will show that, there are three distinct phases iu 
generalization dynanics. These results are given in the following in form by several corollaries of 
Theerein 3.1. 
Without loss of generality, we assume the initial weight vector is picked up in a region with 
hq/[-x'd)t then for all 0 < t < t  
II*l 5 M,, = 0(,,�), and in particular, I1 = 0(.). Lot
