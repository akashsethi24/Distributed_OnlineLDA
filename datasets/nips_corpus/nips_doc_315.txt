Learning Trajectory and Force Control 
of an Artificial Muscle Arm 
by Parallel-hierarchical Neural Network Model 
Masazumi Katayama Mitsuo Kawato 
Cognitive Processes Department 
ATR Auditory and Visual Perception Research Laboratories 
Seika-cho, Soraku-gun, Kyoto 619-02, JAPAN 
Abstract 
We propose a new parallel-hierarchical neural network model to enable motor 
learning for simultaneous control of both trajectory and force, by integrating 
Hogan's control method and our previous neural network control model using a 
feedback-error-learning scheme. Furthermore, two hierarchical control laws 
which apply to the model, are derived by using the Moore-Penrose pseudo- 
inverse matrix. One is related to the minimum muscle-tension-change trajectory 
and the other is related to the minimum motor-command-change trajectory. The 
human arm is redundant at the dynamics level since joint torque is generated by 
ag0nist and antagonist muscles. Therefore, acquisition of the inverse model is 
an ill-posed problem. However, the combination of these control laws and 
feedback-error-learning resolve the ill-posed problem. Finally, the efficiency of 
the parallel-hierarchical neural network model is shown by learning experiments 
using an artificial muscle arm and computer simulations. 
1 INTRODUCTION 
For humans to properly interact with the environment using their arms, both arm posture 
and exerted force must be skillfully controlled. The hierarchical neural network model 
which we previously proposed was successfully applied to trajectory control of an 
industrial manipulator (Kawato et al., 1987). However, this model could not directly be 
applied to force control, because the manipulator mechanism was essentially different 
from the musculo-skeletal system of a human arm. Hogan proposed a biologically 
motivated control method which specifies both the virtual trajectory and the mechanical 
impedance of a musculo-skeletal system (Hogan, 1984, 1985). One of its advantages is 
that both trajectory and force can be simultaneously controlled. However, this control 
method does not explain motor learning. 
436 
Learning Trajectory and Force Control of an Artificial Muscle Arm 437 
In this paper, by integrating these two previous studies, we propose a new Parallel- 
Hierarchical Neural network Model (PHNM) using a./kedback-error-learning scheme we 
previously proposed (Kawato et al., 1987), as shown in Fig. l. PHNM explains the 
biological motor learning for simultaneous control of both trajectory and force. Arm 
movement depends on the static and dynamic properties of a musculo-skeletal system. 
From this viewpoint, its inverse model which computes a motor command from a desired 
trajectory and force, consists of two parallel inverse models: the Inverse Statics Model 
(ISM) and the Inverse Dynamics Model (ISM) (see Fig. l). 
The human arm is redundant at the dynamics level since joint torque is generated by 
agonist and antagonist muscles. Therefore, acquisition of the inverse model is an ill- 
posed problem in the sense that the muscle tensions can not be uniquely determined from 
the prescribed trajectory and force. The central nervous system can resolve the ill-posed 
problem by applying suitable constraints. Based on behavioral data of human multi-joint 
arm movement, Uno et al. (1989) found that the trajectory was generated on the criterion 
that the time integral of the squared sum of the rate of change of muscle tension is 
minimized. From this point of view, we assume that the central nervous system controls 
the arm by using two hierarchical objective functions. One objective function is related 
to the minimum muscle-tension-change trajectory. The other objective function is related 
to the minimum motor-command-change trajectory. From this viewpoint, we propose 
two hierarchical control laws which apply to the feedback controller shown in Fig. l. 
These control laws are calculated with the Moore-Penrose pseudo-inverse matrix of the 
Jacobian matrix from muscle tensions or motor commands to joint torque. The 
combination of these control laws and the feedback-error-learning resolve the ill-posed 
problem. As a result, the inverse model related to hierarchical objective functions can be 
acquired by PHNM. We ascertained the efficiency of PHNM by performing experiments 
in learning control using an artificial-muscle arm with agohist and antagonist muscle-like 
rubber actuators as shown in Fig.2 (Katayama et al., 1990). 
2 PARALLEL-HIERARCHICAL NEURAL NETWORK MODEL 
In a simple case, the dynamics equation of a human multi-joint arm is described as 
follows: 
R(0) + B(0, ) = r + G(0), 1, 1 a ) 
t =af(O)Tf(Mf,O,)-ae(O)Te(Me,O,) ' ( 1 b ) 
Here, R(O) is the inertia matrix, B(O, O) expresses a matrix of centrifugal, coriolis and 
friction forces and G(O) is the vector of joint torque due to gravity. Mj and M are agohist 
and antagonist motor commands, Tj and T e are agohist and antagonist muscle tensions, 0 
is the joint-angle,  is joint torque generated from the tensions of a pair of muscles and 
at(O ) and adO) are moment arms. 
If the arm is static ( = t = 0), (la) and (lb) are reduced to the following: 
0 =aj(O)Tj(Mf,O,O)-a(O)Te(M,O,O)+G(O) ' (2) 
Therefore, (2) is a statics equation. The problem, which calculates the motor commands 
from joint angles based on (2), is called the inverse statics. There are two difficulties: 
first, (2) including nonlinear functions (af, ae, Tj, T and G), must be solved. Second, 
the inverse statics is an ill-posed problem as mentioned above. These difficulties are 
resolved by'the ISM. The problem of computing dynamic torque other than (2) is called 
438 Katayama and Kawato 
Od Fd  '. Mism 
if 0=0',. --- 
J If ,-; , 
. : +_. Desired Trajectory- Minor Command 
and Fore  
Or Fr 
Realized Trajectory 
and Force 
Figure 1: Parallel-Hierarchical Neural Network Model 
the inverse dynamics and it is resolved by the IDM. The main role of the ISM is to 
control the equilibrium posture and mechanical stiffness (Hogan, 1984), and that of the 
IDM is to compensate for dynamic properties of the arm in fast movements. PHNM, in 
addition to a feedback controller, hierarchically arranges these parallel inverse models. 
The motor command is the sum of three outputs (Mism, Midm and Mfc ) calculated by the 
ISM, the IDM and the feedback controller, respectively, as shown in Fig. 1. The outputs 
from the ISM and IDM are calculated by feedforward neural networks with synaptic 
weights w from desired trajectory 0d and desired force Fd. These neural network models 
can be described as the mapping from inputs 0,/and F,/ to motor commands. In order to 
acquire the parallel inverse model, synaptic weights change according to the following 
feedback-error-learning algorithm. 
.. = Mfc 
dt (3) 
The ISM learns when the arm is static and the IDM learns when it is moving. The 
feedback motor command Mfc is fed only to the ISM when  = 0 and only to the IDM 
when 0 � 0 as an error signal for synaptic modification. The arm is mainly controlled by 
the feedback controller before learning, and the feedforward control is basically performed 
only by the parallel inverse model after learning because the output Mj. of the feedback 
controller is minimized after learning. Two control laws which apply to the feedback 
controller, are derived below. 
3 HIERARCHICAL CONTROL MECHANISM 
In order to acquire the parallel inverse models related to hierarchical objective functions, 
we propose two control laws reducing the redundancy at the dynamics level, which apply 
to a feedback controller in the PHNM. 
3.1 MATHEMATICAL MUSCLE MODEL 
Tensions (Ti, Te) of agonist and antagonist muscles are generally modeled as follows: 
Ti = K(MI ){0o,f(M f )- 0}- B(Mj ), ( 4a ) 
Te = K(M){O- Oo,e(Me) } + B(Me) ' (4 b ) 
Learning Trajectory and Force Control of an Artificial Muscle Arm 439 
Here, M consists of Mf and Me for agonist and antagonist muscles, respectively. The 
mechanical impedance of a human arm can be manipulated by the stiffness K(M) and 
viscosity B(M) of the muscle itself, depending on their motor commands. O(�(Mf) and 
Oo,e(Me) are joint angles at equilibrium position. K(M), B(M), Ood(Mf) and O0,e(Me) are 
approximately given as K ( M ) --_ k o + kM , B( M ) _= b o + bM , 0o, f ( M f )  00 + cM f and 
O0,e(Me)=-0o-�Me, respectively. k and b are coefficients which, respectively, 
determine elasticity and viscosity. ko and bo are intrinsic elasticity and viscosity, 
respectively. 00 is the intrinsic equilibrium angle and c is a constant. Small changes in 
joint torque are expressed by using the Jacobian matrix A from small changes in motor 
command to small changes in joint torque. Therefore, by using the Moore-Penrose 
pseudo-inverse matrix A #, small changes in motor command are calculated as follows: 
= A#A' = )2 1 
e af(O)2(f +gf +ae(O)2(f -ge) 2 ae(O)(f --ge)) 
� ..c =-(k0 + oh), 
A # =Ar(AAr) -', 
g f = koc + kO o + 2kcM f 
ge = ko c + kOo + 2kcMe 
(5) 
3.2 HIERARCHICAL CONTROL LAWS 
Two feedback control laws are explained below, which apply to the feedback controller 
shown in Fig. 1. Firstly, ATf=AMf and ATe=AMe are given from (4a) and (4b) by 
assuming k=b=0, c;'O, aj(O)=ae(O)=a and gf=g=l in the simplest case. The solution 
A#zI ' in which the norm (ZlTj2+ZlTe2) /2 of vector AT is minimized by using the 
pseudo-inverse matrix A #, is selected. Therefore, the control law related to the minimum 
muscle-tension-change trajectory is derived from (5). Then the feedback control law is 
acquired by using AT= Kp(O d --Or)-[' Ka(b a -- br)-[' Kf(F d - Fr). Here, Kt,, Ka and Kf 
are feedback gains. Learning is performed by applying the motor commands calculated by 
this feedback control law to the learning algorithm of (3). As a result, the inverse model 
is acquired by the PHNM after learning. Only when aj(O)=ae(O)=a does, the inverse model 
strictly give the optimal solution based on the minimum mus
