Factorial Learning by Clustering Features 
Joshua B. Tenenbaum and Emanuel V. Todorov 
Department of Brain and Cognitive Sciences 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
{jbt, emo}psyche. mir. edu 
Abstract 
We introduce a novel algorithm for �actorial learning, motivated 
by segmentation problems in computational vision, in which the 
underlying factors correspond to clusters of highly correlated input 
features. The algorithm derives from a new kind of competitive 
clustering model, in which the cluster generators compete to ex- 
plain each feature of the data set and cooperate to explain each 
input example, rather than competing for examples and cooper- 
ating on features, as in traditional clustering algorithms. A natu- 
ral extension of the algorithm recovers hierarchical models of data 
generated from multiple unknown categories, each with a differ- 
ent, multiple causal structure. Several simulations demonstrate 
the power of this approach. 
I INTRODUCTION 
Unsupervised learning is the search for structure in data. Most unsupervised learn- 
ing systems can be viewed as trying to invert a particular generative model of the 
data in order to recover the underlying causal structure of their world. Differ- 
ent learning algorithms are then primarily distinguished by the different generative 
models they embody, that is, the different kinds of structure they look for. 
Factorial learning, the subject of this paper, tries to find a set of independent causes 
that cooperate to produce the input examples. We focus on strong �actorial learning, 
where the goal is to recover the actual degrees of freedom responsible for generating 
the observed data, as opposed to the more general weak approach, where the goal 
562 Joshua B. Tenenbaum, Emmanuel V. Todorov 
Figure 1: A simple factorial learning problem. The learner observes an articulated 
hand in various configurations, with each example specified by the positions of 16 
tracked features (shown as black dots). The learner might recover four underlying 
factors, corresponding to the positions of the fingers, each of which claims respon- 
siblity for four features of the data set. 
is merely to recover some factorial model that explains the data efficiently. Strong 
factorial learning makes a claim about the nature of the world, while weak facto- 
rial learning only makes a claim about the nature of the learner's representations 
(although the two are clearly related). Standard subspace algorithms, such as prin- 
cipal component analysis, fit a linear, factorial model to the input data, but can 
only recover the true causal structure in very limited situations, such as when the 
data are generated by a linear combination of independent factors with significantly 
different variances (as in signal-from-noise separation). 
Recent work in factorial learning suggests that the general problem of recovering the 
true, multiple causal structure of an arbitrary, real-world data set is very difficult, 
and that specific approaches must be tailored to specific, but hopefully common, 
classes of problems (Foldiak, 1990; Saund, 1995; Dayan and Zemel, 1995). Our 
own interest in multiple cause learning was motivated by segmentation problems in 
computational vision, in which the underlying factors correspond ideally to disjoint 
clusters of highly correlated input features. Examples include the segmentation 
of articulated objects into functionally independent parts, or the segmentation of 
multiple-object motion sequences into tracks of individual objects. These problems, 
as well as many other problems of pattern recognition and analysis, share a common 
set of constraints which makes factorial learning both appropriate and tractable. 
Specifically, while each observed example depends on some combination of several 
factors, any one input feature always depends on only one such factor (see Figure 
1). Then the generative model decomposes into independent sets of functionally 
grouped input features, or functional parts (Tenenbaum, 1994). 
In this paper, we propose a learning algorithm that extracts these functional parts. 
The key simplifying assumption, which we call the membership constraint, states 
that each feature belongs to at most one functional part, and that this membership 
is constant over the set of training examples. The membership constraint allows 
us to treat the factorial learning problem as a novel kind of clustering problem. 
The cluster generators now compete to explain each feature of the data set and 
cooperate to explain each input example, rather than competing for examples and 
cooperating on features, as in traditional clustering systems such as K-means or 
mixture models. The following sections discuss the details of the feature cluster- 
ing algorithm for extracting functional parts, a simple but illustrative example, and 
extensions. In particular, we demonstrate a natural way to relax the strict member- 
ship constraint and thus learn hierarchical models of data generated from multiple 
unknown categories, each with a different multiple causal structure. 
Factorial Learning by Clustering Features 563 
2 THE FEATURE CLUSTERING ALGORITHM 
Our algorithm for extracting functional parts derives from a statistical mechanics 
formulation of the soft clustering problem (inspired by Rose, Gurewitz, and Fox, 
1990; Hinton and Zemel, 1994). We take as input a data set {x � }, with I examples 
of J real-valued features. The best K-cluster representation of these J features is 
given by an optimal set of cluster parameters, {0k}, and an optimal set of assign- 
ments, {Pjk}. The assignment Pjk specifies the probability of assigning feature j 
to cluster k, and depends directly on Ejk = 
jk , the total squared dif- 
ference (over the I training examples) between the observed feature values -(') and 
cluster k's predictions t(') The parameters 19k define cluster k's generative model, 
e() [Ok 
and thus determine the predictions jk  
If we limit functional parts to clusters of linearly correlated features, then the 
appropriate generative model has t(o _ w k  (') with cluster parameters 
jk -- J Yk + uj, 
19k -- {g), wj,uj} to be estimated. That is, for eh example i, pt k predicts the 
vue of input feature j  a linear function of some pt-specific factor- 
finger position in Figure 1). For the purposes of this paper, we sume zeromean 
features and ignore the u i terms. Then Ej 
J y) � 
The optimal cluster pareters and signments can now be found by mimizing 
the complete log likelihood of the data given the K-cluster representation, or equiv- 
ently, in the framework of' statistical mechanics, by minimizing the free energy 
I 1 
F = E - H =  pj(E +  logp) (1) 
subject to the membership constr&nts, p = 1, (j). Mimizing the ener, 
E =  pjEj, (2) 
reduces the expected reconstruction error, leading to more accurate representations. 
Mimizing the entropy, 
H = -   pj logp/, (3) 
distributes responsibility for each feature across many pts, thus decreeing the 
independence of the parts d leading to simpler representations (with fewer degrees 
of keedom). In line with Occam's zor, minimizing the energy-entropy tradeoff 
finds the representation that, at a particul temperature 1/, best satisfies the 
conflicting requirements of low error and low complexity. 
We minimize the free energy with a generMized EM procedure (NeM d Hinton, 
1994), setting derivatives to zero and iterating the resulting update equations: 
564 Joshua B. Tenenbaum, Emmanuel V. Todorov 
This update procedure assumes a normalization step ' 
! = ! /(Y-i' in 
each iteration, because without some additional constraint on the magnitudes of 
' (') is an ill-posed problem. 
-(') (or wjk), inverting the generative model fJ) = 
Yk 
This algorithm maps naturally onto a simple network architecture. The hidden unit 
activities, representing the part-specific factors - 
y , are computed from the obser- 
vations -(') via bottom-up weights pjwj, normalized, and multiplied by top-down 
weights wik to generate the network's predictions 
j. The weights adapt accord- 
ing to a hybrid learning rule, with wj determined by a Hebb rule (as in subspace 
learning algorithms), and pj determined by a competitive, softmax function of the 
reconstruction error Ej (as in soft mixture models). 
3 LEARNING A HIERARCHY OF PARTS 
The following simulation illustrates the algorithm's behavior on a simple, part seg- 
mentation task. The training data consist of 60 examples with 16 features each, 
representing the horizontal positions of 16 points on an articulated hand in various 
configurations (as in Figure 1). The data for this example were generated by a 
hierarchical, random process that produced a low correlation between all 16 fea- 
tures, a moderate correlation between the four features on each finger, and a high 
correlation between the two features on each joint (two joints per finger). To fully 
explain this data set, the algorithm should be able to find a corresponding hierarchy 
of increasingly complex functional part representations. 
To evaluate the network's representation of this data set, we inspect the learned 
weights pjkwjk, which give the total contribution of feature j to part k in (5). 
In Figure 2, these weights are plotted for several different values of/, with gray 
boxes indicating zero weights, white indicating strong positive weights, and black 
indicating strong negative weights. The network was configured with K - 16 part 
units, to ensure that all potential parts could be found. When fewer than K distinct 
parts are found, some of the cluster units have identical parameters (appearing 
as identical columns in Figure 2). These results were generated by deterministic 
annealing, starting with/  1, and perturbing the weights slightly each time / 
was increased, in order to break symmetries. 
Figure 2 shows that the number
