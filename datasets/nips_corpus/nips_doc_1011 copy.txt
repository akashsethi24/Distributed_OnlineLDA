A Unified Learning Scheme: 
Bayesian-Kullback Ying-Yang Machine 
Lei Xu 
1. Computer Science Dept., The Chinese University of HK, Hong Kong 
2. National Machine Perception Lab, Peking University, Beijing 
Abstract 
A Bayesian-Kullback learning scheme, called Ying-Yang Machine, 
is proposed based on the two complement but equivalent Bayesian 
representations for joint density and their Kullback divergence. 
Not only the scheme unifies existing major supervised and unsu- 
pervised learnings, including the classical maximum likelihood or 
least square learning, the maximum information preservation, the 
EM &em algorithm and information geometry, the recent popular 
Helmholtz machine, as well as other learning methods with new 
variants and new results; but also the scheme provides a number 
of new learning models. 
1 INTRODUCTION 
Many different learning models have been developed in the literature. We may 
come to an age of searching a unified scheme for them. With a unified scheme, 
we may understand deeply the existing models and their relationships, which may 
cause cross-fertilization on them to obtain new results and variants; We may also be 
guided to develop new learning models, after we get better understanding on which 
cases we have already studied or missed, which deserve to be further explored. 
Recently, a Baysian-Kullback scheme, called the YING-YANG Machine, has been 
proposed as such an effort(Xu, 1995a). It bases on the Kullback divergence and two 
complement but equivalent Baysian representations for the joint distribution of the 
input space and the representation space, instead of merely using Kullback diver- 
gence for matching un-structuralized joint densities in information geometry type 
learnings (Amari, 1995a&b; Byrne, 1992; Csiszar, 1975). The two representations 
consist of four different components. The different combinations of choices of each 
component lead the YING-YANG Machine into different learning models. Thus, 
it acts as a general learning scheme for unifying the existing major unsupervised 
and supervised learnings. As shown in Xu(1995a), its one special case reduces to 
the EM algorithm (Dempster et al, 1977; Hathaway, 1986; Neal & Hinton, 1993) 
A Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine 445 
and the closely related Information Geometry theory and the em algorithm (Amari, 
1995a&b), to MDL autoencoder with a bits-back argument by Hinton & Zemel 
(1994) and its alternative equivalent form that minimizes the bits of uncoded resid- 
ual errors and the unused bits in the transmission channel's capacity (Xu, 1995d), 
as well as to Multisets modeling learning (Xu, 1995e)-a unified learning framework 
for clustering, PCA-type learnings and self-organizing map. It other special case 
reduces to maximum information preservation (Linsker, 1989; Atick & Redlich, 
1990; Bell & Sejnowski, 1995). More interestingly its another special case reduces 
to Helmholtz machine (Dayan et a1,1995; Hinton, 1995) with new understandings. 
Moreover, the YING-YANG machine includes also maximum likelihood or least 
square learning. 
Furthermore, the YING-YANG Machine has also been extended to temporal pat- 
terns with a number of new models for signal modeling. Some of them are the 
extensions of Helmholtz machine or maximum information preservation learning to 
temporal processing. Some of them include and extend the Hidden Markov Model 
(HMM), AMAR and AR models (Xu, 1995b). In addition, it has also been shown in 
Xu(1995a&c, 1996a) that one special case of the YING-YANG machine can provide 
us three variants for clustering or VQ, particularly with criteria and an automatic 
procedure developed for solving how to select the number of clusters in clustering 
analysis or Gaussian mixtures -- a classical problem that remains open for decades. 
In this paper, we present a deep and systematical further study. Section 2 re- 
describes the unified scheme on a more precise and systematical basis via discussing 
the possible marital status of the two Bayesian representations for joint density. 
Section 3 summarizes and explains those existing models under the unified scheme, 
particularly we have clarified some confusion made in the previous papers (Xu, 
1995a&b) on maximum information preservation learning. Section 4 proposed and 
summarizes a number of possible new models suggested by the unified scheme. 
2 BAYESIAN-KULLBACK YING-YANG MACHINE 
As argued in Xu (1995a), unsupervised and supervised learning problems can be 
summarized into the problem of estimating joint density P(x, y) of patterns in 
the input space X and the representation space Y, as shown in Fig.1. Under the 
Bayesian framework, we have two representations for P(x,y). One is PM1 (gg,Y) -- 
PM1 (ylgg)PM1 (gO), implemented by a model Mi called YANG/(male) part since it 
performs the task of transferring a pattern/(a real body) into a code/(a seed). The 
other is PM2(x,y) = PM2(xlY)PM(y), implemented by a model M2 called YING 
part since it performs the task of generating a pattern/(a real body) from a code/(a 
seed). They are complement to each other and together implement an entire circle 
x -+ y -+ x. This compliments to the ancient chinese YING-YANG philosophy. 
Here we have four components PMi(X), PMl(Ylx), PM2(xly) and PM2(Y). The 
PM1 (x) can be fixed at some density estimate on input data, e.g., we have at least 
two choices-Parzen window estimate Ph(x) or empirical estimate P0(x): 
-- [' ( n ), Po(x) -- limn-o Pn(x) =  N 
Nh d 1  t=l (5(X -- Xi). (1) 
For PMi(Ylgg), PM2(ggly), each can have three choices: (1) from a parametric fam- 
ily specified by model M1 or M2; (2) free of model with PMi(Ylgg) -- P(ylgc) or 
PM2(ggly) -- P(zly); (3) broken channel PMl(Ylgg) -- PMi(Y) or PM2(ggly) ---- PM2(gg). 
Finally, PM= (Y) with its y consistent to PM1 (Yl x) can also being from a parametric 
family or free of model. Any combinations of the choices of the four components 
forms a potential YING-YANG pair. We at least have 2 x 3 x 3 x 2 = 36 pairs. 
A YING-YANG pair has four types of marital status: (a) marry, i.e., YING and 
446 L. XU 
I Reprelentitlon 8ploe Y 
8ymboll, Integerl, Binary Codell 
Re11 
Enoodlng [ 
Recognition Px(ylx) 
Representation 
I Deoodlng 
Gieneretl ng 
Fleoonitruotlon 
PxxCx) Input pttern mpm X 
Figure 1 The joint spaces X, Y and the YING-YANG Machine 
YANG match each other; (b) divorce, i.e., YING and YANG go away from each 
other; (c) YING chases YANG, YANG escapes; (d) YANG chases YING, but YING 
escapes. The four types can be described by a combination of minimization (chas- 
ing) and maximization (escaping) on one of the two Kullback divergences below: 
IC ( M1, M2 ) --- fx,y PM1 (Yix)PMx (x) log vq (ul)vq ().. (2a) 
vt (xlu)v (u) axay 
K ( M2, M ) = f,u PM2(xlY)PM2(Y) log r.(lu)r. (u) dxdy (2b) 
Table 1 Mathematical description for marital status of a YING-YANG pair 
' K(M,M2) (a) maxM minMK(M1,M2) I maxM minM K(M1,M2 l 
mnM1,M K'(M1,M2) (b) minM maxM1K(M1,M2) I minM1 maxM s/�(M1 M2 
maxM 1 Mg , 
We can replace K(M1, M2) by K(M2, M1) in the table. The 2nd & 3rd columns are for 
(c) (d) respectively, each has two cases depending on who starts the act and the two 
are usually not equivalent. Their results are undefined depending on initial condition for 
M1,M2, except of two special cases: (i) Free PMt(yl x) and parametric PM2(xly), with 
mina42 maxa4t t� being the same as (b) with broken Pa4x (ylx), and with maxa42 mina4t K 
defined but useless. (ii) Free Pa42(xly) and parametric Pa4t (y[x), with minMt max K 
the same as case (a) with broken P2(xly), with mina4t maxa42 K defined but useless. 
Therefore, we will focus on the status marry and divorce. Even so, not all of the 
above mentioned 2 x 3 x 3 x 2 = 36 YING-YANG pairs provide sensible learning 
models although minm,m K and maxmx,m K are always well defined. Fortunately, 
a quite number of them indeed lead us to useful learning models, as will be shown 
in the sequent sections. 
We can implement minmx,m K(M, M2) by the following Alternative Minimization 
(ALTMIN) procedure: 
Step 1 Fix M2 = M 'ta, to get M 0 = arg MinM KL(M1, M 'ta) 
Step 2 Fix M1 = M 'ta, to get M 0 = arg MinM KL(M 'ta, M2) 
The ALTMIN iteration will finally converge to a local minimum of K(M, M2). We can 
have a similar procedure for maxM,M2/'(M1, M2) via replacing Min by Max. 
Since the above scheme bases on the two complement YING and YANG Bayesian 
representations and their Kullback divergence for their marital status, we call it 
Bayesian-Kullback YING-YANG learning scheme. Furthermore, under this scheme 
we call each obtained YING-YANG pair that is sensible for learning purpose as a 
Bayesian-Kullback YING-YANG Machine or YING-YANG machine shortly. 
3 UNIFIED EXISTING LEARNINGS 
Let PM(X)= Po(x) by eq.(1) and put it into eq.(2), through certain mathematics 
we can get K(M, M2) = hM -- haux - qM, + D with D independent of M, M2 
and hM, hax, qMl,2 given by Eqs.(E1)(E2)&(E4) in Tab.2 respectively. The larger 
A Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine 447 
is the hMt, the more discriminative or separable are the representations in Y for the 
input data set. The larger is the hM, the more concentrated the representations 
in Y. The larger is the qm,2, the better Pm2(xly ) fits the input data. 
Therefore, minm,mK(Mi,M2) consists of (1) best fitting of Pm(zly ) on input 
data via maxqm,, which is desirable, (2) producing more concentrated representa- 
tions in Y to occupy less resource, which is also desirable and is the behind reason for 
solving the problem of selecting cluster number in clustering analysis Xu(1995a&c, 
1996a), (3) but with the cost of less discriminative representations in Y for the input 
data. Inversely, maxM,M K(M, M2) consists of (1) producing best discriminative 
or separable representation PM (ylx) in Y for the input data set, which is desirable, 
in 
