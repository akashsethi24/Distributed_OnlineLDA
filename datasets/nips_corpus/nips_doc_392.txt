CAM Storage of Analog Patterns and 
Continuous Sequences with 3N 2 Weights 
Bill Baird 
Dept Mathematics and 
Dept Molecular and Cell Biology, 
129 LSA, U.C.Berkeley, 
Berkeley, Ca. 94720 
Frank Eeckman 
Lawrence Livermore 
National Laboratory, 
P.O. Box 808 (L-426), 
Livermore, Ca. 94550 
Abstract 
A simple architecture and algorithm for analytically guaranteed associa- 
tive memory storage of analog patterns, continuous sequences, and chaotic 
attractors in the same network is described. A matrix inversion determines 
network weights, given prototype patterns to be stored. There are N units 
of capacity in an N node network with 3N 2 weights. It costs one unit per 
static attractor, two per Fourier component of each sequence, and four per 
chaotic attractor. There are no spurious attractors, and there is a Lia- 
punov function in a special coordinate system which governs the approach 
of transient states to stored trajectories. Unsupervised or supervised incre- 
mental learning algorithms for pattern classification, such as competitive 
learning or bootstrap Widrow-Hoff can easily be implemented. The archi- 
tecture can be folded into a recurrent network with higher order weights 
that can be used as a model of cortex that stores oscillatory and chaotic 
attractors by a Hebb rule. Hierarchical sensory-motor control networks 
may be constructed of interconnected cortical patches of these network 
modules. Network performance is being investigated by application to the 
problem of real time handwritten digit recognition. 
1 Introduction 
We introduce here a projection network which is a new network for implementa- 
tion of the normal form projection algorithm discussed in [Bai89, Bai90b]. The 
autoassociative case of this network is formally equivalent to the previous higher 
order network realization used as a biological model [Bai90a]. It has 3N 2 weights 
instead of N  + N 4, and is more useful for engineering applications. All the math- 
ematical results proved for the projection algorithm in that case carry over to this 
91 
92 Baird and Eeckman 
INPUT 
Network Coordinates 
p-i matrix 
A matrix 
Dynamic 
winner-take-all 
Network 
Vl 
Memory Coordinates 
Normal Form: 
6i = ctvi -- vi - aidv] 
P matrix 
OUTPUT 
Pij 
Z-= Pff 
Network Coordinates 
Figure 1: Projection Network - 3N 2 weights. The A matrix determines a k-winner- 
take-all net - programs attractors, basins of attraction, and rates of convergence. 
The columns of P contain the ouptut patterns associated to these attractors. The 
rows of p-1 determine category centroids 
new architecture, but more general versions can be trained and applied in novel 
ways. The discussion here will be informal, since space prohibits technical detail 
and proofs may be found in the references above. 
A key feature of a net constructed by this algorithm is that the underlying dynamics 
is explicitly isomorphic to any of a class of standard, well understood nonlinear 
dynamical systems - a normal form [GH83]. This system is chosen in advance, 
independent of both the patterns to be stored and the learning algorithm to be 
used. This control over the dynamics permits the design of important aspects of 
the network dynamics independent of the particular patterns to be stored. Stability, 
basin geometry, and rates of convergence to attractors can be programmed in the 
standard dynamical system. 
Here we use the normal form for the Hopf bifurcation [GH83] as a simple recurrent 
competitive k-winner-take-all network with a cubic nonlinearity. This network lies 
in what might considered diagonalized or overlap or memory coordinates (one 
memory per k nodes). For temporal patterns, these nodes come in complex conju- 
gate pairs which supply Fourier components for trajectories to be learned. Chaotic 
dynamics may be created by specific programming of the interaction of two pairs 
of these nodes. 
Learning of desired spatial or spatio-temporal patterns is done by projecting sets of 
CAM Storage of Analog Patterns and Continuous Sequences with 3N 2 Weights 93 
these nodes into network coordinates(the standard basis) using the desired vectors 
as corresponding columns of a transformation matrix P. In previous work, the 
differential equations of the recurrent network itself are linearly transformed or 
projected, leading to new recurrent network equations with higher order weights 
corresponding to the cubic terms of the recurrent network. 
2 The Projection Network 
In the projection net for autoassociation, this algebraic projection operation into 
and out of memory coordinates is done explicitly by a set of weights in two feed- 
forward linear networks characterized by weight matrices p-1 and P. These map 
inputs into and out of the nodes of the recurrent dynamical network in memory 
coordinates sandwiched between them. This kind of network, with explicit input 
and output projection maps that are inverses, may be considered an unfolded 
version of the purely recurrent networks described in the references above. 
This network is shown in figure 1. Input pattern vectors Z  are applied as pulses 
which project onto each vector of weights (row of the p-i matrix) on the input 
to each unit i of the dynamic network to establish an activation level vi which 
determines the initial condition for the relaxation dynamics of this network. The 
recurrent weight matrix A of the dynamic network can be chosen so that the unit 
or predefined subspace of units which recieves the largest projection of the input 
will converge to some state of activity, static or dynamic, while all other units are 
supressed to zero activity. 
The evolution of the activity in these memory coordinates appears in the original 
network coordinates at the output terminals as a spatio-temporal pattern which 
may be fully distributed accross all nodes. Here the state vector of the dynamic 
network has been transformed by the P matrix back into the coordinates in which 
the input was first applied. At the attractor if. in memory coordinates, only a lin- 
ear combination of the columns of the P weight matrix multiplied by the winning 
nonzero modes of the dynamic net constitute the network representation of the out- 
put of the system. Thus the attractor retrieved in memory coordinates reconstructs 
its learned distributed representation Z* through the corresponding columns of the 
output matrix P, e.g. p-igOr = . , ff --. 9'* , P9'* = Z* 
For the special case of content addressable memory or autoassociation, which we 
have been describing here, the actual patterns to be learned form the columns 
of the output weight matrix P, and the input matrix is its inverse p-i. These 
are the networks that can be folded into higher order recurrent networks. For 
orthonormal patterns, the inverse is the transpose of this output matrix of memories, 
p-i = pT, and no computation of P- is required to store or change memories - 
just plug the desired patterns into appropriate rows and columns of P and pT. 
In the autoassociative network, the input space, output space and normal form 
state space are each of dimension N. The input and output linear maps require 
N 2 weights each, while the normal form coefficients determine another N 2 weights. 
Thus the net needs only 3N 2 weights, instead of the N 2 + N 4 weights required by 
the folded recurrent network. The 2N 2 input and output weights could be stored 
off-chip in a conventional memory, and the fixed weights of the dynamic normal 
form network could be implemented in VLSI for fast analog relaxation. 
94 Baird and Eeckman 
3 Learning Extensions 
More generally, for a heteroassociative net (i. e., a net designed to perform a map 
from input space to possibly different output space) the linear input and output 
maps need not be inverses, and may be noninvertible. They may be found by any 
linear map learning technique such as Widrow-Hoff or by finding pseudoinverses. 
Learning of all desired memories may be instantaneous, when they are known in 
advance, or may evolve by many possible incremental methods, supervised or un- 
supervised. The standard competitive learning algorithm where the input weight 
vector attached to the winning memory node is moved toward the input pattern 
can be employed. We can also decrease the tendency to choose the most frequently 
selected node, by adjusting paratmeters in the normal form equations, to realize the 
more effective frequency selective competitive learning algorithm [AKCM90]. Su- 
pervised algorithms like bootstrap Widrow Itoff may be implemented as well, where 
a desired output category is known. The weight vector of the winning normal form 
node is updated by the competitive rule, if it is the right category for that input, 
but moved away from the input vector, if it is not the desired category, and the 
weight vector of the desired node is moved toward the input. 
Thus the input map can be optimized for clustering and classification by these 
algorithms, as the weight vectors (row vectors of the input matrix) approach the 
centroids of the clusters in the input environment. The output weight matrix may 
then be constructed with any desired output pattern vectors in appropriate columns 
to place the attractors corresponding to these categories anywhere in the state space 
in network coordinates that is required to achieve a desired heteroassociation. 
If either the input or the output matrix is learned, and the other chosen to be its 
inverse, then these competitive nets can be folded into oscillating biological versions, 
to see what the competive learning algorithms correspond to there. Now either the 
rows of the input matrix may be optimized for recognition, or the columns of the 
output matrix may be chosen to place attractors, but not both. We hope to be able 
to derive a kind of Hebb rule in the biological network, using the unfolded form of 
the network, which we can prove will accomplish competitive lea
