Sparse Code Shrinkage: Denoising by 
Nonlinear Maximum Likelihood Estimation 
Aapo HyvSrinen, Patrik Hoyer and Erkki Oja 
Helsinki University of Technology 
Laboratory of Computer and Information Science 
P.O. Box 5400, FIN-02015 HUT, Finland 
aapo. hyvar inenhut. f i, patrik. hoyerhut. f i, erkki. oj ahut. f i 
http://www. cis. hut. f i/proj ects/ica/ 
Abstract 
Sparse coding is a method for finding a representation of data in 
which each of the components of the representation is only rarely 
significantly active. Such a representation is closely related to re- 
dundancy reduction and independent component analysis, and has 
some neurophysiological plausibility. In this paper, we show how 
sparse coding can be used for denoising. Using maximum likelihood 
estimation of nongaussian variables corrupted by gaussian noise, we 
show how to apply a shrinkage nonlinearity on the components of 
sparse coding so as to reduce noise. Furthermore, we show how to 
choose the optimal sparse coding basis for denoising. Our method 
is closely related to the method of wavelet shrinkage, but has the 
important benefit over wavelet methods that both the features and 
the shrinkage parameters are estimated directly from the data. 
1 Introduction 
A fundamental problem in neural network research is to find a suitable representa- 
tion for the data. One of the simplest methods is to use linear transformations of the 
observed data. Denote by x = (Xl, x2, ..., xn) T the observed n-dimensional random 
vector that is the input data (e.g., an image window), and by s = (s, s2, ...,sn) T 
the vector of the linearly transformed component variables. Denoting further the 
n x n transformation matrix by W, the linear representation is given by 
s- Wx. (1) 
474 A. Hyviirinen, P Hoyer and E. Oja 
We assume here that the number of transformed components equals the number of 
observed variables, but this need not be the case in general. 
An important representation method is given by (linear) sparse coding [1, 10], in 
which the representation of the form (1) has the property that only a small number 
of the components si of the representation are significantly non-zero at the same 
time. Equivalently, this means that a given component has a 'sparse' distribution. 
A random variable si is called sparse when si has a distribution with a peak at zero, 
and heavy tails, as is the case, for example, with the double exponential (or Laplace) 
distribution [6]; for all practical purposes, sparsity is equivalent to supergaussianity 
or leptokurtosis [8]. Sparse coding is an adaptive method, meaning that the matrix 
W is estimated for a given class of data so that the components si are as sparse as 
possible; such an estimation procedure is closely related to independent component 
analysis [2]. 
Sparse coding of sensory data has been shown to have advantages from both phys- 
iological and information processing viewpoints [1]. However, thorough analyses of 
the utility of such a coding scheme have been few. In this paper, we introduce and 
analyze a statistical method based on sparse coding. Given a signal corrupted by 
additive gaussian noise, we attempt to reduce gaussian noise by soft thresholding 
('shrinkage') of the sparse components. Intuitively, because only a few of the com- 
ponents are significantly active in the sparse code of a given data point, one may 
assume that the activities of components with small absolute values are purely noise 
and set them to zero, retaining just a few components with large activities. This 
method is closely connected to the wavelet shrinkage method [3]. In fact, sparse 
coding may be viewed as a principled way for determining a wavelet-like basis and 
the corresponding shrinkage nonlinearities, based on data alone. 
2 Maximum likelihood estimation of sparse components 
The starting point of a rigorous derivation of our denoising method is the fact that 
the distributions of the sparse components are nongaussian. Therefore, we shall 
begin by developing a general theory that shows how to remove gaussian noise from 
nongaussian variables, making minimal assumptions on the data. 
Denote by s the original nongaussian random variable (corresponding here to a 
noise-free version of one of the sparse components si), and by  gaussian noise of 
zero mean and variance a2. Assume that we only observe the random variable y: 
(2) 
and we want to estimate the original s. Denoting by p the probability density of s, 
and by f - -logp its negative log-density, the maximum likelihood (ML) method 
gives the following estimator for s: 
1 
 = argmin -(y - u) 2 + f(u). 
u 
(3) 
Assuming f to be strictly convex and differentiable, this can be solved [6] to yield 
 = g(y), where the function g can be obtained from the relation 
(4) 
This nonlinear estimator forms the basis of our method. 
Sparse Code Shrinkage: Denoising by Nonlinear Maximum Likelihood Estimation 475 
o 
Figure 1: Shrinkage nonlinearities and associated probability densities. Left: Plots 
of the different shrinkage functions. Solid line: shrinkage corresponding to Laplace 
density. Dashed line: typical shrinkage function obtained from (6). Dash-dotted 
line: typical shrinkage function obtained from (8). For comparison, the line x = y is 
given by dotted line. All the densities were normalized to unit variance, and noise 
variance was fixed to .3. Right: Plots of corresponding model densities of the sparse 
components. Solid line: Laplace density. Dashed line: a typical moderately super- 
gaussian density given by (5). Dash-dotted line: a typical strongly supergaussian 
density given by (7). For comparison, gaussian density is given by dotted line. 
3 Parameterizations of sparse densities 
To use the estimator defined by (3) in practice, the densities of the si need to 
be modelled with a parameterization that is rich enough. We have developed two 
parameterizations that seem to describe very well most of the densities encountered 
in image denoising. Moreover, the parameters are easy to estimate, and the inversion 
in (4) can be performed analytically. Both models use two parameters and are thus 
able to model different degrees of supergaussianity, in addition to different scales, 
i.e. variances. The densities are here assumed to be symmetric and of zero mean. 
The first model is suitable for supergaussian densities that are not sparser than the 
Laplace distribution [6], and is given by the family of densities 
p(s) - C exp(-as 2/2 - b[s[), (5) 
where a, b  0 are parameters to be estimated, and C is an irrelevant scaling 
constant. The classical Laplace density is obtained when a - 0, and gaussian 
densities correspond to b - 0. A simple method for estimating a and b was given 
in [6]. For this density, the nonlinearity g takes the form: 
1 
g(u) = 1 + 2a sign(u) max(O, [u[ - b ) (6) 
where  is the noise variance. The effect of the shrinkage function in (6) is to 
reduce the absolute value of its argument by a certain amount, which depends on 
the parameters, and then rescale. Small arguments are thus set to zero. Examples 
of the obtained shrinkage functions are given in Fig. 1. 
The second model describes densities that are sparser than the Laplace density: 
i (a+2)[a(a+ 1)/2] (a/+) 
p(s)- - [V/a ( + 1)/2--[s/d[](+3) (7) 
476 A. Hyvirinen, P. Hoyer and E. Oja 
When c - x, the Laplace density is obtained as the limit. A simple consistent 
method for estimating the parameters d,c > 0 in (7) can be obtained from the 
relations d =  and c = (2 - k + V/k(k + 4))/(2k - 1) with k = d2ps(O) 2, 
see [6]. The resulting shrinkage function can be obtained as [6[ 
sign(u) max(O, lul - ad 
2 
+ v/(lul + ad) 2 - 4a2(a + 3)) 
(8) 
where a - V/a(a + 1)/2, and g(u) is set to zero in case the square root in (8) is 
imaginary. This is a shrinkage function that has a certain hard-thresholding flavor, 
as depicted in Fig. 1. 
Examples of the shapes of the densities given by (5) and (7) are given in Fig. 1, 
together with a Laplace density and a gaussian density. For illustration purposes, 
the densities in the plot are normalized to unit variance, but these parameterizations 
allow the variance to be choosen freely. 
Choosing whether model (5) or (7) should be used can be based on moments of 
the distributions; see [6]. Methods for estimating the noise variance a 2 are given in 
]a, 6[. 
4 Sparse code shrinkage 
The above results imply the following sparse code shrinkage method for denoising. 
Assume that we observe a noisy version  = x+v of the data x, where v is gaussian 
white noise vector. To denoise , we transform the data to a sparse code, apply the 
above ML estimation procedure component-wise, and then transform back to the 
original variables. Here, we constrain the transformation to be orthogonal; this is 
motivated in Section 5. To summarize: 
First, using a noise-free training set of x, use some sparse coding method 
for determining the orthogonal matrix W so that the components si in 
s = Wx have as sparse distributions as possible. Estimate a density model 
pi(si) for each sparse component, using the models in (5) and (7). 
Compute for each noisy observation (t) of x the corresponding noisy sparse 
components y(t) = W(t). Apply the shrinkage non-linearity gi(.) as de- 
fined in (6), or in (8), on each component Yi (t), for every observation index 
t. Denote the obtained components by i(t) - gi(Yi(t)). 
Invert the relation (1) to obtain estimates of the noise-free x, given by 
= wry(t). 
To estimate the sparsifying transform W, we assume that we have access to a noise- 
free realization of the underlying random vector. This assumption is not unrealistic 
on many applications: for example, in image denoising it simply means that we 
can observe noise-free images that are somewhat similar to the noisy image to be 
treated, i.e., they belong to the same environment or context. This assumption can 
be, however, relaxed in many cases, see [
