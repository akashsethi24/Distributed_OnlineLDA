Inference in Multilayer Networks via 
Large Deviation Bounds 
Michael Kearns and Lawrence Saul 
AT&T Labs -- Research 
Shannon Laboratory 
180 Park Avenue A-235 
Florham Park, NJ 07932 
{mkearns, lsaul}�research. art. tom 
Abstract 
We study probabilistic inference in large, layered Bayesian net- 
works represented as directed acyclic graphs. We show that the 
intractability of exact inference in such networks does not preclude 
their effective use. We give algorithms for approximate probabilis- 
tic inference that exploit averaging phenomena occurring at nodes 
with large numbers of parents. We show that these algorithms 
compute rigorous lower and upper bounds on marginal probabili- 
ties of interest, prove that these bounds become exact in the limit 
of large networks, and provide rates of convergence. 
I Introduction 
The promise of neural computation lies in exploiting the information processing 
abilities of simple computing elements organized into large netq�orks. Arguably one 
of the most important types of information processing is the capacity for proba- 
bilistic reasoning. 
The properties of undirected probabilistic models represented as symmetric networks 
have been studied extensively using methods from statistical mechanics (Hertz et 
al, 1991). Detailed analyses of these models are possible by exploiting averaging 
phenomena that occur in the thermodynamic limit of large networks. 
In this paper, we analyze the limit of large, multilayer networks for probabilistic 
models represented as directed acyclic graphs. These models are known as Bayesian 
networks (Pearl, 1988; Neal, 1992), and they have different probabilistic semantics 
than symmetric neural networks (such as Hopfield models or Boltzmann machines). 
We show that the intractability of exact inference in multilayer Bayesian networks 
Inference in Multilayer Networks via Large Deviation Bounds 261 
does not preclude their effective use. Our work builds on earlier studies of varia- 
tional methods (Jordan et al, 1997). We give algorithms for approximate proba- 
bilistic inference that exploit averaging phenomena occurring at nodes with N >> 1 
parents. We show that these algorithms compute rigorous lower and upper bounds 
on marginal probabilities of interest, prove that these bounds become exact in the 
limit N ---> oc, and provide rates of convergence. 
2 Definitions and Preliminaries 
A Bayesian network is a directed graphical probabilistic model, in which the nodes 
represent random variables, and the links represent causal dependencies. The joint 
distribution of this model is obtained by composing the local conditional probability 
distributions (or tables), Pr[child[parents], specified at each node in the network. 
For networks of binary random variables, so-called transfer functions provide a 
convenient way to parameterize conditional probability tables (CPTs). A transfer 
function is a mapping f: [-cx>, cx>] --+ [0, 1] that is everywhere differentiable and 
satisfies f'(x) _> 0 for all x (thus, f is nondecreasing). If ft(x) < a for all x, we say 
that f has slope a. Common examples of transfer functions of bounded slope include 
the sigmoid fix) = 1/(1 + e-X), the cumulative gaussian fix): fdt e- /v/-, 
and the noisy-OR f(x) - 1 - e -x. Because the value of a transfer function f 
is bounded between 0 and 1, it can be interpreted as the conditional probability 
that a binary random variable takes on a particular value. One use of transfer 
functions is to endow multilayer networks of soft-thresholding computing elements 
with probabilistic semantics. This motivates the following definition: 
Definition 1 For a transfer function f, a layered probabilistic f-network has: 
� Nodes representing binary variables {Xit}, � = 1,..., L and i = 1,..., N. 
Thus, L is the number of layers, and each layer contains N nodes. 
� For every pair of nqdes XJ -1 and Xi  in adjacent layers, a real-val[ted weight 
t-1 from t-1 to Xi t . 
Oij Xj 
� For every node Xi  in the first layer, a bias Pi. 
We will sometimes refer to nodes in layer I as inputs, and to nodes in layer L as 
outputs. A layered probabilistic f-network defines a joint probability distribution 
over all of the variables {X/} as follows: each input node X is independently set 
to I with probability pi, and to 0 with probability 1 -pi. Inductively, given binary 
t-1 �-1 
values Xj : xj G {0,1} for all of the nodes in layer �- l, the node Xi t is set 
N ,-1 t-lx 
to I with probability f(Y'}j=l lYij Xj 
Among other uses, multilayer networks of this form have been studied as hierarchi- 
cal generatire models of sensory data (Hinton et al, 1995). In such applications, 
the fundamental computational problem (known as inference) is that of estimating 
the marginal probability of evidence at some number of output nodes, say the first 
It _< N. (The computation of conditional probabilities, such as diagnostic queries, 
can be reduced to marginals via Bayes rule.) More precisely, one wishes to estimate 
PRIX1  = xl,... ,X -- XK] (where xi  {0, 1}), a quantity whose exact computa- 
tion involves an exponential sum over all the possible settings of the uninstantiated 
nodes in layers 1 through L - 1, and is known to be computationally intractable 
(Cooper, 1990). 
262 M. Kearns and L. Saul 
3 Large Deviation and Union Bounds 
One of our main weapons will be the theory of large deviations. As a first illustration 
of this theory, consider the input nodes {XJ } (which are independently set to 0 or 1 
according to their biases pj) and the weighted sum y']Y= Oj X) that feeds into the 
ith node X in the second layer. A typical large deviation bound (Kearns & Saul, 
1997) states that for all e > 0, er[I y']7= 10j(X) -pj)[ > e] < 2e -22/(:v02) where 
6) is the largest weight in the network. If we make the scaling assumption that 
each weight Oj is bounded by r/N for some constant r (thus, 6) < r/N), then we 
see that the probability of large (order 1) deviations of this weighted sum from its 
mean decays exponentially with N. (Our methods can also provide results under 
the weaker assumption that all weights are bounded by O(N -a) for a > 1/2.) 
How can we apply this observation to the problem of inference? Suppose we are 
interested in the marginal probability Pr[X = 1]. Then the large deviation bound 
tells us that with probability at least i - 5 (where we define 5 = 2e--2V2/T2), the 
weighted sum at node X will be within e of its mean value tti - YY= Ojpj. Thus, 
with probability at least 1 - 5, we are assured that Pr[X = 1] is at least f(tti - e) 
and at most f(lui + e). Of course, the flip side of the large deviation bound is that 
with probability at most 5, the weighted sum may fall more than e away from tti. 
In this case we can make no guarantees on Pr[X = 1] aside from the trivial lower 
and upper bounds of 0 and 1. Combining both eventualities, however, we obtain 
the overall bounds: 
(1-5)f(lui-e) _< Pr[X/2 = 1] _< (1-5)f(lui+e)+& (1) 
Equation (1) is based on a simple two-point approximation to the distribution over 
the weighted sam of inputs, -'7= OjXJ. This approximation places one point, 
with weight i - 5, at either e above or below the mean/i (depending on whether 
we are deriving the upper or lower bound); and the other point, with weight 5, at 
either -x> or +. The value of 5 depends on the choice of e: in particular, as e 
becomes smaller, we give more weight to the + point, with the trade-off governed 
by the large deviation bound. We regard the weight given to the + point as a 
throw-away probability, since with this weight we resort to the trivial bounds of 0 
or i on the marginal probability Pr[X = 1]. 
Note that the very simple bounds in Equation (1) already exhibit an interesting 
trade-off, governed by the choice of the parameter e--namely, as e becomes smaller, 
the throw-away probability 5 becomes larger, while the terms f(lui + e) converge to 
the same value. Since the overall bounds involve products of f(lui + e) and 1 - 5, 
the optimal value of e is the one that balances this competition between probable 
explanations of the evidence and improbable deviations from the mean. This trade- 
off is reminiscent of that encountered between energy and entropy in mean-field 
approximations for symmetric networks (Hertz et al, 1991). 
So far we have considered the marginal probability involving a single node in the 
second layer. We can also compute bounds on the marginal probabilities involving 
K > i nodes in this layer (which without loss of generality we take to be the nodes 
X12through X:). This is done by considering the probability that one or more 
of the weighted sums entering these I  nodes in the second layer deviate by more 
than e from their means. We can upper bound this probability by I5 by appealing 
to the so-called union bound, which simply states that the probability of a union of 
events is bounded by the sum of their individual probabilities. The union bound 
allows us to bound marginal probabilities involving multiple variables. For example, 
Inference in Multilayer Networks via Large Deviation Bounds 263 
consider the marginal probability PRIX12 = 1,...,X: = 1]. Combining the large 
deviation and union bounds, we find: 
K K 
(i-K6) H f(]li--e ) _< Pr[X 2 = 1,...,X} = 1] _< (1-KS)f(i+6)+I6. (2) 
i=1 i=1 
A number of observations are in order here. First, Equation (2) directly leads to 
efficient algorithms for computing the upper and lower bounds. Second, although 
for simplicity we have considered e-deviations of the same size at each node in the 
second layer, the same methods apply to different choices of (i (and therefore 5i) 
at each node. Indeed, variations in ei can lead to significantly tighter bounds, and 
thus we exploit the freedom to choose different ei in the rest of the paper. This 
results, for example, in bounds of the form: 
1-Si H f(i--fi)  er[X = 1,...,X} = 1], where 5i = 2e -2/. 
i=1 
