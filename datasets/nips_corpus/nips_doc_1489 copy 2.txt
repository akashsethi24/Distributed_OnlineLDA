On the optimality of incremental neural network 
algorithms 
Ron Meir* 
Department of Electrical Engineering 
Technion, Haifa 32000, Israel 
rmeir@dumbo. technion. ac. il 
Vitaly Maiorov + 
Department of Mathematics 
Technion, Haifa 32000, Israel 
maiorov@tx. technion. ac. il 
Abstract 
We study the approximation of functions by two-layer feedforward neu- 
ral networks, focusing on incremental algorithms which greedily add 
units, estimating single unit parameters at each stage. As opposed to 
standard algorithms for fixed architectures, the optimization at each stage 
is performed over a small number of parameters, mitigating many of the 
difficult numerical problems inherent in high-dimensional non-linear op- 
timization. We establish upper bounds on the error incurred by the al- 
gorithm, when approximating functions from the Sobolev class, thereby 
extending previous results which only provided rates of convergence for 
functions in certain convex hulls of functional spaces. By comparing our 
results to recently derived lower bounds, we show that the greedy algo- 
rithms are nearly optimal. Combined with estimation error results for 
greedy algorithms, a strong case can be made for this type of approach. 
1 Introduction and background 
A major problem in the application of neural networks to real world problems is the ex- 
cessively long time required for training large networks of a fixed architecture. Moreover, 
theoretical results establish the intractability of such training in the worst case [9][4]. Ad- 
ditionally, the problem of determining the architecture and size of the network required to 
solve a certain task is left open. Due to these problems, several authors have considered 
incremental algorithms for constructing the network by the addition of hidden units, and 
estimation of each unit's parameters incrementally. These approaches possess two desir- 
able attributes: first, the optimization is done step-wise, so that only a small number of 
parameters need to be optimized at each stage; and second, the structure of the network 
*This work was supported in part by the a grant from the Israel Science Foundation 
+The author was partially supported by the center for Absorption in Science, Ministry of Immi- 
grant Absorption, State of Israel. 
296 R. Meir and V. Maiorov 
is established concomitantly with the learning, rather than specifying it in advance. How- 
ever, until recently these algorithms have been rather heuristic in nature, as no guaranteed 
performance bounds had been established. Note that while there has been a recent surge 
of interest in these types of algorithms, they in fact date back to work done in the early 
seventies (see [3] for a historical survey). 
The first theoretical result establishing performance bounds for incremental approximations 
in Hilbert space, was given by Jones [8]. This work was later extended by Barron [2], and 
applied to neural network approximation of functions characterized by certain conditions 
on their Fourier coefficients. The work of Barron has been extended in two main direc- 
tions. First, Lee et al. [10] have considered approximating general functions using Hilbert 
space techniques, while Donahue et al. [7] have provided powerful extensions of Jones' 
and Barron's results to general Banach spaces. One of the most impressive results of the 
latter work is the demonstration that iterative algorithms can, in many cases, achieve nearly 
optimal rates of convergence, when approximating convex hulls. 
While this paper is concerned mainly with issues of approximation, we comment that it is 
highly relevant to the statistical problem of learning from data in neural networks. First, 
Lee et al. [10] give estimation error bounds for algorithms performing incremental opti- 
mization with respect to the training error. Under certain regularity conditions, they are 
able to achieve rates of convergence comparable to those obtained by the much more com- 
putationally demanding algorithm of empirical error minimization. Moreover, it is well 
known that upper bounds on the approximation error are needed in order to obtain per- 
formance bounds, both for parametric and nonparametric estimation, where the latter is 
achieved using the method of complexity regularization. Finally, as pointed out by Don- 
ahue et al. [7], lower bounds on the approximation error are crucial in establishing worst 
case speed limitations for learning. 
The main contribution of this paper is as follows. For functions belonging to the Sobolev 
class (see definition below), we establish, under appropriate conditions, near-optimal rates 
of convergence for the incremental approach, and obtain explicit bounds on the parameter 
values of the network. The latter bounds are often crucial for establishing estimation error 
rates. In contrast to the work in [10] and [7], we characterize approximation rates for 
functions belonging to standard smoothness classes, such as the Sobolev class. The former 
work establishes rates of convergence with respect to the convex hulls of certain subsets 
of functions, which do not relate in a any simple way to standard functional classes (such 
as Lipschitz, Sobolev, H61der, etc.). As far as we are aware, the results reported here are 
the first to report on such bounds for incremental neural network procedures. A detailed 
version of this work, complete with the detailed proofs, is available in [ 13]. 
2 Problem statement 
We make use of the nomenclature and definitions from [7]. Let 7/be a Banach space of 
functions with norm II ' II. For concreteness we assume henceforth that the norm is given 
by the Lq norm, 1 < q < oo, denoted by l[ ' IIq. Let lin,,7/consist of all sums of the form 
i= aig, gi E 7/and arbitrary ai, and co,,7/is the set of such sums with ai E [0, 1] and 
i ai = 1. The distances, measured in the Lq norm, from a function f are given by 
dist(lin,7/, f) = inf {lib- fllq:  lin,7/), 
dist(co,7/, f) = inf {lib - fllq: h  
The linear span of 7/is given by lin7/ = U,lin,7/, while the convex-hull of 7/is co7/ = 
U,co,7/. We follow standard notation and denote closures of sets by a bar, e.g. co7/is the 
closure of the convex hull of 7/. In this work we focus on the special case where 
7/__ 7/. __a +b), 14 _< I1(-)11 _< 1), (1) 
On the Optimality of Incremental Neural Network Algorithms 297 
corresponding to the basic building blocks of multilayer neural networks. The restriction 
Ila(')11 _<  is not very demanding as many sigmoidal functions can be expressed as a sum 
of functions of bounded norm. It should be obvious that lin,N, corresponds to a two-layer 
neural network with a linear output unit and a-activation functions in the single hidden 
layer, while co,N, is equivalent to a restricted form of such a network, where restrictions 
are placed on the hidden-to-output weights. In terms of the definitions introduced above, 
the by now well known property of universal function approximation over campacta can 
be stated as linN: C(M), where C(M) is the class of continuous real valued functions 
defined over M, a compact subset of p,a. A necessary and sufficient condition for this 
has been established by Leshno et al. [11], and essentially requires that a(.) be locally 
integrable and non-polynomial. We comment that if r = oo in (1), and c is unrestricted in 
sign, then coNoo = linNoo. The distinction becomes important only if r < oo, in which 
case coN, C linN,. 
For the purpose of incremental approximation, it turns out to be useful to consider the con- 
vex hull coN, rather than the usual linear span, as powerful algorithms and performance 
bounds can be developed in this case. In this context several authors have considered 
bounds for the approximation of a function f belonging to con by sequences of functions 
belonging to co,N. However, it is not clear in general how well convex hulls of bounded 
functions approximate general functions. One contribution of this work is to show how 
one may control the rate of growth of the bound r in (1), so that general functions, belong- 
ing to certain smoothness classes (e.g. Sobolev), may be well approximated. In fact, we 
show that the incremental approximation scheme described below achieves nearly optimal 
approximation error for functions in the Sobolev space. 
Following Danahue et al. [7], we consider s-greedy algorithms. Let s - (s, 62,... ) be a 
positive sequence, and similarly for (c, c2,... ), 0 < c, < 1. A sequence of functions 
h, h2, � � � is s-greedy with respect to f if for n = 0, 1, 2, .... 
II/n+l -- fllq < inf {I]c,h, + (1 - c,)9 - fllq:  E N.) q- 6,,, (2) 
where we set h0 = 0. For simplicity we set c, -- (n - 1)In, although other schemes are 
also possible. It should be clear that at each stage n, the function h, belongs to co,N,. 
Observe also that at each step, the infimum is taken with respect to 9 E N,, the function 
h, being fixed. In terms of neural networks, this implies that the optimization over each 
hidden unit parameters (a, b, c) is performed independently of the others. We note in pass- 
ing, that while this greatly facilitates the optimization process in practice, no theoretical 
guarantee can be made as to the convexity of the single-node error function (see [1] for 
counter-examples). The variables s, are slack variables, allowing the extra freedom of 
only approximate minimization. In this paper we do not optimize over c,, but rather fix a 
sequence in advance, forfeiting some generality at the price of a simpler presentation. In 
any event, the rates we obtain are unchanged by such a restriction. 
In the sequel we consider s-greedy approximations of smooth functions belonging to the 
Sobolev class of functions, 
W = { f ' max 
0<<r 
where 17c = (]Cl,... , ]Cd) , ]C i _ 0 and l]7cl = k +... ka. IIere 7 ) is the partial derivative 
operator of order 17c. All functions are defined over a compact domain K C p
