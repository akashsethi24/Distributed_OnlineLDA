A New Approach to Hybrid HMM/ANN Speech 
Recognition Using Mutual Information Neural 
Networks 
G. Rigoil, C. Neukirchen 
Gerhard-Mercator-University Duisburg 
Faculty of Electrical Engineering 
Department of Computer Science 
Bismarckstr. 90, Duisburg, Germany 
ABSTRACT 
This paper presents a new approach to speech recognition with hybrid 
HMM/ANN technology. While the standard approach to hybrid 
HMM/ANN systems is based on the use of neural networks as 
posterior probability estimators, the new approach is based on the use 
of mutual information neural networks trained with a special learning 
algorithm in order to maximize the mutual information between the 
input classes of the network and its resulting sequence of firing output 
neurons during training. It is shown in this paper that such a neural 
network is an optimal neural vector quantizer for a discrete hidden 
Markov model system trained on Maximum Likelihood principles. 
One of the main advantages of this approach is the fact, that such 
neural networks can be easily combined with HMM's of any 
complexity with context-dependent capabilities. It is shown that the 
resulting hybrid system achieves very high recognition rates, which 
are now already on the same level as the best conventional HMM 
systems with continuous parameters, and the capabilities of the 
mutual information neural networks are not yet entirely exploited. 
1 INTRODUCTION 
Hybrid HMM/ANN systems deal with the optimal combination of artificial neural 
networks (ANN) and hidden Markov models (HMM). Especially in the area of automatic 
speech recognition, it has been shown that hybrid approaches can lead to very powerful 
and efficient systems, combining the discriminative capabilities of neural networks and 
the superior dynamic time warping abilities of HMM's. The most popular hybrid 
approach is described in (Hochberg, 1995) and replaces the component modeling the 
emission probabilities of the HMM by a neural net. This is possible, because it is shown 
Mutual Information Neural Networks for Hybrid HMM/ANN Speech Recognition 773 
in (Bourlard, 1994) that neural networks can be trained so that the output of the m-th 
neuron approximates the posterior probability p(fmlX). In this paper, an alternative 
method for constructing a hybrid system is presented. It is based on the use of discrete 
HMM's which are combined with a neural vector quantizer (VQ) in order to form a hybrid 
system. Each speech feature vector is presented to the neural network, which generates a 
firing neuron in its output layer. This neuron is processed as VQ label by the HMM's. 
There are the following arguments for this alternative hybrid approach: 
� The neural vector quantizer has to be trained on a special information theory criterion, 
based on the mutual information between network input and resulting neuron firing 
sequence. It will be shown that such a network is the optimal acoustic processor for a 
discrete HMM system, resulting in a profound mathematical theory for this approach. 
� Resulting from this theory, a formula can be derived which jointly describes the 
behavior of the HMM and the neural acoustic processor. In that way, both systems can 
be described in a unified manner and both major components of the hybrid system can 
be trained using a unified learning criterion. 
� The above mentioned theoretical background leads to the development of new neural 
network paradigms using novel training algorithms that have not been used before in 
other areas of neurocomputing, and therefore represent major challenges and issues in 
learning and training for neural systems. 
� The neural networks can be easily combined with any HMM system of arbitrary 
complexity. This leads to the combination of optimally trained neural networks with 
very powerful HMM's, having all features useful for speech recognition, e.g. triphones, 
function words, crossword triphones, etc.. Context-dependency, which is very desirable 
but relatively difficult to realize with a pure neural approach, can be left to the HMM's. 
� The resulting hybrid system has still the basic structure of a discrete system, and 
therefore has all the effective features associated with discrete systems, e.g. quick and 
easy training as well as recognition procedures, real-time capabilities, etc.. 
� The work presented in this paper has been also successfully implemented for a 
demanding speech recognition problem, the 1000 word speaker-independent continuous 
Resource Management speech recognition task. For this task, the hybrid system 
produces one of the best recognition results obtained by any speech recognition system. 
In the following section, the theoretical foundations of the hybrid approach are briefly 
explained. A unified probabilistic model for the combined HMM/ANN system is derived, 
describing the interaction of the neural and the HMM component. Furthermore, it is 
shown that the optimal neural acoustic processor can be obtained from a special 
information theoretic network training algorithm. 
2 INFORMATION THEORY PRINCIPLES FOR NEURAL 
NETWORK TRAINING 
We are considering now a neural network of arbitrary topology used as neural vector 
quantizer for a discrete HMM system. If K patterns are presented to the hybrid system 
during training, the feature vectors resulting from these patterns using any feature 
extraction method can be denoted as x(k), k=l ...K. If these feature vectors are presented to 
the input layer of a neural network, the network will generate one firing neuron for each 
presentation. Hence, all K presentations will generate a stream of firing neurons with 
length K resulting from the output layer of the neural net. This label stream is denoted as 
Y=y(1)...y(K). The label stream Y will be presented to the HMM's, which calculate the 
probability that this stream has been observed while a pattern of a certain class has been 
presented to the system. It is assumed, that M different classes fm are active in the 
774 G. Rigoil and C. Neukirchen 
system, e.g. the words or phonemes in speech recognition. Each feature vector x(k) will 
belong to one of these classes. The class frn, to which feature vector x(k) belongs is 
denoted as f(k). The major training issue for the neural network can be now formulated 
as follows: How should the weights of the network be trained, so that the network 
produces a stream of firing neurons that can be used by the discrete HMM's in an optimal 
way? It is known that HMM's are usually trained with information theory methods which 
mostly rely on the Maximum Likelihood (ML) principle. If the parameters of the hybrid 
system (i.e. transition and emission probabilities and network weights) are summarized in 
the vector _0, the probability p0_(x(k)lf(k)) denotes the probability of the pattern x at 
discrete time k, under the assumption that it has been generated by the model representing 
class f(k), with parameter set 0_. The ML principle will then try to maximize the joint 
probability of all presented training patterns x(k), according to the following Maximum 
Likelihood function: 
-- k=l - (1) 
where _0' is the optimal parameter vector maximizing this equation. Our goal is to feed 
the feature vector x into a neural network and to present the neural network output to the 
Markov model. Therefore, one has to introduce the neural network output in a suitable 
manner into the above formula. If the vector x is presented to the network input layer, and 
we assume that there is a chance that any neuron Yn, n=l...N (with network output layer 
size N) can fire with a certain probability, then the output probability p(xlf) in (1) can 
be written as: 
N N 
p(..l.Q) =  p(X,YnlZl)=  p(YnlZl) .p(xlYn,Zl) 
n=l n=l (2) 
Now, the combination of the neural component with the HMM can be made more 
obvious: In (2), typically the probability p(ynlf) will be described by the Markov model, 
in terms of the emission probabilities of the HMM. For instance, in continuous 
parameter HMM's, these probabilities are interpreted as weights for Gaussian mixtures. In 
the case of semi-continuous systems or discrete HMM's, these probabilities will serve as 
discrete emission probabilities of the codebook labels. The probability p(xlyn,f) 
describes the acoustic processor of the system and is characterizing the relation between 
the vector x as input to the acoustic processor and the label Yn, which can be considered 
as the n-th output component of the acoustic processor. This n-th output component may 
characterize e.g. the n-th Gaussian mixture component in continuous parameter HMM's, 
or the generation of the n-th label of a vector quantizer in a discrete system. This 
probability is often considered as independent of the class f and can then be expressed as 
p(xlYn). It is exactly this probability, that can be modeled efficiently by our neural 
network. In this case, the vector x serves as input to the neural network and Yn 
characterizes the n-th neuron in the output layer of the network. Using Bayes law, this 
probability can be written as: 
P(Yn Ix). P x(Z.) 
p(xlYn) = P(Yn) (3) 
yielding for (2): 
N p(YnlX) 
p(..l.(2) = p(x).  p(YnlZl). 
n=l P(Yn) 
(4) 
Using again Bayes law to express 
Mutual Information Neural Networks for Hybrid HMM/ANN Speech Recognition 775 
one obtains from (4): 
P(YnI.Q) = 
P(ZilYn )' P(Yn) 
p(ll) (5) 
px(.) N 
p(.l.O)= p(Zl' - P(ZilYn) 'P(YnlX) 
n=l (6) 
We have now modified the class-dependent probability of the feature vector x in a way 
that allows the incorporation of the probability p(ynlx). This probability allows a better 
chactefization of the behavior of the neural network, because it describes the probability 
of the vious neurons Yn, if the vector  is presented to the network input. Therefore, 
these probabilities give a good description of the inpudoutput behavior of the neural 
network. Eq. (6) can therefore be considered as probabilistic mel for the 
