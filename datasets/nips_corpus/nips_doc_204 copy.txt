396 Le Cun, Boser, Denker, Henderson, Howard, Hubbard and Jackel 
Handwritten Digit Recognition with 
Back-Propagation Network 
a 
Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, 
R. E. Howard, W. Hubbard, and L. D. Jackel 
AT&T Bell Laboratories, Holmdel, N. J. 07733 
ABSTRACT 
We present an application of back-propagation networks to hand- 
written digit recognition. Minimal preprocessing of the data was 
required, but architecture of the network was highly constrained 
and specifically designed for the task. The input of the network 
consists of normalized images of isolated digits. The method has 
1% error rate and about a 9% reject rate on zipcode digits provided 
by the U.S. Postal Service. 
1 INTRODUCTION 
The main point of this paper is to show that large back-propagation (BP) net- 
works can be applied to real image-recognition problems without a large, complex 
preprocessing stage requiring detailed engineering. Unlike most previous work on 
the subject (Denker et al., 1989), the learning network is directly fed with images, 
rather than feature vectors, thus demonstrating the ability of BP networks to deal 
with large amounts of low level information. 
Previous work performed on simple digit images (Le Gun, 1989) showed that the 
architecture of the network strongly influences the network's generalization ability. 
Good generalization can only be obtained by designing a network architecture that 
contains a certain amount of a priori knowledge about the problem. The basic de- 
sign principle is to minimize the number of free parameters that must be determined 
by the learning algorithm, without overly reducing the computational power of the 
network. This principle increases the probability of correct generalization because 
Handwritten Digit Recognition with a Back-Propagation Network 397 
� tt t .r' I 
Figure 1: Examples of original zipcodes from the testing set. 
it results in a specialized network architecture that has a reduced entropy (Denker 
et al., 1987; Patarnello and Carnevali, 1987; Tishby, Levin and Solla, 1989; Le Gun, 
1989). On the other hand, some effort must be devoted to designing appropriate 
constraints into the architecture. 
2 ZIPCODE RECOGNITION 
The handwritten digit-recognition application was chosen because it is a relatively 
simple machine vision task: the input consists of black or white pixels, the digits 
are usually well-separated from the background, and there are only ten output 
categories. Yet the problem deals with objects in a real two-dimensional space and 
the mapping from image space to category space has both considerable regularity 
and considerable complexity. The problem has added attraction because it is of 
great practical value. 
The database used to train and test the network is a superset of the one used in 
the work reported last year (Denker et al., 1989). We emphasize that the method 
of solution reported here relies more heavily on automatic learning, and much less 
on hand-designed preprocessing. 
The database consists of 9298 segmented numerals digitized from handwritten zip- 
codes that appeared on real U.S. Mail passing through the Buffalo, N.Y. post office. 
Examples of such images are shown in figure 1. The digits were written by many 
different people, using a great variety of sizes, writing styles and instruments, with 
widely varying levels of care. This was supplemented by a set of 3349 printed dig- 
its coming from 35 different fonts. The training set consisted of 7291 handwritten 
digits plus 2549 printed digits. The remaining 2007 handwritten and 700 printed 
digits were used as the test set. The printed fonts in the test set were different from 
the printed fonts in the training set. One important feature of this database, which 
398 Le Cun, Boser, Denker, Henderson, Howard, Hubbard and Jackel 
20 
I07 $( 
Figure 2: Examples of normalized digits from the testing set. 
is a common feature to all real-world databases, is that both the training set and 
the testing set contain numerous examples that are ambiguous, unclassifiable, or 
even misclassified. 
3 PREPROCESSING 
Acquisition, binarization, location of the zipcode, and preliminary segmentation 
were performed by Postal Service contractors (Wang and Srihari, 1988). Some of 
these steps constitute very hard tasks in themselves. The segmentation (separating 
each digit from its neighbors) would be a relatively simple task if we could assume 
that a character is contiguous and is disconnected from its neighbors, but neither 
of these assumptions holds in practice. Many ambiguous characters in the database 
are the result of mis-segmentation (especially broken 5's) as can be seen on figure 2. 
At this point, the size of a digit varies but is typically around 40 by 60 pixels. Since 
the input of a back-propagation network is fixed size, it is necessary to normalize 
the size of the characters. This was performed using a linear transformation to 
make the characters fit in a 16 by 16 pixel image. This transformation preserves 
the aspect ratio of the character, and is performed after extraneous marks in the 
image have been removed. Because of the linear transformation, the resulting image 
is not binary but has multiple gray levels, since a variable number of pixels in the 
original image can fall into a given pixel in the target image. The gray levels of 
each image are scaled and translated to fall within the range -1 to 1. 
4 THE NETWORK 
The remainder of the recognition is entirely performed by a multi-layer network. All 
of the connections in the network are adaptive, although heavily constrained, and 
are trained using back-propagation. This is in contrast with earlier work (Denker 
et al., 1989) where the first few layers of connections were hand-chosen constants. 
The input of the network is a 16 by 16 normalized image and the output is composed 
Handwritten Digit Recognition with a Back-Propagation Network 399 
of 10 units: one per class. When a pattern belonging to class i is presented, the 
desired output is +1 for the ith output unit, and -1 for the other output units. 
........................ ,'.' ........ :.:-.:.:.:.:.:.:.:+x-:+:+:+:-: ':..::i$i....*... . ....... ''' ' -' :' :':':' 
::::::::::::::::::::::::::::::::::: :+:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:. :::::::::::::::::::::::::::::: ...... ..::::.:.:.:.:....:.:.:. .:.:.:. :,:.:.:.::.: 
......................................................................... ............,........ : .........:.:....:.,..: ........ :.:. � .:.:.:....: ....... 
Figure 3: Input image (left), weight vector (center), and resulting feature map 
(right). The feature map is obtained by scanning the input image with a single 
neuron that has a local receptive field, as indicated. White represents -1, black 
represents +1. 
A fully connected network with enough discriminative power for the task would have 
far too many parameters to be able to generalize correctly. Therefore a restricted 
connection-scheme must be devised, guided by our prior knowledge abofit shape 
recognition. There are well-known advantages to performing shape recognition by 
detecting and combining local features. We have required our network to do this 
by constraining the connections in the first few layers to be local. In addition, if 
a feature detector is useful on one part of the image, it is likely to be useful on 
other parts of the image as well. One reason for this is that the salient features of a 
distorted character might be displaced slightly from their position in a typical char- 
acter. One solution to this problem is to scan the input image with a single neuron 
that has a local receptive field, and store the states of this neuron in corresponding 
locations in a layer called a feature map (see figure 3). This operation is equivalent 
to a convolution with a small size kernel, followed by a squashing function. The 
process can be performed in parallel by implementing the feature map as a plane 
of neurons whose weight vectors are constrained to be equal. That is, units in a 
feature map are constrained to perform the same operation on different parts of the 
image. An interesting side-effect of this weight sharing technique, already described 
in (Rumelhart, Hinton and Williams, 1986), is to reduce the number of free param- 
eters by a large amount, since a large number of units share the same weights. In 
addition, a certain level of shift invariance is present in the system: shifting the 
input will shift the result on the feature map, but will leave it unchanged otherwise. 
In practice, it will be necessary to have multiple feature maps, extracting different 
features from the same image. 
400 Le Cun, Boser, Denker, Henderson, Howard, Hubbard and Jackel 
1 2 3 4 5 6 7 8 9 10 11 12 
I X X X X X 
2 X X X X X 
3 X X X X X 
4 X X X X X 
Table 1: Connections between H2 and H3. 
The idea of local, convolutional feature maps can be applied to subsequent hidden 
layers as well, to extract features of increasing complexity and abstraction. Inter- 
esting]y, higher level features require ]ess precise coding of their location. Reduced 
precision is actually advantageous, since a slight distortion or translation of the in- 
put will have reduced effect on the representation. Thus, each feature extraction in 
our network is followed by an additional layer which performs a local averaging and 
a subsampling, reducing the resolution of the feature map. This layer introduces 
a certain level of invariance to distortions and translations. A functional module 
of our network consists of a layer of shared-weight feature maps followed by an 
averaging/subsampling layer. This is reminiscent of the Neocognitron architecture 
(Fukushima and Miyake, 1982), with the notable difference that we use backprop 
(rather than unsupervised learning) which we feel is more appropriate to this sort 
of classification problem. 
The network architecture, represented in figure 4, i
