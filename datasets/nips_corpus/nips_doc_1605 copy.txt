Uniqueness of the SVM Solution 
Christopher J.C. Burges 
Advanced Technologies, 
Bell Laboratories, 
Lucent Technologies 
Holmdel, New Jersey 
burges lucent. corn 
David J. Crisp 
Centre for Sensor Signal and 
Information Processing, 
Deptartment of Electrical Engineering, 
University of Adelaide, South Australia 
dcrisp eleceng. adelaide. edu. au 
Abstract 
We give necessary and sufficient conditions for uniqueness of the 
support vector solution for the problems of pattern recognition and 
regression estimation, for a general class of cost functions. We show 
that if the solution is not unique, all support vectors are necessarily 
at bound, and we give some simple examples of non-unique solu- 
tions. We note that uniqueness of the primal (dual) solution does 
not necessarily imply uniqueness of the dual (primal) solution. We 
show how to compute the threshold b when the solution is unique, 
but when all support vectors are at bound, in which case the usual 
method for determining b does not work. 
1 Introduction 
Support vector machines (SVMs) have attracted wide interest as a means to imple- 
ment structural risk minimization for the problems of classification and regression 
estimation. The fact that training an SVM amounts to solving a convex quadratic 
programming problem means that the solution found is global, and that if it is not 
unique, then the set of global solutions is itself convex; furthermore, if the objec- 
tive function is strictly convex, the solution is guaranteed to be unique [1] x. For 
quadratic programming problems, convexity of the objective function is equivalent 
to positive semi-definiteness of the Hessian, and strict convexity, to positive definite- 
ness [1]. For reference, we summarize the basic uniqueness result in the following 
theorem, the proof of which can be found in [1]: 
Theorem 1: The solution to a convex programming problem, for which the ob- 
jective function is strictly convex, is unique. Positive definiteness of the Hessian 
implies strict convexity of the objective function. 
Note that in general strict convexity of the objective function does not neccesarily 
imply positive definiteness of the Hessian. Furthermore, the solution can still be 
unique, even if the objective function is loosely convex (we will use the term loosely 
convex to mean convex but not strictly convex). Thus the question of uniqueness 
XThis is in contrast with the case of neural nets, where local minima of the objective 
function can occur. 
224 C. J. C. Burges and D. J. Crisp 
for a convex programming problem for which the objective function is loosely convex 
is one that must be examined on a case by case basis. In this paper we will give 
necessary and sufficient conditions for the support vector solution to be unique, 
even when the objective function is loosely convex, for both the clasification and 
regression cases, and for a general class of cost function. 
One of the central features of the support vector method is the implicit mapping 
ï¿½ of the data z  Rn to some feature space 5, which is accomplished by replacing 
dot products between data points zi, zj, wherever they occur in the train and test 
algorithms, with a symmetric function K(zi, zj), which is itself an inner product in 
 [2]: K(zi, zj) -- ((I)(zi), (I)(zj)) -- (xi,xj), where we denote the mapped points in 
 by x - (z). In order for this to hold the kernel function K must satisfy Mercer's 
positivity condition [3]. The algorithms then amount to constructing an optimal 
separating hyperplane in 5, in the pattern recognition case, or fitting the data to a 
linear regression tube (with a suitable choice of loss function [4]) in the regression 
estimation case. Below, without loss of generality, we will work in the space 5, 
whose dimension we denote by dr. The conditions we will find for non-uniqueness 
of the solution will not depend explicitly on  or . 
Most approaches to solving the support vector training problem employ the Wolfe 
dual, which we describe below. By uniqueness of the primal (dual) solution, we 
mean uniqueness of the set of primal (dual) variables at the solution. Notice that 
strict convexity of the primal objective function does not imply strict convexity of 
the dual objective function. For example, for the optimal hyperplane problem (the 
problem of finding the maximal separating hyperplane in input space, for the case 
of separable data), the primal objective function is strictly convex, but the dual 
objective function will be loosely convex whenever the number of training points 
exceeds the dimension of the data in input space. In that case, the dual Hessian 
H will necessarily be positive semidefinite, since H (or a submatrix of H, for the 
cases in which the cost function also contributes to the (block-diagonal) Hessian) is a 
Gram matrix of the training data, and some rows of the matrix will then necessarily 
be linearly dependent [5] 2 . In the cases of support vector pattern recognition and 
regression estimation studied below, one of four cases can occur: (1) both primal 
and dual solutions are unique; (2) the primal solution is unique while the dual 
solution is not; (3) the dual is unique but the primal is not; (4) both solutions 
are not unique. Case (2) occurs when the unique primal solution has more than 
one expansion in terms of the dual variables. We will give an example of case (3) 
below. It is easy to construct trivial examples where case (1) holds, and based on 
the discussion below, it will be clear how to construct examples of (4). However, 
since the geometrical motivation and interpretation of SVMs rests on the primal 
variables, the theorems given below address uniqueness of the primal solution s . 
2 The Case of Pattern Recognition 
We consider a slightly generalized form of the problem given in [6], namely to 
minimize the objective function 
F = (1/2)[[w[[ 2 +  Cif (1) 
i 
aRecall that a Gram matrix is a matrix whose ij'th element has the form {xi,xj} for 
some inner product {, ), where xi is an element of a vector space, and that the rank of a 
Gram matrix is the maximum number of linearly independent vectors xi that appear in it 
[6]. 
aDue to space constraints some proofs and other details will be omitted. Complete 
details will be given elsewhere. 
Uniqueness of the SVM Solution 225 
with constants p  [1, cw), Ci > 0, subject to constraints: 
yi(w ' xi + b) _ 1 - i, i = 1,...,l (2) 
i k 0, i=l,---,l (3) 
where w is the vector of weights, b a scalar threshold, i are positive slack variables 
which are introduced to handle the case of nonseparable data, the Yi are the polar- 
ities of the training samples (Yi 6 (+1)), xi are the images of training samples in 
the space  by the mapping , the Ci determine how much errors are penalized 
(here we have allowed each pattern to have its own penalty), and the index i labels 
the I training patterns. The goal is then to find the values of the primal variables 
(w, b, i) that solve this problem. Most workers choose p = 1, since this results in 
a particularly simple dual formulation, but the problem is convex for any p _ 1. 
We will not go into further details on support vector classification algorithms them- 
selves here, but refer the interested reader to [3], [7]. Note that, at the solution, b 
is determined from w and i by the Karush Kuhn Tucker (KKT) conditions (see 
below), but we include it in the definition of a solution for convenience. 
Note that Theorem i gives an immediate proof that the solution to the optimal 
hyperplane problem is unique, since there the objective function is just (1/2)l[w[I 2, 
which is strictly convex, and the constraints (Eq. (2) with the  variables removed) 
are linear inequality constraints which therefore define a convex set 4. 
For the discussion below we will need the dual formulation of this problem, for the 
case p = 1. It takes the following form: minimize  Eij OtiOtjYiYj{Xi,Xj) -- -].i Oti 
subject to constraints: 
_Y 0, alP_0 (4) 
Ci = -i + m (5) 
) = o (6) 
i 
and where the solution takes the form w = -']-i otiYiXi, and the KKT conditions, 
which are satisfied at the solution, are rlii = O, oq(yi(w.xi + b) - 1 + i) = 0, where 
r/i are Lagrange multipliers to enforce positivity of the i, and ci are Lagrange 
multipliers to enforce the constraint (2). The r/i can be implicitly encapsulated 
in the condition 0 _ oi _ Ci, but we retain them to emphasize that the above 
equations imply that whenever i  0, we must have ci - Ci. Note that, for a 
given solution, a support vector is defined to be any point xi for which ci > 0. Now 
suppose we have some solution to the problem (1), (2), (3). Let Af denote the set 
{i: Yi = 1, w 'xi + b < 1}, Af2 the set {i: Yi = -1, w 'xi + b > -1}, JV'a the set 
(i: Yi = 1, w.xi + b = 1), Af4 the set (i: Yi = -1, w. xi + b = -1), JV'5 the set 
(i: yi = 1, w.xi +b  1), and Af6 the set (i: yi = -1, w.xi + b (-1). Then we 
have the following theorem: 
Theorem 2: The solution to the soft-margin problem, (1), (2) and (3), is unique 
for p > 1. For p = 1, the solution is not unique if and only if at least one of the 
following two conditions holds: 
c, = (7) 
Furthermore, whenever the solution is not unique, all solutions share the same w, 
and any support vector xi has Lagrange multiplier satisfying cq = Ci, and when (7) 
4This is of course not a new result: see for example [3]. 
226 C. J. C. Burges and D. J.. Crisp 
holds, then Afa contains no support vectors, and when (8) holds, then iV'4 contains 
no support vectors. 
Proof: For the case p > 1, the objective function F is strictly convex, since a 
sum of strictly convex functions is a strictly convex function, and since the function 
g(v) - v p, v  + is strictly convex for p > 1. Furthermore the constraints define 
a convex set, since any set of simultaneous linear inequality constraints defines a 
convex set. Hence by Theorem i the solution is unique. 
For the case p
