Bias, Variance and the Combination of 
Least Squares Estimators 
Ronny Meir 
Faculty of Electrical Engineering 
Technion, Haifa 32000 
Israel 
rmer)ee. t echnon. ac. 1 
Abstract 
We consider the effect of combining several least squares estimators 
on the expected performance of a regression problem. Computing 
the exact bias and variance curves as a function of the sample size 
we are able to quantitatively compare the effect of the combination 
on the bias and variance separately, and thus on the expected error 
which is the sum of the two. Our exact calculations, demonstrate 
that the combination of estimators is particularly useful in the case 
where the data set is small and noisy and the function to be learned 
is unrealizable. For large data sets the single estimator produces 
superior results. Finally, we show that by splitting the data set 
into several independent parts and training each estimator on a 
different subset, the performance can in some cases be significantly 
improved. 
Key words: Bias, Variance, Least Squares, Combination. 
1 INTRODUCTION 
Many of the problems related to supervised learning can be boiled down to the 
question of balancing bias and variance. While reducing bias can usually be ac- 
complished quite easily by simply increasing the complexity of the class of models 
studied, this usually comes at the expense of increasing the variance in such a way 
that the overall expected error (which is the sum of the two) is often increased. 
296 Ronny Meir 
Thus, many efforts have been devoted to the issue of decreasing variance, while at- 
tempting to keep the concomitant increase in bias as small as possible. One of the 
methods which has become popular recently in the neural network community is 
variance reduction by combining estimators, although the idea has been around in 
the statistics and econometrics literature at least since the late sixties (see Granger 
1989 for a review). Nevertheless, it seems that not much analytic work has been 
devoted to a detailed study of the effect of noise and an effectively finite sample size 
on the bias/variance balance. It is the explicit goal of this paper to study in detail 
a simple problem of linear regression, where the full bias/variance curve can be 
computed exactly for any effectively finite sample size and noise level. We believe 
that this simple and exactly solvable model can afford us insight into more complex 
non-linear problems, which are at the heart of much of the recent work in neural 
networks. 
A further aspect of our work is related to the question of the partitioning of the 
data set between the various estimators. Thus, while most studies assume the each 
estimator is trained on the complete data set, it is possible to envisage a situation 
where the data set is broken up into several subsets, using each subset of data to form 
a different estimator. While such a scheme seems wasteful from the bias point of 
view, we will see that in fact it produces superior forecasts in some situations. This, 
perhaps suprising, result is due to a large decrease in variance resulting from the 
independence of the estimators, in the case where the data subsets are independent. 
2 ON THE COMBINATION OF ESTIMATORS 
The basic objective of regression is the following: given a finite training set, D, com- 
posed of n input/output pairs, D = {(x,y)}:, drawn according to an unkown 
distribution P(x, y), find a function ('estimator'), f(x; D), which 'best' approxi- 
mates y. Using the popular mean-squared error criterion and taking expectations 
with respect to the data distribution one finds the well-known separation of the 
error into a bias and variance terms respectively (Geman et al. 1992) 
�(x) = (ED f(x; D) - E[ylx])  + ED If(x; D) - E f(x; D)]  (1) 
We consider a data source of the form y = g(x) + r/, where the 'target' function g(x) 
is an unknown (and potentially non-linear) function and r/ is a Gaussian random 
variable with zero mean and variance a2. Clearly then E[ylx] = g(x). 
In the usual scenario for parameter estimation one uses the complete data set, D, 
to form an estimator f(x; D). In this paper we consider the case where the data set 
nc 
D is broken up into K subsets (not necessarily disjoint), such that D =, ,k=._. , 
and a separate estimator is found for each subset. The full estimator is then given 
by the linear combination (Granger 1989) 
K 
k=l 
The optimal values of the parameters b can be easily obtained if the data distribu- 
tion, P(x,y), is known, by simply minimizing the mean-squared error (Granger 
Bias, Variance and the Combination of Least Squares Estimators 297 
1989). In the more typical case where this distribution is unkown, one may 
resort to other schemes such as least-squares fitting for the parameter vector 
b = {b,..., bK}. The bias and variance of the combined estimator can be simply 
expressed in this case, and are given by 
k=l k,k  
(3) 
where the overbars denote an average with respect to the data. It is immediately 
apparent that the variance term is composed of two contributions. The first term, 
corresponding to k = k , simply computes a weighted average of the single estimator 
variances, while the second term measures the average covariance between the dif- 
ferent estimators. While the first term in the variance can be seen to decay as 
in the case where all the weights bk are of the same order of magnitude, the second 
term is finite unless the covariances between estimators are very small. It would 
thus seem beneficial to attempt to make the estimators as weakly correlated as pos- 
sible in order to decrease the variance. Observe that in the extreme case where the 
data sets are independent of each other, the second term in the variance vanishes 
identically. Note that the bias term depends only on single estimator properties 
and can thus be computed from the theory of the single estimator. As mentioned 
above, however, the second term in the variance expression explicitly depends on 
correlations between the different estimators, and thus requires the computation of 
quantities beyond those of single estimators. 
3 THE SINGLE LINEAR ESTIMATOR 
Before considering the case of a combination of estimators, we first review the case 
of a single linear estimator, given by f(x; D) = T . x, where  is estimated from 
the data set D. Following BSs et al. (1993) we further assume that the data arises 
through an equation of the form y = g(x) + /with g = g(w0 T � x). Looking back 
at equations (3) it is clear that the bias and variance are explicit functions of x 
and the weight vector w0. In order to remove the explicit dependence we compute 
in what follows expectations with respect to the probability distribution of x and 
w0, denoted respectively by ELI. ] and E0[.]. Thus, we define the averaged bias and 
variance by B = EoEr[B(x;wo)] and V = EoEr[V(x;wo)] and the expected error 
is then � = B + V. 
In this work we consider least-squares estimation which corresponds to minimizing 
the empirical error, �emp(W, D) = IIXw - Y]l 2, where X is the n x d data matrix, 
Y is the n x I output vector and w is a d x I weight vector. The components of 
the 'target' vector Y are given by y, = g(w0 T � x) + /, where /, are i.i.d normal 
random variables with zero mean and variance a 2. Note that while we take the 
estimator itself to be linear we allow the target function g(.) to be non-linear. This 
is meant to model the common situation where the model we are trying to fit is 
inadequate, since the correct model (even it exists) is usually unkown. 
Thus, the least squares estimator is given by   argminw �emp(W, D). Since in 
this case the error-function is quadratic it possesses either a unique global minimum 
298 Ronny Meir 
or a degenerate manifold of minima, in the case where the Hessian matrix, X'X, 
is singular. 
The solution to the least squares problem is well known (see for example Scharf 
1991), and will be briefly summarized. When the number of examples, n, is smaller 
than the input dimension, d, the problem is underdetermined and there are many 
solutions with zero empirical error. The solutions can be written out explicitly in 
the form 
r = XT(XXT)-Y + (I- XT(XXT)-X) V (n < d), (4) 
where V is an arbitrary d-dimensional vector. It should be noted that any vector w 
satisfying this equation (and thus any least-squares estimator) becomes singular as 
n approaches d from below, since the matrix XX T becomes singular. The minimal 
norm solution, often referred to as the Moore-Penrose solution, is given in this case 
by the choice V = 0. It is common in the literature to neglect the study of the 
underdetermined regime since the solution is not unique in this case. We however 
will pay specific attention to this case, corresponding to the often prevalent situa- 
tion where the amount of data is small, attempting to show that the combination 
of estimators approach can significantly improve the quality of predictors in this 
regime. Moreover, many important inverse problems in signal processing fall into 
this category (Scharf 1991). 
In the overdetermined case, n > d (assuming the matrix X to be of full rank), 
a zero error solution is possible only if the function g(.) is linear and there is no 
noise, namely Eta/a] = 0. In any other case, the problem becomes unrealizable and 
the minimum error is non-zero. In any event, in this regime the unique solution 
minimizing the empirical error is given by 
= (xTx)-xY (n > d). 
It is eay to see that this estimator is unbiased for linear g(.). 
In order to compute the bias and variance for this model we use Eqs. (3) with 
K = 1 and b} = 1. In order to actually compute the expectations with respect to 
x and the weight vector w0 we assume in what follows that the random vector x 
is distributed according to a multi-dimensional normal distributions of zero mean 
and covariance matrix (1/d)I. The vector w0 is similarly d
