Exponentially many local minima for single 
neurons 
Peter Auer 
Mark Herbster 
Manfred K. Warmuth 
Department of Computer Science 
Santa Cruz, California 
{pauer, mark,manfred} @cs.ucsc.edu 
Abstract 
We show that for a single neuron with the logistic function as the transfer 
function the number of local minima of the error function based on the 
square loss can grow exponentially in the dimension. 
1 INTRODUCTION 
Consider a single artificial neuron with d inputs. The neuron has d weights w E R e. The 
output of the neuron for an input pattern x E R e is 0 = 5(x � w), where 5: R -- R 
is a transfer function. For a given sequence of training examples ((xt, yt))st_<,, each 
consisting of a pattern xt E R e and a desired output yt E R, the goal of the training phase 
for neural networks consists of minimizing the error function with respect to the weight 
vector w E R e. This function is the sum of the losses between outputs of the neuron and 
the desired outputs summed over all training examples. In notation, the error function is 
m 
where L � R x R -- [0, oc) is the loss function. 
A common example of a transfer function is the logistic function logistic(z) =  which 
l+e-z 
has the bounded range (0, 1). In contrast, the identity function id(z) = z has unbounded 
range. One of the most common loss functions is the square loss L(y, 0) - (Y - )2. Other 
examples are the absolute loss lY - 01 and the entropic loss yln 
We show that for the square loss and the logistic function the error function of a single 
neuron for n training examples may have Ln/dJ t local minima. More generally, this holds 
for any loss and transfer function for which the composition of the loss function with the 
transfer function (in notation L(y, qS(x. w)) is continuous and has bounded range. This 
Exponentially Many Local Minima for Single Neurons 317 
0.62 
0.6 
0.58 
0.56 
0.54 
0.52 
0.5 
0.48 
-12 
-lO 
-8 _ -8 
log w2 log wl 
-2 
o o 
14 
-12 
Figure 1: Error Function with 25 Local Minima (16 Visible), Generated by 10 Two- 
Dimensional Examples. 
proves that for any transfer function with bounded range exponentially many local minima 
can occur when the loss function is the square loss. 
The sequences of examples that we use in our proofs have the property that they are non- 
realizable in the sense that there is no weight vector w 6 R  for which the error function 
is zero, i.e. the neuron cannot produce the desired output for all examples. We show with 
some minimal assumptions on the loss and transfer functions that for a single neuron there 
can be no local minima besides the global minimum if the examples are realizable. 
If the transfer function is the logistic function then it has often been suggested in the 
literature to use the entropic loss in artificial neural networks in place of the square loss 
[BW88, WD88, SLF88, Wat92]. In that case the error function of a single neuron is 
convex and thus has only one minimum even in the non-realizable case. We generalize this 
observation by defining a matching loss for any differentiable increasing transfer functions 
05: 
--- / O-- 1 () 
LO(Y, .0) (05(z) - y) dz . 
Jb-l(y) 
The loss is the area depicted in Figure 2a. If 05 is the identity function then L 0 is the square 
loss likewise if 05 is the logistic function then L0 is the entropic loss. For the matching loss 
the gradient descent update for minimizing the error function for a sequence of examples 
is simply 
Whew := Wola -- r/ 05(xt' Wola) - yt)xt , 
where r/is a positive learning rate. Also the second derivatives are easy to calculate for 
this general setting: L(y,,O(x,.w)) = 05'(xt � w)zt,izt,j. Thus, if Ht(w) is the Hessian 
Ow, Owj 
of Lo(yt, 05(x, � w)) with respect to w then v 'Ht(w)v = 05'(x, � w)(v- x,) =. Thus 
318 P. AUER, M. HERBSTER, M. K. WARMUTH 
y 
1 
0.8 
0.6 
0.4 
0.2 
-4 -2 0 2 4 6 
(a) (b) 
Figure 2: (a) The Matching Loss Function L,. 
(b) The Square Loss becomes Saturated, the Entropic Loss does not. 
Ht is positive semi-definite for any increasing differentiable transfer function. Clearly 
yt= Ht(w) is the Hessian of the error function E(w) for a sequence of m examples and 
it is also positive semi-definite. It follows that for any differentiable increasing transfer 
function the error function with respect to the matching loss is always convex. 
We show that in the case of one neuron the logistic function paired with the square loss 
can lead to exponentially many minima. It is open whether the number of local minima 
grows exponentially for some natural data. However there is another problem with the 
pairing of the logistic and the square loss that makes it hard to optimize the error function 
with gradient based methods. This is the problem of flat regions. Consider one example 
(x, y) consisting of a pattern x (such that x is not equal to the all zero vector) and the 
desired output y. Then the square loss (logistic(x � w) - y)2, for y E [0, 1] and w E R a, 
turns flat as a function of w when .0 = logistic(x � w) approaches zero or one (for example 
see Figure 2b where d = 1 and y = 0). It is easy to see that for all bounded transfer 
functions with a finite number of minima and corresponding bounded loss functions, the 
same phenomenon occurs. In other words, the composition L(y, 5(x. w)) of the square 
loss with any bounded transfer function 5 which has a finite number of extrema turns flat as 
Ix. w[ becomes large. Similarly, for multiple examples the error function E(w) as defined 
above becomes flat. In flat regions the gradients with respect to the weight vector w are 
small, and thus gradient-based updates of the weight vector may have a hard time moving 
the weight vector out of these flat regions. This phenomenon can easily be observed in 
practice and is sometimes called saturation [Hay94]. In contrast, if the logistic function 
is paired with the entropic loss (see Figure 2b), then the error function turns flat only at the 
global minimum. The same holds for any increasing differentiable transfer function and its 
matching loss function. 
A number of previous papers discussed conditions necessary and sufficient for multiple 
local minima of the error function of single neurons or otherwise small networks [WD88, 
SS89, BRS89, Blu89, SS91, GT92]. This previous work only discusses the occurrence of 
multiple local minima whereas in this paper we show that the number of such minima can 
grow exponentially with the dimension. Also the previous work has mainly been limited 
to the demonstration of local minima in networks or neurons that have used the hyperbolic 
tangent or logistic function with the square loss. Here we show that exponentially many 
minima occur whenever the composition of the loss function with the transfer function is 
continuous and bounded. 
The paper is outlined as follows. After some preliminaries in the next section, we give formal 
Exponentially Many Local Minima for Single Neurons 319 
0 35 
03 
0 25 
uJ 
O2 
0.15 
0.1 
0.05 
-2 -1 0 1 
Figure 3: 
1 
0.9 
0.6 
07 
too6 
o.5 
04 
03 
02 
ol 
2 3 4 6 7 -6 -6 -4 -2 0 2 4 6 8 
w Iogw 
(a) (b) 
(a) Error Function for the Logistic Transfer Function and the 
Square Loss with Examples ((10, .55), (.7, .25)) 
(b) Sets of Minima can be Combined. 
statements and proofs of the results mentioned above in Section 3. At first (Section 3.1) we 
show that n one-dimensional examples might result in n local minima of the error function 
(see e.g. Figure 3a for the error function of two one-dimensional examples). From the local 
minima in one dimension it follows easily that n d-dimensional examples might result in 
Ln/d j a local minima of the error function (see Figure 1 and discussion in Section 3.2). 
We then consider neurons with a bias (Section 4), i.e. we add an additional input that is 
clamped to one. The error function for a sequence of examples $ = ((xt,ye))<e<,,, is 
now 
/92 
Es(B, w) - E L(yt,cfi(B + wxt)), 
t--1 
where B denotes the bias, i.e. the weight of the input that is clamped to one. We can prove 
that the error function might have [n/2dl a local minima if loss and transfer function are 
symmetric. This holds for example for the square loss and the logistic transfer function. 
The proofs are omitted due to space constraints. They are given in the full paper [AHW96], 
together with additional results for general loss and transfer functions. 
Finally we show in Section 5 that with minimal assumptions on transfer and loss functions 
that there is only one minimum of the error function if the sequence of examples is realizable 
by the neuron. 
The essence of the proofs is quite simple. At first observe that if loss and transfer function are 
bounded and the domain is unbounded, then there exist areas of saturation where the error 
function is essentially flat. Furthermore the error function is additive i.e. the error function 
produced by examples in $ U $' is simply the error function produced by the examples in 
$ added to the error function produced by the examples in $, Esus, = Es + Es,. Hence 
the local minima of Es remain local minima of Eau.s, if they fall into an area of saturation 
of Es. Similarly, the local minima of Es, remain local minima of Esus, as well (see 
Figure 3b). In this way sets of local minima can be combined. 
2 PRELIMINARIES ' 
We introduce the notion of minimum-containing set which will prove useful for counting 
the minima of the error function. 
320 P. AUER, M. HERBSTER, M. K. WARMUTH 
Definition 2.1 Let f � Ra-R be a continuous function. Then an open and bounded set 
U  R e is called a minimum-containing set for f if for each w on the boundary of U there 
is a w* 6 U such that f(w*) < f(w). 
Obviously any minimum-containing set contains alocal minimum of the respective function. 
Furthermore each of n disjoint minimum-containing sets contains a distinct local minimum. 
Thus it is sufficient to find n disjoint minimum-containing sets in order to show 
