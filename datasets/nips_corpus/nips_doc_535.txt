Adaptive Soft Weight Tying 
using Gaussian Mixtures 
Steven J. Nowlan 
Computational Neuroscience Laboratory 
The Salk Institute, P.O. Box 5800 
San Diego, CA 92186-5800 
Geoffrey E. Hinton 
Department of Computer Science 
.University of Toronto 
Toronto, Canada M5S 1A4 
Abstract 
One way of simpli'ing neural networks so they generalize better is to add 
an extra term Io the error function that will penalize complexity. We 
propose a new penalty term in which the distribution of weight values 
is modelled as a mixture of multiple gaussians. Under this model, a set 
of weights is simple if the weights can be clustered into subsets so that 
the weights in each cluster have similar values. We allow the parameters 
of the mixture model to adapt at t. he same time as the network learns. 
Simulations demonstrate that this complexity term is more effective than 
previous complexity terms. 
1 Introduction 
A major problem in training artificial neural networks is to ensure that they will 
generalize well to cas{' that lhey hv  not been Irareed on. Some recent theoretical 
results (Baum and ttaussler, 19S9) Iave ugge[ed that in order to guarautec good 
generaJizalion lhe anlounl of lt'orll;tio required o dr,'cl iecit} lh  ouLput 
vectors of all the training cases ml.qt be considerably larger than the nmilber of 
independent weight> n the network in many practical probles there s only 
a small amount of labelled data available for trmning and this creates problems 
for any approach that uses a large, homogeneous network with many independent 
weights. As a result. there h been mubh recent, interest in techniques [hat can 
train large networks with relatively sinall amounts of labelled data and still provide 
good generalization performance. 
In order to improve generalization, the number of free parameters in [he network 
must be reduced. One of the oldest and simplest approaches to removing excess 
degrees of freedom from a network is to add an extra term o the error ftmcton 
993 
994 Nowlan and Hinton 
that penalizes complexity: 
cost = data-misfit + X complexity (1) 
During learning, the network is trying to find a locally optimal trade-off between 
the data-misfit (the usual error term) and the complexity of the net. The relative 
importance of these two terms can be estimated by finding the value of X that 
optimizes generalization to a validation set. Probably the simplest approximation 
to complexity is the sum of the squares of the weights, i w. Differentiating 
this complexity measure leads to simple weight decay (Plaut, Nowlan and Hinton, 
1986) in which each weight decays towards zero at a rate that is proportional to its 
magnitude. This decay is countered by the gradient of the error term, so weights 
which are not critical to network performance, and hence always have small error 
gradients, decay away leaving only the weights necessary to solve the problem. 
The use of a. i w? penalty term can also be interpreted from a Bayesian 
perspective. l The complexity of a set of weights, Xi w, may be described 
as its negal.iw.  log probal)ility density under a radially symmetric gaussian prior 
distribution on the weights. The distribution is centered at the origin a. nd has vari- 
ance 1/X. For multilayer networks, it is hard to find a good theoretical justification 
for this prior, but Ilinton (1987)justities it empirically by showing that it greatly 
improves generalization on a very diIficult task. More recently, Mackay (1991) has 
shown that even better generalization can be achieved by using different values of 
X for the weights in different layers. 
2 A more complex measure of network complexity 
If we wish to elimina. te sinall weights without forcing large weights away from the 
values they need to model the data, we can use a prior which is a mixture of a 
narrow (n) and a broad (b) gaussian, both centered at zero. 
I 0 1 - ' 
= vr;,, + 
where . and  are the mixing proportions of the two gaussians and are therefore 
constrained to sum to 1. 
Assuming that, the weight values were generated Dom a gaussian mixture, the con- 
ditional probability that a particular weight, wi, was generated by a particular 
gaussian, j, is called the responsibility of that gaussian for the weight and is: 
(3) 
= 
where pj(wi) is the probability density of wi under gaussian j. 
When the mixing proportions of the two ga. ussians are comparable, the ua. rrow gus- 
sian gets most of the responsibility for a snall weight. Adopting the Bayesian per- 
2  
spective, the cost of a weight under the narrow gaussian is proportional to w /2a5. 
As long as a, is quite small there will be strong pressure to reduce the magnitude 
R. Szeliski, personal communication, 1985. 
Adaptive Soft Weight Tying using Gaussian Mixtures 995 
of small weights even further. Conversely, the broad gaussian takes most of the 
responsibility for large weight values, so there is much less pressure to reduce them. 
In the limiting case when the broad gaussian becomes a uniform distribution, there 
is ahnost no pressure to reduce very large weights because they are almost certainly 
generated by the unitbrm distribution. A complexity term very similar to this limit- 
ing case is used in the weight elimination technique of (Weigend, Huberman and 
Rumelhart, 1990) to improve generalization for a time series prediction task. '-' 
3 Adaptive Gaussian Mixtures and Soft Weight-Sharing 
A mixture of a narrow, zero-mean gaussian with a broad gaussian or a uniform allows 
us to favor networks with many near-zero weights, and this improves generalization 
on many tasks. But practical experience with hand-coded weight constraints has 
also shown that great improvements can be achieved by constraining particular 
subsets of the weights to share the same value (Lang, Waibel and Hinton, 1990; Le 
Cun, 1989). Mixtures of zero-mean gausstans and uniforms cannot imllement this 
type of symmetry constraiut. If however, we use multiple gausstans and allow their 
means and variances to adapt as the network learns, we can implenient a soft 
version of weight.-sharing in which tile learning algorit. lnn decides for itself whicl 
weights should be tied together. (We may also allow the nixing proportions to 
adapt so that we are uot assuing all sets of tied weights are the same size.) 
The basic idea is that a gaussian which takes responsibility for a subset of the 
weights will squeeze those weights together since it can then have a lower variance 
and assign a ligher probability density to each weight. If the gaussias all start 
with high variance, the iuitiaI division of weights into subsets will be very soft. As 
the variances shrink and the network learns, the decisions about how to group the 
weights into subsets are influenced by the task the network is learning to perlbrn. 
To make these intuitive ideas a bit more concrete, we lnay define a cost function of 
the general form giwm in (1): 
- . , :(y - d) 2 - log 7rjpj(wi) (4) 
where a is the variauce of the squared error and each pj (wi) is a gaussian density 
with mean pj and standard deviation aj. We optimize this function by ad.justing 
the wi and the mixture parameters ?rj, pj, and aj, and ay.a 
The partial derivative of C vith respect to each weight is the sum of the usual 
squared error derivative and a term due to the complexity cost for the weight: 
2See (Nowlan, 1991) for a precise description of the relationship between mixture models 
and the model use(t by (Weigend, l:luberman and Rumelhart, 1990). 
3 2 
1/er s may be thought of as playing the same role Ks X in equation 1 in determining a 
trade-off between the misfit and complexity costs. K is a normalizing factor based on a 
gaussian error model. 
996 Nowlan and Hinton 
Method Train % Correct Test % Correct 
Vanilla Back Prop. 100.0 q- 0.0 67.3 q- 5.7 
Cross Valid. 98.8 q- 1.1 83.5 q- 5.1 
Weight Elimination 100.0 q- 0.0 89.8 q- 3.0 
Soft-share - 5 Comp. 100.0 q- 0.0 95.6 q- 2.7 
Soft-share - 10 Conq). 100.0 q- 0.0 97.1 q- 2.1 
Table 1: Summary of generalization performance of 5 different training techniques 
on the shift detection problem. 
The derivative of the complexity cost term is simply a weighted sum of the difference 
between the weight value and the center of each of the gaussians. The weighting 
factors are the responsibility measures defined in equation 3 and if over time a 
single gaussian claims most of the responsibility for a particular weight the effect 
of the complexity cost term is simply to pull the weight towards the center of the 
responsible gaussian. The strength of this tbrce is inversely proportional to the 
variance of the gaussian. 
In the simulations described below, all of the parameters (wi, Ij, o'j, Zrj) are updated 
simultaneousl using a conjugate gradient descent procedure. To prevent variances 
shrinking too fast or going negative we optimize logj rather than j. To ensure 
that the mixing proportions sum to I and are positive, we optimize xj where zrj = 
exp(xj)/ exp(zi). For further details see (Nowlan and Hinton, 1992). 
4 Simulation Results 
We compared the generalization performance of soft weight-tying to other tech- 
niques on two differeut problends. The first problem, a 20 input, one output shift 
detection network, was chosen because it was binary problen for whicl solutions 
which generalize well exhibit a lot of repeated weight structure. The generalization 
performmce ot' et, works traied using the cost criterion given in equation 4 was 
compared to networks trained in three other ways: No cost term to penalize com- 
plexity; No explicit complexity cost. term, but use of a validation set to terminate 
learning; Weight elimination (\Veiged, Huberman a. nd Rumelhart, 1990) 4. The 
simulation results are summarized in Table 1. 
The network had 20 input units, 10 hidden units, and a single output unit and 
contained 101 weights. The first 10 input u
