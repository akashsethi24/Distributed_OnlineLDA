How to Dynamically Merge Markov 
Decision Processes 
Satinder Singh 
Department of Computer Science 
University of Colorado 
Boulder, CO 80309-0430 
baveja@cs.colorado.edu 
David Cohn 
Adaptive Systems Group 
Harlequin, Inc. 
Menlo Park, CA 94025 
cohn@harlequin. com 
Abstract 
We are frequently called upon to perform multiple tasks that com- 
pete for our attention and resource. Often we know the optimal 
solution to each task in isolation; in this paper, we describe how 
this knowledge can be exploited to efficiently find good solutions 
for doing the tasks in parallel. We formulate this problem as that of 
dynamically merging multiple Markov decision processes (MDPs) 
into a composite MDP, and present a new theoretically-sound dy- 
namic programming algorithm for finding an optimal policy for the 
composite MDP. We analyze various aspects of our algorithm and 
illustrate its use on a simple merging problem. 
Every day, we are faced with the problem of doing multiple tasks in parallel, each 
of which competes for our attention and resource. If we are running a job shop, 
we must decide which machines to allocate to which jobs, and in what order, so 
that no jobs miss their deadlines. If we are a mail delivery robot, we must find the 
intended recipients of the mail while simultaneously avoiding fixed obstacles (such 
as walls) and mobile obstacles (such as people), and still manage to keep ourselves 
sufficiently charged up. 
Frequently we know how to perform each task in isolation; this paper considers how 
we can take the information we have about the individual tasks and combine it to 
efficiently find an optimal solution for doing the entire set of tasks in parallel. More 
importantly, we describe a theoretically-sound algorithm for doing this merging 
dynamically; new tasks (such as a new job arrival at a job shop) can be assimilated 
online into the solution being found for the ongoing set of simultaneous tasks. 
1058 S. Singh and D. Cohn 
1 The Merging Framework 
Many decision-making tasks in control and operations research are naturally formu- 
lated as Markov decision processes (MDPs) (e.g., Bertsekas & Tsitsiklis, 1996). Here 
we define MDPs and then formulate what it means to have multiple simultanous 
MDPs. 
1.1 Markov decision processes (MDPs) 
An MDP is defined via its state set $, action set A, transition probability matrices 
P, and payoff matrices R. On executing action a in state s the probability of 
transiting to state s  is denoted Pa(SS ) and the expected payoff associated with 
that transition is denoted Ra(sst). We assume throughout that the payoffs are 
non-negative for all transitions. A policy assigns an action to each state of the 
MDP. The value of a state under a policy is the expected value of the discounted 
sum of payoffs obtained when the policy is followed on starting in that state. The 
objective is to find an optimal policy, one that maximizes the value of every state. 
The optimal value of state s, V*(s), is its value under the optimal policy. 
The optimal value function is the solution to the Bellman optimality equations: for 
all s G $, V(s) - maxaeA(-.s, pa(sst)[Ra(ss)+fV(s)]), where the discount factor 
0 _ 7 < I makes future payoffs less valuable than more immediate payoffs (e.g., 
Bertsekas & Tsitsiklis, 1996). It is known that the optimal policy r* can be de- 
termined from V* as follows: r*(s) = argmaxeA(7'.s, P(ss)[R(sg) + 7V*(st)]). 
Therefore solving an MDP is tantamount to computing its optimal value function. 
1.2 Solving MDPs via Value Iteration 
Given a model ($, A, P, R) of an MDP value iteration (e.g., Bertsekas & Tsitsiklis, 
1996) can be used to determine the optimal value function. Starting with an initial 
guess, V0, iterate for all s V+(s) = maxaeA('.8,es P(sst)[R(sg) + 7V(g)]). It 
is known that maxses [V+(s)- V*(s)l _ 7maxss IVy(s)- V*(s)[ and therefore 
V converges to V* as k goes to infinity. Note that a Q-value (Watkins, 1989) based 
version of value iteration and our algorithm presented below is also easily defined. 
1.3 Multiple Simultaneous MDPs 
The notion of an optimal policy is well defined for a single task represented as 
an MDP. If, however, we have multiple tasks to do in parallel, each with its own 
state, action, transition probability, and payoff spaces, optimal behavior is not 
automatically defined. We will assume that payoffs sum across the MDPs, which 
means we want to select actions for each MDP at every time step so as to maximize 
the expected discounted value of this summed payoff over time. If actions can be 
chosen independently for each MDP, then the solution to this composite MDP 
is obvious -- do what's optimal for each MDP. More typically, choosing an action 
for one MDP constrains what actions can be chosen for the others. In a job shop 
for example, actions correspond to assignment of resources, and the same physical 
resource may not be assigned to more than one job simultaneously. 
Formally, we can define a composite MDP as a set of N MDPs {Mi} N. We will use 
superscripts to distinguish the component MDPs, e.g., $i, Ai, pi, and R i are the 
state, action, transition probability and payoff parameters of MDP Mi. The state 
space of the composite MDP, $, is the cross product of the state spaces of the com- 
ponent MDPs, i.e., $ = $ x $2 x ... x $N. The constraints on actions implies that 
How to Dynamically Merge Markov Decision Processes 1059 
the action set of the composite MDP, A, is some proper subset of the cross product 
of the N component action spaces. The transition probabilities and the payoffs of 
the composite MDP are factorial because the following decompositions hold: for 
all s, s   $ and a  A, Pa(ss) N 
= IIi=xP a' (sis r) and R(ss ') = Y=i Ra' (ssr) ï¿½ 
Singh (1997) has previously studied such factoffal MDPs but only for the case of a 
fixed set of components. 
The optimal value function of a composite MDP is well defined, and satisfies the 
following Bellman equation: for all s  S, 
N 
V(s) = max E (IIiN=1Pa'(sisi')[ERa'(sisi')+/V(s)])' (1) 
aA 
s$ i=1 
Note that the Bellman equation for a composite MDP assumes an identical discount 
factor across component MDPs and is not defined otherwise. 
1.4 The Dynamic Merging Problem 
Given a composite MDP, and the optimal solution (e.g. the optimal value function) 
for each of its component MDPs, we would like to efficiently compute the optimal 
solution for the composite MDP. More generally, we would like to compute the 
optimal composite policy given only bounds on the value functions of the component 
MDPs (the motivation for this more general version will become clear in the next 
section). To the best of our knowledge, the dynamic merging question has not been 
studied before. 
Note that the traditional treatment of problems such as job-shop scheduling would 
formulate them as nonstationary MDPs (however, see Zhang and Dietterich, 1995 
for another learning approach). This normally requires augmenting the state space 
to include a time component which indexes all possible state spaces that could 
arise (e.g., Bertsekas, 1995). This is inefficient, and potentially infeasible unless we 
know in advance all combinations of possible tasks we will be required to solve. One 
contribution of this paper is the observation that this type of nonstationary problem 
can be reformulated as one of dynamically merging (individually) stationary MDPs. 
1.4.1 The naive greedy policy is suboptimal 
Given bounds on the value functions of the component MDPs, one heuristic com- 
posite policy is that of selecting actions according to a one-step greedy rule: 
N 
'(s) = argmax(E rliN=l Pa (sisi' )[E(Ra (si, ai) + ffXi(si' ))]), 
a s i=1 
where X i is the upper or lower bound of the value function, or the mean of the 
bounds. It is fairly easy however, to demonstrate that these policies are substantially 
suboptimal in many common situations (see Section 3). 
2 Dynamic Merging Algorithm 
Consider merging N MDPs; job-shop scheduling presents a special case of merging 
a new single MDP with an old composite MDP consisting of several factor MDPs. 
One obvious approach to finding the optimal composite policy would be to directly 
perform value iteration in the composite state and action space. A more efficient 
approach would make use of the solutions (bounds on optimal value functions) of 
the existing components; below we describe an algorithm for doing this. 
1060 S. Singh and D. Cohn 
Our algorithm will assume that we know the optimal values, or more generally, 
upper and lower bounds to the optimal values of the states in each component 
MDP. We use the symbols L and U for the lower and upper bounds; if the optimal 
value function for the i th factor MDP is available then L i = U i = V*'i. 1 
Our algorithm uses the bounds for the component MDPs to compute bounds on 
the values of composite states as needed and then incrementally updates and nar- 
rows these initial bounds using a form of value iteration that allows pruning of 
actions that are not competitive, that is, actions whose bounded values are strictly 
dominated by the bounded value of some other action. 
Initial State: The initial composite state so is composed from the start state of 
all the factor MDPs. In practice (e.g. in job-shop scheduling) the initial composite 
state is composed of the start state of the new job and whatever the current state 
of the set of old jobs is. Our algorithm exploits the initial state by only updating 
states that can occur from the initial state under competitive actions. 
Initial Value Step: When we need the value of a composite state s for the first 
time, we compute upper and lower bounds to its optimal value as follows: L(s) - 
N Li(si), and U(s)= -.ilv=l Ui(s). 
max/= 1 
Initial Update Step: We dynamically allocate upper and lower bound storage 
space for composite states as we first update them. We also create the initial set
