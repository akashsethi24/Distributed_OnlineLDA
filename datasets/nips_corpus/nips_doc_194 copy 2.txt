282 Kanerva 
Contour-Map Encoding of Shape for Early Vision 
Pentti Kanerva 
Research Institute for Advanced Computer Science 
Mail Stop 230-5, NASA Ames Research Center 
Moffett Field, California 94035 
ABSTRACT 
Contour maps provide a general method for 
recognizing two-dimensional shapes. All but 
blank images give rise to such maps, and people 
are good at recognizing objects and shapes 
from them. The maps are encoded easily in 
long feature vectors that are suitable for 
recognition by an associative memory. These 
properties of contour maps suggest a role for 
them in early visual perception. The prevalence 
of direction-sensitive neurons in the visual 
cortex of mammals supports this view. 
INTRODUCTION 
Early vision refers here to the first stages of visual 
perception of an experienced (adult human) observer. 
Overall, visual perception results in the identification of 
what is being viewed: We recognize an image as the letter A 
because it looks to us like other As we have seen. Early 
vision is the beginning of this process of identification-- 
the making of the first guess. 
Early vision cannot be based on special or salient 
features. For example, we normally think of the letter A 
as being composed of two slanted strokes, / and \, meeting 
at the top and connected in the middle by a horizontal 
stroke, -. The strokes and their coincidences define all 
the features of A. However, we recognize the As in Figure 1 
even though the strokes and the features, if present at all, 
do not stand out in the images. 
Contour. Map Encoding of Shape for Early Vision 283 
Most telling about human vision is that we can recognize 
such As after seeing more or less normal As only. The 
challenge of early vision, then, is to find general encoding 
mechanisms that turn these quite dissimilar images of the 
same object into similar internal representations while 
leaving the representations of different objects dissimilar; 
and to find basic pattern-recognition mechanisms that work 
with these representations. Since our main work is on 
associative memories, we have been interested in ways to 
encode images into long feature vectors suitable for such 
memories. The contour-map method of this paper encodes a 
variety of images into vectors for associative memories. 
REPRESENTING AN IMAGE AS A CONTOUR MAP 
Images take many forms: line drawings, silhouettes, 
outlines, dot-matrix pictures, gray-scale pictures, color 
pictures, and the like, and pictures that combine all these 
elements. Common to all is that they occupy a region of 
(two-dimensional) space. An early representation of an 
image should therefore be concerned with how the image 
controls its space or, in technical terms, how might it be 
represented as a field. 
Let us consider first a gray-scale image. It defines 
a field by how dark it is in different places (image 
intensity--a scalar field--the image itself is the field). 
A related field is given by how the darkness changes from 
place to place (gradient of intensity--a vector field). 
Neither one is quite right for recognizing As because 
reversing the field (turning dark to light and light to 
dark) leaves us with the same A. However, the dark- 
and-light reversal leaves the contour lines of the image 
unchanged (i.e., lines of uniform intensity--technically 
a tangent field perpendicular to the gradient field). My 
proposal is to base initial recognition on the contour 
lines. 
In line drawings and black-and-white images, which have 
only two darkness levels or colors, the contour lines are 
not well defined. This is overcome by propagating the lines 
and the edges of the image outward and inward over areas of 
o 
o o 
o o 
o o o 0 
o o 
::::::::::::::::::::: 
::::::::::::::::::::: 
FIGURE 1. Various kinds of As. 
284 Kanerva 
uniform image intensity, in the manner of contour lines, 
roughly parallel to the lines and the edges. Figure 2 shows 
only a few such lines, but, in fact, the image is covered 
with them, running roughly parallel to each other. As a 
rule, exactly one contour line runs through any given point. 
Computing its direction is discussed near the end of the 
paper. 
ENCODING THE CONTOUR MAP 
Table 1 shows how the direction of the contour at a point 
can be encoded in three trits (-1, 0, 1 ternary variables). 
The code divides 180 degrees into six equal sectors and 
assigns a codeword to each sector. The distance between 
two codewords is the number of (Hamming) units by which 
the words differ (L1 distance). The code is circular, and 
the distance between codewords is related directly to the 
difference in direction: Directions 30, 60, and 90 degrees 
apart are encoded with words that are 2, 4, and 6 units 
apart, respectively. The code wraps around, as do tangents, 
so that directions 180 degrees apart are encoded the same. 
For finer discrimination we would use some finer circular 
code. The zero-word 000, which is equally far from all 
other words in the code, is used for points at which the 
direction of the contour is ill-defined, such as the very 
centers of circles. 
This encoding makes the direction of the contour at any 
point on a map into a three-component vector. To encode 
the entire map, the vector field is sampled at a fixed, 
finite set of points, and the encodings of the sample points 
are concatenated in fixed order into a long vector. In 
preliminary studies we have used small sample sizes: 7 x 5 
(= 35) sample points, each encoded into three trits, for a 
total vector of (3 x 35 =) 105 trits, and 8 x 8 sample 
points by three trits for a total vector of 192 trits. 
FIGURE 2. Propagating the contour. 
Contour-Map Encoding of Shape for Early Vision 285 
For an example, Figure 3 shows the digit 4 drawn on a 
21-by-15-pixel grid. It also shows a 7 x 5 sampling grid 
laid over the image and the direction of the contour at 
the sample points (shown by short line segments). Below 
the image are the three-trit encodings of the sample points 
starting at the upper left corner and progressing by rows, 
concatenated into a 105-trit encoding of the entire image. 
In this encoding, + means +1 and - means -1. 
From Positions of the Code to Directional Sensors 
Each position of the three-trit code can be thought of as a 
directional sensor. For example, the center position senses 
contours at 90 degrees, plus or minus 45 degrees: It is 1 
when the direction of the contour is closer to vertical than 
to horizontal (see Table 1). Similarly, each position of 
the long (105-trit) code for the entire map can be thought 
of as a sensor for a specific direction--plus or minus--at 
a specific location on the map. 
An array of sensors will thus encode an image. The 
sensors are like the direction-sensitive cells of the visual 
cortex. Such cells, of course, are not laid down with 
perfect regularity over the cortex, but that does not mean 
TABLE 1 
Coarse Circular Code for 
Direction of Contour 
Direction, 
degrees 
Codeword 
0 + 15 1 -1 1 
30 + 15 -1 -1 1 
60 + 15 -1 1 1 
90 + 15 -1 1 -1 
120 + 15 1 1 -1 
150 + 15 1 -1 -1 
180 + 15 1 -1 1 
Undefined 0 0 0 
-++ -++ -++ --+ ++- 
-++ -++ -++ -+- -+- 
__+ --+ --+ -+- -+- 
--+ -++ 000 -+- -+- 
000 +-+ +-+ +-+ --+ 
+-- +-+ +-+ -+- -+- 
+-- +-- ++- ++- -++ 
FIGURE 3. Encoding an image. 
286 Kanerva 
that they could not perform as encoders. Accordingly, a 
direction-sensitive cell can be thought of as a feature 
detector that encodes for a certain direction at a certain 
location in the visual or attentional field. An irregular 
array of randomly oriented sensors laid over images would 
produce perfectly good encodings of their contour maps. 
COMPARING TWO CONTOUR MAPS 
How closely do two contour maps resemble each other? For 
simplicity, we will compare maps of equal size (and shape) 
only. The maps are compared point to point. The difference 
at a point is the difference in the direction of the contour 
at that point on the two maps--that is, the magnitude of the 
lesser of the two angles made by the two contour lines that 
run through the two points that correspond to each other 
on the two maps. The maximum difference at a point is 
therefore 90 degrees. The entire maps are then compared 
by adding the pointwise differences over all the points (by 
integrating over the area of the map). 
The purpose of the encoding is to make the comparing of 
maps simple. The code is so constructed that the difference 
of two maps at a point is roughly proportional to the 
distance between the two (3-trit) codewords--one from each 
map--for that point. We need not even concern ourselves 
with the finding of the lesser of the two angles made by the 
crossing of the two contours; the distance between codewords 
accounts for that automatically. 
Entire maps are then compared by adding together the 
distances at the (35) sample points. This is equivalent 
to computing the distance between the (105-trit) codewords 
for the two maps. This distance is proportional to the 
difference between the maps, and it is approximately so 
because the maps are sampled at a small number of points 
and because the direction at each point is coded coarsely. 
COMPUTING THE DIRECTION OF THE CONTOUR 
We have not explored widely how to compute contours from 
images and merely outline here one method, not exactly 
biological, that works for line drawings and two-tone images 
and that can be generalized to gray-scale images and even 
to many multicolor images. We have also experimented with 
oriented, difference-of-Gaussian filters of Parent and 
Zucker (1985) and with cortex transforms of Watson (1987). 
The contours are based on a simple model of attraction, 
akin to gravity, by assuming that the lines and the edges 
of the image attract according to their distance from the 
point. The net attraction at any point on the image defines 
Contour-Map Encoding of Shape for Early Vision 287 
a gradient field, and the contours are perpendicular to it. 
In practice we work with pixe
