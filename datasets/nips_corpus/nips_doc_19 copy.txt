534 
The Performance of Convex Set Projection Based Neural Networks 
Robert J. Marks II, Les E. Atlas, Seho Oh and James A. Ritcey 
Interactive Systems Design Lab, FT-10 
University of Washington, Seattle, Wa 98195. 
ABSTRACT 
We donsider a class of neural networks whose performance can be 
analyzed and geometrically visualized in a signal space 
environment. Alternating projection neural networks (APNN's) 
perform by alternately projecting between two or more constraint 
sets. Criteria for desired and unique convergence are easily 
established. The network can be configured in either a homogeneous 
or layered form. The number of patterns that can be stored in the 
network is on the order of the number of input and hidden neurons. 
If the output neurons can take on only one of two states, then the 
trained layered APNN can be easily configured to converge in one 
iteration. More generally, convergence is at an exponential rate. 
Convergence can be improved by the use of sigmoid type 
nonlinearities, network relaxation and/or increasing the number of 
neurons in the hidden layer. The manner in which the network 
responds to data for which it was not specifically trained (i.e. 
how it generalizes) can be directly evaluated analytically. 
1. INTRODUCTION 
In this paper, we depart from the performance analysis 
techniques normally applied to neural networks. Instead, a signal 
space approach is used to gain new insights via ease of analysis 
and geometrical interpretation. Building on a foundation laid 
elsewhere 1-3 , we demonstrate that alternating projecting neural 
network's (APNN's) formulated from such a viewpoint can be 
configured in layered form or homogeneously. 
Significiantly, APNN's have advantages over other neural 
network architectures. For example, 
(a) APNN's perform by alternatingly projecting between two or more 
constraint sets. Criteria can be established for proper 
iterative convergence for both synchronous and asynchronous 
operation. This is in contrast to the more conventional 
technique of formulation of an energy metric for the neural 
networks, establishing a lower energy bound and showing that 
the energy reduces each iteration 4-7 Such procedures generally 
do not address the accuracy of the final solution. In order to 
assure that such networks arrive at the desired globally 
minimum energy, computationaly lengthly procedures such as 
simulated annealing are used s-l� For synchronous networks, 
steady state oscillation can occur between two states of the 
same energy ll 
(b) Homogeneous neural networks such as Hopfield's content 
addressable memory 4'12-4 do not scale well, i.e. the capacity 
@ American Institute of Physics 1988 
535 
of Hopfield's neural networks less than doubles when the number 
of neurons is doubled 15-6 Also, the capacity of previously 
proposed layered neural networks 17,18 is not well understood. 
The capacity of the layered APNN'S, on the other hand, is 
roughly equal to the number of input and hidden neurons 19 
(c) The speed of backward error propagation learning 17-1s can be 
painfully slow. Layered APNN's, on the other hand, can be 
trained on only one pass through the training data 2 . If the 
network memory does not saturate, new data can easily be 
learned without repeating previous data. Neither is the 
effectiveness of recall of previous data diminished. Unlike 
layered back propagation neural networks, the APNN recalls by 
iteration. Under certain important applications, however, the 
APNN will recall in one iteration. 
(d) The manner in which layered APNN's generalizes to data for 
which it was not trained can be analyzed straightforwardly. 
The outline of this paper is as follows. After establishing the 
dynamics of the APNN in the next section, sufficient criteria for 
proper convergence are given. The convergence dynamics of the APNN 
are explored. Wise use of nonlinearities, e.g. the sigmoidal type 
nonlinearities 2, improve the network's performance. Establishing a 
hidden layer of neurons whose states are a nonlinear function of 
the input neurons' states is shown to increase the network's 
capacity and the network's convergence rate as well. The manner in 
which the networks respond to data outside of the training set is 
also addressed. 
2. THE ALTERNATING PROJECTION NEURAL NETWORK 
In this section, we established the notation for the APNN. 
Nonlinear modificiations to the network made to impose certain 
performance attributes are considered later. 
Consider a set of N continuous level linearly independent 
library vectors (or patterns) of length L > N: { 3 n I 0nN }. We form 
the library matrix  = [31 I I.%13 N ] and the neural network 
interconnect matrix a T = F T F F T where the superscript T 
denotes transposition. We divide the L neurons into two sets: one 
in which the states are known and the remainder in which the states 
are unknown. This partition may change from application to 
application. Let Sk (M) be the state of the k th node at time M. If 
the k th node falls into the known catego, its state is clamped to 
the known value (i.e. Sk (M) = /k where f is some library vector). 
The states of the remaining floating neurons are equal to the sum 
of the inputs into the node. That is, s k (M) = ik, where 
L 
i k = Z tpk S (1) 
P =1 P 
a The interconnect matrix is better trained iteratively 2 . To include 
a new library vector 3, the interconnects are updated as 
T . T 
+ () / ( ) where 
536 
If all neurons change state simultaneously (i.e. sp = sp (M-l)), then 
the net is said to operate synchronously� If only one neuron changes 
state at a time, the network is operating asynchronously. 
Let P be the number of clamped neurons� We have proven 1 that the 
neural states converge strongly to the extrapolated library vector 
if the first P rows of  (denoted F_m) form a matrix of full column 
rank� That is, no column of F_m can be expressed as a linear 
combination of those remaining. By strong convergence b we mean 
lira II (M) -  [[ 0 where II [[Z   ' 
  X T X. 
M ) 
Lastly, note that subsumed in the criterion that F_m be full 
rank is the condition that the number of library vectors not exceed 
the number of known neural states (Pk N). Techniques to bypass this 
restriction by using hidden neurons are discussed in section 5. 
Partition Notation: Without loss of generality� we will assume 
that neurons 1 through P are clamped and the remaining neurons are 
floating. We adopt the vector partitioning notation 
where p is the P-tuple of the first P elements of ? and Q is a 
vector of the remaining Q=L-P. We can thus write� for example� F_m = 
[  I I...IN  ]. Using this partition notation� we can define 
the neural clamping operator by: 
Thus� the first P elements of  are clamped to P The remaining Q 
nodes float 
Partition notation for the interconnect matrix will also prove 
useful. Define 
where 2 is a P by P and 4 a Q by Q matrix. 
3. STEADY STATE CONVERGENCE PROOFS 
For purposes of later reference, we address convergence of the 
network for synchronous operation. Asynchronous operation is 
addressed in reference 2. For proper convergence, both cases 
require that F_m be full rank. For synchronous operation, the 
network iteration in (1) followed by clamping can be written as: 
s(M+l) =   s(M) (2) 
As is illustrated in 1-3 this operation can easily be visualized 
� 
in an L dimensional signal space� 
b The referenced convergence proofs prove strong convergence in an 
infinite dimensional Hilbert space. In a discrete finite 
dimensional space, both strong, and weak convergence imply 
uniform convergence 9'2�, i.e. s(M) ) as M--. 
537 
For a given partition with P clamped neurons, 
written in partitioned form as 
(2) can be 
=  (3) 
Q (M+I 3 4 iM 
The states of the P clamped neurons are not affected by their input 
sum. Thus, there is no contribution to the iteration by 1 and 2' 
We can equivalently write (3) as 
s Q (M+I) = T 3 P + T 4 s Q (M) (4) 
We show in that if F_m is full rank, then the spectral radius 
(magnitude of the maximum eigenvalue) of 4 is strictly less than 
one 19 It follows that the steady state solution of (4) is: 
= - T f 
-- --4 (5) 
where, since F_m is full rank, we have made use of our claim that 
s Q () = Q (6) 
4. CONVERGENCE DYNAMICS 
In this section, we explore different convergence dynamics of 
the APNN when F_m is full column rank. If the library matrix 
displays certain orthogonality characteristics, or if there is a 
single output (floating) neuron, convergence can be achieved in a 
single iteration. More generally, convergence is at an exponential 
rate. Two techniques are presented to improve convergence. The 
first is standard relaxation. Use of nonlinear convex constraint at 
each neuron is discussed elsewhere 2,19 
One Step Converqence: There are at least two important cases where 
the APNN converges other than uniformly in one iteration. Both 
require that the output be bipolar (�1). Convergence is in one 
step in the sense that 
Q 
7 0 = sign s (1) (7) 
where the vector operation sign takes the sign of each element of 
the vector on which it operates. 
CASE 1: If there is a single output neuron, then, from (4), (5) and 
(6), s�(1) = (1 - tLL ) /O Since the eigenvalue of the (scalar) 
matrix, 4 = tLL /ies between zero and one 19, we conclude that 1 - 
tLL> O. Thus, if is restricted to �1, (7) follows immediately. A 
technique to extend this result to an arbitrary number of output 
neurons in a layered network is discussed in section 7. 
CASE 2: For certain library matrices, the APNN can also display one 
step convergence. We showed that if the columns of  are orthogonal 
and the columns of F_m are also orthogonal, then one synchronous 
iteration results in floating states proportional to the steady 
538 
state values 19 Specifically, for the floating neurons, 
An important special case of (8) is when the elements of F are 
all +_1 and or
