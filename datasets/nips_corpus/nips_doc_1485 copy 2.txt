A Precise Characterization of the Class of 
Languages Recognized by Neural Nets under 
Gaussian and other Common Noise Distributions 
Wolfgang Maass* 
Inst. for Theoretical Computer Science, 
Technische Universitit Graz 
Klosterwiesgasse 32/2, 
A-S010 Graz, Austria 
email: maass@igi.tu-graz.ac.at 
Eduardo D. Sontag 
Dep. of Mathematics 
Rutgers University 
New Brunswick, NJ 08903, USA 
email: sontag @hilbert.rutgers.edu 
Abstract 
We consider recurrent analog neural nets where each gate is subject to 
Gaussian noise, or any other common noise distribution whose probabil- 
ity density function is nonzero on a large set. We show that many regular 
languages cannot be recognized by networks of this type, for example 
the language {w C {0, 1}* I w begins with 0}, and we give a precise 
characterization of those languages which can be recognized. This result 
implies severe constraints on possibilities for constructing recurrent ana- 
log neural nets that are robust against realistic types of analog noise. On 
the other hand we present a method for constructingfeedforward analog 
neural nets that are robust with regard to analog noise of this type. 
1 Introduction 
A fairly large literature (see [Omlin, Giles, 1996] and the references therein) is devoted 
to the construction of analog neural nets that recognize regular languages. Any physical 
realization of the analog computational units of an analog neural net in technological or 
biological systems is bound to encounter some form of imprecision or analog noise at 
its analog computational units. We show in this article that this effect has serious conse- 
quences for the computational power of recurrent analog neural nets. We show that any 
analog neural net whose computational units are subject to Gaussian or other common 
noise distributions cannot recognize arbitrary regular languages. For example, such analog 
neural net cannot recognize the regular language {w C {0, 1}'1 w begins with 0}. 
* Partially supported by the Fonds zur FOrderung der wissenschaftlichen Forschnung (FWF), Aus- 
tria, project P12153. 
282 W. Maass and E. D. Sontag 
A precise characterization of those regular languages which can be recognized by such 
analog neural nets is given in Theorem 1.1. In section 3 we introduce a simple technique 
for making feedforward neural nets robust with regard to the same types of analog noise. 
This method is employed to prove the positive part of Theorem 1.1. The main difficulty in 
proving Theorem 1.1 is its negative part, for which adequate theoretical tools are introduced 
in section 2. 
Before we can give the exact statement of Theorem 1.1 and discuss related preceding work 
we have to give a precise definition of computations in noisy neural networks. From the 
conceptual point of view this definition is basically the same as for computations in noisy 
boolean circuits (see [Pippenger, 1985] and [Pippenger, 1990]). However it is technically 
more involved since we have to deal here with an infinite state space. 
We will first illustrate this definition for a concrete case, a recurrent sigmoidal neural net 
with Gaussian noise, and then indicate the full generality of our result, which makes it 
applicable to a very large class of other types of analog computational systems with analog 
noise. Consider a recurrent sigmoidal neural net ./V' consisting of n units, that receives 
at each time step t an input ut from some finite alphabet U (for example U = {0, 1}). 
The internal state of./V' at the end of step t is described by a vector xt E [-1, 1] n, which 
consists of the outputs of the n sigmoidal units at the end of step t. A computation step of 
the network ./V' is described by 
Xt+ 1 --- G(Wx t q- h q- utc q- Vt) 
where W  nxn and c, h  11 n represent weight matrix and vectors, o' is a sigmoidal 
activation function (e.g., a(y) = 1/(1 + e-U)) applied to each vector component, and 
V1, V2,. � � is a sequence of n-vectors drawn independently from some Gaussian distribu- 
tion. In analogy to the case of noisy boolean circuits [Pippenger, 1990] one says that this 
network ./V' recognizes a language L C_ U* with reliability s (where s  (0, �] is some 
given constant) if immediately after reading an arbitrary word w  U* the network ./V' is 
with probability _>  + s in an accepting state in case that w  L, and with probability 
< � - s in an accepting state in case that w  L . 
We will show in this article that even if the parameters of the Gaussian noise distribution for 
each sigmoidal unit can be determined by the designer of the neural net, it is impossible to 
find a size n, weight matrix W, vectors h, c and a reliability s E (0, -}] so that the resulting 
recurrent sigmoidal neural net with Gaussian noise accepts the simple regular language 
{to {0, 1}*l to begins with 0} with reliability s. This result exhibits a fundamental 
limitation for making a recurrent analog neural net noise robust, even in a case where the 
noise distribution is known and of a rather benign type. This quite startling negative result 
should be contrasted with the large number of known techniques for making a feedforward 
boolean circuit robust against noise, see [Pippenger, 1990]. 
Our negative result turns out to be of a very general nature, that holds for virtually all related 
definitions of noisy analog neural nets and also for completely different models for analog 
computation in the presence of Gaussian or similar noise. Instead of the state set [-1, 1] n 
one can take any compact set  C_ 11 ' , and instead of the map (x, u)  Wx + h + uc one 
can consider an arbitrary map ]' �  x U .  for a compact set  C- 11 n where f (., u) is 
Borel measurable for each fixed u  U. Instead of a sigmoidal activation function a and a 
Gaussian distributed noise vector V it suffices to assume that a � 11 n .  is some arbitrary 
Borel measurable function and V is some 11 n-valued random variable with a density b(.) 
that has a wide support 2. In order to define a computation in such system we consider for 
According to this definition a network iV' that is after reading some w  U* in an accepting state 
1 1 
with probability strictly between [ - s and [ + s does not recognize any language L C_ U*. 
2More precisely: We assume that there exists a subset fl0 of fl and some constant co > 0 such 
Analog Neural Nets with Gaussian Noise 283 
each u  U the stochastic kernel K, defined by K,(x,A):= Prob [a(f(x, u) + V)  A] 
for a:  ft and A C_ fl. For each (signed, Borel) measure bt on ft, and each u  U, we 
let  be the (signed, Borel) measure defined on fl by ()(A) := f K(, A)d() . 
Note that  is a probability measure whenever  is. For any sequence of inputs w = 
u, ..., u, we consider the composition of the evolution operators ' 
 :2 or_ 1 o...o 1 (1) 
If the probability distribution of states at any given instant is given by the measure , then 
the distribution of states after a single computation step on input u E U is given by  , 
and after r computation steps on inputs w = u,..., u, the new distribution is , 
where we are using the notation (1). In picular, if the system stats at a pticular initial 
state , then the distribution of states after r computation steps on w is  , where  is 
the probability measure concentrated on {}. at is to say, for each measurable subset 
FCfi 
Prob[z+ E F lz = , input = w] = (8)(F). 
We fix an initial state  E fi, a set of accepting or final states F, and a reliability 
level s > 0, and say that the resulting noisy analog computational system M recognizes 
the language L C U* if for all w E U* � 
- 1 
1 
In general a neural network that simulates a DFA will carry out not just one, but a fixed 
number k of computation steps (=state transitions) of the form a:' = a(Wx + h + 
ttc+ V) for each input symbol tt  U that it reads (see the constructions described in 
[Omlin, Giles, 1996], and in section 3 of this article). This can easily be reflected in our 
model by formally replacing any input sequence w = u, tt2, ..., tt,. from U* by a padded 
sequence tb u, b t-, tt2 b t- , b t- from (U U {b})* where b is a blank sym- 
bol not in U, and b k-  denotes a sequence of k - 1 copies of b (for some arbitrarily fixed 
k _> 1). This completes our definition of language recognition by a noisy analog compu- 
tational system M with discrete time. This definition essentially agrees with that given in 
[Maass, Orponen, 1997]. 
We employ the following common notations from formal language theory: We write w w2 
for the concatenation of two strings w and w2, U for the set of all concatenations of r 
strings from U, U* for the set of all concatenations of any finite number of strings from U, 
and UV for the set of all strings ww2 with w 6 U and w2 6 V. The main result of this 
article is the following: 
Theorem 1.1 Assume that U is some arbitrary finite alphabet. A language L C_ U* can 
be recognized by a noisy analog computational system of the previously specified type if 
and only if L = E [J U'E2 for two finite subsets E and E2 of U*. 
A corresponding version of Theorem 1.1 for discrete computational systems was previously 
shown in [Rabin, 1963]. More precisely, Rabin had shown that probabilistic automata with 
strictly positive matrices can recognize exactly the same class of languages L that occur 
in our Theorem 1.1. Rabin referred to these languages as definite languages. Language 
recognition by analog computational systems with analog noise has previously been in- 
vestigated in [Casey, 1996] for the special case of bounded noise and perfect reliability 
that the following two properties hold: b(v) > co for all v 6 Q := a - (f0) -  (that is, Q is the 
set consisting of all possible differences z - y, with a(z)  fo and y 6 2) and a - (f0) has finite 
and nonzero Lebesgue measure m0 = X (a -1 (f0)) � 
284 W. Maass and E. D. Son tag 
(i.e. fll-ll_<,O(v)dv = 1 
