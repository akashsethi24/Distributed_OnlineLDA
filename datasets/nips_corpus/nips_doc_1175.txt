For valid generalization, the size of the 
weights is more important than the size 
of the network 
Peter L. Bartlett 
Department of Systems Engineering 
Research School of Information Sciences and Engineering 
Australian National University 
Canberra, 0200 Australia 
Peter.Bartlettllanu.edu.au 
Abstract 
This paper shows that if a large neural network is used for a pattern 
classification problem, and the learning algorithm finds a network 
with small weights that has small squared error on the training 
patterns, then the generalization performance depends on the size 
of the weights rather than the number of weights. More specifi- 
cally, consider an t-layer feed-forward network of sigmoid units, in 
which the sum of the magnitudes of the weights associated with 
each unit is bounded by A. The misclassification probability con- 
verges to an error estimate (that is closely related to squared error 
on the training set) at rate O((cA)t(t+l)/2v/(logn)/m ) ignoring 
log factors, where m is the number of training patterns, n is the 
input dimension, and c is a constant. This may explain the gen- 
eralization performance of neural networks, particularly when the 
number of training examples is considerably smaller than the num- 
ber of weights. It also supports heuristics (such as weight decay 
and early stopping) that attempt to keep the weights small during 
training. 
I Introduction 
Results from statistical learning theory give bounds on the number of training exam- 
ples that are necessary for satisfactory generalization performance in classification 
problems, in terms of the Vapnik-Chervonenkis dimension of the class of functions 
used by the learning system (see, for example, [13, 5]). Baum and Haussler [4] 
used these results to give sample size bounds for multi-layer threshold networks 
Generalization and the Size of the Weights in Neural Networks 135 
that grow at least as quickly as the number of weights (see also [7]). However, 
for pattern classification applications the VC-bounds seem loose; neural networks 
often perform successfully with training sets that are considerably smaller than the 
number of weights. This paper shows that for classification problems on which neu- 
ral networks perform well, if the weights are not too big, the size of the weights 
determines the generalization performance. 
In contrast with the function classes and algorithms considered in the VC-theory, 
neural networks used for binary classification problems have real-valued outputs, 
and learning algorithms typically attempt to minimize the squared error of the 
network output over a training set. As well as encouraging the correct classification, 
this tends to push the output away from zero and towards the target values of 
{-1, 1}. It is easy to see that if the total squared error of a hypothesis on m 
examples is no more than me, then on no more than me/(1 - )2 of these examples 
can the hypothesis have either the incorrect sign or magnitude less than a. 
The next section gives misclassification probability bounds for hypotheses that are 
distinctly correct in this way on most examples. These bounds are in terms of 
a scale-sensitive version of the VC-dimension, called the fat-shattering dimension. 
Section 3 gives bounds on this dimension for feedforward sigmoid networks, which 
imply the main results. The proofs are sketched in Section 4. Full proofs can be 
found in the full version [2]. 
2 Notation and bounds on misclassification probability 
Denote the space of input patterns by X. The space of labels is {-1, 1}. We 
assume that there is a probability distribution P on the product space X x {-1, 1}, 
that reflects both the relative frequency of different input patterns and the relative 
frequency of an expert's classification of those patterns. The learning algorithm 
uses a class of real-valued functions, called the hypothesis class H. An hypothesis h 
is correct on an example (x,y) if sgn(h(x)) = y, where sgn(c)' ll -+ {-1, 1} takes 
value i iff a >_ 0, so the misclassification probability (or error) of h is defined as 
erp(h) = P{(x,y)  X x {-1,1}'sgn(h(x))  y}. 
The crucial quantity determining misclassification probability is the fat-shattering 
dimension of the hypothesis class H. We say that a sequence x,..., xd of d points 
from X is shattered by H if functions in H can give all classifications of the sequence. 
That is, for all b = (bl,...,bin)  {-1, 1} m there is an h in H satisfyingsgn(h(xi)) = 
bi. The VC-dimension of H is defined as the size of the largest shattered sequence.  
For a given scale parameter 7 > 0, we say that a sequence x,..., xd of d points 
from X is 7-shattered by H if there is a sequence r,..., rd of real values such that 
for all b = (b,...,bm)  {-1, 1} m there is an h in H satisfying (h(xi)- ri)bi _ 7' 
The fat-shattering dimension of H at '7, denoted fatH(7), is the size of the largest 
y-shattered sequence. This dimension reflects the complexity of the functions in the 
class H, when examined at scale 7. Notice that fatH(7) is a nonincreasing function 
of 7. The following theorem gives generalization error bounds in terms of fatH (7). 
A related result, that applies to the case of no errors on the training set, will appear 
in [12]. 
Theorem I Define the input space X, hypothesis class H, and probability distri- 
bution P on X x {-1,1} as above. Let 0 < J < 1/2, and 0 < 7 < 1. Then, 
with probability 1 -  over the training sequence (x, y),..., (xm, ym) of rn labelled 
In fact, according to the usual definition, this is the VC-dimension of the class of 
thresholded versions of functions in H. 
136 P. L. Bartlett 
examples, every hypothesis h in H satisfies 
1 
erp(h) < --I{i'lh(x)l < 7 orsgn(h(xi))  Y}l + e(7, rn, J), 
m 
where 
e(7, re, J) = 2 (dln(50em/d) log(1250m) + ln(4/5)) 
-- , 
m 
and d-- fatH(7/16). 
(1) 
2.1 Comments 
It is informative to compare this result with the standard VC-bound. In that case, 
the bound on misclassification probability is 
1 (c )/2 
ere(h) < --I{i 'sgn(h(xi))  Yi}l+ (dlog(m/d) +log(i/J)) , 
m 
where d = VCdim(H) and c is a constant. We shall see in the next section that 
there are function classes H for which VCdim(H) is infinite but fatH(7) is finite 
for all 7 > 0; an example is the class of functions computed by any two-layer neu- 
ral network with an arbitrary number of parameters but constraints on the size of 
the parameters. It is known that if the learning algorithm and error estimates are 
constrained to make use of the sample only by considering the proportion of train- 
ing examples that hypotheses misclassify, there are distributions P for which the 
second term in the VC-bound above cannot be improved by more than log factors. 
Theorem i shows that it can be improved if the learning algorithm makes use of 
the sample by considering the proportion of training examples that are correctly 
classified and have Ih(x)l < 7. it is possible to give a lower bound (see the full 
paper [2]) which, for the function classes considered here, shows that Theorem 1 
also cannot be improved by more than log factors. 
The idea of using the magnitudes of the values of h(xi) to give a more precise 
estimate of the generalization performance was first proposed by Vapnik in [13] 
(and was further developed by Vapnik and co-workers). There it was used only for 
the case of linear hypothesis classes. Results in [13] give bounds on misclassification 
probability for a test sample, in terms of values of h on the training and test data. 
This result is extended in [11], to give bounds on misclassification probability (that 
is, for unseen data) in terms of the values of h on the training examples. This is 
further extended in [12] to more general function classes, to give error bounds that 
are applicable when there is a hypothesis with no errors on the training examples. 
Lugosi and Pintdr [9] have also obtained bounds on misclassification probability in 
terms of similar properties of the class of functions containing the true regression 
function (conditional expectation of y given x). However, their results do not extend 
to the case when the true regression function is not in the class of real-valued 
functions used by the estimator. 
It seems unnatural that the quantity 7 is specified in advance in Theorem 1, since 
it depends on the examples. The full paper [2] gives a similar result in which the 
statement is made uniform over all values of this quantity. 
3 The fat-shattering dimension of neural networks 
Bounds on the VC-dimension of various neural network classes have been established 
(see [10] for a review), but these are all at least linear in the number of parameters. 
In this section, we give bounds on the fat-shattering dimension for several neural 
network classes. 
Generalization and the Size of the Weights in Neural Networks 137 
We assume that the input space X is some subset of 1 n. Define a sigmoid unit 
as a function from 1 n to 1, parametrized by a vector of weights to 6 1 n. The 
unit computes z  rr(z � w), where rr is a fixed bounded function satisfying a 
Lipchitz condition. (For simplicity, we ignore the offset parameter. It is equivalent 
to including an extra input with a constant value.) A multi-layer feed-forward 
sigmoid network of depth � is a network of sigmoid units with a single output unit, 
which can be arranged in a layered structure with � layers, so that the output of 
a unit passes only to the inputs of units in later layers. We will consider networks 
in which the weights are bounded. The relevant norm is the �1 norm: for a vector 
w E 11 , define Iltollx - g Itoil. The following result gives a bound on the fat- 
shattering dimension of a (bounded) linear combination of real-valued functions, in 
terms of the fat-shattering dimension of the basis function class. We can apply this 
result in a recursive fashion to give bounds for single output feed-forward networks. 
Theorem 2
