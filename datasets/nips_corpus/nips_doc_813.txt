Estimating analogical similarity by dot-products 
of Holographic Reduced Representations. 
Tony A. Plate 
Department of Computer Science, University of Toronto 
Toronto, Ontario, Canada M5S 1A4 
email: tap @ ai.utoronto.ca 
Abstract 
Models of analog retrieval require a computationally cheap method of 
estimating similarity between a probe and the candidates in a large pool 
of memory items. The vector dot-product operation would be ideal for 
this purpose if it were possible to encode complex structures as vector 
representations in such a way that the superficial similarity of vector 
representations reflected underlying structural similarity. This paper de- 
scribes how such an encoding is provided by Holographic Reduced Rep- 
resentations (HRRs), which are a method for encoding nested relational 
structures as fixed-width distributed representations. The conditions un- 
der which structural similarity is reflected in the dot-product rankings of 
HRRs are discussed. 
1 INTRODUCTION 
Gentnet and Markman (1992) suggested that the ability to deal with analogy will be a 
Watershed or Waterloo for connectionist models. They identified structural alignment 
as the central aspect of analogy making. They noted the apparent ease with which people 
can perform structural alignment in a wide variety of tasks and were pessimistic about the 
prospects for the development of a distributed connectionist model that could be useful in 
performing structural alignment. 
In this paper I describe how Holographic Reduced Representations (HRRs) (Plate, 1991; 
Plate, 1994), a fixed-width distributed representation for nested structures, can be used 
to obtain fast estimates of analogical similarity. A HRR is a high dimensional vector, 
1109 
1110 Plate 
and the vector dot-product of two HRRs is an efficiently computable estimate of the 
overall similarity between the two structures represented. This estimate reflects both 
surface similarity and some aspects of structural similarity,  even though alignments are 
not explicitly calculated. I also describe contextualization, an enrichment of HRRs designed 
to make dot-product comparisons of HRRs more sensitive to structural similarity. 
2 STRUCTURAL ALIGNMENT & ANALOGICAL REMINDING 
People appear to perform structural alignment in a wide variety of tasks, including per- 
ception, problem solving, and memory recall (Gentner and Markman, 1992; Markman, 
Gentner and Wisniewski, 1993). One task many researchers have investigated is analog 
recall. A subject is shown a number of stories and later is shown a probe story. The task 
is to recall stories that are similar to the probe story (and sometimes evaluate the degree of 
similarity and perform analogical reasoning). 
MAC/FAC, a computer models of this process, has two stages(Gentner and Forbus, 1991). 
The first stage selects a few likely analogs from a large number of potential analogs. The 
second stage searches for an optimal (or at least good) mapping between each selected story 
and the probe story and outputs those with the best mappings. Two stages are necessary 
because it is too computationally expensive to search for an optimal mapping between the 
probe and all stories in memory. An important requirement for a first stage is that its 
performance scale well with both the size and number of episodes in long-term memory. 
This prevents the first stage of MAC/FAC from considering any structural features. 
Large pool of items in memory Probp 
 () () / Potentia% Good 
() 'J. () 'J() / analogies X analogies 
 __() _,  O Ex-ensive selection ro 
()  _.( ()- ,Chea,p filtenng process cesPs based on struclPurai 
k_2 0 (10'0 () tasea on sunace eatures features 
Figure 1: General architecture of a two-stage retrieval model. 
While it is indisputable that people take structural correspondences into account when 
evaluating and using analogies (Gentnet, Rattermann and Forbus, 1993), it is less certain 
whether structural similarity influences access to long term memory (i.e., the first-stage 
reminding process). Some studies have found little effect of analogical similarity on 
reminding (Gentnet and Forbus, 1991; Gentnet, Rattermann and Forbus, 1993), while 
others have found some effect (Wharton et al., 1994). 
Surface features of stories are the features of the entities and relations involved, and structural 
features are the relationships among the relations and entities. 
Estimating Analogical Similarity by Dot-Products of Holographic Reduced Representations 1111 
In any case, surface features appear to influence the likelihood of a reminding far more than 
do structural features. Studies that have found an effect of structural similarity on reminding 
seem to indicate the effect only exists, or is greater, in the presence of surface similarity 
(Gentnet and Forbus, 1991; Gentnet, Rattermann and Forbus, 1993; Thagard et al., 1990). 
2.1 EXAMPLES OF ANALOGY BETWEEN NESTED STRUCTURES. 
To test how well the HRR dot-product works as an estimate of analogical similarity between 
nested relational structures I used the following set of simple episodes (see Plate (1993) 
for the full set). The memorized episodes are similar in different ways to the probe. These 
examples are adapted from (Thagard et al., 1990). 
Probe: 
Episodes in long-term 
E1 (LS) 
E2 (AN ) 
E3 (AN) 
E6 (SS) 
E7 (FA) 
Spot bit Jane, causing Jane to flee from Spot. 
memory: 
Fido bit John, causing John to flee from Fido. 
Fred bit Rover, causing Rover to flee from Fred. 
Felix bit Mort, causing Mort to flee from Felix. 
John fled from Fido, causing Fido to bite John. 
Mort bit Felix, causing Mort to flee from Felix. 
In these episodes Jane, John, and Fred are people, Spot, Fido and Rover are dogs, Felix is 
a cat, and Mort is a mouse. All of these are objects, represented by token vectors. Tokens 
of the same type are considered to be similar to each other, but not to tokens of other types. 
Bite, flee, and cause are relations. The argument structure of the cause relation, and the 
patterns in which objects fill multiple roles constitutes the higher-order structure. 
The second column classifies the relationship between each episode and the probe using 
Gentner et al's types of similarity: LS (Literal Similarity) shares relations, object features, 
and higher-order structure; AN (Analogy, also called True Analogy) shares relations and 
higher-order structure, but not object features; SS (Surface Similarity, also called Mere 
Appearance) shares relations and object features, but not higher-order structure; FA (False 
Analogy) shares relations only. AN cm denotes a cross-mapped analogy - it involves the 
same types of objects as the probe, but the types of corresponding objects are swapped. 
2.2 MAC/FAC PERFORMANCE ON TEST EXAMPLES 
The first stage of MAC/FAC (the Many Are Called stage) only inspects object features 
and relations. It uses a vector representation of surface features. Each location in the vector 
corresponds to a surface feature of an object, relation or function, and the value in the 
location is the number of times the feature occurs in the structure. The first-stage estimate 
of the similarity between two structures is the dot-product of their feature-count vectors. A 
threshold is used to select likely analogies. It would give E1 (LS), E2 (ANtre), and E6 
(SS) equal and highest scores, i.e., (LS, AN , SS) > (AN, FA) 
The Structure Mapping Engine (SME) (Falkenhainer, Forbus and Gentner, 1989) is used 
as the second stage of MAC/FAC (the Few Are Chosen stage). The rules of SME are 
that mapped relations must match, all the arguments of mapped relations must be mapped 
consistently, and mapping of objects must be one-to-one. SME would detect structural 
correspondences between each episode and the probe and give the literally similar and 
analogous episodes the highest rankings, i.e., LS > AN > (SS, FA). 
1112 Plate 
A simplified view of the overall similarity scores from MAC and the full MAC/FAC is 
shown in Table 1. There are four conditions - the two structures being compared can be 
similar in structure and/or in object attributes. In all four conditions, the structures are 
assumed to involve similar relations - only structural and object attribute similarities are 
varied. Ideally, the responses to the mixed conditions should be flexible, and controlled by 
which aspects of similarity are currently considered important. Only the relative values of 
the scores are important, the absolute values do not matter. 
Structural Object Attribute Similarity 
Similarity YES NO 
YES (LS) High (AN) Low 
NO (SS) High (FA) Low 
(a) Scores from MAC. 
Structural Object Attribute Similarity 
Similarity YES NO 
YES (LS) High (AN) $Med-High 
NO (SS) $Med-Low (FA) Low 
(b) Ideal similarity scores. 
Table 1: (a) Scores from the fast (MAC) similarity estimator in MAC/FAC. (b) Scores from 
an ideal structure-sensitive similarity estimator, e.g., SME as used in MAC/FAC. 
In the remainder of this paper I describe how HRRs can be used to compute fast similarity 
estimates that are more like ratings in Table lb, i.e., estimates that are flexible and sensitive 
to structure. 
3 HOLOGRAPHIC REDUCED REPRESENTATIONS 
A distributed representation for nested relational structures requires a solution to the binding 
problem. The representation of a relation such as b�te (spot, jane) (Spot bit Jane.) 
must bind 'Spot' to the agent role and 'Jane' to the object role. In order to represent nested 
structures it must also be possible to bind a relation to a role, e.g., b�to (spot, j ano) and 
the antecedent role of the cause relation. 
z_=x�y 
n--1 
Zi '-- E XkYj-k 
k----O 
(Subscript are modulo-n) 
zo xy� Y 
(a) z2 (b) 
z=x(by 
zo = xoYo + x2yl q- xly2 
z = xyo + xoy + x2y2 
z2 = x2yo + xly q- xoy2 
Figure 2: (a) Circular convolution. (b) Circular convolution illustrated as a compressed 
outer product for n: 3. Each of the small 
