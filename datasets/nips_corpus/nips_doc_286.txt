Learning to See Rotation and 
Dilation with a Hebb Rule 
Martin I. Sereno and Margaret E. Sereno 
Cognitive Science D-01S 
University of California, San Diego 
La Jolla, CA 92093-0115 
Abstract 
Previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) showed that a 
feedforward network with area Vl-like input-layer units and a Hebb rule 
can develop area MT-like second layer units that solve the aperture 
problem for pattern motion. The present study extends this earlier work 
to more complex motions. Saito et al. (1986) showed that neurons with 
large receptive fields in macaque visual area MST are sensitive to 
different senses of rotation and dilation, irrespective of the receptive field 
location of the movement singularity. A network with an MT-like 
second layer was trained and tested on combinations of rotating, dilating, 
and translating patterns. Third-layer units learn to detect specific senses 
of rotation or dilation in a position-independent fashion, despite having 
position-dependent direction selectivity within their receptive fields. 
1 INTRODUCTION 
The visual systems of mammals and especially primates are capable of prodigious feats of 
movement, object, and scene recognition under noisy conditions--feats we would like to 
copy with artificial networks. We are just beginning to understand how biological 
networks are wired up during development and during learning in the adult. Even at this 
stage, however, it is clear that explicit error signals and the apparatus for propagating them 
backwards across layers are probably not involved. On the other hand, there is a growing 
body of evidence for connections whose strength can be modified (via NMDA channels) 
as functions of the correlation between pre- and post-synaptic activity. The present project 
was to try to learn to detect pattern rotation and dilation by example, using a simple Hebb 
320 
Learning to See Rotation and Dilation with a Hebb Rule 321 
rule. By building up complex filters in stages using a simple, realistic learning rule, we 
reduce the complexity of what must be learned with more explicit supervision at higher 
levels. 
1.1 ORIENTATION SELECTIVITY 
Some of the connections responsible for the selectivity of cortical neurons to local stimulus 
features develop in the absence of patterned visual experience. For example, primary 
visual cortex (V1 or area 17) contains orientation-selective neurons at birth in several 
animals. Linsker (1986a,b) has shown that feedforward networks with gaussian 
topographic interlayer connections, linear summation, and simple hebb rules, develop 
orientation selective units in higher layers when trained on noise. In his linear system, 
weight updates for a layer can be written as a function of the two-point correlation 
characterizing the previous layer. Noise applied to the input layer causes the emergence of 
connections that generate gaussian correlations at the second layer. This in turn drives the 
development of more complex correlation functions in the third layer (e.g., difference-of- 
gaussians). Rotational symmetry is broken in higher layers with the emergence of Gabor- 
function-like connection patterns reminiscent of simple cells in the cortex. 
1.2 PATTERN MOTION SELECTIVITY 
The ability to see coherent motion fields develops late in primates. Human babies, for 
example, fail to see the transition from unstructured to structured motion--e.g., the 
transition between randomly moving dots and circular 2-D motion--for several months. 
The transition from horizontally moving dots with random y-axis velocities to dots with 
sinusoidal y-axis velocities (which gives the percept of a rotating 3-D cylinder) is seen even 
later (Spitz, Stiles-Davis, & Siegel, 1988). This suggests that the cortex requires many 
experiences of moving displays in order to learn how to recognize the various types of 
coherent texture motions. 
However, orientation gradients, shape from shading, and pattern translation, dilation, and 
rotation cannot be detected with the kinds of filters that can be generated solely by noise. 
The correlations present in visual scenes are required in order for these higher level filters 
to arise. 
1.3 NEUROPHYSIOLOGICAL MOTIVATION 
Moving stimuli are processed in successive stages in primate visual cortical areas. The first 
cortical stage is layer 4Ca of V1, which receives its main ascending input from the 
magnocellular layers of the lateral geniculate nucleus. Layer 4Ca projects to layer 4B, 
which contains many tightly-tuned direction-selective neurons. These neurons, however, 
respond to moving contours as if these contours were moving perpendicular to their local 
orientation (Movshon et al., 1985). 
Layer 4B neurons project directly and indirectly to area MT, where a subset of neurons 
show a relatively narrow peak in the direction tuning curve for a plaid that is lined up with 
the peak for a single grating. These neurons therefore solve the aperture problem for 
pattern translation presented to them by the local motion detectors in layer 4B of V1. MT 
neurons, however, appear to be largely blind to the sense of pattern rotation or dilation 
(Saito et al., 1986). Thus, there is a higher order 'aperture problem' that is solved by the 
neurons in the parts of areas MST and 7a that distinguish senses of pattern rotation and 
322 Sereno and Sereno 
dilation. The present model provides a rationale for how these stages might naturally arise 
in development. 
2 RESULTS 
In previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) a simple 2-layer feedforward 
architecture sufficed for an MT-like solution to the aperture problem for local translational 
motion. Units in the first layer were granted tuning curves like those in V 1, layer 4B. Each 
first-layer unit responded to a particular range of directions and speeds of the component 
of movement perpendicular to a local contour. Second layer units developed MT-like 
receptive fields that solved the aperture problem for local pattern translation when trained 
on locally jiggled gratings rigidly moving in randomly chosen pattern directions. 
2.1 NETWORK ARCHITECTURE 
A similar architecture was used for second-to-third layer connections (see Fig. 1--a sample 
network with 5 directions and 3 speeds). As with Linsker, a new input layer was 
constructed from a canonical unit, suitably transformed. Thus, second-layer units were 
granted tuning curves resembling those found in MT (as well as those generated by first- 
to-second layer leaming)--that is, they responded to the local pattern translation but were 
blind to particular senses of local rotation, dilation, and shear. There were 12 different local 
Third 
Layer 
(=MST) 
Second Layer  
(=MT) x/ \ , ..: 
Firs 
(=V 1, Layer 4B) 
Figure 1: Network Architecture 
pattern directions and 4 different local pattern speeds at each x-y location (48 different units 
at each of 100 x-y points). Second-layer excitatory tuning curves were piecewise linear 
with half-height overlap for both direction and speed. Direction tuning was set to be 2-3 
times as important as speed tuning in determining the activation of input units. Input units 
Learning to See Rotation and Dilation with a Hebb Rule 323 
generated untuned feedforward inhibition for off-directions and off-speeds. Total 
inhibition was adjusted to balance total excitation. The probability that a unit in the first 
layer connected to a unit in the second layer fell off as a gaussian centered on the 
retinotopically equivalent point in the second layer. Since receptive fields in areas MST 
and 7a are large, the interlayer divergence was increased relative to the divergence in the 
first-to-second layer connections. Third layer units received several thousand connections. 
The network is similar to that of Linsker except that there is no activity-independent decay 
(kj) for synaptic weights and no offset (k2) for the correlation term. The activation, outj, 
for each unit is a linear weighted sum of its inputs, in i scaled by tx, and clipped to maximum 
and minimum values: 
out j= tï¿½t  ' iniweightij 
[ OUtmax, rain 
Weights are also clipped to maximum and minimum values. The change in each weight, 
Aweightij, is a simple fraction, 5, of the product of the pre- and post-synaptic values: 
Aweightij - 1Sinlout j 
The learning rate, 5, was set so that about 1,000 patterns could be presented before most 
weights saturated. The stable second-layer weight patterns seen by Linsker (1986a) are 
reproduced by this model when it is trained on noise input. However, since it lacks k2, it 
cannot generate center-surround weight structures given only gaussian correlations as 
input. 
2.2 TRAINING PATTERNS 
Second-to-third layer connections were trained with full or partial field rotations, dilations, 
and translations. Each stimulus consisted of a set of local pattern motions at each x-y point 
that were: 1) rotating clockwise or counterclockwise around, 2) dilating or contracting 
toward, or 3) translating through a randomly chosen location. The singularity was always 
within the input array. Both full and partial field rotations and dilations were effective 
training stimuli for generating rotation and dilation selectivity. 
2.3 POSITION-INDEPENDENT TUNING CURVES 
Post-training rotation and dilation tuning curves for different receptive-field locations were 
generated for many third-layer units using paradigms similar to those used on real neurons. 
The location of the motion singularity of the test stimulus was varied across layer two. 
Third-layer units often responded selectively to a particular sense of rotation or dilation at 
each visual field test location. A sizeable fraction of units (10-60%) responded in a 
position-independent way after unsupervised learning on rotating and dilating fields. 
Similar responses were found using both partial- and full-field test stimuli. 
These units thus resemble the neurons in primate visual area MSTd
