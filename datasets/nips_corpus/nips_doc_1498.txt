Divisive Normalization, 
Networks and Ideal 
Line Attractor 
Observers 
Sophie Deneve  Alexandre Pouget , and P.E. Latham 2 
1Georgetown Institute for Computational and Cognitive Sciences, 
Georgetown University, Washington, DC 20007-2197 
2Dpt of Neurobiology, UCLA, Los Angeles, CA 90095-1763, U.S.A. 
Abstract 
Gain control by divisive inhibition, a.k.a. divisive normalization, 
has been proposed to be a general mechanism throughout the vi- 
sual cortex. We explore in this study the statistical properties 
of this normalization in the presence of noise. Using simulations, 
we show that divisive normalization is a close approximation to a 
maximum likelihood estimator, which, in the context of population 
coding, is the same as an ideal observer. We also demonstrate ana- 
lytically that this is a general property of a large class of nonlinear 
recurrent networks with line attractors. Our work suggests that 
divisive normalization plays a critical role in noise filtering, and 
that every cortical layer may be an ideal observer of the activity in 
the preceding layer. 
Information processing in the cortex is often formalized as a sequence of a linear 
stages followed by a nonlinearity. In the visual cortex, the nonlinearity is best de- 
scribed by squaring combined with a divisive pooling of local activities. The divisive 
part of the nonlinearity has been extensively studied by Heeger and colleagues [1], 
and several authors have explored the role of this normalization in the computation 
of high order visual features such as orientation of edges or first and second order 
motion[4]. We show in this paper that divisive normalization can also play a role in 
noise filtering. More specifically, we demonstrate through simulations that networks 
implementing this normalization come close to performing maximum likelihood es- 
timation. We then demonstrate analytically that the ability to perform maximum 
likelihood estimation, and thus efficiently extract information from a population of 
noisy neurons, is a property exhibited by a large class of networks. 
Maximum likelihood estimation is a framework commonly used in the theory of 
ideal observers. A recent example comes from the work of Itti et al., 1998, who have 
shown that it is possible to account for the behavior of human subjects in simple 
discrimination tasks. Their model comprised two distinct stages: 1) a network 
Divisive Normalization, Line Attractor Networks and Ideal Observers 105 
which models the noisy response of neurons with tuning curves to orientation and 
spatial frequency combined with divisive normalization, and 2) an ideal observer (a 
maximum likelihood estimator) to read out the population activity of the network. 
Our work suggests that there is no need to distinguish between these two stages, 
since, as we will show, divisive normalization comes close to providing a maximum 
likelihood estimation. More generally, we propose that there may not be any part 
of the cortex that acts as an ideal observer for patterns of activity in sensory areas 
but, instead, that each cortical layer acts as an ideal observer of the activity in the 
preceding layer. 
I The network 
Our network is a simplified model of a cortical hypercolumn for spatial frequency 
and orientation. It consists of a two dimensional array of units in which each unit 
is indexed by its preferred orientation, Oi, and spatial frequency, j. 
1.1 LGN model 
Units in the cortical layer are assumed to receive direct inputs from the lateral 
geniculate nucleus (LGN). Here we do not model explicitly the LGN, but focus 
instead on the pooled LGN input onto each cortical unit. The input to each unit 
is denoted aij. We distinguish between the mean pooled LGN input, fij(O,,k), as 
a function of orientation, 0, and spatial frequency, X, and the noise distribution 
around this mean, P(aijlO, ). 
In response to a stimulus of orientation, 0, spatial frequency, X, and contrast, C, 
the mean LGN input onto unit ij is a circular Gaussian with a small amount of 
spontaneous activity, t,: 
fij(O,,k)-- KCexp (cos(A- Aj)- 1 cos(O-Oi)- 1) 
+ + 
(1) 
where K is a constant. Note that spatial frequency is treated as a periodic variable; 
this was done for convenience only and should have negligible effects on our results 
as long as we keep  far from 27rn, n an integer. 
On any given trial the LGN input to cortical unit i j, aij, is sampled from a Gaussian 
noise distribution with variance aj' 
p(aijlO,,k) = 1 ( [aij -- fij(Ok)]2 ) 
In our simulations, the variance of the noise was either kept fixed (a/2j = a 2) or set 
to the mean activity (a - fij(O, X)). The latter is more consistent with the noise 
that has been measureff experimentally in the cortex. We show in figure 1-A an 
example of a noisy LGN pattern of activity. 
1.2 Cortical Model: Divisive Normalization 
Activities in the cortical layer are updated over time according to: 
106 S. Deneve, A. Pouget and P E. Latham 
A. CORTEX 
LGN t 0 
e o 
o. 0.2 o.3 0.4 o o. o.? o. o.s 
Contrast 
Figure 1: A- LGN input (bottom) and stable hill in the cortical network after 
relaxation (top). The position of the stable hill can be used to estimate orientation 
() and spatial frequency (). B- Inverse of the variance of the network estimate for 
orientation using Gaussian noise with variance equal to the mean as a function of 
contrast and number of iterations (0, dashed; 1, diamond; 2, circle; and 3, square). 
The continuous curve corresponds to the theoretical upper bound on the inverse 
of the variance (i.e. an ideal observer). C- Gain curve for contrast for the cortical 
units after 1, 2 and 3 iterations. 
+ 1) 2 (3) 
uij(t + 1): y Wij,klOkl(t), Oij(t + 1) = $ + I Ekt Ul(t + 1) 2' 
kl 
where {Wij,kl} are the filtering weights, Oij(t) is the activity of unit ij at time t, 
S is a constant, and/ is what we call the divisive inhibition weight. The filtering 
weights implement a two dimensional Gaussian filter: 
Wij,k! = Wi-k,j-I -- Kwexp (cos[2z-(i- k)/P]- 1 cos[2z-(j- l)/P]- 1) 
2 + or2 
O'wO w X 
(4) 
where Kw is a constant, awO and ax control the width of the filtering weights, and 
there are p2 units. 
On each iteration the activity is filtered by the weights, squared, and then nor- 
malized by the total local activity. Divisive normalization per se only involves the 
squaring and division by local activity. We have added the filtering weights to ob- 
tain a local pooling of activity between cells with similar preferred orientations and 
spatial frequencies. This pooling can easily be implemented with cortical lateral 
connections and it is reasonable to think that such a pooling takes place in the 
cortex. 
Divisive Normalization, Line Attractor Networks and Ideal Observers 107 
2 Simulation Results 
Our simulations consist of iterating equation 3 with initial conditions determined by 
the presentation orientation and spatial frequency. The initial conditions are chosen 
as follows: For a given presentation angle, 00, and spatial frequency, 0, determine 
the mean cortical activity, fij(0o,o), via equation 1. Then generate the actual 
cortical activity, {aij }, by sampling from the distribution given in equation 2. This 
serves as our set of initial conditions: oij (t = O) = aij. 
Iterating equation 3 with the above initial conditions, we found that for very low 
contrast the activity of all cortical units decayed to zero. Above some contrast 
threshold, however, the activities converged to a smooth stable hill (see figure 1-A 
for an example with parameters awO = ax = a0 = ax = 1/x/, K = 74, C = 1, 
/: 0.01). The width of the hill is controlled by the width of the filtering weights. 
Its peak, on the other hand, depends on the orientation and spatial frequency of the 
LGN input, 00 and 0. The peak can thus be used to estimate these quantities (see 
figure l-A). To compute the position of the final hill, we used a population vector 
estimator [3] although any unbiased estimator would work as well. In all cases we 
looked at, the network produced an unbiased estimate of 00 and 0. 
In our simulations we adjusted ao and awX so that the stable hill had the same 
profile as the mean LGN input (equation 1). As a result, the tuning curves of the 
cortical units match the tuning curves specified by the pooled LGN input. For this 
case, we found that the estimate obtained from the network has a variance close 
to the theoretical minimum, known as the Cramr-Rao bound [3]. For Gaussian 
noise of fixed variance, the variance of the estimate was 16.6% above this bound, 
compared to 3833% for the population vector applied directly to the LGN input. 
In a 1D network (orientation alone), these numbers go to 12.9% for the network 
versus 613% for population vector. For Gaussian noise with variance proportional 
to the mean, the network was 8.8% above the bound, compared to 722% for the 
population vector applied directly to the input. These numbers are respectively 9% 
and 108% for the 1-D network. The network is therefore a close approximation to 
a maximum likelihood estimator, i.e., it is close to being an ideal observer of the 
LGN activity with respect to orientation and spatial frequency. 
As long as the contrast, C, was superthreshold, large variations in contrast did not 
affect our results (figure l-B). However, the tuning of the network units to contrast 
after reaching the stable state was found to follow a step function whereas, for real 
neurons, the curves are better described by a sigmoid [2]. Improved agreement 
with experiment was achieved by taking only 2-3 iterations, at which point the 
performance of the network is close to optimal (figure l-B) and the tuning curves to 
contrast are more realistic and closer to sigmoids (figure 1-C). Therefore, reaching 
a stable state is not required for optimal performance, and in fact leads to contrast 
tuning curves that are inconsistent with experiment. 
3 Mathematical Analysis 
W
