554 
STABILITY RESULTS FOR NEURAL NETWORKS 
A. N. Michel  , J. A. Farrell  , and W. Porod 2 
Department of Electrical and Computer Engineering 
University of Notre Dame 
Notre Dame, IN 46556 
ABSTRACT 
In the present paper we survey mad utilize results from the qualitative theory of large 
scale interconnected dynamical systems in order to develop a qualitative theory for the 
Hop field model of neural networks. In our approach we view such networks as an inter- 
connection of many single neurons. Our results are phrased in terms of the qualitative 
properties of the individual neurons and in terms of the properties of the interconnecting 
structure of the neural networks. Aspects of neural networks which we address include 
asymptotic stability, exponential stability, and instability of an equilibrium; estimates 
of trajectory bounds; estimates of the domain of attraction of an asymptotically stable 
equilibrium; and stability of neural networks under structural perturbations. 
INTRODUCTION 
In recent years, neural networks have attracted considerable attention as candidates 
for novel computational systems -3. These types of large-scale dynamical systems, in 
analogy to biological structures, take advantage of distributed information processing 
and their inherent potential for parallel computation 4,5. Clearly, the design of such 
neural-network-based computational systems entails a detailed understanding of the 
dynamics of large-scale dynamical systems. In particular, the stability and instability 
properties of the various equilibrium points in such networks are of interest, as well 
as the extent of associated domains of attraction (basins of attraction) and trajectory 
bounds. 
In the present paper, we apply and survey results from the qualitative theory of large 
scale interconnected dynamical systems 6-9 in order to develop a qualitative theory for 
neural networks. We will concentrate here on the popular Hopfield model 3, however, 
this type of analysis may also be applied to other models. In particular, we will address 
the following problems: (i) determine the stability properties of a given equilibrium 
point; (ii) given that a specific equilibrium point of a neural network is asymptotically 
stable, establish an estimate for its domain of attraction; (iii) given a set of initial condi- 
tions and external inputs, establish estimates for corresponding trajectory bounds; (iv) 
give conditions for the instability of a given equilibrium point; (v) investigate stability 
properties under structural perturbations. The present paper contains local results. A 
more detailed treatment of local stability results can be found in Ref. 10, whereas global 
results are contained in Ref. 11. 
In arriving at the results of the present paper, we make use of the method of anal- 
ysis advanced in Ref. 6. Specifically, we view high dimensional neural network as an 
XThe work of A. N. Michel and J. A. Farrell was supported by NSF under grant ECS84-19918. 
2The work of W. Porod was supported by ONR under grant N00014-86-K-0506. 
American Institute of Physics 1988 
555 
interconnection of individual subsystems (neurons). This interconnected systems view- 
point makes our results distinct from others derived in the literature L2. Our results 
are phrased in terms of the qualitative properties of the free subsystems (individual 
neurons, disconnected from the network) and in terms of the properties of the intercon- 
necting structure of the neural network. As such, these results may constitute useful 
design tools. This approach makes possible the systematic analysis of high dimensional 
complex systems and it frequently enables one to circumvent difficulties encountered in 
the analysis of such systems by conventional methods. 
The structure of this paper is as follows. We start out by defining the Hop field 
model and we then introduce the interconnected systems viewpoint. We then present 
representative stability results, including estimates of trajectory bounds and of domains 
of attraction, results for instability, and conditions for stability under structural pertur- 
bations. Finally, we present concluding remarks. 
THE HOPFIELD MODEL FOR NEURAL NETWORKS 
In the present paper we consider neural networks of the Hopfield type 3. Such systems 
can be represented by equations of the form 
N 
= '--biu + &j Cj(uj) + lot i = ,:v, 
j=l 
where A,j T Ui(t) = (t) and bi =  As usual, Ci > O,Tij = 
= c,, c c,' Pdj ' RijeR = 
__1 ._ 1 
(-oc, oc),r,  + Z= Ijl, Ri > O,Ii : R + = [0,)  R,Ii is continuous, 
ai =  Gi : R  (-1, 1),Gi is continuously differentiable and strictly monotoni- 
dt  
t 
cMly increasing (i.e., Ci(uti) > Ci(uti t) if and only if u i > utf),uiCi(ui) > 0 for all ui  0, 
and Gi(O) = 0. In (1), Ci denotes capacitance, R 0 denotes resistance (possibly includ- 
ing a sign inversion due to an inverter), Gi(.) denotes an amplifier nonlinearity, and 
denotes an externM input. 
In the terature it is frequently assumed that Tij = Tji for M1 i,j = 1,... ,N and 
that i = 0 for M1 i = 1,..., N. We will me these sumptions only when explicitly 
stated. 
We are interested in the quMitative behavior of solutions of (1) near equibrium 
points (rest positions where ai  O, for i = 1,..., N). By setting the extemM inputs 
Ui(t), i = 1,... ,N, equM to zero, we define u* = [u,... ,u]TeR N to be an equilibrium 
* N 
for (1) provided that -biu i + j= Aij Gj(u) = O, for i = 1,...,N. The locations 
of such equibria in R N are determined by the interconnection pattern of the neural 
network (i.e., by the pameters Aij, i,j = 1,..., N)  well as by the parameters bi and 
the nature of the nonlinearities Gi('), i = 1,..., N. 
Throughout, we will sume that a given equibrium u* being anMyzed is an isolated 
equilibrium for (1), i.e., there ests an r > 0 such that in the neighborhood B(u*, r) = 
{(u - u*)eR N '1 u - u* I < r} no equilibrium for (1), other than u = u*, effists. 
When analyzing the stability properties of a given equilibrium point, we will be able 
to assume, without loss of generMity, that this equibrium is located at the origin u = 0 
of R N. If this is not the ce, a triviM trsformation can be employed which shifts the 
equilibrium point to the origin and which leaves the structure of (1) the same. 
556 
INTERCONNECTED SYSTEMS VIEWPOINT 
We will find it convenient to view system (1) as an interconnection of N free sub- 
systems (or isolated subsystems) described by equations of the form 
Pi = -bipi + Zii Gi(pi) + Ui(t). (2) 
Under this viewpoint, the interconnecting structure of the system (1) is given by 
N 
i#j 
AijGj(xj), i= 1,...,N. (3) 
Following the method of analysis advanced in 6, we will establish stability results 
which are phrased in terms of the quMitative properties of the free subsystems (2) and 
in terms of the properties of the interconnecting structure given in (3). This method 
of aaalysis makes it often possible to circumvent difficulties that arise in the analysis 
of complex high-dimensional systems. Furthermore, results obtained in this manner 
frequently yield insight into the dynamic behavior of systems in terms of system com- 
ponents and interconnections. 
GENERAL STABILITY CONDITIONS 
We demonstrate below an example of a result for exponential stability of an equi- 
librium point. The principal Lyapunov stability results for such systems are presented, 
e.g., in Chapter 5 of Ref. 7. 
We will utilize the following hypotheses in our first result. 
(A-l) For system (1), the external inputs are all zero, i.e., 
(A-2) 
Ui(t) = O, i=l,...,N. 
For system (1), the interconnections satisfy the estimate 
xiAij Gj(xj) < xi aijxj 
for all ]xi[ < ri, [xj[ < rj, i,j = 1,...,N, where the aij are real constants. 
(A-a) There exists an N-vector a > 0 (i.e., a T = (a,...,aN) and a > 0, for all i: 
1,..., N) such that the test matrix S = [sj] 
{ ai(-bi +aii), i = j 
Sij = (oti aij + otj aji)/2, i  j 
is negative definite, where the bi are defined in (1) and the aij are given in (A-2). 
557 
We are now in a position to state and prove the following result. 
Theorem 1 The equilibrium x = 0 of the neural network (1) is exponentially stable 
if hypotheses (A-l), (A-2) and (A-3) are satisfied. 
Proof. For (1) we choose the Lyanpunov function 
v(x)= . i 2 
i=1 igi 
where the ai are given in (A-3). This function is clearly positive definite. 
derivative of v along the solutions of (1) is given by 
N i N 
Dvo)(x ) =  .i(2xi)[-bixi +  Aij 
i=1 j=l 
where (A-l) has been invoked. In view of (A-2) we have 
(4) 
The time 
Dv()(x) 
N N 
<_ + 
i--1 j=l 
= TRx for U 112 < r 
2 /2 
where r - min(r), I1 - E=  J , and the matrix R = [ro] is given by 
i(-bi + aii), i = j 
rij -- i aij, i  j. 
But it follows that 
= - , � = xrs < A4(s) Il (5) 
where S is the matrix given in (A-3) and AM(S) denotes the largest eigenvalue of 
the real symmetric matrix S. Since S is by assumption negative definite, we have 
AM(S) < 0. It follows from (4) and (5) that in some neighborhood of the origin x = 0, 
we have c1122 < v() < c21xl22 and Dvo)(x ) <_ -calxl, where c = -} mini-/ > 0, 
c2 = � maxi i > 0, and ca = --AM(S) > 0. Hence, the equilibrium x = 0 of the neural 
network (1) is exponentially stable (c.f. Theorem 9.10 in Ref. 7). 
Consistent with the philosophy of viewing the neural network (1) as an intercon- 
nection of N free subsystems (2), we think of the Lyapunov function (4) as consisting 
of a weighted sum of Lyapunov functions for eax:h free subsystem (2) (with Ui(t) -- 0). 
The weighting vector , > 0 provides flexibility to emphasize the relative importance 
of the qualitative properties of the various individual subsystems. Hypothesis (A - 2) 
provides a measure of interaction between the various subsystems (3). Furthermore, it 
is emphasized that Theorem 1 does not require that the parameters Aij in (1) form a 
symmetric matrix. 
558 
WEAK COUPLING CONDITIONS 
The t
