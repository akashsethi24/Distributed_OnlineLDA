642 
LEARNING BY STATE RECURRENCE DETECFION 
Bruce E. Rosen, James M. Goodwin*, and Jacques J. Vidal 
University of California, Los Angeles, Ca. 90024 
ABSTRACT 
This research investigates a new technique for unsupervised learning of nonlinear 
control problems. The approach is applied both to Michie and Chambers BOXES 
algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and 
has significantly improved the convergence rate of stochastically based learning 
automata. 
Recurrence learning is a new nonlinear reward-penalty algorithm. It exploits 
information found during learning trials to reinforce decisions resulting in the 
recurrence of nonfailing states. Recurrence learning applies positive reinforcement 
during the exploration of the search space, whereas in the BOXES or ASE algorithms, 
only negative weight reinforcement is applied, and then only on failure. Simulation 
results show that the added information from recurrence learning increases the learning 
rate. 
Our empirical results show that recurrence learning is faster than both basic failure 
driven learning and failure prediction methods. Although recurrence learning has only 
been tested in failure driven experiments, there are goal directed learning applications 
where detection of recurring oscillations may provide useful information that reduces 
the learning time by applying negative, instead of positive reinforcement. 
Detection of cycles provides a heuristic to improve the balance between evidence 
gathering and goal directed search. 
INTRODUCFION 
This research investigates a new technique for unsupervised learning of nonlinear 
control problems with delayed feedback. Our approach is compared to both Michie and 
Chambers BOXES algorithm 1, to the extension by Barto, et al., the ASE (Adaptive 
Search Element) and to their ASE/ACE (Adaptive Critic Element) system 2, and shows 
an improved learning time for stochastically based learning automata in failure driven 
tasks. 
We consider adaptively controlling the behavior of a system which passes 
through a sequence of states due to its internal dynamics (which are not assumed to be 
known a priori) and due to choices of actions made in visited states. Such an adaptive 
controller is often referred to as a learning automaton. The decisions can be 
deterministic or can be made according to a stochastic rule. A learning automaton has 
to discover which action is best in each circumstance by producing actions and 
observing the resulting information. 
This paper was motivated by the previous work of Barto, et al. to investigate 
neuron like adaptive elements that affect and learn from their environment. We were 
inspired by their current work and the recent attention to neural networks and 
connectionist systems, and have chosen to use the cart-pole control problem 2, to enable 
a comparison of our results with theirs. 
'Permanent address: California State University, Stanislaus; Turlock, California. 
@ American Institute of Physics 1988 
643 
THE CART-POLE PROBLEM 
In their work on the cart-pole problem, Barto, Sutton and Anderson considered a 
learning system composed of an automaton interacting with an environment. The 
problem requires the automaton to balance a pole acting as an inverted pendulum hinged 
on a moveable cart. The cart travels left or right along a bounded one dimensional track; 
the pole may swing to the left or right about a pivot attached to the cart. The automaton 
must learn to keep the pole balanced on the cart, and to keep the cart within the bounds 
of the track. The parameters of the cart/pole system are the cart position and velocity, 
and the pole angle and angular velocity. The only actions available to the automaton are 
the applications of a fixed impulsive force to the cart in either right or left direction; one 
of these actions must be taken. 
This balancing is an extremely difficult problem if there is no a priori knowledge 
of the system dynamics, if these dynamics change with time, or if there is no 
preexisting controller that can be imitated (e.g. Widrow and Smith's 3 ADALINE 
controller). We assumed no a priori knowledge of the dynamics nor any preexisting 
controller and anticipate that the system will be able to deal with any changing 
dynamics. 
Numerical simulations of the cart-pole solution via recurrence learning show 
substantial improvement over the results of Barto et al., and of Michie and Chambers, 
as is shown in figure 1. The algorithms used, and the results shown in figure 1, will 
be discussed in detail below. 
Time 
Until 
Failu 
/ 
 ,.. Hr 
i I 
50 75 110 
Trial No. 
Figure 1' Performance of the ASE, ASE/ACE, Constant Recurrence (HI) and 
Short Recurrence (H2) Algorithms. 
THE GENERAL PROBLEM: ASSIGNMENT OF CREDIT 
The cart-pole problem is one of a class of problems known as credit 
assignment 4, and in particular temporal credit assignment. The recurrence learning 
algorithm is an approach to the general temporal credit assignment problem. It is 
characterized by seeking to improve learning by making decisions about early actions. 
The goal is to find actions responsible for improved or degraded performance at a much 
later time. 
An example is the bucket brigade algorithm 5. This is designed to assign credit to 
rules in the system according to their overall usefulness in attaining their goals. This is 
done by adjusting the strength value (weight) of each rule. The problem is of 
modifying these strengths is to permit rules activated early in the sequence to result in 
successful actions later. 
644 
Samuels considered the credit assignment problem for his checkers playing 
program 6. He noted that it is easy enough to credit the rules that combine to produce a 
triple jump at some point in the game; it is much harder to decide which rules active 
earlier were responsible for changes that made the later jump possible. 
State recurrence learning assigns a strength to an individual rule or action and 
modifies that action's strength (while the system accumulates experience) on the basis 
of the action's overall usefulness in the situations in which it has been invoked. In this 
it follows the bucket brigade paradigm of Holland. 
PREVIOUS WORK 
The problems of learning to control dynamical systems have been studied in the 
past by Widrow and Smith 3, Michie and Chambers 1, Barto, Sutton, and Anderson 2, 
and Connell 7. Although different approaches have been taken and have achieved 
varying degrees of success, each investigator used the cart/pole problem as the basis for 
empirically measuring how well their algorithms work. 
Michie and Chambers 1 built BOXES, a program that learned to balance a pole on 
a cart. The BOXES algorithm choose an action that had the highest average time until 
failure. After 600 trials (a trial is a run ending in eventual failure or by some time limit 
expiration), the program was able to balance the pole for 72,000 time steps. Figure 2a 
describes the BOXES learning algorithm. States are penalized (after a system failure) 
according to recency. Active states immediately preceding a system failure are 
punished most. 
Barto, Sutton and Anderson 2 used two neuron like adaptive elements to solve the 
control problem. Their ASE/ACE algorithm chose the action with the highest 
probability of keeping the pole balanced in the region, and was able to balance the pole 
for over 60,000 time steps before completion of the 100th trial. 
Figure 2a and 2b: The BOXES and ASE/ACE (Associative Search Elelement - 
Adpafive Critic Element) algorithms 
Figure 2a shows the BOXES (and ASE) learning algorithm paradigm When the 
automaton enters a failure state (C), all states that it has traversed (shaded rectangles) 
are punished, although state B is punished more than state A. (Failure states are those 
at the edges of the diagram.) Figure 2b describes the ASE/ACE learning algorithm. If 
a system failure occurs before a state's expected failure time, the state is penalized. If a 
system failure occurs after its expected failure time, the state is rewarded. State A is 
penalized because a failure occurred at B sooner than expected. State A's expected 
645 
failure time is the time for the automaton to traverse from state A to failure point C. 
When leaving state A, the weights are updated if the new state's expected failure time 
differs from that of state A. 
Anderson 8 used a connectionist system to learn to balance the pole. Unlike the 
previous experiments, the system did provide well-chosen states a priori. On the 
average, 10,000 trials were necessary to learn to balance the pole for 7000 time steps. 
Connell and Utgoff 7 developed an approach that did not depend on partitioning 
the state space into discrete regions. They used Shepard's function9,10 to interpolate 
the degree of desirability of a cart-pole state. The system learned the control task after 
16 trials. However, their system used a knowledge representation that had a priori 
information about the system. 
OTHER RELATED WORK 
Klopfl 1 proposed a more neurological class of differential learning mechanisms 
that correlates earlier changes of inputs with later changes of outputs. The adaptation 
formula used multiplies the change in outputs by the weighted sum of the absolute 
value of the t previous inputs weights (Awj), the '1: previous differences in inputs (Axj), 
and the '1: previous time coefficients (cj). 
Sutton's temporal differences (TD) 12 approach is one of a class of adaptive 
prediction methods. Elements of this class use the sum of previously predicted output 
values multiplied by the gradient and an exponentially decaying coefficient to modify 
the weights. Barto and Sutton 13 used temporal differences as the underlying learning 
procedure for classical conditioning. 
THE RECURRENCE LEARNING METHOD 
DEFINITIONS 
A state is the set of values (or ranges) of parameters sufficient to specify the 
instantaneous condition of the 
