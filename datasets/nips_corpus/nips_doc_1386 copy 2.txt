Minimax and Hamiltonian Dynamics of 
Excitatory-Inhibitory Networks 
H. S. Seung, T. J. Richardson 
Bell Labs, Lucent Technologies 
Murray Hill, NJ 07974 
{seung [ t j r}bell-labs. com 
J. C. Lagarias 
AT&T Labs-Research 
180 Park Ave. D-130 
Florham Park, NJ 07932 
j cl(reseaxch. art. com 
J. J. Hop field 
Dept. of Molecular Biology 
Princeton University 
Princeton, NJ 08544 
j hopf ieldwat son. princet on. edu 
Abstract 
A Lyapunov function for excitatory-inhibitory networks is constructed. 
The construction assumes symmetric interactions within excitatory and 
inhibitory populations of neurons, and antisymmetric interactions be- 
tween populations. The Lyapunov function yields sucient conditions 
for the global asymptotic stability of fixed points. If these conditions 
axe violated, limit cycles may be stable. The relations of the Lyapunov 
function to optimization theory and classical mechanics axe revealed by 
minimax and dissipative Hamiltonian forms of the network dynamics. 
The dynamics of a neural network with symmetric interactions provably converges to 
fixed points under very general assumptions[I, 2]. This mathematical result helped 
to establish the paradigm of neural computation with fixed point attractors[3]. But 
in reality, interactions between neurons in the brain are asymmetric. Furthermore, 
the dynamical behaviors seen in the brain are not confined to fixed point attractors, 
but also include oscillations and complex nonperiodic behavior. These other types 
of dynamics can be realized by asymmetric networks, and may be useful for neural 
computation. For these reasons, it is important to understand the global behavior 
of asymmetric neural networks. 
The interaction between an excitatory neuron and an inhibitory neuron is clearly 
asymmetric. Here we consider a class of networks that incorporates this fundamen- 
tal asymmetry of the brain's microcircuitry. Networks of this class have distinct 
populations of excitatory and inhibitory neurons, with antisymmetric interactions 
330 H. S. $eung, T. J. Richardson, J. C. Lagarias and J. J. Hopfield 
between populations and symmetric interactions within each population. Such net- 
works display a rich repertoire of dynamical behaviors including fixed points, limit 
cycles[4, 5] and traveling waves[6]. 
After defining the class of excitatory-inhibitory networks, we introduce a Lyapunov 
function that establishes sufficient conditions for the global asymptotic stability 
of fixed points. The generality of these conditions contrasts with the restricted 
nature of previous convergence results, which applied only to linear networks[5], or 
to nonlinear networks with infinitely fast inhibition[7]. 
The use of the Lyapunov function is illustrated with a competitive or winner-take-all 
network, which consists of an excitatory population of neurons with recurrent inhi- 
bition from a single neuron[8]. For this network, the sufficient conditions for global 
stability of fixed points also happen to be necessary conditions. In other words, 
we have proved global stability over the largest possible parameter regime in which 
it holds, demonstrating the power of the Lyapunov function. There exists another 
parameter regime in which numerical simulations display limit cycle oscillations[7]. 
Similar convergence proofs for other excitatory-inhibitory networks may be obtained 
by tedious but straightforward calculations. All the necessary tools are given in the 
first half of the paper. But the rest of the paper explains what makes the Lyapunov 
function especially interesting, beyond the convergence results it yields: its role in 
a conceptual framework that relates excitatory-inhibitory networks to optimization 
theory and classical mechanics. 
The connection between neural networks and optimization[3] was established by 
proofs that symmetric networks could find minima of objective functions[I, 2]. Later 
it was discovered that excitatory-inhibitory networks could perform the minimax 
computation of finding saddle points[9, 10, 11], though no general proof of this was 
given at the time. Our Lyapunov function finally supplies such a proof, and one of 
its components is the objective function of the network's minimax computation. 
Our Lyapunov function can also be obtained by writing the dynamics of excitatory- 
inhibitory networks in HamiltonJan form, with extra velocity-dependent terms. If 
these extra terms are dissipative, then the energy of the system is nonincreasing, 
and is a Lyapunov function. If the extra terms are not purely dissipative, limit 
cycles are possible. Previous HamiltonJan formalisms for neural networks made 
the more restrictive assumption of purely antisymmetric interactions, and did not 
include the effect of dissipation[12]. 
This paper establishes sufficient conditions for global asymptotic stability of fixed 
points. The problem of finding sufficient conditions for oscillatory and chaotic 
behavior remains open. The perspectives of minimax and Hamiltonian dynamics 
may help in this task. 
I EXCITATORY-INHIBITORY NETWORKS 
The dynamics of an excitatory-inhibitory network is defined by 
Txk + x -- f(u + Ax - By) , 
+ y = g(v + BTx - Cy) . 
(1) 
(2) 
The state variables are contained in two vectors x  R m and y  R n, which represent 
the activities of the excitatory and inhibitory neurons, respectively. 
The symbol f is used in both scalar and vector contexts. The scalar function 
f: R - R is monotonic nondecreasing. The vector function f: R m - R m is 
Minimax and Hamilt onian Dynamics of Excitato ry-Inhib ito ry Networks 331 
defined by applying the scalar function f to each component of a vector argument, 
i.e., f(x) = (f(x),..., f(xm)). The symbol g is used similarly. 
The symmetry of interaction within each population is imposed by the constraints 
A = A T and C - C T. The antisymmetry of interaction between populations is 
manifest in the occurrence of -B and B T in the equations. The terms excitatory 
and inhibitory are appropriate with the additional constraint that the entries of 
matrices A, B, and C are nonnegative. Though this assumption makes sense in 
a neurobiological context the mathematics does not depends on it. The constant 
vectors u and v represent tonic input from external sources, or alternatively bias 
intrinsic to the neurons. 
The time constants Tx and Ty set the speed of excitatory and inhibitory synapses, 
respectively. In the limit of infinitely fast inhibition, -y = 0, the convergence 
theorems for symmetric networks are applicable[I, 2], though some effort is required 
in applying them to the case C  0. If the dynamics converges for y = 0, then 
there exists some neighborhood of zero in which it still converges[7]. Our Lyapunov 
function goes further, as it is valid for more general y. 
The potential for oscillatory behavior in excitatory-inhibitory networks like (1) has 
long been known[4, 7]. The origin of oscillations can be understood from a simple 
two neuron model. Suppose that neuron I excites neuron 2, and receives inhibition 
back from neuron 2. Then the effect is that neuron I suppresses its own activity 
with an effective delay that depends on the time constant of inhibition. If this delay 
is long enough, oscillations result. However, these oscillations will die down to a 
fixed point, as the inhibition tends to dampen activity in the circuit. Only if neuron 
I also excites itself can the oscillations become sustained. 
Therefore, whether oscillations are damped or sustained depends on the choice of 
parameters. In this paper we establish sufficient conditions for the global stability of 
fixed points in (1). The violation of these sufficient conditions indicates parameter 
regimes in which there may be other types of asymptotic behavior, such as limit 
cycles. 
2 LYAPUNOV FUNCTION 
We will assume that f and g are smooth and that their inverses f- and g- exist. 
If the function f is bounded above and/or below, then its inverse f-1 is defined on 
the appropriate subinterval of R. Note that the set of (x, y) lying in the range of 
(f, g) is a positive invariant set under (1) and that its closure is a global attractor 
for the system. 
The scalar function F is defined as the antiderivative of f, and _ as the Legendre 
transform '(x) -- maxp{px - F(p)}. The derivatives of these conjugate convex 
functions are, 
F'(x) - f(x) , '(x) -- f-l(x) . (3) 
The vector versions of these functions are defined componentwise, as in the definition 
of the vector version of f. The conjugate convex pair G,  is defined similarly. 
The Lyapunov function requires generalizations of the standard kinetic energies 
zxk2/2 and 'ryo2/2. These are constructed using the functions �: R m x R m -- R 
and F � R n x R n -+ R, defined by 
(I,(p,x) -' 1TF(p)--xTp+ 1T'(x), (4) 
F(q,y) = 1TG(q)--yTq+lT(y). (5) 
332 H. S. Seung, T. J. Richardson, J. C. Lagarias and J. J. Hopfield 
The components of the vector 1 are all ones; its dimensionality should be clear 
from context. The function (p,x) is lower bounded by zero, and vanishes on 
the manifold f(p) - x, by the definition of the Legendre transform. Setting p = 
u + Ax - By, we obtain the generalized kinetic energy r-l(u + Ax - By, x), which 
vanishes when  = 0 and is positive otherwise. It reduces to rx5:2/2 in the special 
case where f is the identity function. 
To construct the Lyapunov function, a multiple of the saddle function 
$=--uTx--xTAx+vTy--yTCy+ITp(x)+yTBTx--IT(y) (6) 
is added to the kinetic energy. The reason for the name saddle function will be 
explained later. Then 
L = r-(u + Ax - By,x) + rF(v + BTx - Cy, y) + rS (7) 
is a Lyapunov function provided that it is lower bounded, nonincreasing, and  only 
vanishes at fixed points of the dynamics. Roughly speaking, this is enough to prove 
the global asymptotic stability of fixed points, although some additional technical 
details may be involved. 
In the next section, the Lya
