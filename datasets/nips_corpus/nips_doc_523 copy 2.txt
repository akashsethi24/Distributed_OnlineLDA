3D Object Recognition Using Unsupervised 
Feature Extraction 
Nathan Intrator 
Center for Neural Science, 
Brown University 
Providence, RI 02912, USA 
Josh I. Gold 
Center for Neural Science, 
Brown University 
Providence, RI 02912, USA 
Heinrich H. Biilthoff 
Dept. of Cognitive Science, 
Brown University, 
and Center for 
Biological Information Processing, 
MIT, Cambridge, MA 02139 USA 
Shimon Edelman 
Dept. of Applied Mathematics 
and Computer Science, 
Weizmann Institute of Science, 
Rehovot 76100, Israel 
Abstract 
Intrator (1990) proposed a feature extraction method that is related to 
recent statistical theory (Huber, 1985; Friedman, 1987), and is based on 
a biologically motivated model of neuronal plasticity (Bienenstock et al., 
1982). This method has been recently applied to feature extraction in the 
context of recognizing 3D objects from single 2D views (Intrator and Gold, 
1991). Here we describe experiments designed to analyze the nature of the 
extracted features, and their relevance to the theory and psychophysics of 
object recognition. 
1 Introduction 
Results of recent computational studies of visual recognition (e.g., Poggio and Edel- 
man, 1990) indicate that the problem of recognition of 3D objects can be effectively 
reformulated in terms of standard pattern classification theory. According to this 
approach, an object is represented by a few of its 2D views, encoded as clusters in 
multidimentional space. Recognition of a novel view is then carried out by interpo- 
460 
3D Object Recognition Using Unsupervised Feature Extraction 461 
lating among the stored views in the representation space. A major characteristic 
of the view interpolation scheme is its sensitivity to viewpoint: the farther the novel 
view is from the stored views, the lower the expected recognition rate. 
This characteristic performance in the recognition of novel views of synthetic 3D 
stimuli was indeed found in human subjects by Biilthoff and Edelman (1991), who 
also replicated it in simulated psychophysical experiments that involved a computer 
implementation of the view interpolation model. Because of the high dimensionality 
of the raster images seen by the human subjects, it was impossible to use them di- 
rectly for classification in the simulated experiments. Consequently, the simulations 
were simplified, in that the views presented to the model were encoded as lists of 
vertex locations of the objects (which resembled 3D wire frames). 
This simplification amounts to what is referred to in the psychology of recognition 
as the feature extraction step (LaBerge, 1976). The discussion of the issue of fea- 
tures of recognition in recent psychological literature is relatively scarce, probably 
because of the abandonment of invariant feature theories (which postulate that ob- 
jects are represented by clusters of points in multidimensional feature spaces (Duda 
and Wart, 1973)) in favor of structural models (see review in (Edelman, 1991)). tl- 
though some attempts have been made to generate and verify specific psychophysical 
predictions based on the feature space approach (see especially (Shepard, 1987)), 
current feature-based theories of perception seem to be more readily applicable to 
lower-level visual tasks than to the problem of object recognition. 
In the present work, our aim was to explore a computationally tractable model 
of feature extraction conceived as dimensionality reduction, and to test its psy- 
chophysical validity. This work was guided by previous successful applications in 
pattern recognition of dimensionality reduction by a network model implement- 
ing Exploratory Projection Pursuit (Intrator, 1990; Intrator and Gold, 1991). We 
were also motivated by results of recent psychophysical experiments (Edelman and 
Biilthoff, 1990; Edelman et al., 1991) that found improvement in subjects' perfor- 
mance with increasing stimulus familiarity. These results are compatible with a 
feature-based recognition model which extracts problem-specific features in addi- 
tion to universal ones. Specifically, the subjects' ability to discern key elements 
of the solution appears to increase as the problem becomes more familiar. This 
finding suggests that some of the features used by the visual system are based on 
the task-specific data, and therefore raises the question of how can such features be 
extracted. It was our conjecture that features found by the EPP model would turn 
out to be similar to the task-specific features in human vision. 
1.1 Unsupervised Feature Extraction - The BCM Model 
The feature extraction method briefly described below emphasizes dimensionality 
reduction, while seeking features of a set of objects that would best distinguish 
among the members of the set. This method does not rely on a general pre-defined 
set of features. This is not to imply, however, that the features are useful only in 
recognition of the original set of images from which they were extracted. In fact, the 
potential importance of these features is related to their invariance properties, or 
their ability to generalize. Invariance properties of features extracted by this method 
have been demonstrated previously in speech recognition (Intrator and Tajchman, 
462 Intrator, Gold, Btilthof�, and Edelman 
1991; Intrator, 1992). 
From a mathematical viewpoint, extracting features from gray level images is related 
to dimensionality reduction in a high dimensional vector space, in which an n x k 
pixel image is considered to be a vector of length n x k. The dimensionality reduction 
is achieved by replacing each image (or its high dimensional equivalent vector) by a 
low dimensional vector in which each element represents a projection of the image 
onto a vector of synaptic weights (constructed by a BCM neuron). 
1 
m 
d 2 
 2 
m 
1 
m 
1 
Projections through rn 
Projected distrlbutlol 
Figure 1: The stable solutions for a two dimensional two input problem are rn and 
rn2 (left) and similarly with a two-cluster data (right). 
The feature extraction method we used (Intrator and Cooper, 1991) seeks multi- 
modality in the projected distribution of these high dimensional vectors. A simple 
example is illustrated in Figure 1. For a two-input problem in two dimensions, the 
stable solutions (projection directions) are rn and rn2, each has the property of 
being orthogonal to one of the inputs. In a higher dimensional space, for n linearly 
independent inputs, a stable solution is one that it is orthogonal to all but one of 
the inputs. In case of noisy but clustered inputs, a stable solution will be orthogonal 
to all but one of the cluster centers. As is seen in Figure i (right), this leads to 
a bimodal, or, in general, multi-modal, projected distribution. Further details are 
given in (Intrator and Cooper, 1991). In the present study, the features extracted 
by the above approach were used for classification as described in (Intrator and 
Gold, 1991; Intrator, 1992). 
1.2 Experimental paradigm 
We have studied the features extracted by the BCM model by replicating the experi- 
ments of Biilthoff and Edelman (1991), designed to test generalization from familiar 
to novel views of 3D objects. As in the psychophysical experiments, images of novel 
wire-like computer-generated objects (Biilthoff and Edelman, 1991; Edelman and 
Biilthoff, 1990) were used as stimuli. These objects proved to be easily manipulated, 
and yet complex enough to yield interesting results. Using wires also simplified the 
problem for the feature extractor, as they provided little or no occlusion of the 
key features from any viewpoint. The objects were generated by the Symbolics 
S-Geometry TM modeling package, and rendered with a visualization graphics tool 
(AVS, Stardent, Inc.). Each object consisted of seven connected equal-length seg- 
ments, pointing in random directions and distributed equally around the origin (for 
further details, see Edelman and Biilthoff, 1990). 
In the psychophysical experiments of Biilthoff and Edelman (1991), subjects were 
3D Object Recognition Using Unsupervised Feature Extraction 463 
shown a target wire from two standard views, located 750 apart along the equa- 
tor of the viewing sphere. The target oscillated around each of the two standard 
orientations with an amplitude of -4-15 � about a fixed vertical axis, with views 
spaced at 30 increments. Test views were located either along the equator - on 
the minor arc bounded by the two standard views (INTER condition) or on the 
corresponding major arc (EXTRA condition) - or on the meridian passing through 
one of the standard views (ORTHO condition). Testing was conducted according to 
a two-alternative forced choice (2AFC) paradigm, in which subjects were asked to 
indicate whether the displayed image constituted a view of the target object shown 
during the preceding training session. Test images were either unfamiliar views of 
the training object, or random views of a distractor (one of a distinct set of objects 
generated by the same procedure). 
To apply the above paradigm to the BCM network, the objects were rendered in 
a 63 x 63 array, at 8 bits/pixel, under simulated illumination that combined ambi- 
ent lighting of relative strength 0.3 with a point source of strength 1.0 at infinity. 
The study described below involved six-way classification, which is more difficult 
than the 2AFC task used in the psychophysical experiments. The six wires used 
Figure 2: The six wires used in the computational experiments, as seen from a 
single view point. 
in the experiments are depicted in Figure 2. Given the task of recognizing the six 
wires, the network extracted features that corresponded to small patches of the 
different images, namely areas that either remained relatively invariant under the 
rotation performed during training, or represented distinctive features of specific 
wires (Intrator and Gold, 1991). The classification results were i
