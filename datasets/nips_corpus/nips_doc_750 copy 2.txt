Identifying Fault-Prone Software 
Modules Using Feed-Forward Networks: 
A Case Study 
N. Karunanithi 
Room 2E-378, Bellcore 
435 South Street 
Morristown, NJ 07960 
E-mail: karun@faline. bellcore. com 
Abstract 
Functional complexity of a software module can be measured in 
terms of static complexity metrics of the program text. Classify- 
ing software modules, based on their static complexity measures, 
into different fault-prone categories is a difficult problem in soft- 
ware engineering. This research investigates the applicability of 
neural network classifiers for identifying fault-prone software mod- 
ules using a data set from a commercial software system. A pre- 
liminary empirical comparison is performed between a minimum 
distance based Gaussian classifier, a perceptton classifier and a 
multilayer layer feed-forward network classifier constructed using 
a modified Cascade-Correlation algorithm. The modified version 
of the Cascade-Correlation algorithm constrains the growth of the 
network size by incorporating a cross-validation check during the 
output layer training phase. Our preliminary results suggest that 
a multilayer feed-forward network can be used as a tool for iden- 
tifying fault-prone software modules early during the development 
cycle. Other issues such as representation of software metrics and 
selection of a proper training samples are also discussed. 
793 
794 Karunanithi 
1 Problem Statement 
Developing reliable software at a low cost is an important issue in the area of soft- 
ware engineering (Karunanithi, Whitley and Malaiya, 1992). Both the reliability 
of a software system and the development cost can be reduced by identifying trou- 
blesome software modules early during the development cycle. Many measurable 
program attributes have been identified and studied to characterize the intrinsic 
complexity and the fault proneness of software systems. The intuition behind soft- 
ware complexity metrics is that complex program modules tend to be more error 
prone than simple modules. By controlling the complexity of software modules 
during development, one can produce software systems that are easy to maintain 
and enhance (because simple program modules are easy to understand). Static 
complexity metrics are measured from the passive program texts early during the 
development cycle and can be used as a valuable feedback for allocating resources 
in future development efforts (future releases or new projects). 
Two approachs can be applied to relate static complexity measures with faults 
found or program changes made during testing. In the estimative approach regres- 
sions models are used to predict the actual number of faults that will be disclosed 
during testing (Lipow, 1982; Gaffney, 1984; Shen et al., 1985; Crawford et al., 1985; 
Munson and Khoshgoftaar, 1992). Regression models assume that the metrics that 
constitute independent variables are independent and normally distributed. How- 
ever, most practical measures often violate the normality assumptions and exhibit 
high correlation with other metrics (i.e., multicollinearity). The resulting fit of the 
regression models often tend to produce inconsistent predictions. 
Under the classification approach software modules are categorized into two or more 
fault-prone classes (Rodriguez and Tsai, 1987; Munson and Khoshgoftaar, 1992; 
Karunanithi, 1993; Khoshgoftaar et al., 1993). A special case of the classifica- 
tion approach is to classify software modules into either low-fault (non-complex) or 
high-fault (complex) categories. The main rationale behind this approach is that 
the software managers are often interested in getting some approximate feedback 
from this type of models rather than accurate predictions of the number of faults 
that will be disclosed. Existing two-class categorization models are based on lin- 
ear discriminant principle (Rodriguez and Tsai, 1987; Munson and Khoshgoftaar, 
1992). Linear discriminant models assume that the metrics are orthogonal and that 
they follow a normal distribution. To reduce multicollinearity, researchers often use 
principle component analysis or some other dimensionality reduction techniques. 
However, the reduced metrics may not explain all the variability if the original 
metrics have nonlinear relationship. 
In this paper, the applicability of neural network classifiers for identifying fault 
proneness of software modules is examined. The motivation behind this research is 
to evaluate whether classifiers can be developed without usual assumptions about 
the input metrics. In order to study the usefulness of neural network classifiers, a 
preliminary comparison is made between a simple minimum distance based Gaus- 
sian classifier, a single layer perceptton and a multilayer feed-forward network devel- 
oped using a modified version of Fahlman's Cascade Correlation algorithm (Fahlman 
and Lebiere, 1990). The modified algorithm incorporates a cross-validation for con- 
straining the growth of the size of the network. In this investigation, other issues 
Identifying Fault-Prone Software Modules Using Feed-Forward Networks: A Case Study 795 
such as selection of proper training samples and representation of metrics are also 
considered. 
2 Data Set Used 
The metrics data used in this study were obtained from a research conducted by Lind 
and Vairavan (Lind and Vairavan, 1989) for a Medical Imaging System software. 
The complete system consisted of approximately 4500 modules amounting to about 
400,000 lines of code written in Pascal, FORTRAN, PL/M and assembly level. 
From this set, a random sample of 390 high level language routines was selected 
for the analysis. For each module in the sample, program changes were recorded 
as an indication of software fault. The number of changes in the program modules 
varied from zero to 98. In addition to changes, 11 software complexity metrics 
were extracted from each module. These metrics range from total lines of code 
to Belady's bandwidth metric. (Readers curious about these metrics may refer to 
Table I of Lind and Vairavan, 1989.) For the purpose of our classification study, 
these metrics represent 11 input (both real and integer) variables of the classifier. 
A software module is considered as a low fault-prone module (Category I) if there 
are 0 or 1 changes and as a high fault-prone module (Category II) if there are 10 
or more changes. The remaining modules are considered as medium fault category. 
For the purpose of this study we consider only the low and high fault-prone modules. 
Our extreme categorization and deliberate discarding of program modules is similar 
to the approach used in other studies (Rodriguez and Tsai, 1987; Munson and 
Khoshgoftaar, 1992). After discarding medium fault-prone modules, there are 203 
modules left in the data set. Of 203 modules, 114 modules belong to the low 
fault-prone category while the remaining 89 modules belong to the high fault-prone 
category. The output layer of the neural nets had two units corresponding to two 
fault categories. 
3 Training Data Selection 
We had two objectives in selecting training data: 1) to evaluate how well a neural 
network classifier will perform across different sized training sets and 2) to select 
the training data as much unbiased as possible. The first objective was motivated 
by the need to evaluate whether a neural network classifier can be used early in the 
software development cycle. Thus the classification experiments were conducted 
using training samples of sizeS= �,-,l 2 a 9 � 
3' ' ' if0 fractmn of 203 samples belonging 
to Categories I and II. The remaining (I-S) fraction of the samples were used for 
testing the classifiers. In order to avoid bias in the training data, we randomly 
selected 10 different training samples for each fraction S. This resulted in 6 X 10 
(=60) different training and test sets. 
796 Karunanithi 
4 Classifiers Compared 
4.1 A Minimum Distance Classifier 
In order to compare neural network classifiers and linear discriminant classifiers we 
implemented a simple minimum distance based two-class Gaussian classifier of the 
form (Nilsson, 1990): 
IX - C,I = ((X - C,)(X - C,)') 
where Ci, i = 1, 2 represent the prototype points for the Categories I and II, X is 
a 11 dimensional metrics vector, and t is the transpose operator. The prototype 
points C and C2 are calculated from the training set based on the normality as- 
sumption. In this approach a given arbitrary input vector X is placed in Category 
I if IX - C[ < I X - C21 and in Category II otherwise. 
All raw component metrics had distributions that are asymmetric with a positive 
skew (i.e., long tail to the right) and they had different numerical ranges. Note 
that asymmetric distributions do not conform to the normality assumption of a 
typical Gaussian classifier. First, to remove the extreme asymmetry of the original 
distribution of the individual metric we transformed each metric using a natural 
logarithmic base. Second, to mask the influence of individual component metric on 
the distance score, we divided each metric by its standard deviation of the training 
set. These transformations considerably improved the performance of the Gaussian 
classifier. To be consistent in our comparison we used the log transformed inputs 
for other classifiers also. 
4.2 A Perceptron Classifier 
A perceptron with a hard-limiting threshold can be considered as a realization of a 
non-parametric linear discriminant classifier. If we use a sigmoidal unit, then the 
continuous valued output of the perceptton can be interpreted as a likelihood or 
probability with which inputs are assigned to different classes. In our experiment we 
implemented a perceptton with two sigmoidal units (outputs 1 and 2) corresponding 
to two categories. A given arbitrary vector X is assigned to Category I if the value 
of the output unit 1 is greater than the output of the unit 2 
