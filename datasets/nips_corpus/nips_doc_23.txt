233 
HIGH ORDER NEURAL NETWORKS FOR EFFICIENT 
ASSOCIATIVE MEMORY DESIGN 
I. GUYON*, L. PERSONNAZ*, J.P. NADAL** and G. DREYFUS* 
* Ecole SupOrieure de Physique et de Chimie Industrielles de la Ville de Pads 
Laboratoire d'Electronique 
10, rue Vauquelin 
75005 Paris (France) 
** Ecole Normale Sup0rieure 
Groupe de Physique des Solides 
24, rue Lhomond 
75005 Paris (France) 
ABSTRACT 
We propose learning rules for recurrent neural networks with high-order 
interactions between some or all neurons. The designed networks exhibit the 
desired associative memory function: perfect storage and retrieval of pieces 
of information and/or sequences of information of any complexity. 
INTRODUCTION 
In the field of information processing, an important class of potential 
applications of neural networks arises from their ability to perform as 
associative memories. Since the publication of J. Hopfield's seminal paper 1, 
investigations of the storage and retrieval properties of recurrent networks 
have led to a deep understanding of their properties. The basic limitations of 
these networks are the following: 
- their storage capacity is of the order of the number of neurons; 
- they are unable to handle structured problems; 
- they are unable to classify non-linearly separable data. 
@ American Institute of Physics 1988 
234 
In order to circumvent these limitations, one has to introduce additional 
non-linearities. This can be done either by using hidden, non-linear units, or 
by considering multi-neuron interactions 2. This paper presents learning rules 
for networks with multiple interactions, allowing the storage and retrieval, 
either of static pieces of information (autoassociative memory), or of temporal 
sequences (associative memory), while preventing an explosive growth of the 
number of synaptic coefficients. 
AUTOASSOCIATIVE MEMORY 
The problem that will be addressed in this paragraph is how to design an 
autoassociative memory with a recurrent (or feedback) neural network when 
the number p of prototypes is large as compared to the number n of neurons. 
We consider a network of n binary neurons, operating in a synchronous 
mode, with period . The state of neuron i at time t is denoted by (i(t), and the 
state of the network at time t is represented by a vector (t)whose 
components are the (i(t). The dynamics of each neuron is governed by the 
following relation: 
(i(t+) = sgn vi(t ). (1) 
In networks with two-neuron interactions only, the potential vi(t ) is a linear 
function of the state of the network: 
n 
For autoassociative memory design, it has been shown 3 that any set of 
correlated patterns, up to a number of patterns p equal to 2 n, can be made the 
stable states of the system, provided the synaptic matrix is computed as the 
orthogonal projection matrix onto the subspace spanned by the stored 
vectors. However, as p increases, the rank of the family of prototype vectors 
will increase, and finally reach the value of n. In such a case, the synaptic 
matrix reduces to the identity matrix, so that all 2 n states are stable and the 
energy landscape becomes flat. Even if such an extreme case is avoided, the 
attractivity of the stored states decreases with increasing p, or, in other terms, 
235 
the number of fixed points which are not the stored patterns increases; this 
problem can be alleviated to a large extent by making a useful use of these 
spurious fixed points 4. Another possible solution consists in gardening the 
state space in order to enlarge the basins of attraction of the fixed points 5. 
Anyway, no dramatic improvements are provided by all these solutions since 
the storage capacity is always O(n). 
We now show that the introduction of high-order interactions between 
neurons, increases the storage capacity proportionally to the number of 
connections per neuron. The dynamical behaviour of neuron i is still governed 
by (1). We consider two and three-neuron interactions, extension to higher 
order are straightforward. 
The potential vi(t ) is now defined as 
vi(t ) = ,.,j Ci, j oj(t) + ,T_.,j, I Ci,jl oj(t) ol(t). 
It is more convenient, for the derivation of learning rules, to write the potential 
in the matrix form ' 
v(t) = C /(t), 
where 3(t) is an m dimensional vector whose components are taken among 
the set of the (n2+n)/2 values � (1 , ..-, On , (1(2 , ', (j (1 , -.-, (n-1 On' 
As in the case of the two-neuron interactions model, we want to compute the 
interaction coefficients so that the prototypes are stable and attractor states. 
A condition to store a set of states (k (k=l to p) is that v_k= (k for all k. Among 
the solutions, the most convenient solution is given by the (n,m) matrix 
C = y.,[ 1 (2) 
where 7_., is the (n,p) matrix whose columns are the _q.k and .1 is the (p,m) 
pseudoinverse of the (m,p) matrix ' whose columns are the 3 k . This solution 
satisfies the above requirements, up to a storage capacity which is related to 
the dimension m of vectors 35 Thus, in a network with three-neuron 
236 
interactions, the number of patterns that can be stored is O(n2). Details on 
these derivations are published in Ref.6. 
By using only a subset of the products {(j (1}, the increase in the number of 
synaptic coefficients can remain within acceptable limits, while the attractivity 
of the stored patterns is enhanced, even though their number exceeds the 
number of neurons ; this will be examplified in the simulations presented 
below. 
Finally, it can be noticed that, if vector :ycontains all the {(i (j}, i=1 ,...n, j=l ,...n, 
only, the computation of the vector potential v=C:ycan be performed after the 
following expression: 
V : [ {(I]T[))} I ([T_)) 
where  stands for the operation which consists in squaring all the matrix 
coefficients. Hence, the computation of the synaptic coefficients is avoided, 
memory and computing time are saved if the simulations are performed on a 
conventional computer. This formulation is also meaningful for optical 
implementations, the function  being easily performed in optics 7. 
In order to illustrate the capabilities of the learning rule, we have performed 
numerical simulations which show the increase of the size of the basins of 
attraction when second-order interactions, in addition to the first-order ones, 
are used. The simulations were carried out as follows. The number of neurons 
n being fixed, the amount of second-order interactions was chosen ; p 
prototype patterns were picked randomly, their components being _+1 with 
probability 0.5; the second-order interactions were chosen randomly. The 
synaptic matrix was computed from relation (2). The neural network was 
forced into an initial state lying at an initial Hamming distance H i from one of 
the prototypes _k; it was subsequently left to evolve until it reached a stable 
state at a distance Hf from _k. This procedure was repeated many times for 
each prototype and the Hf were averaged over all the tests and all the 
prototypes. 
Figures la. and lb. are charts of the mean values of Hf as a function of the 
number of prototypes, for n = 30 and for various values of m (the dimension of 
237 
vector :y). These curves allowed us to determine the maximum number of 
prototype states which can be stored for a given quality of recall. Perfect recall 
implies Hf =0; when the number of prototypes increases, the error in recall 
may reach Hf--Hi: the associative memory is degenerate. The results 
obtained for H i/n =10% are plotted on Figure l a. When no high-order 
interactions were used, Hf reached H i for p/n = 1, as expected; conversely, 
virtually no error in recall occured up to p/n = 2 when all second-order 
interactions were taken into account (m=465). Figure lb shows the same 
quantities for Hi=20%; since the initial states were more distant from the 
prototypes, the errors in recall were more severe. 
1,2 
1.0 
0.8 
^ 0.6 
� 
0.4 
0.2 
0.0 
0 1 2 3 
pin 
(a) 
1.2 
1.0 
0,8 
0.6 
0.4 
0.2 
0.0 
0 
i I 
pin 
(b) 
Fig. 1. Improvement of the attractivity by addition of three-neuron interactions 
to the two-neuron interactions. All prototypes are always stored exactly (all 
curves go through the origin). Each point corresponds to an average over 
min(p,10) prototypes and 30 tests for each prototype. 
r'l Projection' m = n = 30; � m = 120; � m = 180; {) m = 465 (all interactions) 
la: H i/n=10% ; lb:H i/n=20%. 
TEMPORAL SEQUENCES (ASSOCIATIVE MEMORY) 
The previous section was devoted to the storage and retrieval of items of 
information considered as fixed points of the dynamics of the network 
(autoassociative memory design). However, since fully connected neural 
networks are basically dynamical systems, they are natural candidates for 
238 
storing and retrieving information which is dynamical in nature, i.e., temporal 
sequences of patterns8: In this section, we propose a general solution to the 
problem of storing and retrieving sequences of arbitrary complexity, in 
recurrent networks with parallel dynamics. 
Sequences consist in sets of transitions between states _k_> (k+l, k=l ,..., p. 
A sufficient condition to store these sets of transitions is that vk= (k+l for all k. 
In the case of a linear potential v=C _, the storage prescription proposed in 
ref.3 can be used: C=7_.,+ I , 
where 7., is a matrix whose columns are the (k and 7., + is the matrix whose 
columns are the successors (k+l of (k. If p is larger than n, one can use 
high-order interactions, which leads to introduce a non-linear potential v=C , 
with as previously defined. We proposed in ref.10 the following storage 
prescription: 
C=Z+F I (3) 
The two above prescriptions are only valid for storing simple sequences, 
where no patterns occur twice (or more). Suppose that one pattern occurs 
twice; when the network reaches this b/furcat/on po/nt, it is unable to make a 
decision according the deterministic dynamics described in (1), since the 
knowledge of the pr
