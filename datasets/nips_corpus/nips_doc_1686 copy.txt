Spiking Boltzmann Machines 
Geoffrey E. Hinton 
Gatsby Computational Neuroscience Unit 
University College London 
London WCiN 3AR, UK 
hinton @gatshy. ucl. ac. uk 
Andrew D. Brown 
Department of Computer Science 
University of Toronto 
Toronto, Canada 
andy@cs. utoronto. ca 
Abstract 
We first show how to represent sharp posterior probability distribu- 
tions using real valued coefficients on broadly-tuned basis functions. 
Then we show how the precise times of spikes can be used to con- 
vey the real-valued coefficients on the basis functions quickly and 
accurately. Finally we describe a simple simulation in which spik- 
ing neurons learn to model an image sequence by fitting a dynamic 
generafive model. 
I Population codes and energy landscapes 
A perceived object is represented in the brain by the activities of many neurons, but 
there is no general consensus on how the activities of individual neurons combine to 
represent the multiple properties of an object. We start by focussing on the case of 
a single object that has multiple instantiation parameters such as position, velocity, 
size and orientation. We assume that each neuron has an ideal stimulus in the space 
of instantiation parameters and that its activation rate or probability of activation 
falls off monotonically in all directions as the actual stimulus departs from this ideal. 
The semantic problem is to define exactly what instantiation parameters are being 
represented when the activities of many such neurons are specified. 
Hinton, Rumelhart and McClelland (1986) consider binary neurons with receptive 
fields that are convex in instantiation space. They assume that when an object 
is present it activates all of the neurons in whose receptive fields its instantiation 
parameters lie. Consequently, if it is known that only one object is present, the 
parameter values of the object must lie within the feasible region formed by the 
intersection of the receptive fields of the active neurons. This will be called a con- 
junctive distributed representation. Assuming that each receptive field occupies 
only a small fraction of the whole space, an interesting property of this type of 
coarse coding is that the bigger the receptive fields, the more accurate the repre- 
sentation. However, large receptive fields lead to a loss of resolution when several 
objects are present simultaneously. 
When the sensory input is noisy, it is impossible to infer the exact parameters of 
objects so it makes sense for a perceptual system to represent the probability dis- 
tribution across parameters rather than just a single best estimate or a feasible 
region. The full probability distribution is essential for correctly combining infor- 
Spiking Boltzmann Machines 123 
E(x) 
P(x) 
Figure 1: a) Energy landscape over a one- 
dimensional space. Each neuron adds a 
dimple (dotted line) to the energy land- 
scape (solid line). b) The corresponding 
probability density. Where dimples over- 
lap the corresponding probability density 
becomes sharper. Since the dimples decay 
to zero, the location of a sharp probabili- 
ty peak is not affected by distant dimples 
and multimodal distributions can be rep- 
resented. 
marion from different times or different sources. One obvious way to represent this 
distribution (Anderson and van Essen, 1994) is to allow each neuron to represent 
a fairly compact probability distribution over the space of instantiation parameters 
and to treat the activity levels of neurons as (unnormalized) mixing proportions. 
The semantics of this disjunctive distributed representation is precise, but the per- 
cepts it allows are not because it is impossible to represent distributions that are 
sharper than the individual receptive fields and, in high-dimensional spaces, the 
individual fields must be broad in order to cover the space. Disjunctive represen- 
tations are used in Kohonen's self-organizing map which is why it is restricted to 
very low dimensional latent spaces. 
The disjunctive model can be viewed as an attempt to approximate arbitrary smooth 
probability distributions by adding together probability distributions contributed 
by each active neuron. Coarse coding suggests a multiplicative approach in which 
the addition is done in the domain of energies (negative log probabilities). Each 
active neuron contributes an energy landscape over the whole space of instantiation 
parameters. The activity level of the neuron multiplies its energy landscape and the 
landscapes for all neurons in the population are added (Figure 1). If, for example, 
each neuron has a full covariance Gaussian tuning function, its energy landscape 
is a parabolic bowl whose curvature matrix is the inverse of the covariance matrix. 
The activity level of the neuron scales the inverse covariance matrix. If there are 
k instantiation parameters then only k -F k(k -F 1)/2 real numbers are required to 
span the space of means and inverse covariance matrices. So the real-valued activ- 
ities of O(k 2) neurons are sufficient to represent arbitrary full covariance Gaussian 
distributions over the space of instantiation parameters. 
Treating neural activities as multiplicative coefficients on additive contributions to 
energy landscapes has a number of advantages. Unlike disjunctive codes, vague 
distributions are represented by low activities so significant biochemical energy is 
only required when distributions are quite sharp. A central operation in Bayesian 
inference is to combine a prior term with a likelihood term or to combine two 
conditionally independent likelihood terms. This is trivially achieved by adding 
two energy landscapes  . 
We thank Zoubin Ghahramani for pointing out that another important operation, 
convolving a probability distribution with Gaussian noise, is a difficult non-linear operation 
on the energy landscape. 
124 G. E. Hinton and A.D. Brown 
2 Representing the coefficients on the basis functions 
To perform perception at video rates, the probability distributions over instantiation 
parameters need to be represented at about 30 frames per second. This seems 
difficult using relatively slow spiking neurons because it requires the real-valued 
multiplicative coefficients on the basis functions to be communicated accurately and 
quickly using all-or-none spikes. The trick is to realise that when a spike arrives 
at another neuron it produces a postsynaptic potential that is a smooth function 
of time. So from the perspective of the postsynaptic neuron, the spike has been 
convolved with a smooth temporal function. By adding a number of these smooth 
functions together, with appropriate temporal offsets, it is possible to represent any 
smoothly varying sequence of coefficient values on a basis function, and this makes 
it possible to represent the temporal evolution of probability distributions as shown 
in Figure 2. The ability to vary the location of a spike in the single dimension of 
time thus allows real-valued control of the representation of probability distributions 
over multiple spatial dimensions. 
a) b) 
Encoded Value Time 
time 
neuron 2 
neuron 1  ..,.' / 
/.., 
Figure 2: a)Two spiking neurons centered at 0 and 1 can represent the time-varying 
mean and standard deviation on a single spatial dimension. The spikes are first 
convolved with a temporal kernel and the resulting activity values are treated as 
exponents on Gaussian distributions centered at 0 and 1. The ratio of the activi- 
ty values determines the mean and the sum of the activity values determines the 
inverse variance. b) The same method can be used for two (or more) spatial di- 
mensions. Time flows from top to bottom. Each spike makes a contribution to the 
energy landscape that resembles an hourglass (thin lines). The waist of the hour- 
glass corresponds to the time at which the spike has its strongest effect on some 
post-synaptic population. By moving the hourglasses in time, it is possible to get 
whatever temporal cross-sections are desired (thick lines) provided the temporal 
sampling rate is comparable to the time course of the effect of a spike. 
Our proposed use of spike timing to convey real values quickly and accurately does 
not require precise coincidence detection, sub-threshold oscillations, modifiable time 
delays, or any of the other paraphernalia that has been invoked to explain how the 
brain could make effective use of the single, real-valued degree of freedom in the 
timing of a spike (Hopfield, 1995). 
The coding scheme we have proposed would be far more convincing if we could 
show how it was learned and could demonstrate that it was effective in a simula- 
tion. There are two ways to design a learning algorithm for such spiking neurons. 
We could work in the relatively low-dimensional space of the instantiation param- 
eters and design the learning to produce the right representations and interactions 
between representations in this space. Or we could treat this space as an implicit 
emergent property of the network and design the learning algorithm to optimize 
Spiking Boltzmann Machines 125 
some objective function in the much higher-dimensional space of neural activities 
in the hope that this will create representations that can be understood using the 
implicit space of instantiation parameters. We chose the latter approach. 
3 A learning algorithm for restricted Boltzmann machines 
Hinton (1999) describes a learning algorithm for probabilistic generative models 
that are composed of a number of experts. Each expert specifies a probability 
distribution over the visible variables and the experts are combined by multiplying 
these distributions together and renormalizing. 
IlmPm(dlOm) 
P(dlO'O') = Y-]i IlmPm(CilOm) 
(1) 
where d is a data vector in a discrete space, 0, is all the parameters of individual 
model m, pm(dlOm ) is the probability of d under model m, and i is an index over 
all possible vectors in the data space
