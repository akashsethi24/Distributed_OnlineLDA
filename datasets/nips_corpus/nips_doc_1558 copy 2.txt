Dynamically Adapting Kernels in Support 
Vector Machines 
Nello Cristianini 
Dept. of Engineering Mathematics 
University of Bristol, UK 
nello. crist ianinibristol. ac. uk 
Colin Campbell 
Dept. of Engineering Mathematics 
University of Bristol, UK 
c. campbellbristol. ac. uk 
John Shawe-Taylor 
Dept. of Computer Science 
Royal Holloway College 
j ohnldcs. rhbnc. ac. uk 
Abstract 
The kernel-parameter is one of the few tunable parameters in Sup- 
port Vector machines, controlling the complexity of the resulting 
hypothesis. Its choice amounts to model selection and its value is 
usually found by means of a validation set. We present an algo- 
rithm which can automatically perform model selection with little 
additional computational cost and with no need of a validation set. 
In this procedure model selection and learning are not separate, 
but kernels are dynamically adjusted during the learning process 
to find the kernel parameter which provides the best possible upper 
bound on the generalisation error. Theoretical results motivating 
the approach and experimental results confirming its validity are 
presented. 
I Introduction 
Support Vector Machines (SVMs) are learning systems designed to automatically 
trade-off accuracy and complexity by minimizing an upper bound on the general- 
isation error provided by VC theory. In practice, however, SVMs still have a few 
tunable parameters which need to be determined in order to achieve the right bal- 
ance and the values of these are usually found by means of a validation set. One 
of the most important of these is the kernel-parameter which implicitly defines the 
structure of the high dimensional feature space where the maximal margin hyper- 
plane is found. Too rich a feature space would cause the system to overfit the data, 
Dynamically Adapting Kernels in Support Vector Machines 205 
and conversely the system can be unable to separate the data if the kernels are too 
poor. Capacity control can therefore be performed by tuning the kernel parameter 
subject to the margin being maximized. For noisy datasets, yet another quantity 
needs to be set, namely the soft-margin parameter C. 
SVMs therefore display a remarkable dimensionality reduction for model selection. 
Systems such as neural networks need many different architectures to be tested and 
decision trees are faced with a similar problem during the pruning phase. On the 
other hand SVMs can shift from one model complexity to another by simply tuning 
a continuous parameter. 
Generally, model selection by SVMs is still performed in the standard way: by 
learning different SVMs and testing them on a validation set in order to determine 
the optimal value of the kernel-parameter. This is expensive in terms of computing 
time and training data. In this paper we propose a different scheme which dy- 
namically adjusts the kernel-parameter to explore the space of possible models at 
little additional computational cost compared to fixed-kernel learning. Futhermore 
this approach only makes use of training-set information so it is more efficient in a 
sample complexity sense. 
Before proposing the model selection procedure we first prove a theoretical result, 
namely that the margin and structural risk minimization (SRM) bound on the gen- 
eralization error depend smoothly on the kernel parameter. This can be exploited 
by an algorithm which keeps the system close to maximal margin while the kernel 
parameter is changed smoothly. During this phase, the theoretical bound given by 
SRM theory can be computed. The best kernel-parameter is the one which gives the 
lowest possible bound. In section 4 we present experimental results showing that 
model selection can be efficiently performed using the proposed method (though we 
only consider Gaussian kernels in the simulations outlined). 
2 Support Vector Learning 
The decision function implemented by SV machines can be written as: 
where the (? are obtained by maximising the following Lagrangian (where m is the 
number of patterns)' 
L _._ 
E (i- 1/2 E oqojYiYjK(xi,xj) 
i=1 i,j=l 
with respect to the oq, subject to the constraints 
Oi _> 0  oqy i -- 0 
i----1 
and where the functions K(x, x') are called kernels. The kernels provide an expres- 
sion for dot-products in a high-dimensional feature space [1]: 
206 N. Cristianini, C. Campbell and d. Shawe-Taylor 
and also implicitly define the nonlinear mapping Cg(x) of the training data into 
feature space where they may be separated using the maximal margin hyperplane. 
A number of choices of kernel-function can be made e.g. Gaussians kernels: 
K(x, x') = e 
The following upper bound can be proven from VC theory for the generalisation 
error using hyperplanes in feature space [7, 9]' 
where/ is the radius of the smallest ball containing the training set, m the number 
of training points and '), the margin (cf. [2] for a complete survey of the generaliza- 
tion properties of SV machines). 
The Lagrange multipliers cq are usually found by means of a Quadratic Program- 
ming optimization routine, while the kernel-parameters are found using a validation 
set. As illustrated in Figure 1 there is a minimum of the generalisation error for 
that value of the kernel-parameter which has the best trade-off between overfitting 
and ability to find an efficient solution. 
013 
012 
O09 
0 O8 
0 O7 
006 
0 O5 
0 O4 
3 4 
5 6 7 8 9 10 
Figure 1' Generalization error (y-axis) as a function of a (x-axis) for the mirror sym- 
metry problem (for Gaussian kernels with zero training error and maximal margin, 
m = 200, n = 30 and averaged over 105 examples). 
3 Automatic Model Order Selection 
We now prove a theorem which shows that the margin of the optimal hyperplane is a 
smooth function of the kernel parameter, as is the upper bound on the generalisation 
error. First we state the Implicit Function Theorem. 
Implicit Function Theorem [10]: Let F(x,) be a continuously differentiable 
function, 
F'UCxVCP--} 
and let (a,) C U x V be a solution to the equation (x,) = 0. Let the partial 
OF  . 
derivatives matrix mid = (b--, w.r.t y be full rank at (a,). Then, near (a,), 
Dynamically Adapting Kernels in Support Vector Machines 207 
there exists one and only one function  = (x) such that F(x,(x)) - 0, and such 
function is continuous. 
Theorem: The margin'7 of SV machines depends smoothly on the kernel parameter 
0'. 
Proof: Consider the function  � E C_ R - A C_ RP,  � rr  (5o, A) which given the 
data maps the choice of rr to the optimal parameters (o and lagrange parameter A 
of the SV machine with Kernel matrix Gij - yiYjK(o'; xi, xj)). Let 
p 
ra() =  i - 1/2  ijiYjK(ff; xi,xj) + ( Yii) 
i=1 ,j i 
be the functional that the SV machine maximizes. Fix a value of a and let o(a) be 
the corresponding solution of W(5). Let I be the set of indices for which 5j(a)  O. 
We may assume that the submatrix of G indexed by I is non-singular since otherwise 
the maximal margin hyperplane could be expressed in terms of a subset of indices. 
Now choose a mimal set of indices J containing I such that the corresponding 
submatrix of G is non-singular and all of the points indexed by J have margin 1. 
Now consider the function F(a, 5, A)i = ()j, ,i k 1, F(a, 5, A)0 = Ej YjSj in 
the neighbourhood of a, where ji is an enumeration of the elements of J, 
OW 
= 1 - + 
i 
and satisfies the equation F(a,(a),A(a)) = 0 at the extremal points of W(5). 
Then the SV function is the implicit function, (5o, A) = (a), and is continuous 
(and unique) iff F is continuously differentiable and the partial derivatives matrix 
w.r.t. 5, A is full rank. But the partial derivatives matrix H is given by 
Sij -- Ojz = Yji Yjj K(a; Xj,, Xj ) = Hji, i, j  1, 
for Ji,j  J, which was non-degenerate by definition of J, while 
0F0 0F0 0Fj 
H00 - OA - 0 and H0j - 0oj - yjj = 0& - Hj0,j  1. 
Consider any non-zero ( satisfying y.j cUy j = O, and any A. We have 
(o, ,k)TH(o, A) = oTG + 2AoTy : TGo > O. 
Hence, the matrix H is non-singular for ( satisfying the given linear constraint. 
Hence, by the implicit function theorem g is a continuous function of rr. The 
following is proven in [2]' 
--1 
-- O i 
i----1 
which shows that 3' is a continuous function of rr. As the radius of the ball containing 
the points is also a continuous function of rr, and the generalization error bound has 
the form  _< C�)ll�()11 for some constant C, we have the following corollary. 
Corollary: The bound on the generalization error is smooth in 
This means that, when the margin is optimal, small variations in the kernel pa- 
rameter will produce small variations in the margin (and in the bound on the 
generalisation error). Thus % m 3',+ and after updating the a, the system will 
208 N. Cristianini, C. Campbell and J. Shawe-Taylor 
still be in a sub-optimal position. This suggests the following strategy for Gaussian 
kernels, for instance: 
Kernel Selection Procedure 
1. Initialize a to a very small value 
2. Maximize the margin, then 
� Compute the SRM bound (or observe the validation error) 
� Increase the kernel parameter: a - a + 6a 
3. Stop when a predetermined value of a is reached else repeat step 2. 
This procedure takes advantage of the fact that for very small rr convergence is 
generally very rapid (overfitting the data, of course), and that once the system is 
near the equilibrium, few iterations will always be sufficient to move it back to the 
maximal margin situation. In other words, this system is brought to a maximal 
margin state in the beginning, when this is computationally very cheap, and then it 
is actively kept in that situation by continuously adjusting the ct while the kernel- 
parameter is gradually increased. 
In the next section we will experimentally investigate this procedure for real-life 
datasets. In the numerical simulations we have used the Kernel-Adatron (KA) 
algorithm recently developed by two of
