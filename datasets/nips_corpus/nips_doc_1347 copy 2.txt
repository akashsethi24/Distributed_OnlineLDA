Asymptotic Theory for Regularization: 
One-Dimensional Linear Case 
Petri Koistinen 
Roll Nevanlinna Institute, P.O. Box 4, FIN-00014 University of Helsinki, 
Finland. Emaih Petri.Koistinen@rni.helsinki.fi 
Abstract 
The generalization ability of a neural network can sometimes be 
improved dramatically by regularization. To analyze the improve- 
ment one needs more refined results than the asymptotic distri- 
bution of the weight vector. Here we study the simple case of 
one-dimensional linear regression under quadratic regularization, 
i.e., ridge regression. We study the random design, misspecified 
case, where we derive expansions for the optimal regularization pa- 
rameter and the ensuing improvement. It is possible to construct 
examples where it is best to use no regularization. 
1 INTRODUCTION 
Suppose that we have available training data (X, Y1),..., (X,,, Y,,) consisting of 
pairs of vectors, and we try to predict Y/on the basis of Xi with a neural network 
with weight vector w. One popular way of selecting w is by the criterion 
n 
I , t(Xi, Yi, w) + AQ(w) - min!, 
(1)  
1 
where the loss (x,y,w) is, e.g., the squared error ]]y- g(x,w)]] 2, the function 
g(.,w) is the input/output function of the neural network, the penalty Q(w) is 
a real function which takes on small values when the mapping g(-, w) is smooth 
and high values when it changes rapidly, and the regularization parameter A is a 
nonnegative scalar (which might depend on the training sample). We refer to the 
setup (1) as (training with) regularization, and to the same setup with the choice 
A - 0 as training without regularization. Regularization has been found to be very 
effective for improving the generalization ability of a neural network especially when 
the sample size n is of the same order of magnitude as the dimensionality of the 
parameter vector w, see, e.g., the textbooks (Bishop, 1995; Ripley, 1996). 
Asymptotic Theory for Regularization: One-Dimensional Linear Case 295 
In this paper we deal with asymptotics in the case where the architecture of the 
network is fixed but the sample size grows. To fix ideas, let us assume that the 
training data is part of an i.i.d. (independent, identically distributed) sequence 
(X,�); (Xl,�1), (X2,�2),... of pairs of random vectors, i.e., for each i the pair 
(Xi,Y/) has the same distribution as the pair (X,�) and the collection of pairs is 
independent (X and � can be dependent). Then we can define the (prediction) risk 
of a network with weights w as the expected value 
:= 
Let us denote the minimizer of (1) by tb,,(,k), and a minimizer of the risk r by 
w*. The quantity r(,,(,k)) is the average prediction error for data independent of 
the training sample. This quantity r(t,,(,k)) is a random variable which describes 
the generalization performance of the network: it is bounded below by r(w*) and 
the more concentrated it is about r(w*), the better the performance. We will 
quantify this concentration by a single number, the expected value E r( (h)). We 
are interested in quantifying the gain (if any) in generalization for training with 
versus training without regularization defined by 
(3) 
- 
When regularization helps, this is positive. 
However, relatively little can be said about the quantity (3) without specifying in 
detail how the regularization parameter is determined. We show in the next section 
that provided  converges to zero sufficiently quickly (at the rate op(n-/2)), then 
F. r(n (0)) and F. r(n ()) are equal to leading order. It turns out, that the optimal 
regularization parameter resides in this asymptotic regime. For this reason, delicate 
analysis is required in order to get an asymptotic approximation for (3). In this 
article we derive the needed asymptotic expansions only for the simplest possible 
case: one-dimensional linear regression where the regularization parameter is chosen 
independently of the training sample. 
2 REGULARIZATION IN LINEAR REGRESSION 
We now specialize the setup (1) to the case of linear regression and a quadratic 
smoothness penalty, i.e., we take (x, y, w) -- [y- xTw] 2 and Q(w) - wTRw, where 
now y is scalar, x and w are vectors, and R is a symmetric, positive definite matrix. 
It is well known (and easy to show) that then the minimizer of (1) is 
(4)  (,k) XiX + ,kR 1 ' 
= - XY. 
n 
i 1 
This is called the generalized ridge regression estimator, see, e.g., (Titterington, 
1985); ridge regression corresponds to the choice R = I, see (Hoerl and Kennard, 
1988) for a survey. Notice that (generalized) ridge regression is usually studied in 
the fixed design ca.e, where Xi:s are nonrandom. Further, it is usually assumed 
that the model is correctly specified, i.e., that there exists a parameter such that 
Yi = XiTw * + ei, and such that the distribution of the noise term ei does not depend 
on Xi. In contrast, we study the random design, misspecified case. 
Assuming that E IIxII 2 <  and that E [XX T] is invertible, the minimizer of the 
risk (2) and the risk itself can be written as 
(5) w* -- A-iI[XY], with A := I[XX T] 
(6) r(w) = r(w*) + (w - w*) n(w - $*). 
296 P Koistinen 
If Z, is a sequence of random variables, then the notation Z,, = Op(n -a) means 
that naZ,, converges to zero in probability as n  vo. For this notation and the 
mathematical tools needed for the following proposition see, e.g., (Serfling, 1980, 
Ch. 1) or (Brockwell and Davis, 1987, Ch. 6). 
Proposition 1 Suppose that EY 4 < oo, E IlXll 4 < oo and that A - E [XX T] is in- 
vertible. If A = op(n-1/2), then both v/-(�.(0)-w*) and v/-0b.(A)- w *) converge 
in distribution to N(O,C), a normal distribution with mean zero and covariance 
matrix C. 
The previous proposition also generalizes to the nonlinear case (under more compli- 
cated conditions). Given this proposition, it follows (under certain additional con- 
ditions) by Taylor expansion that both Er(b,())-r(w*) and Er(b(0))- r(w*) 
admit the expansion fin -1 + o(n -) with the same constant fi. Hence, in the 
regime A = Op(n -1/2) we need to consider higher order expansions in order to 
compare the performance of b, (A) and b (0). 
3 ONE-DIMENSIONAL LINEAR REGRESSION 
We now specialize the setting of the previous section to the case where x is scalar. 
Also, from now on, we only consider the case where the regularization parameter 
for given sample size n is deterministic; especially A is not allowed to depend on 
the training sample. This is necessary, since coefficients in the following type of 
asymptotic expansions depend on the details of how the regularization parameter 
is determined. The deterministic case is the easiest one to analyze. 
We develop asymptotic expansions for the criterion 
(7) 
&(k) := - 
where now the regularization parameter k is deterministic and nonnegative. The 
expansions we get turn out to be valid uniformly for k > 0. We then develop 
asymptotic formulas for the minimizer of J, and also for J,(0) - inf J. The last 
quantity can be interpreted as the average improvement in generalization perfor- 
mance gained by optimal level of regularization, when the regularization constant 
is allowed to depend on n but not on the training sample. 
From now on we take Q(w) - w 2 and assume that A - E X 2 = i (which could be 
arranged by a linear change of variables). Referring back to formulas in the previous 
section, we see that 
(8) 
whence J,(k) = Eh(r,,,,,k), where we have introduced the function h (used 
heavily in what follows) as well as the arithmetic means , and z, 
(9) 
(10) 
r,,:=-yUi, z '--1 V/, with 
1 1 
ui := 
For convenience, also define U := X 2 - 1 and V := XY -w*X 2. Notice that 
U; U, U2,... are zero mean i.i.d. random variables, and that V; �, V2,... satisfy 
the same conditions. Hence f, and c, converge to zero, and this leads to the idea 
of using the Taylor expansion of h(u, v, k) about the point (u, v) = (0, 0) in order 
to get an expansion for J,,(k). 
Asymptotic Theory for Regutarization: One-Dimensional Linear Case 297 
To outline the ideas, let Tj (u, v, k) be the degree j Taylor polynomial of (u, v)  
h(u,v,k) about (0,0), i.e., Tj(u,v,k) is a polynomial in u and v whose coeffi- 
cients are functions of k and whose degree with respect to u and v is j. Then 
E Tj(rn, zn, k) depends on n and moments of U and V. By deriving an upper 
bound for the quantity llg ]h(n, zn, k)- Tj(n,Zn, k)l we get an upper bound for 
the error committed in approximating Jn(k) by E Tj(n, n, k). It turns out that 
for odd degrees j the error is of the same order of magnitude in n as for degree 
j - 1. Therefore we only consider even degrees j. It also turns out that the error 
bounds are uniform in k _ 0 whenever j _ 2. To proceed, we need to introduce 
assumptions. 
Assumption 1 E iXl r < o and E IYI s < c for high enough r and s. 
Assumption 2 Either (a) for some constant  > 0 almost surely IX] >/ or (b) 
X has a density which is bounded in some neighborhood of zero. 
Assumption 1 guarantees the existence of high enough moments; the values r = 20 
and s = 8 are sufficient for the following proofs. E.g., if the pair (X, Y) has a 
normal distribution or a distribution with compact support, then moments of all 
orders exist and hence in this case assumption 1 would be satisfied. Without some 
condition such as assumption 2, Jn(0) might fail to be meaningful or finite. The 
following technical result is stated without proof. 
Proposition 2 Let p > 0 and let 0 < EX 2 < oo. If assumption 2 holds, then 
--> [I (X2)] --p , as l], --> 00, 
where the expectation on the left is finite (a) for n >_ 1 (b) for n > 2p provided that 
assumption 2 (a), respectively 2 (b) holds. 
Proposition 3 Let assumptions i and � hold. Then there exist constants no and 
M such that 
w* kE UV 
PROOF SKETCH The formula for ET2(On,'n,k) follows easily by integrating the 
degree two Taylor polynomial term by term. To get the upper bound for R(n, k), 
cons
