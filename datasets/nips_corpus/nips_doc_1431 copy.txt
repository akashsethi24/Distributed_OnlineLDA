Radial Basis Functions: 
a Bayesian treatment 
David Barber* 
Bernhard Schottky 
Neural Computing Research Group 
Department of Applied Mathematics and Computer Science 
Aston University, Birmingham B4 7ET, U.K. 
http://www. ncrg. aston. ac. uk/ 
{D. Barber, B. Schottky}aston. ac. uk 
Abstract 
Bayesian methods have been successfully applied to regression and 
classification problems in multi-layer percepttons. We present a 
novel application of Bayesian techniques to Radial Basis Function 
networks by developing a Gaussian approximation to the posterior 
distribution which, for fixed basis function widths, is analytic in 
the parameters. The setting of regularization constants by cross- 
validation is wasteful as only a single optimal parameter estimate 
is retained. We treat this issue by assigning prior distributions to 
these constants, which are then adapted in light of the data under 
a simple re-estimation formula. 
I Introduction 
Radial Basis Function networks are popular regression and classification tools[10]. 
For fixed basis function centers, RBFs are linear in their parameters and can there- 
fore be trained with simple one shot linear algebra techniques[10]. The use of 
unsupervised techniques to fix the basis function centers is, however, not generally 
optimal since setting the basis function centers using density estimation on the input 
data alone takes no account of the target values associated with that data. Ideally, 
therefore, we should include the target values in the training procedure[7, 3, 9]. Un- 
fortunately, allowing centers to adapt to the training targets leads to the RBF being 
a nonlinear function of its parameters, and training becomes more problematic. 
Most methods that perform supervised training of RBF parameters minimize the 
*Present address: SNN, University of Nijmegen, Geert Grooteplein 21, Nijmegen, The 
Netherlands. http://w. mbfys. kun. nl/snn/ email: dav�dbmbfyz. kun. nl 
Radial Basis Functions: A Bayesian Treatment 403 
training error, or penalized training error in the case of regularized networks[7, 3, 9]. 
The setting of the associated regularization constants is often achieved by compu- 
tationally expensive approaches such as cross-validation which search through a set 
of regularization constants chosen a priori. Furthermore, much of the information 
contained in such computation is discarded in favour of keeping only a single regu- 
larization constant. A single set of RBF parameters is subsequently found by mini- 
mizing the penalized training error with the determined regularization constant. In 
this work, we assign prior distributions over these regularization constants, both for 
the hidden to output weights and the basis function centers. Together with a noise 
model, this defines an ideal Bayesian procedure in which the beliefs expressed in the 
distribution of regularization constants are combined with the information in the 
data to yield a posterior distribution of network parameters[6]. The beauty of this 
approach is that none of the information is discarded, in contrast to cross-validation 
type procedures. Bayesian techniques applied to such non-linear, non-parametric 
models, however, can also be computationally extremely expensive, as predictions 
require averaging over the high-dimensional posterior parameter distribution. One 
approach is to use Markov chain Monte Carlo techniques to draw samples from the 
posterior[8]. A simpler approach is the Laplace approximation which fits a Gaussian 
distribution with mean set to a mode of the posterior, and covariance set to the 
inverse Hessian evaluated at that mode. This can be viewed as a local posterior 
approximation, as the form of the posterior away from the mode does not affect the 
Gaussian fit. A third approach, called ensemble learning, also fits a Gaussian, but is 
based on a less local fit criterion, the Kullback-Leibler divergence[4, 5]. As shown in 
[1], this method can be applied successfully to multi-layer perceptrons, whereby the 
KL divergence is an almost analytic quantity in the adaptable parameters. For fixed 
basis function widths, the KL divergence for RBF networks is completely analytic 
in the adaptable parameters, leading to a relatively fast optimization procedure. 
2 Bayesian Radial Basis Function Networks 
For an N dimensional input vector x, we consider RBFs that compute the linear 
combination of K Gaussian basis functions, 
K 
f(x, rn) =  wtexp {-At[Ix- �tll 2 } (1) 
/=1 
where we denote collectively the centers c..v  and weights w = w...wk by 
the parameter vector m = [c[,..., e, w,.. , ?. We consider the basis function 
widths ,... k to be fixed although, in principle, they can also be adapted by a 
similar technique to the one presented below. The data set that we wish to regress 
is a set of P input-output pairs D = {x u, yU,/ = 1... P}. Assuming that the target 
outputs y have been corrupted with additive Gaussian noise of variance/-x, the 
likelihood of the data is  
p(DIrn,/ ) = exp (-/Et))/Zt), (2) 
where the training error is defined, 
I p 
Er) =  y (f(xt',m)- yt,)2 (3) 
/=l 
To discourage overfitting, we choose a prior regularizing distribution for m 
p(mla) = exp (-Em(rn))/Zp (4) 
In the following, Zt>, Zp and Z, are normalising constants 
404 D. Barber and B. Schottky 
where we take Era(m) = �mq:Am for a matrix A of hyperparameters. More compli- 
cated regularization terms, such as those that penalize centers that move away from 
specified points are easily incorporated in our formalism. For expositional clarity, 
we deal here with only the simple case of a diagonal regularizer matrix A - aI. 
The conditional distribution p(mlD , a, 3) is then given by 
p(m[D,a,/) - exp(--/ED(m) -- Em(m))/ZF (5) 
We choose to model the hyperparameters a and/ by Gamma distributions, 
p(a) cr aa-Xe -c/b p() (X oC--llg --/d , (6) 
where a, b, c, d are chosen constants. This completely specifies the joint posterior, 
p(m, a,/ID) - p(mlD, a, )p(a)p(). (7) 
A Bayesian prediction for a new test point x is then given by the posterior average 
(f(x, m))p(m,c,,BiD ). If the centers are fixed, p(wlD , a,) is Gaussian and com- 
puting the posterior average is trivial. However, with adaptive centers,the posterior 
distribution is typically highly complex and computing this average is difficult 2. We 
describe below approaches that approximate the posterior by a simpler distribution 
which can then be used to find the Bayesian predictions and error bars analytically. 
3 Approximating the posterior 
3.1 Laplace's method 
Laplace's method is an approximation to the Bayesian procedure that fits a Gaussian 
to the mode m0 of p(m, ID, ,/) by extremizing the exponent in (5) 
T- l[mll 2 + ED(m) (8) 
with respect to m. The mean of the approximating distribution is then set to the 
mode too, and the covariance is taken to be the inverse Hessian around too; this is 
then used to approximately compute the posterior average. This is a local method 
as no account is taken for the fit of the Gaussian away from the mode. 
3.2 Kullback-Leibler method 
The Kullback-Leibler divergence between the posterior p(m, a,/[D) and an approx- 
imating distribution q(m, a,/) is defined by 
KL[q]--/q(m,a,)ln/p(m'-'--[D)) 
, q(m, a,/) ' (9) 
KL[q] is zero only if p and q are identical, and is greater than zero otherwise. Since 
in (5) ZF is unknown, we can compute the KL divergence only up to an additive 
constant, L[q] = KL[q] - In ZF. We seek then a posterior approximation of the 
form q(m, a, ) = Q(m)R(a)S(/) where Q(m) is Gaussian and the distributions R 
and $ are determined by minimization of the functional L[q][5]. 
We first consider optimizing L with respect to the mean  and covariance C of 
X(m- )Tc-X(m-- )}. Omitting all 
the Gaussian distribution Q(m) cr exp {-3 
constant terms and integrating out a and/, the Q(m) dependency in L is, 
/ 1 
L[Q(m)] = - Q(m) -/JED(m) -- &llmll 2 - In Q(m) dm + const. (10) 
2The fixed and adaptive center Bayesian approaches are contrasted more fully in [2]. 
Radial Basis Functions: A Bayesian Treatment 405 
where 
a = faR()&,  = f fiS(fi)dfi (11) 
are the mean values of the hyperparameters. For Gaussian basis functions, the 
remaining integration in (10) over Q(m) can be evaluated analytically, giving 3 
1 
L[Q(m)] = & {tr(C) + I1112 } q-/(ED(m))Q --  ln(det C) + const. (12) 
where 
(Ez(rn))Q:  (Y' - 2Y' E s' + E s, (13) 
/----1 /----1 kl----1 
The analytical formulae for 
d' = (wexp{-'xllx-cl12}>Q (14) 
sk -- <wkwexp{-,xllx-clla}exp{-,xllx-clla}>Q (15) 
are straightforward to compute, requiring only Gaussian integration[2]. The values 
for C and  can then be found by optimizing (12). 
We now turn to the functional optimisation of (9) with respect to R. Integrating 
out rn and fi leaves, up to a constant, 
tr(C) [K(/.+ 1) a-1] lna lnR(a) 
(16) 
As the first two terms in (16) constitute the log of a Gamma distribution (6), the 
functional (16) is optimized by choosing a Gamma distribution for a, 
R() o --/ (17) 
with 
K(N + 1) I 11112 + tr(C)+ -, G rs (18) 
r: 2 +a, - = : . 
s 2 
The same procedure for S(fi) yields 
$()-'e -/ (lS) 
with 
P I 1 
u =  + c, -: (S(m)> + 5' j = u, (20) 
where the averaged trning error is ven by (13). The optimization of the ap- 
proximating distribution Q(m)R(a)S(fi) can then be performed using  iterative 
procedure in which we first optimize (12) with respect to  d C for fixed , fi, 
and then update  d j according to the re-estimation formulae (18,20). 
After this iterative procedure h converged, we have an approximating distribution 
of peters, both for the hidden to output weights d center positions (fire 
1 (a)). The tual predictions e then given by the posterior average over this distri- 
bution of networks. The model averaging effect inherent in the Bayesian procedure 
produces a finM function potentially much more complex than t
