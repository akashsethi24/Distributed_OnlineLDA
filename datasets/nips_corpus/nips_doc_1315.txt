Comparison of Human and Machine Word 
Recognition 
M. Schenkel 
Dept of Electrical Eng. 
University of Sydney 
Sydney, NSW 2006, Australia 
schenkel@sedal.usyd.edu.au 
C. Latimer 
Dept of Psychology 
University of Sydney 
Sydney, NSW 2006, Australia 
M. Jabri 
Dept of Electrical Eng. 
University of Sydney 
Sydney, NSW 2006, Australia 
marwan@sedal.usyd.edu.au 
Abstract 
We present a study which is concerned with word recognition rates for 
heavily degraded documents. We compare human with machine read- 
ing capabilities in a series of experiments, which explores the interaction 
of word/non-word recognition, word frequency and legality of non-words 
with degradation level. We also study the influence of character segmen- 
tation, and compare human performance with that of our artificial neural 
network model for reading. We found that the proposed computer model 
uses word context as efficiently as humans, but performs slightly worse 
on the pure character recognition task. 
i Introduction 
Optical Character Recognition (OCR) of machine-print document images 'has matured 
considerably during the last decade. Recognition rates as high as 99.5% have been re- 
ported on good quality documents. However, for lower image resolutions (200 DpI and 
below), noisy images, images with blur or skew, the recognition rate declines consider- 
ably. In bad quality documents, character segmentation is as big a problem as the actual 
character recognition. In many cases, characters tend either to merge with neighbouring 
characters (dark documents) or to break into several pieces (light documents) or both. We 
have developed a reading system based on a combination of neural networks and hidden 
Markov models (HMM), specifically for low resolution and degraded documents. 
To assess the limits of the system and to see where possible improvements are still to be 
Comparison of Human and Machine Word Recognition 95 
expected, an obvious comparison is between its performance and that of the best reading 
system known, the human reader. It has been argued, that humans use an extremely 
wide range of context information, such as current topics, syntax and semantic analysis in 
addition to simple lexical knowledge during reading. Such higher level context is very hard 
to model and we decided to run a first comparison on a word recognition task, excluding 
any context beyond word knowledge. 
The main questions asked for this study are: how does human performance compare with 
our system when it comes to pure character recognition (no context at all) of bad quality 
documents? How do they compare when word context can be used? Does character 
segmentation information help in reading? 
2 Data Preparation 
We created as stimuli 36 data sets, each containing 144 character strings, 72 words and 
72 non-words, all lower case. The data sets were generated from 6 original sets, each 
con. taining 144 unique words/non-words. For each original set we used three ways to 
divide the words into the different degradation levels such that each word appears once in 
each degradation level. We also had two ways to pick segmented/non-segmented so that 
each word is presented once segmented and once non-segmented. This counterbalancing 
creates the 36 sets out of the six original ones. The order of presentation within a test set 
was randomized with respect to degradation, segmentation and lexical status. 
All character strings were printed in 'times roman 10 pt' font. Degradation was achieved 
by photocopying and faxing the printed doc,ument before scanning it at 200DpI. Care was 
taken to randomize the print position of the words such that as few systematic degradation 
differences as possible were introduced. 
Words were picked from a dictionary of the 44,000 most frequent words in the 'Sydney 
Morning Herald'. The length of the words was restricted to be between 5 and 9 characters. 
They were divided in a 3x3x2 mixed factorial model containing 3 word-frequency groups, 
3 stimulus degradation levels and visually segmented/non-segmented words. The three 
word-frequency groups were: I to 10 occurences/million (o/m) as low frequency, 11 to 
40 o/m as medium frequency and 41 or more o/m as high frequency. Each participant 
was presented with four examples per stimulus class (e.g. four high frequency words in 
medium degradation level, not segmented). 
The non-words conformed to a 2x3x2 model containing legal]illegal non-words, 3 stimulus 
degradation levels and visually segmented/non-segmented strings. The illegal non-words 
(e.g. 'ptvca') were generated by randomly selecting a word length between 5 and 9 char- 
acters (using the same word length frequencies as the dictionary has) and then randomly 
picking characters (using the same character frequencies as the dictionary has) and keep- 
ing the unpronouncable sequences. The legal non-words (e.g. 'slunk') were generated by 
using trigrams (using the dictionary to compute the trigram probabilities) and keeping 
pronouncable sequences. Six examples per non-word stimulus class were used in each test 
set. (e.g. six illegal non-words in high degradaton level, segmented). 
3 Human Reading 
There were 36 participants in the study. Participants were students and staff of the 
University of Sydney, recruited by advertisement and paid for their service. They were 
all native English speakers, aged between 19 and 52 with no reported uncorrected visual 
deficits. 
The participants viewed the images, one at a time, on a computer monitor and were asked 
to type in the character string they thought would best fit the image. They had been 
96 M. Schenkel, C. Latimer and M. Jabri 
instructed that half of the character strings were English words and half non-words, and 
they were informed about the degradation levels and the segmentation hints. Participants 
were asked to be as fast and as accurate as possible. After an initial training session of 30 
randomly picked character strings not from an independent training set, the participants 
had a short break and were then presented with the test set, one string at a time. After a 
Carriage Return was typed, time was recorded and the next word was displayed. Training 
and testing took about one hour. The words were about 1-1.5cm large on the screen and 
viewed at a distance of 60cm, which corresponds to a viewing angle of 1 ï¿½. 
4 Machine Reading 
For the machine reading tests, we used our integrated segmentation/recognition system, 
using a sliding window tectinique with a combination of a neural network and an HMM [6]. 
In the following we describe the basic workings without going into too much detail on the 
specific algorithms. For more detailed description see [6]. 
A sliding window approach to word recognition performs no segmentation on the input 
data of the recognizer. It consists basically of sweeping a window over the input word in 
small steps. At each step the window is taken to be a tentative character and corresponding 
character class scores axe produced. Segmentation and recognition decisions are then made 
on the basis of the sequence of character scores produced, possibly taking contextual 
information into account. 
In the preprocessing stage we normalize the word to a fixed height. The result is a 
grey-normalized pixel map of the word. This pixel map is the input to a neural network 
which estimates a posteriori probabilities of occurrence for each character given the input 
in the sliding window whose length corresponds approximately to two characters. We use 
a space displacement neural network (SDNN) which is a multi-layer feed-forward network 
with local connections and shared weights, the layers of which perform successively higher- 
level feature extraction. SDNN's are derived from Time Delay Neural Networks which have 
been successfully used in speech recognition [2] and handwriting recognition [4, 1]. Thanks 
to its convolutional structure the computational complexity of the sliding window approach 
is kept tractable. Only about one eighth of the network connections are reevaluated for 
each new input window. The outputs of the SDNN are processed by an HMM. In our 
case the HMM implements character duration models. It tries to align the best scores of 
the SDNN with the corresponding expected character durations. The Viterbi algorithm is 
used for this alignment, determining simultaneously the segmentation and the recognition 
of the word. Finding this state sequence is equivalent to finding the most probable path 
through the graph which represents the HMM. Normally additive costs are used instead of 
multiplicative probabilities. The HMM then selects the word causing the smallest costs. 
Our best architecture contains 4 convolutional layers with a total of 50,000 parameters [6]. 
The training set consisted of a subset of 180,000 characters from the SEDAL database, a 
low resuloution degraded document database which was collected earlier and is indepen- 
dent of any data used in this experiment. 
4.1 The Dictionary Model 
A natural way of including a dictionary in this process, is to restrict the solution space 
of the HMM to words given by the dictionary. Unfortunately this means calculating the 
cost for each word in the dictionary, which becomes prohibitively slow with increasing 
dictionary size (we use a combination of available dictionaries with a total size of 98,000 
words). We thus chose a two step process for the dictionary search: in a first step a list of 
the most probable words is generated, using a fast-matcher technique. In the second step 
the HMM costs are calculated for the words in the proposed list. 
Comparison of Human and Machine Word Recognition 97 
To generate the word list, we take the character string as found by the HMM without the 
dictionary and calculate the edit-distance between that string and all the words in the 
dictionary. The edit-distance measues how many edit operations (insertion, deletion and 
substit
