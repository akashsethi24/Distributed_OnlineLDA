The Effective Number of Parameters: 
An Analysis of Generalization and Regularization 
in Nonlinear Learning Systems 
John E. Moody 
Department of Computer Science, Yale University 
P.O. Box 2158 Yale Station, New Haven, CT 06520-2158 
Internet: moody@cs.yale.edu, Phone: (203)432-1200 
Abstract 
We present an analysis of how the generalization performance (expected 
test set error) relates to the expected training set error for nonlinear learn- 
ing systems, such as multilayer perceptrons and radial basis functions. The 
principal result is the following relationship (computed to second order) 
between the expected test set and training set errors: 
(1) 
2 is the effective noise 
Here, n is the size of the training sample , O'eff 
variance in the response variable(s),  is a regularization or weight decay 
parameter, and p,ij(A) is the effective number of parameters in the non- 
linear model. The expectations ( ) of training set and test set errors are 
taken over possible training sets  and training and test sets f t respec- 
tively. The effective number of parameters p,ij, (A) usually differs from the 
true number of model parameters p for nonlinear or regularized models; 
this theoretical conclusion is supported by Monte Carlo experiments. In 
addition to the surprising result that p,ii (A)  p, we propose an estimate 
of (1) called the generalized prediction error(GPE) which generalizes well 
established estimates of prediction risk such as Akaike's FPE and AIC, 
Mallows Cp, and Barron's PSE to the nonlinear setting. 1 
GPE and p,fi(A) were previously introduced in Moody (1991). 847 
848 Moody 
1 Background and Motivation 
Many of the nonlinear learning systems of current interest for adaptive control, 
adaptive signal processing, and time series prediction, are supervised learning sys- 
tems of the regression type. Understanding the relationship between generalization 
performance and training error and being able to estimate the generalization per- 
formance of such systems is of crucial importance. We will take the prediction risk 
(expected test set error) as our measure of generalization performance. 
2 Learning from Examples 
Consider a set of n real-valued input/output data pairs (n) = {i _ (xi, yi); i _ 
1,..., n} drawn from a stationary density E(). The observations can be viewed as 
being generated according to the signal plus noise model 2 
yi : It(x i) + ei (2) 
where yi is the observed response (dependent variabl. e), x i are the independent 
variables sampled with input probability density f(x), e' is independent, identically- 
distributed (iid) noise sampled with density (I)(e) having mean 0 and variance or2, 3 
and It(z) is the conditional mean, an 
perspective, the density E() - E(x 
components, the conditional density 
Z(x,y) : 
unknown function. From the signal plus noise 
, y) can be represented as the product of two 
� (yl x) and the input density 
� (ylz) 
q?(y - it(x)) f(x) (3) 
The learning problem is then to find an estimate fi(x) of the conditional mean It(x) 
on the basis of the training set (n). 
In many real world problems, few a priori assumptions can be made about the 
functional form of It(x). Since a parametric function class is usually not known, 
one must resort to a nonparametric regression approach, whereby one constructs an 
estimate fi(x) = f(x) for It(x) from a large class of functions . known to have good 
approximation properties (for example, . could be all possible radial basis func- 
tion networks and multilayer perceptrons). The class of approximation functions is 
usually the union of a countable set of subclasses (specific network architectures) 4 
,4 C - for which the elements of each subclass f(w, x)  ,4 are continuously 
parametrized by a set of p = p(.A) weights w = {w; c = 1,...,p}. The task of 
finding the estimate f(x) thus consists of two problems: choosing the best architec- 
h 
ture A and choosing the best set of weights  given the architecture. Note that in 
2The assumption of additive noise e which is independent of x is a standard assumption 
and is not overly restrictive. Many other conceivable signal/noise models can be trans- 
formed into this form. For example, the mnltiphcative model y = it(x)(1 + e) becomes 
t 
y : it'(x) + for the transformed variable y' = log(y). 
3Note that we have made only a minimal assumption about the noise e, that it is has 
finite variance a2 independent of x. Specifically, we do not need to make the assumption 
that the noise density (I)(e) is of known form (e.g. ganssian) for the following development. 
4For example, a fully connected two layer perceptton with five internal units. 
The Ejectire Number of Parameters 849 
the nonparametric setting, there does not typically exist a function f(w*, z) 
with a finite number of parameters such that f(w*,z): p(z) for arbitrary p(z). 
For this reason, the estimators (z) = f(, z) will be biased estimators of 
The first problem (finding the architecture A) requires a search over possible ar- 
chitectures (e.9. network sizes and topologies), usually starting with small archi- 
tectures and then considering larger ones. By necessity, the search is not usually 
exhaustive and must use heuristics to reduce search complexity. (A heuristic search 
procedure for two layer networks is presented in Moody and Utans (1992).) 
The second problem (finding a good set of weights for f(w,z)) is accomplished by 
minimizing an objective function: 
x = argmin U(, w,(n)) (4) 
The objective function U consists of an error function plus a regularizer: 
u(, w, (.)) =. ,,.(w, (.)) +  S(w) () 
Here, the error S,.(w,(n)) menures the distance between the target response 
values y and the fitted values f(w, zi) � 
  [v' f(w, )] , (o) 
,,.(w,(.)) =  , 
i=1 
and S(w) is a regularization or weight-decay function which bies the solution 
toward functions with a priori desirable characteristics, such as smoothness. The 
parameter  ) 0 is the regularization or weight decay parameter and must itself be 
optimizedfi 
The most familiar example of an objective function uses the squared erroff 
[u, f(w,)]: [u- f(w, )]  nd  wright dy tm: 
n p 
v(X,w,e(.)) = (-i(w,))+  (w ) (7) 
i=1 
The first term is the sum of squared errors (SSE) of the model f(w,z) with respect 
to the training data, while the second term penalizes either small, medium, or 
large weights, depending on the form of 9(w). Two common examples of weight 
decay hnctions are the ridge regression form 9(w ) = (w) 2 (which penalizes large 
a a 2 0 2 a 2 
wigh) nd h aumh fom a( ) = () /[( ) + () ] (which pni 
weights of intermediate values near w�). 
� By biased, we mean that the mean squared bi is nonzero: MSB = f p(x)(((x)}- 
p())2d > 0. Here, p() is some positive weighting function on the input space and 
(} denotes an expected vMued taken over possible trning sets (n). For unbiedness 
(MSB = O) to occur, there must effist a set of weights w* such that 
and the learned weights  must be close to w*. For near unbiedness, we must have 
w*  argminMSB(w) such that (MSB(w*)  O) and  close to w*. 
aThe optimization of A will be discussed in Moody (1992). 
� Other error functions, such  those used in generzed linear models (see for example 
McCuagh and Nelder 1983) or robust statistics (see for example Huber 1981) are more 
appropriate than the squared error if the noise is known to be non-gaussian or the data 
contMns many outers. 
8 50 Moody 
An example of a regularizer which is not explicitly a weight decay term is: 
S(w) - d:Cz(:)llO=f(w, x)11 = This is a smoothing term which penalizes functional fits with high curvature. 
(8) 
3 Prediction Risk 
With fi(x)= f(&[(n)],x)denoting an estimate of the true regression function/(x) 
trained on a data set �(n), we wish to estimate the prediction risk P, which is the 
expected error of fi(x) in predicting future data. In principle, we can either define 
P for models fi(x) trained on arbitrary training sets of size n sampled from the 
unknown density (y[x)f(x) or for training sets of size n with input density equal 
to the empirical density defined by the single training set available: 
i2'() -- _1  5(- ') (9) 
For such training sets, the n inputs x i are held fixed, but the response variables yi 
are sampled with the conditional densities (y[z). Since () is known, but () 
is generally not known a priori, we adopt the latter approach. 
For a large ensemble of such training sets, the expected training set error is 8 
i:1 
1 -.�[yi,f([{(n)],xi)] ' qqyilxi)dy i (10) 
i--1 
For a future exemplar (x,z) sampled with density q(zlx)(x), the prediction risk 
P is defined as: 
P=/�[z,.f([{(n)],x)]q(zlx)(x)(Iq(yilxi)dyi}dzdx (11) 
i=1 
Again, however, we don't assume that (x) is known, so computing (11) is not 
possible. 
Following Akaike (1970), Barron (1984), and numerous other authors (see Eubank 
1988), we can define the prediction risk P as the expected test set error for test sets 
of size n '(n) = {i, _ (xi,zi); i = 1,...,n} having the empirical input density 
f'(x). This expected test set error has form: 
1  �[zi ' f([(n)],xi)] (12) 
<�test(,)){{, ---- i=1 , 
= /l�[zi,f([(n)],xi)]{Iql(yi,xi)ql(zi,xi)dyidzi } 
i=1 i=1 
SFollowing the physics convention, we use angled brackets ( ) to denote expected values. 
The subscripts denote the random variables being integrated over. 
The Elective Number of Parameters 8 51 
We take (12) as a proxy for the true prediction risk P. 
In order to compute (12), it will not be necessary to know the precise functional 
form of the noise density (I)(e). Knowing just the noise variance cr 2 will enable an 
exact calculation for linear models trained with the SSE error and an approximate 
calculation correct to second order for general nonlinear models. The results of 
these calculations are presented in the next two sections. 
4 The Expected 
