Neural Learning in Structured 
Parameter Spaces 
Natural Riernannian Gradient 
Shun-ichi Amari 
RIKEN Frontier Research Program, RIKEN, 
Hirosawa 2-1, Wako-shi 351-01, Japan 
amari@zoo.riken.go.jp 
Abstract 
The parameter space of neural networks has a Riemannian met- 
ric structure. The natural Riemannian gradient should be used 
instead of the conventional gradient, since the former denotes the 
true steepest descent direction of a loss function in the Riemannian 
space. The behavior of the stochastic gradient learning algorithm 
is much more effective if the natural gradient is used. The present 
paper studies the information-geometrical structure of perceptrons 
and other networks, and prove that the on-line learning method 
based on the natural gradient is asymptotically as efficient as the 
optimal batch algorithm. Adaptive modification of the learning 
constant is proposed and analyzed in terms of the Riemannian mea- 
sure and is shown to be efficient. The natural gradient is finally 
applied to blind separation of mixtured independent signal sources. 
I Introduction 
Neural learning takes place in the parameter space of modifiable synaptic weights 
of a neural network. The role of each parameter is different in the neural network 
so that the parameter space is structured in this sense. The Riemannian structure 
which represents a local distance measure is introduced in the parameter space by 
information geometry (Amari, 1985). 
On-line learning is mostly based on the stochastic gradient descent method, where 
the current weight vector is modified in the gradient direction of a loss function. 
However, the ordinary gradient does not represent the steepest direction of a loss 
function in the Riemannian space. A geometrical modification is necessary, and it is 
called the natural Riemannian gradient. The present paper studies the remarkable 
effects of using the natural Riemannian gradient in neural learning. 
128 $. Amari 
We first studies the asymptotic behavior of on-line learning (Opper, NIPS'95 Work- 
shop). Batch learning uses all the examples at any time to obtain the optimal weight 
vector, whereas on-line learning uses an example once when it is observed. Hence, 
in general, the target weight vector is estimated more accurately in the case of batch 
learning. However, we prove that, when the Riemannian gradient is used, on-line 
learning is asymptotically as efficient as optimal batch learning. 
On-line learning is useful when the target vector fluctuates slowly (Am. ari, 1967). 
In this case, we need to modify a learning constant r h depending on how far the 
current weight vector is located from the target function. We show an algorithm 
adaptive changes in the learning constant based on the Riemannian criterion and 
prove that it gives asymptotically optimal behavior. This is a generalization of the 
idea of Sompolinsky et al. [1995]. 
We then answer the question what is the Riemannian structure to be introduced in 
the parameter space of synaptic weights. We answer this problem from the point of 
view of information geometry (Amari [1985, 1995], Amari et al [1992]). The explicit 
form of the Riemannian metric and its inverse matrix are given in the case of simple 
perceptrons. 
We finally show how the Riemannian gradient is applied to blind separation of mix- 
tured independent signal sources. Here, the mixing matrix is unknown so that the 
parameter space is the space of matrices. The Riemannian structure is introduced 
in it. The natural Riemannian gradient is computationary much simpler and more 
effective than the conventional gradient. 
2 Stochastic Gradient Descent and On-Line Learning 
Let us consider a neural network which is specified by a vector parameter w - 
(w,--. w,)  R '. The parameter w is composed of modifiable connection weights 
and thresholds. Let us denote by l(;, w) a loss when input signal ; is processed 
by a network having parameter w. In the case of multilayer percepttons, a desired 
output or teacher signal y is associated with ;, and a typical loss is given 
(1) 
where z = jf(a, w) is the output from the network. 
When input ;, or input-output training pair (;, y), is generated from a fixed prob- 
ability distribution, the expected loss L(w) of the network specified by w is 
(2) 
where E denotes the expectation. A neural network is trained by using training 
examples (a,y), (;r2,y2),- to obtain the optimal network parameter w* that 
minimizes L(w). If L(w) is known, the gradient method'is described by 
tOt+l: tot -- rltVL(wt), t = 1,2,... 
where r/t is a learning constant depending on t and X7L = OL/e9w. Usually L(w) is 
unknown. The stochastic gradient learning method 
wt+ = wt - rltVl(at+, lt+l ; Wt) 
(3) 
was proposed by an old paper (Amari [1967]). This method has become popular 
since Rumelhart et al. [1986] rediscovered it. It is expected that, when r h converges 
to 0 in a certain manner, the above wt converges to w*. The dynamical behavior of 
Neural Learning and Natural Gradient Descent 129 
(3) was studied by Amari [1967], Heskes and Kappen [1992] and many others when 
r/t is a constant. 
It was also shown in Amari [1967] that 
Wt+l -- wt -- rhG-1Vl(;t+l,Yt+l,Wt) 
(4) 
works well for any positive-definite matrix, in particular for the metric G. Ge- 
ometrically speaking O1/Ow is a covariant vector while Awt = wt+l - wt is a 
contravariant vector. Therefore, it is natural to use a (contravariant) metric tensor 
G -1 to convert the covariant gradient into the contravariant form 
71= G - O1 ( .. O ) 
O- = Eg*3-wj(W) , (5) 
where G- 1 _m (gij) is the inverse matrix of G -- (gij). The present paper studies how 
the matrix tensor matrix G should be defined in neural learning and how effective 
is the new gradient learning rule 
wt+ = wt - qt'l(ah, Yt, wt). 
(6) 
3 Gradient in Riemannian spaces 
Let S = {w) be the parameter space and let l(vo) be a function defined on S. 
When S is a Euclidean space and w is an orthonormal coordinate system, the 
squared length of a small incremental vector dw connecting w and w + dw is given 
by 
r 
Id,12- - y](dw) 2. (7) 
i=1 
However, when the coordinate system is non-orthonormal or the space $ is Rieman- 
nian, the squared length is given by a quadratic form 
Idwl 2- gij(m)dwidwj = mtGm. (8) 
i,j 
Here, the matrix G = (gij) depends in general on w and is called the metric tensor. 
It reduces to the identity matrix I in the Euclidean orthonormal ce. It will be 
shown soon that the parameter space S of neural networks has Riemannian structure 
(see Amari et al. [1992], Amari [1995], etc.). 
The steepest descent direction of a function l(w) at w is defined by a vector dw 
that minimize l(w+dw) under the constraint Idw] 2 = ,2 (see eq.8) for a sumciently 
small constant s. 
Lemma 1. The steepest descent direction of l(w) in a Riemannian space is given 
by 
= -a 
We call 
7l(w) = G-l(w)Vl(w) 
the natural gradient of l(w) in the Riemannian space. It shows the steepest descent 
direction of l, and is nothing but the contravariant form of V'l in the tensor notation. 
When the space is Euclidean and the coordinate system is orthonormal, G is the 
unit matrix I so that rl = V1. 
130 $. Arnari 
4 Natural gradient gives efficient on-line learning 
Let us begin with the simplest case of noisy multilayer analog perceptrons. Given 
input , the network emits output z = jf(, w) + n, where jf is a differentiable 
deterministic function of the multilayer perceptron with parameter w and n is a 
noise subject to the normal distribution N(0, I). The probability density of an 
input-output pair (, z) is given by 
p(ae, z; w) = q(ae)p(z]ae; w), 
where q(a) is the probability distribution of input , and 
P(zla; TM)-- x/-- exp -- II II 2. 
The squared error loss function (1) can be written as 
l(, z, w) = - log p(, z; w) + log q() - log x/-. 
Hence, minimizing the loss is equivalent to maximizing the likelihood function 
Let DT = {(,zl),-'-,(T, ZT)} be T independent input-output examples gener- 
ated by the network having the parameter w*. Then, maximum likelihood estimator 
/VT minimizes the log loss l(, z; w) = -logp(, z; w) over the training data DT, 
that is, it minimizes the training error 
1 T 
Etrain(W ) =   l(at,zt;w). (9) 
t----1 
The maximum likelihood estimator is efficient (or Fisher-efficient), implying that it 
is the best consistent estimator satisfying the Cram4r-Rao bound asymptotically, 
lira TE[(/VT - w*)(,T -- w*)']- G -1 , (10) 
where G- 1 is the inverse of the Fisher information matrix G = (gij) defined by 
gij =El OlOgp('z;w) OlOgp('z;w)] 
OWl  (11) 
in the component form. Information geometry (Amari, 1985) proves that the isher 
information G is the only invariant metric to be introduced in the space 
of the parameters of probability distributions. 
Examples (, z), (, z )... are given one at a time in the case of on-line learning. 
Let t be the estimated value at time t. At the next time t + 1, the estimator t is 
modified to give a new estimator t+ based on the observation (t+, zt+). The 
old observations (, z),.-., (t, zt) cannot be reused to obtain t+, so that the 
learning rule is written  t+ = m(t+,zt+,t). The process {t} is hence 
MarkovJan. Whatever a learning rule m we choose, the behavior of the estimator 
t is never better than that of the optimal batch estimator t because of this 
restriction. The conventional on-line learning rule is given by the following gradient 
form t+l : t -- tVl(t+l, Zt+l;t)' When t satisfies a certain condition, say 
t = c/t, the stochtic approximation guarantees that t is a consistent estimator 
converging to w*. However, it is not efficient in general. 
There arises a question if there exists an on-line learning rule that gives an efficient 
estimator. If it exists, the ymptotic behavior of on-line learning is equivalent to 
Neural Learning and Natural Gradient Descent 131 
'that of the batch esti
