Algorithms for Independent Components 
Analysis and Higher Order Statistics 
Daniel D. Lee 
Bell Laboratories 
Lucent Technologies 
Murray Hill, NJ 07974 
Uri Rokni and Haim Sompolinsky 
Racah Institute of Physics and 
Center for Neural Computation 
Hebrew University 
Jerusalem, 91904, Israel 
Abstract 
A latent variable generative model with finite noise is used to de- 
scribe several different algorithms for Independent Components Anal- 
ysis (ICA). In particular, the Fixed Point ICA algorithm is shown to 
be equivalent to the Expectation-Maximization algorithm for maximum 
likelihood under certain constraints, allowing the conditions for global 
convergence to be elucidated. The algorithms can also be explained by 
their generic behavior near a singular point where the size of the opti- 
mal generarive bases vanishes. An expansion of the likelihood about this 
singular point indicates the role of higher order correlations in determin- 
ing the features discovered by ICA. The application and convergence of 
these algorithms are demonstrated on a simple illustrative example. 
Introduction 
Independent Components Analysis (ICA) has generated much recent theoretical and prac- 
tical interest because of its successes on a number of different signal processing problems. 
ICA attempts to decompose the observed data into components that are as statistically in- 
dependent from each other as possible, and can be viewed as a nonlinear generalization of 
Principal Components Analysis (PCA). Some applications of ICA include blind separation 
of audio signals, beamforming of radio sources, and discovery of features in biomedical 
traces [ 1 ]. 
There have also been a number of approaches to deriving algorithms for ICA [2, 3, 4]. 
Fundamentally, they all consider the problem of recovering independent source signals { '} 
from observations {} such that: 
M 
Xi : E Wijsj' i = 1..N (1) 
j:l 
Here, Wij is a N x M mixing matrix where the number of sources M is not greater than 
the dimcnsionality N of the observations. Thus, the columns of W represent the different 
independent features present in the observed data. 
Bell and Scjnowski formulated their Infomax algorithm for ICA as maximizing the mutual 
information between the data and a nonlinearly transformed version of the data [5]. The 
492 D. D. Lee, U. Rokni and H. Sornpolinsky 
covariant version of this algorithm uses the natural gradient of the mutual information to 
iteratively update the estimate for the demixing matrix W - in terms of the estimated 
components s = W- ix [6]: 
-1 w -1, (2) 
The nonlinearity g(s) differentiates the features learned by the Infomax ICA algorithm 
from those found by conventional PCA. Fortunately, the exact form of the nonlinearity 
used in Eq. 2 is not crucial for the success of the algorithm, as long as it preserves the 
sub-Gaussian or super-Gaussian nature of the sources [7]. 
Another approach to ICA due to Hyvarinen and Oja was derived from maximizing objective 
functions motivated by projection pursuit [8]. Their Fixed Point ICA algorithm attempts 
to self-consistently solve for the extremum of a nonlinear objective function. The simplest 
formulation considers a single source M = 1 so that the mixing matrix is a single vector 
w, constrained to be unit length Iw[ - 1. Assuming the data is first preprocessed and 
whitened, the Fixed Point ICA algorithm iteratively updates the estimate of to as follows: 
w +- 
w 
w +- (3) 
where g(wTx) is a nonlinear function and AG is a constant given by the integral over the 
Gaussian: 
I /_ --r?/2 
AG =  drle gt(rl). (4) 
The Fixed Point algorithm can be extended to an arbitrary number M <_ N of sources by 
using Eq. 3 in a serial deflation scheme. Alternatively, the M columns of the mixing matrix 
W can be updated simultaneously by orthogonalizing the N x M matrix: 
W +- (zg(Wrz) r) - XoW. 
(5) 
Under the assumption that the observed data match the underlying ICA model, x = Ws, it 
has been shown that the Fixed Point algorithm converges locally to the correct solution with 
at least quadratic convergence. However, the global convergence of the generic Fixed Point 
ICA algorithm is uncertain. This is in contrast to the gradient-based Infomax algorithm 
whose convergence is guaranteed as long as a sufficiently small step size is chosen. 
In this paper, we first review the latent variable generative model framework for Indepen- 
dent Components Analysis. We then consider the generarive model in the presence of finite 
noise, and show how the Fixed Point ICA algorithm can be related to an Expectation- 
Maximization algorithm for maximum likelihood. This allows us to elucidate the condi- 
tions under which the Fixed Point algorithm is guaranteed to globally converge. Assuming 
that the data are indeed generated from independent components, we derive the optimal 
parameters for convergence. We also investigate how the optimal size of the ICA mixing 
matrix varies as a function of the added noise, and demonstrate the presence of a singular 
point. By expanding the likelihood about this singular point, the behavior of the ICA algo- 
rithms can be related to the higher order statistics present in the data. Finally, we illustrate 
the application and convergence of these ICA algorithms on some artificial data. 
Generative model 
A convenient method for interpreting the different ICA algorithms is in terms of the hidden, 
or latent, variable generarive model shown in Fig. I [9, 10]. The hidden variables 
ICA Algorithms and Higher Order Statistics 493 
M hidden variables 
Weights 
Noi Wse ... 
N visible variables 
Figure 1: Generative model for ICA algorithms. s are the hidden variables, cr are additive 
Gaussian noise terms, and x = Ws + cr are the visible variables. 
correspond to the different independent components and are assumed to have the factorized 
non-Gaussian prior probability distribution: 
M 
?(s) = H �-y(sj). (6) 
j=l 
Once the hidden variables are instantiated, the visible variables {x,} are generated via a 
linear mapping through the generarive weights W: 
II exp - (x,- 2 , (7) 
i=1 j 
where (7 2 is the variance of the Gaussian noise added to the visible variables. 
The probability of the data given this model is then calculated by integrating over all pos- 
sible values of the hidden variables: 
?(:) = ds ?(s)?(ls): (2./.s ep  (- ws)(8) 
In the limit that the added noise vanishes, a 2  0, it has previously been shown that 
maximizing the likelihood of Eq. 8 is equivalent to the Infomax algorithm in Eq. 2 [11]. 
In the following analysis, we will consider the situation when the viance of the noise is 
nonzero, a 2  0. 
Expectation-Maximization 
We assume that the data has initially been preprocessed and spherized: (XiXj) -- 5ij. 
Unfortunately, for finite noise 0 -2 and an arbitrary prior F($j), deriving a learning rule for 
W in closed form is analytically intractable. However, it becomes possible to derive a 
simple Expectation-Maximization (EM) learning rule under the constraint: 
W = Wo, WorWo=I, (9) 
which implies that W is orthogonal, and  is the length of the individual columns of W. 
Indeed, for data that obeys the ICA model, x = Ws, it can be shown that the optimal W 
must satisfy this orthogonality condition. By assuming the constraint in Eq. 9 for arbitrary 
data, the posterior distribution P(slx) becomes conveniently factorized: 
M [ 1 [(i/vTx)jsj_ 1 2 2 ] 
P(slx ) c H exp -F(sj) + -ff  sj] . (10) 
j----1 
494 D. D. Lee, U. Rokni and H. Sornpolinsky 
For the E-step, this factorized form allows the expectation function f ds P(s[x)s = 
g(WTx) to be analytically evaluated. This expectation is then used in the M-step to find 
the new estimate W': 
(xg(Wrx) r) - AsW' = O, (11) 
where As is a symmetric matrix of Lagrange multipliers that constrain the new W t to be 
orthogonal. Eq. 11 is easily solved by taking the reduced singular value decomposition of 
the rectangular matrix: 
UDV T -- (xg(WTx)T), (12) 
where UTU - VV r = I and D is a diagonal .Mr x .Mr matrix. Then the solution for the 
EM estimate of the mixing matrix is given by' 
W' = UV T (13) 
1 
As = - UDU T. (14) 
As a specific example, consider the following prior for binary hidden variables: P(s) = 
�[6(s - 1) + 6(s + 1)]. In this case, the expectation f ds ?(slx)s = tanh(Wrx/cr 2) and 
so the EM update rule is given by orthogonalizing the matrix: 
W q-- (xtanh(2 WTx)l . (15) 
Fixed Point ICA 
Besides the presence of the linear term AGW in Eq. 5, the EM update rule looks very much 
like that of the Fixed Point ICA algorithm. It turns out that without this linear term, the 
convergence of the naive EM algorithm is much slower than that of Eq. 5. Here we show 
that it is possible to interpret the role of this linear term in the Fixed Point ICA algorithm 
within the framework of this generatire model. 
Suppose that the distribution of the observed data 7o (x) is actually a mixture between an 
isotropic distribution 7o (z) and a non-isotropic distribution 2:'1 (x): 
PD(X) = ozPo(x) q- (1 - o/)P 1 (x). (16) 
Because the isotropic part does not break rotational symmetry, it does not affect the choice 
of the directions of the learned basis W. Thus, it is more efficient to apply the learning 
algorithm to only the non-isotropic portion of the distribution, P (x) oc PD(x) - cPo(x), 
rather than to the whole observed distribution PD(X). Applying EM to Pl(x) results in a 
correction term arising from the subtracted isotropic distribution. With this correction, the 
EM update becomes: 
W +- (xg(WTx)) -- c.GW (17) 
which is equivalent to the Fixed Point ICA algorithm when o = 1. 
Unfortunately, it is not clear how to compute an appropriate value for o to use in fitting data. 
Taking a very small value, o << 1, will result in a learning rule that is very similar to the 
naive EM update rule. This implies that the algorithm will be guaranteed to monotonically 
converge, albeit very slowly, to a local 
