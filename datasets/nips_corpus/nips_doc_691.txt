Neural Network Model Selection Using 
Asymptotic Jackknife Estimator and 
Cross-Validation Method 
Yong Liu 
Department of Physics and 
Institute for Brain and Neural Systems 
Box 1843, Brown University 
Providence, RI, 02912 
Abstract 
Two theorems and a lemma are presented about the use of jackknife es- 
timator and the cross-validation method for model selection. Theorem 1 
gives the asymptotic form for the jackknife estimator. Combined with the 
model selection criterion, this asymptotic form can be used to obtain the 
fit of a model. The model selection criterion we used is the negative of the 
average predictive likehood, the choice of which is based on the idea of the 
cross-validation method. Lemma 1 provides a formula for further explo- 
ration of the asymptotics of the model selection criterion. Theorem 2 gives 
an asymptotic form of the model selection criterion for the regression case, 
when the parameters optimization criterion has a penalty term. Theorem 
2 also proves the asymptotic equivalence of Moody's model selection cri- 
terion (Moody, 1992) and the cross-validation method, when the distance 
measure between response y and regression function takes the form of a 
squared difference. 
1 INTRODUCTION 
Selecting a model for a specified problem is the key to generalization based on the 
training data set. In the context of neural network, this corresponds to selecting 
an architecture. There has been a substantial amount of work in model selection 
(Lindley, 1968; Mallows, 1973; Akaike, 1973; Stone, 1977; Atkinson, 1978; Schwartz, 
599 
600 Liu 
1978; Zellner, 1984; MacKay, 1991; Moody, 1992; etc.). In Moody's paper (Moody, 
1992), the author generalized Akaike Information Criterion (AIC) (Akaike, 1973) 
in the regression case and introduced the term effective number of parameters. It 
is thus of great interest to see what the link between this criterion and the cross- 
validation method (Stone, 1974) is and what we can gain from it, given the fact 
that AIC is asymptotically equivalent to the cross-validation method (Stone, 1977). 
In the method of cross-validation (Stone, 1974), a data set, which has a data point 
deleted from the original training data set, is used to estimate the parameters of a 
model by optimizing a parameters optimization criterion. The optimal parameters 
thus obtained are called the jackknife estimator (Miller, 1974). Then the predictive 
likelihood of the deleted data point is calculated, based on the estimated parame- 
ters. This is repeated for each data point in the original training data set. The fit 
of the model, or the model selection criterion, is chosen as the negative of the aver- 
age of these predictive likelihoods. However, the computational cost of estimating 
parameters for different data point deletion is expensive. In section 2, we obtained 
an asymptotic formula (theorem 1) for the jackknife estimator based on optimizing 
a parameters optimization criterion with one data point deleted from the training 
data set. This somewhat relieves the computational cost mentioned above. This 
asymptotic formula can be used to obtain the model selection criterion by plugging 
it into the criterion. Furthermore, in section 3, we obtained the asymptotic form 
of the model selection criterion for the general case (Lemma 1) and for the special 
case when the parameters optimization criterion has a penalty term (theorem 2). 
We also proved the equivalence of Moody's model selection criterion (Moody, 1992) 
and the cross-validation method (theorem 2). Only sketchy proofs are given when 
these theorems and lemma are introduced. The detail of the proofs are given in 
section 4. 
2 APPROXIMATE JACKKNIFE ESTIMATOR 
Let the parameters optimization criterion, with data set co: {(a:i, Yi), i - 1, ..., n) 
and parameters 0, be Co(O), and let co-i denote the data set with ith data point 
deleted from co. If we denote  and O_i as the optimal parameters for criterion C (0) 
and Co_c(0), respectively, X7 a as the derivative with respect to 0 and superscript t 
as transpose, we have the following theorem about the relationship between  and 
Theorem I If the criterion function Co (a) is an infinite- order differeniiable func- 
tion and its derivatives are bounded around . The estimator -i (also called jack- 
knife estimator (Miller, 197)) can be approzimated as 
- -(v0vco() - 
(1) 
in c(o) = co(o) - co_,(0). 
Proof. Use the Taylor expansion of equation VoCo_(_i): 0 around . Ignore 
terms higher than the second order. 
Model Selection Using Asymptotic Jackknife Estimator & Cross-Validation Method 601 
Ezample 1: Using the generalized mazimum likelihood method from Bayesian 
analysis  (Berger, 1985), if r(19) is the prior on the parameters and the observations 
are mutually independent, for which the distribution is modeled as ylx ....f(ylx, 9), 
the parameters optimization criterion is 
Co(0)= log[ H f(yi[z,,O)r(O) ]=  logf(y,[.z,,O) + log(0). (2) 
Thus C(O) = logf(yz,O). If we ignore the influence of the deleted data point in 
the d nominator of equation 1, we have 
b_, - b  -(VoVC())-XVologf(y, lg,,). (3) 
Ezample �: In the special case of example 1, with noninformative prior r(0): 1, 
the criterion is the ordinary log-likelihood function, thus 
_i-tJm-[ y. VoVlogf(yjla:j, ) ]-Vologf(yilxi,). (4) 
3 
CROSS-VALIDATION METHOD AND MODEL 
SELECTION CRITERION 
Hereafter we use the negative of the average predictive likelihood, or, 
T,(w)- 1 )' logf(y, lm, _,) (5) 
n 
(;gi,yl) 
as the model selection criterion, in which n is the size of the training data set w, 
rn G :M denotes parametric probability models f(yl and :M is the set of all the 
models in consideration. It is well known that T(w) is an unbiased estimator of 
r(90, (-)), the risk of using the model m and estimator , when the true parameters 
are 0 and the training data set is w (Stone, 1974; Efron and Gong, 1983; etc.), i.e., 
: 
= E{-logf(ylx,(w)) } 
1 
: E{ k  l�gf(YJlxJ'(w)) } (6) 
in which w = {(xj,yj) , j = 1, ... k} is the test data set, (-) is an implicit 
function of the training data set w and it is the estimator we decide to use after 
we have observed the training data set w. The expectation above is taken over the 
randomness of w, , y and w. The optimal model will be the one that minimizes 
this criterion. This procedure of using _ and T (w) to obtain an estimation of risk 
is often called the cross-validation method (Stone, 1974; Efron and Gong, 1983). 
Remark: After we have obtained  for a model, we can use equation 1 to calculate 
-i for each i, and put the resulting -i into equation 5 to get the fit of the model, 
thus we will be able to compare different models m 
t Strictly speaking, it is a method to find the posterior mode. 
602 Liu 
Lemma 1 / the probability model f(y]oz, 0), as a function of O, is differeniiable up 
infinite order and its derivatives are bounded around . The approzimaiion go 
model selection criterion, equation 5, can be written as 
1 1 
()  -;  logf(ylx,)-   Vlogf(ylxi,)(_ -) (7) 
(Zi,yi) (0 
Proof. Igoring the terms higher than the second order of the Taylor expansion of 
logf(yj [x,_i) around t} will yield the result. 
Example � (continued): Using equation 4, we have, for the model selection criterion, 
7-,.(w)- 1  logf(yilxi, O)- 
n 
(,y.) 
  Vlogf(yi[i,)A-Vologf(yi[i,). (8) 
in which A = (,y) VsVlogf(yj[j,). If the model f(y[,O) is the true 
one, the second term is asymptotically equal to p, the number of parameters in the 
model. So the model selection criterion is 
- log-likelihood + number of parameters of the model. 
This is the well known Akaike's Information Criterion (AIC) (Akaike, 1973). 
Eample /(continued): Consider the probability model 
1 
f(y[,O) = Zexp(-f(y, W())) (9) 
in which Z is a normalization factor, f(y, W()) is a distance measure between y and 
regression knction W(). f(') as function of 0 is assumed differentiable. Denoting  
U(O,,w) = (,,y,) f(Yi, W(i))- 2alog(0), we have the following theorem, 
Theorem 2 For the model specified in equation 9 and the parameters optimization 
criterion specified in equation � (example 1), under regular condition, the unbiased 
estimator of 
1 
'( i (Y' ()) ) (0) 
asymptotically equals to 
(,y)eo 
+ 
V�(yi,l(xi)){VoVU(,X,w)}-Vo�(yi,l(ozi)). (11) 
'For example, r(O[X)= A/;(O, a'/X), this corresponds to 
U(O,X,w)=  �(yi,ls(zi)) q- XO' + const(A,a'). 
Model Selection Using Asymptotic Jackknife Estimator & Cross-Validation Method 603 
For the case when �(y, r/s(z))= (y-t/s(z)) 2, we get, for the asymptotic equivalency 
of the equation 11, 
2(r 2 1 
+ --- x 
n 2 
0 (12) 
Oyi 
in which w -- {(zi, Yi), i -- 1, ..., n) is the training data set, wn -- {(zi,Yi), i ---- 
1 
1, ..., k} is the test data set, and - 5(Y, 
Proof. This result comes directly from theorem 1 and lemma 1. Some asymptotic 
technique has to be used. 
Remark: The result in equation 12 was first proposed by Moody (Moody, 1992). 
The effective number of parameters formulated in his paper corresponds to the 
summation in equation 12. Since the result in this theorem comes directly from 
the asymptotics of the cross-validation method and the jackknife estimator, it gives 
the equivalency proof between Moody's model selection criterion and the cross- 
validation method. The detailed proof of this theorem, presented in section 4, is 
in spirit the same as the one presented in Stone's paper about the proof of the 
asymptotic equivalence of AIC and the cross-validation method (Stone, 1977). 
4 DETAILED PROOF OF LEMMAS AND THEOREMS 
In order to prove theorem 1, lemma 1 and theorem 2, we will present three auxiliary 
lemmas first. 
Lemma 2 For random variable sequence n and Yn, if limn-ozn = � and 
lirn_o Yn = , then n and Yn are asymptotically equivalent. 
Proof. This comes from the definition of asymptotic equivalence. Because asymp- 
totica
