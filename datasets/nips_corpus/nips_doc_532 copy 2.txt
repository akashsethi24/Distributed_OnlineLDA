Principles of Risk Minimization 
for Learning Theory 
V. Vapnik 
AT&T Bell Laboratories 
Holmdel, NJ 07733, USA 
Abstract 
Learning is posed as a problem of function estimation, for which two princi- 
ples of solution are considered: empirical risk minimization and structural 
risk minimization. These two principles are applied to two different state- 
ments of the function estimation problem: global and local. Systematic 
improvements in prediction power are illustrated in application to zip-code 
recognition. 
1 INTRODUCTION 
The structure of the theory of learning differs from that of most other theories for 
applied problems. The search for a solution to an applied problem usually requires 
the three following steps: 
1. State the problem in mathematical terms. 
2. Formulate a general principle to look for a solution to the problem. 
3. Develop an algorithm based on such general principle. 
The first two steps of this procedure offer in general no major difficulties; the 
third step requires most efforts, in developing computational algorithms to solve 
the problem at hand. 
In the case of learning theory, however, many algorithms have been developed, but 
we still lack a clear understanding of the mathematical statement needed to describe 
the learning procedure, and of the general principle on which the search for solutions 
8Jl 
832 Vapnik 
should be based. This paper is devoted to these first two steps, the statement of 
the problem and the general principle of solution. 
The paper is organized as follows. First, the problem of function estimation is 
stated, and two principles of solution are discussed: the principle of empirical risk 
minimization and the principle of structural risk minimization. A new statement 
is then given: that of local estimation of function, to which the same principles are 
applied. An application to zip-code recognition is used to illustrate these ideas. 
2 FUNCTION ESTIMATION MODEL 
The learning process is described through three components: 
1. A generator of random vectors x, drawn independently from a fixed but unknown 
distribution P(x). 
2. A supervisor which returns an output vector y to every input vector x, according 
to a conditional distribution function P(ylx), also fixed but unknown. 
3. A learning machine capable of implementing a set of functions f(x, w), w  W. 
The problem of learning is that of choosing from the given set of functions the one 
which approximates best the supervisor's response. The selection is based on a 
training set of � independent observations: 
(1) 
The formulation given above implies that learning corresponds to the problem of 
function approximation. 
3 PROBLEM OF RISK MINIMIZATION 
In order to choose the best available approximation to the supervisor's response, 
we measure the loss or discrepancy L(y, f(x,w)) between the response y of the 
supervisor to a given input x and the response f(x, w) provided by the learning 
machine. Consider the expected value of the loss, given by the risk functional 
R(w) - f L(y,f(x,w))dP(x,y). 
(2) 
The goal is to minimize the risk functional R(w) over the class of functions 
f(x,w), w  W. But the joint probability distribution P(x,y) -' P(ylx)P(x) 
is unknown and the only available information is contained in the training set (1). 
4 EMPIRICAL RISK MINIMIZATION 
In order to solve this problem, the following induction principle is proposed: 
risk functional R(w) is replaced by the empirical risk functional 
the 
E(w)= w)) (3) 
i=1 
Principles of Risk Minimization for Learning Theory 833 
constructed on the basis of the training set (1). The induction principle of empirical 
risk minimization (ERM) assumes that the function f(x, w),which minimizes E(w) 
over the set w  W, results in a risk R(w) which is close to its minimum. 
This induction principle is quite general; many classical methods such as least square 
or maximum likelihood are realizations of the ERM principle. 
The evaluation of the soundness of the ERM principle requires answers to the fol- 
lowing two questions: 
1. Is the principle consistent? (Does R(w) converge to its minimum value on the 
set w G W when �-- o?) 
2. How fast is the convergence as � increases? 
The answers to these two questions have been shown (Vapnik et al., 1989) to be 
equivalent to the answers to the following two questions: 
1. Does the empirical risk E(w) converge uniformly to the actual risk R(w) over 
the full set f(x, w), w  W? Uniform convergence is defined as 
Prob{ sup IR(w) - E(w)l > e} ) 0 as � --, o. (4) 
wEW 
2. What is the rate of convergence? 
It is important to stress that uniform convergence (4) for the full set of functions is 
a necessary and sufficient condition for the consistency of the ERM principle. 
5 VC-DIMENSION OF THE SET OF FUNCTIONS 
The theory of uniform convergence of empirical risk to actual risk developed in 
the 70's and 80's, includes a description of necessary and sufficient conditions as 
well as bounds for the rate of convergence (Vapnik, 1982). These bounds, which 
are independent of the distribution function P(x,y), are based on a quantitative 
measure of the capacity of the set of functions implemented by the learning machine: 
the VC-dimension of the set. 
For simplicity, these bounds will be discussed here only for the case of binary pat- 
tern recognition, for which y G {0, 1} and f(x, w), w  W is the class of indicator 
functions. The loss function takes only two values L(y, f(x, w)) = 0 if y = f(x, w) 
and L(y,f(x,w)) -- 1 otherwise. In this case , the risk functional (2) is the prob- 
ability of error, denoted by P(w). The empirical risk functional (3), denoted by 
y(w), is the frequency of error in the training set. 
The VC-dimension of a set of indicator functions is the maximum number h of 
vectors which can be shattered in all possible 2 h ways using functions in the set. 
For instance, h - n + I for linear decision rules in n-dimensional space, since they 
can shatter at most n + 1 points. 
6 RATES OF UNIFORM CONVERGENCE 
The notion of VC-dimension provides a bound to the rate of uniform convergence. 
For a set of indicator functions with VC-dimension h, the following inequality holds: 
834 
Vapnik 
(2re) h exp{--e:�}. (5) 
Prob{ sup IP(w) - v(w)l > ) < h 
w( W 
It then follows that with probability 1 - r/, simultaneously for all w 6 W, 
P(w) < ,(w) + Co(t/h, ,), (6) 
with confidence interval 
Co(t/h, rl) = �/h(ln 2t/h + 1)- lnr/ 
t (7) 
This important result provides a bound to the actual risk P(w) for all w 6 W, 
including the w* which minimizes the empirical risk v(w). 
The deviation IP(w)- v(w)l in (5) is expected to be maximum for P(w) close 
to 1/2, since it is this value of P(w) which maximizes the error variance a(w) = 
v/P(w)(1- P(w)). The worst case bound for the confidence interval (7) is thus 
likely be controlled by the worst decision rule. The bound (6) is achieved for the 
worst case P(w) = 1/2, but not for small P(w), which is the case of interest. A 
uniformly good approximation to P(w) follows from considering 
Prob{ sup P(w)- v(w) 
wEW O'(W) > }' (8) 
The variance of the relative deviation (P(w)-v(w))/o'(w) is now independent of w. 
A bound for the probability (8), if available, would yield a uniformly good bound 
for actual risks for all P(w). 
Such a bound has not yet been established. But for P(w) << 1, the approximation 
a(w) _ V/-w) is true, and the following inequality holds: 
Prob{ sup P(w) - v(w) __ . e2t 
wEW  > e} < ( )n exp/_ - }. (9) 
It then follows that with probability 1 - r/, simultaneously for all w  W, 
P(w) < v(w) + C (t/h, v(w), rl) , (10) 
with confidence interval 
C(g/h,u(w),rl)=2(h(ln2t?/h+l)-lnrl) (1+/1+ u(w)t ) 
t ' 
h(ln 2t/h + 1) -- In r/ 
(11) 
Note that the confidence interval now depends on u(w), and that for u(w) - 0 it 
reduces to 
(t/h, O, = 2Co(t/h, 
which provides a more precise bound for real case learning. 
7 STRUCTURAL RISK MINIMIZATION 
The method of ERM can be theoretically justified by considering the inequalities 
(6) or (10). When t/h is large, the confidence intervals C0 or C1 become small, and 
Principles of Risk Minimization for Learning Theory 835 
can be neglected. The actual risk is then bound by only the empirical risk, and the 
probability of error on the test set can be expected to be small when the frequency 
of error in the training set is small. 
However, if ?/h is small, the confidence interval cannot be neglected, and even 
,(w) = 0 does not guarantee a small probability of error. In this case the minimiza- 
tion of P(w) requires a new principle, based on the simultaneous minimization of 
v(w) and the confidence interval. It is then necessary to control the VC-dimension 
of the learning machine. 
To do this, we introduce a nested structure of subsets Sp = {f(x, w), w E Wp }, such 
that 
& c S2 c ... c S.. 
The corresponding VC-dimensions of the subsets satisfy 
h <h2<...<h,. 
The principle of structure risk minimization (SRM) requires a two-step process: the 
empirical risk has to be minimized for each element of the structure. The optimal 
element $* is then selected to minimize the guaranteed risk, defined as the sum 
of the empirical risk and the confidence interval. This process involves a trade-off: 
as h increases the minimum empirical risk decreases, but the confidence interval 
increases. 
EXAMPLES OF STRUCTURES FOR NEURAL NETS 
The general principle of SRM can be implemented in many different ways. Here 
we consider three different examples of structures built for the set of functions 
implemented by a neural network. 
1. Structure given by the architecture of the neural network. Consider an 
ensemble of fully connected neural networks in which the number of units in one of 
the hidden layers is monotonically increased. The set of implementable functions 
makes a structure as the number of hidden units is increased. 
2. Structure given by the learning procedure. Consider the set of functions 
S = {f(
