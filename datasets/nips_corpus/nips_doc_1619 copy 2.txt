Nonlinear Discriminant Analysis using 
Kernel Functions 
Volker Roth & Volker Steinhage 
University of Bonn, Institut of Computer Science III 
RSmerstrasse 164, D-53117 Bonn, Germany 
{ roth, steinhag} @ cs. uni-bonn. de 
Abstract 
Fishers linear discriminant analysis (LDA) is a classical multivari- 
ate technique both for dimension reduction and classification. The 
data vectors are transformed into a low dimensional subspace such 
that the class centroids are spread out as much as possible. In 
this subspace LDA works as a simple prototype classifier with lin- 
ear decision boundaries. However, in many applications the linear 
boundaries do not adequately separate the classes. We present a 
nonlinear generalization of discriminant analysis that uses the ker- 
nel trick of representing dot products by kernel functions. The pre- 
sented algorithm allows a simple formulation of the EM-algorithm 
in terms of kernel functions which leads to a unique concept for un- 
supervised mixture analysis, supervised discriminant analysis and 
semi-supervised discriminant analysis with partially unlabelled ob- 
servations in feature spaces. 
I Introduction 
Classical linear discriminant analysis (LDA) projects N data vectors that belong to 
c different classes into a (c- 1)-dimensionai space in such way that the ratio of be- 
tween group scatter $$ and within group scatter Sw is maximized [1]. LDA formally 
consists of an eigenvalue decomposition of S  SB leading to the so called canonical 
vatlares which contain the whole class specific information in a (c- 1)-dimensional 
subspace. The canonical variates can be ordered by decreasing eigenvalue size in- 
dicating that the first variates contain the major part of the information. As a 
consequence, this procedure allows low dimensional representations and therefore 
a visualization of the data. Besides from interpreting LDA only as a technique for 
dimensionality reduction, it can also be seen as a multi-class classification method: 
the set of linear discriminant functions define a partition of the projected space into 
regions that are identified with class membership. A new observation x is assigned 
to the class with centroid closest to x in the projected space. 
To overcome the limitation of only linear decision functions some attempts have 
been made to incorporate nonlinearity into the classical algorithm. HASTIE et al. 
[2] introduced the so called model of Flexible Discriminant Analysis: LDA is refor- 
mulated in the framework of linear regression estimation and a generalization of this 
method is given by using nonlinear regression techniques. The proposed regression 
techniques implement the idea of using nonlinear mappings to transform the input 
data into a new space in which again a linear regression is performed. In real world 
Nonlinear Discriminant Analysis Using Kernel Functions 569 
applications this approach has to deal with numerical problems due to the dimen- 
sional explosion resulting from nonlinear mappings. In the recent years approaches 
that avoid such explicit mappings by using kernel functions have become popular. 
The main idea is to construct algorithms that only afford dot products of pattern 
vectors which can be computed efficiently in high-dimensional spaces. Examples of 
this type of algorithms are the Support Vector Machine [3] and Kernel Principal 
Component Analysis [4]. 
In this paper we show that it is possible to formulate classical linear regression 
and therefore also linear discriminant analysis exclusively in terms of dot products. 
Therefore, kernel methods can be used to construct a nonlinear variant of dis- 
criminant analysis. We call this technique Kernel Discriminant Analysis (KDA). 
Contrary to a similar approach that has been published recently [5], our algorithm is 
a real multi-class classifier and inherits from classical LDA the convenient property 
of data ,isualization. 
2 Review of Linear Discriminant Analysis 
Under the assumption of the data being centered (i.e. Y-i xi = 0) the scatter ma- 
trices $s and Sw are defined by 
c 1 nj (x?)(x) T 
SB: Ej=I -. El,m=l ) ) (1) 
c . 1 . x? ) ) 1 n 
- Z --Z (2) 
where nj is the number of patterns x? ) that belong to class j. 
LDA chooses a transformation matrix V that maximizes the objective function 
Ivrssvl 
J(V) = iVTSwV I . (3) 
The columns of an optimal V are the generalized eigenvectors that correspond to 
the nonzero eigenvalues in Ssvi = hi Swvi. 
In [6] and [7] we have shown, that the standard LDA algorithm can be restat- 
ed exclusively in terms of dot products of input vectors. The final equation is an 
eigenvalue equation in terms of dot product matrices which are of size N x N. Since 
the solution of high-dimensional generalized eigenvalue equations may cause numer- 
ical problems (N may be large in real world applications), we present an improved 
algorithm that reformulates discriminant analysis as a regression problem. More- 
over, this version allows a simple implementation of the EM-algorithm in feature 
spaces. 
3 Linear regression analysis 
In this section we give a brief review of linear regression analysis which we use as 
building block for LDA. The task of linear regression analysis is to approximate 
the regression function by a linear function 
r(x) = E(YlA' = x) m c + x T/. (4) 
on the basis of a sample (y,x),... ,(yN,XN). Let now y denote the vector 
(y,...,yN) T and X denote the data matrix which rows are the input vectors. 
Using a quadratic loss function, the optimal parameters c and / are chosen to 
minimize the average squared residual 
= Ilu-  1 + x11 + 
IN denotes a N-vector of ones, [2 denotes a ridge-type penalty matrix [2 = el which 
penalizes the coefficients of/. Assuming the data being centered, i.e y//v= xi = O, 
the parameters of the regression function are given by: 
c = X -1 yN 
i= Yi =: Uy, / = (XrX + eI) -Xry. (6) 
570 V. Roth and V. Steinhage 
4 LDA by optimal scoring 
In this section the LDA problem is linked to linear regression using the framework 
of penalized optimal scoring. We give an overview over the detailed derivation in 
[2] and [8]. Considering again the problem with c classes and N data vectors, 
the class-memberships are represented by a categorical response variable  with 
c levels. It is useful to code the n responses in terms of the indicator matrix 
Z: Zi,5 = 1, if the i-th data vector belongs to class j, and 0 otherwise. The point 
of optimal scoring is to turn categorical variables into quantitative ones by assigning 
scores to classes: the score vector 0 assigns the real number 0 5 to the j-th level of 
. The vector Z then represents a vector of scored training data and is regressed 
onto the data matrix X. The simultaneous estimation of scores and regression 
coefficients constitutes the optimal scoring problem: minimize the criterion 
ASR(O, fi) = N-l[llZO - Still 4. (7) 
under the constraint -[[Z0[[ 2 = 1. According to (6), for a given score 0 the 
minimizing fi is given by 
rios = ( xrx + fi)- x rzo, (8) 
and the partially minimized criterion becomes: 
minASR(,fi) - 1- N-iT zT M(f)Z, 
(9) 
where M(fl) = X(XTX + fl)-X T denotes the regularized hat or smoother matrix. 
Minimizing of (9) under the constraint [[ZO[[  = 1 can be performed by the 
following procedure: 
1. Choose an initial matrix O0 satisfying the constraint N-10oZTZOo = I 
and set O) = ZOo 
2. Run a multi-response regression of ï¿½ onto X: ) = M(f)O) = XB, 
where B is the matrix of regression coefficients. 
3. Eigenanalyze o)T)) to obtain the optimal scores, and update the matrix of 
regression coefficients: B* = BW, with W being the matrix ofeigenvectors. 
It can be shown, that the final matrix B* is, up to a diagonal scale matrix, equivalent 
to the matrix of LDA-vectors, see [8]. 
5 Ridge regression using only dot products 
The penalty matrix  in (5) assures that the penalized d x d covariance matrix 
,, - xTx 4- eI is a symmetric nonsingular matrix. Therefore, it has d eigenvectors 
ai with accomplished positive eigenvalues ?i such that the following equations hold: 
-- ---- --aia i (10) 
j--1 i----1 i 
The first equation implies that the first l leading eigenvectors ai with eigenvalues 
?i  e have an expansion in terms of the input vectors. Note that l is the number 
of nonzero eigenvalues of the unpenalized covariance matrix xTX. Together with 
(6), it follows for the general case, when the dimensionality d may extend l, that fi 
can be written as the sum of two terms: an expansion in terms of the vectors xi 
with coefficients ai and a similar expansion in terms of the remaining eigenvectors: 
N d d 
Z OiXi 4- Z jaj -- XToI 4- Z jaj, (11) 
fi -- i----1 j----l+l j----l+l 
with e = (a ... an) T. However, the last term can be dropped, since every eigen- 
vector aj, j = l + 1,..., d is orthogonal to every vector xi and does not influence 
the value of the regression function (4). 
The problem of penalized linear regression can therefore be stated as minimizing 
Nonlinear Discriminant Analysis Using Kernel Functions 571 
= [ Ilu - I + 
A stationary vector c is determined by 
a = (XX r + 
(12) 
(13) 
Let now the dot product matrix K be defined by Kij -- :riT:rj and let for a given test 
point (a:t) the dot product vector kt be defined by kt = Xxt. With this notation 
the regression function of a test point (xt) reads 
(,)  + [ ( + )- 
= y. (14) 
This equation requires only dot products and we can apply the kernel trick. The 
final equation (14), up to the constant term u, h also been found by SUDERS et 
al., [9]. They restated ridge regression in dual variables and optimized the resulting 
criterion function with a lagrange multiplier technique. Note that our derivation, 
which is a direct generalization of the standard linear regression formalism, leads in 
a natural way to a cls of more general regression functions including the constant 
term. 
6 LDA using only dot prod
