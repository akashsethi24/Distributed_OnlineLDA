Agnostic Classification of Markovian 
Sequences 
Ran E1-Yaniv Shai Fine Naftali Tishby* 
Institute of Computer Science and Center for Neural Computation 
The Hebrew University 
Jerusalem 91904, Israel 
E-ma: {rnni ,fshai ,tishby}cs .huji. ac. il 
Category: Algorithms. 
Abstract 
Classification of finite sequences without explicit knowledge of their 
statistical nature is a fundamental problem with many important 
applications. We propose a new information theoretic approach 
to this problem which is based on the following ingredients: (i) se- 
quences are similar when they are likely to be generated by the same 
source; (ii) cross entropies can be estimated via universal compres- 
sion; (iii) Markovian sequences can be asymptotically-optimally 
merged. 
With these ingredients we design a method for the classification of 
discrete sequences whenever they can be compressed. We introduce 
the method and illustrate its application for hierarchical clustering 
of languages and for estimating similarities of protein sequences. 
I Introduction 
While the relationship between compression (minimal description) and supervised 
learning is by now well established, no such connection is generally accepted for 
the unsupervised case. Unsupervised classification is still largely based on ad-hock 
distance measures with often no explicit statistical justification. This is particu- 
larly true for unsupervised classification of sequences of discrete symbols which is 
encountered in numerous important applications in machine learning and data min- 
ing, such as text categorization, biological sequence modeling, and analysis of spike 
trains. 
The emergence of universal (i.e. asymptotically distribution independent) se- 
*Corresponding author. 
466 R. EI-Yaniv, S. Fine and N. Tishby 
quence compression techniques suggests the existence of universal classification 
methods that make minimal assumptions about the statistical nature of the data. 
Such techniques are potentially more robust and appropriate for real world appli- 
cations. 
In this paper we introduce a specific method that utilizes the connection between 
universal compression and unsupervised classification of sequences. Our only un- 
derlying assumption is that the sequences can be approximated (in the information 
theoretic sense) by some finite order Markov sources. There are three ingredients to 
our approach. The first is the assertion that two sequences are statistically similar 
if they are likely to be independently generated by the same source. This likelihood 
can then be estimated, given a typical sequence of the most likely joint source, using 
any good compression method for the sequence samples. The third ingredient is a 
novel and simple randomized sequence merging algorithm which provably generates 
a typical sequence of the most likely joint source of the sequences, under the above 
Markovian approximation assumption. 
Our similarity measure is also motivated by the known two sample problem 
[Leh59] of estimating the probability that two given samples are taken from the 
same distribution. In the i.i.d. (Bernoulli) case this problem was thoroughly inves- 
tigated and the. optimal statistical test is given by the sum of the empirical cross 
entropies between the two samples and their most likely joint .source. We argue 
that this measure can be extended for arbitrary order Markov sources and use it to 
construct and sample the most likely joint source. 
The similarity measure and the statistical merging algorithm can be naturally com- 
bined into classification algorithms for sequences. Here we apply the method to 
hierarchical clustering of short text segments in 18 European languages and to eval- 
uation of similarities of protein sequences. A complete analysis of the method, with 
further applications, will be presented elsewhere lEFT97]. 
2 Measuring the statistical similarity of sequences 
Estimating the statistical similarity of two individual sequences is traditionally done 
by training a statistical model for each sequence and then measuring the likelihood 
of the other sequence by the model. Training a model entails an assumption about 
the nature of the noise in the data and this is the rational behind most edit 
distance measures, even when the noise model is not explicitly stated. 
Estimating the log-likelihood of a sequence-sample over a discrete alphabet E by 
a statistical model can be done through the Cross Entropy or Kullback-Leibler 
Divergence[CT91] between the sample empirical distribution p and model distri- 
bution q, defined as: 
DK� (p[[q) = E p(ty)log p(ty) 
er q (a- (1) 
The KL-divergence, however, has some serious practical drawbacks. It is non- 
symmetric and unbounded unless the model distribution q is absolutely continuous 
with respect to p (i.e. q = 0 = p - 0). The KL-divergence is therefore highly sensi- 
tive to low probability events under q. Using the empirical (sample) distributions 
for both p and q can result in very unreliable estimates of the true divergences. Es- 
sentially, D/L [p[[q] measures the asymptotic coding inefficiency when coding the 
sample p with an optimal code for the model distribution q. 
The symmetric divergence, i.e. D (p,q) = DKt, llq] + Dit, [qllp], suffers from 
Agnostic Classification of Markovian Sequences 467 
similar sensitivity problems and lacks the clear statistical meaning. 
2.1 The two sample problem 
Direct Bayesian arguments, or alternately the method of types [CK81], suggest that 
the probability that there exists one source distribution 37/for two independently 
drawn samples, x and y [Leh59], 
d/ (M) Pr (xIM) � Pr (ylM) 
where dl(M) is a prior density 
empirical (sample) distributions, 
is proportional to 
= f dl (M) � 2 -(IxlDKL[p=IIM]+IyIDKL[pvIIM]), (2) 
of all candidate distributions, px and Pu are the 
and Ixl and lyl are the corresponding sample sizes. 
For large enough samples this integral is dominated (for any non-vanishing prior) 
by the maximal exponent in the integrand, or by the most likely joint source of x 
and y, Mx, defined as 
Mx = argmin {[XlDKL (pIIM') + lYlDKL (pyllM')}. (3) 
M' 
where 0 <_ A = Ixl/(lx I + lyl) <_ i is the sample mixture ratio. The convexity of the 
KL-divergence guarantees that this minimum is unique and is given by 
Mx = 
the A - mixture of p and 
The similarity measure between two samples, d(x, y), naturally follows as the min- 
imal value of the above exponent. That is, 
Definition I The similarity measure, d(x, y) = 7)x(Pz,Pu), of two samples x and 
y, with empirical distributions px and py respectively, is defined as 
d(x,y) = Vx(pz,py) = ADK� (PlIM) + (1 - A) DK� (pyllMx) (4) 
where Mx is the A-mixture of pz and py. 
The function Dx (p, q) is an extension of the Jensen-Shannon divergence (see e.g. 
[Lin91]) and satisfies many useful analytic properties, such as symmetry and bound- 
edness on both sides by the L-norm, in addition to its clear statistical meaning. 
See [Lin91, EFT97] for a more complete discussion of this measure. 
2.2 Estimating the T)x similarity measure 
The key component of our classification method is the estimation of Dx for individ- 
ual finite sequences, without an explicit model distribution. 
Since cross entropies, D;�, express code-length differences, they can be estimated 
using any efficient compression algorithm for the two sequences. The existence 
of universal compression methods, such as the Lempel-Ziv algorithm (see e.g. 
[CT91]) which are provably asymptotically optimal for any sequence, give us the 
means for asymptotically optimal estimation of D, provided that we can obtain a 
typical sequence of the most-likely joint source, M. 
We apply an improvement of the method of Ziv and Merhav [ZM93] for the esti- 
mation of the two cross-entropies using the Lempel-Ziv algorithm given two sample 
sequences [BE97]. Notice that our estimation of D is as good as the compression 
method used, namely, closer to optimal compression yields better estimation of the 
similarity measure. 
It remains to show how a typical sequence of the most-likely joint source can be 
generated. 
468 R. EI-Yaniv, $. Fine and N. Tishby 
3 Joint Sources of Markovian Sequences 
In this section we first explicitly generalize the notion of the joint statistical source to 
finite order Markov probability measures. We identify the joint source of Markovian 
sequences and show how to construct a typical random sample of this source. 
More precisely, let x and y be two sequences generated by Markov processes with 
distributions P and Q, respectively. We present a novel algorithm for the merging 
the two sequences, by generating a typical sequence of an approximation to the 
most likely joint source of x and y. The algorithm does not require the parameters 
of the true sources P and Q and the computation of the sequence is done directly 
from the sequence samples x and y. 
As before, E denotes a finite alphabet and P and Q denote two ergodic Markov 
sources over E of orders Kp and KQ, respectively. By equation 3, the A-mixture 
joint source Mx of P and Q is Mx = argminM, ADK�(PI]M')+(1-A)DK�(QIIM'), 
I Px 
where for sequences DK�(PIIM ) = limsuPn_m  -].xeEn P(x)log M(' The fol- 
lowing theorem identifies the joint source of P and Q. 
Theorem I The unique A-mixture joint source Mx of P and Q, 
max{Kp, KQ}, is given by the following conditional distribution. 
EK, a 6 E, 
of order K = 
For each s  
AP(s) 
Mx(als) = AP(s) + (1 - A)O(s) 
(1- A)Q(s) 
P(als) + Q(als) � 
AP(s) + (1- 
This distribution can be naturally extended to n sources with priors A1,..., An. 
3.1 The sequence merging algorithm 
The above theorem can be easily translated into an algorithm. Figure I describes a 
randomized algorithm that generates from the given sequences x and y, an asymp- 
totically typical sequence z of the most likely joint source, as defined by Theorem 
1, of P and Q. 
Initialization: 
� z [0] -- choose a
