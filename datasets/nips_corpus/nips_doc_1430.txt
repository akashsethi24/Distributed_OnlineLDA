A Neural Network Model of Naive Preference 
and Filial Imprinting in the Domestic Chick 
Lucy E. Hadden 
Department of Cognitive Science 
University of California, San Diego 
La Jolla, CA 92093 
haddencogsci.ucsd.edu 
Abstract 
Filial imprinting in domestic chicks is of interest in psychology, biology, 
and computational modeling because it exemplifies simple, rapid, in- 
nately programmed learning which is biased toward learning about some 
objects. Horn et al. have recently discovered a naive visual preference 
for heads and necks which develops over the course of the first three 
days of life. The neurological basis of this predisposition is almost en- 
tirely unknown; that of imprinting-related learning is fairly clear. This 
project is the first model of the predisposition consistent with what is 
known about learning in imprinting. The model develops the predisposi- 
tion appropriately, learns to approach a training object, and replicates 
one interaction between the two processes. Future work will replicate 
more interactions between imprinting and the predisposition in chicks, 
and analyze why the system works. 
1 Background 
Filial imprinting ifi domestic chicks is of interest in psychology, biology, and computational 
modeling (O'Reilly and Johnson, 1994; Bateson and Horn, 1994) because it exemplifies 
simple, rapid, innately programmed learning which is biased toward learning about some 
particular objects, and because it has a sensitive period in which learning is most efficient. 
Domestic chicks will imprint on almost anything (including boxes, chickens, and humans) 
which they see for enough time (Horn, 1985). Horn and his colleagues (Horn, 1985) have 
recently found a naive visual preference (predisposition) for heads and necks which devel- 
ops over the course of the first three days of life. In particular, the birds prefer to approach 
objects shaped like heads and necks, even if they are the heads and necks of other species, 
including ducks and polecats (Horn, 1985). This preference interacts interestingly with fil- 
ial imprinting, or learning to recognize a parent. Chicks can still learn about (and imprint 
32 L E. Hadden 
on) other objects even in the presence of this predisposition, and the predisposition can 
override previously learned preferences (Johnson et al., 1985), which is usually hard with 
imprinted chicks. These interactions are like other systems which rely on naive preferences 
and learning. 
While the neurological basis of imprinting is understood to some extent, that of the predis- 
position for heads and necks is only beginning to be investigated. Imprinting learning is 
known to take place in IMHV (intermediate and medial portions of the hyperstriatum ven- 
trale) (Horn, 1985), and to rely on noradrenaline (Davies et al., 1992). The predisposition's 
location is currently unknown, but its strength correlates with plasma testosterone levels 
(Horn, 1985). 
1.1 Previous Models 
No previous models of imprinting have incorporated the predisposition in any meaningful 
way. O'Reilly & Johnson's (1994) model focussed on accounting for the sensitive period 
via an interaction between hysteresis (slow decay of activation) and a Hebbian learning rule, 
and ignored the predisposition. The only model which did try to include a predisposition 
(Bateson and Horn, 1994) was a 3-layer Hebbian network with real-valued input vectors, 
and outputs which represented the strehgth of an approach behavior. Bateson and Horn 
(1994) found a predisposition in their model by comparing networks trained on input 
vectors of Os and ls (High) to vectors where non-zero entries were 0.6 (Low). Untrained 
networks preferred (produced a higher output value for) the high-valued input (hen), 
and trained networks preferred the stimulus they were trained on (box). Of course, in a 
network with identical weights, an input with higher input values will naturally excite an 
output unit more than one with lower input values. Thus, this model's predisposition is 
implicit in the input values, and is therefore hard to apply to chicks. 
In this project, I develop a model which incorporates both the predisposition and imprint- 
ing, and which is as consistent as possible with the known neurobiology. The overall goals 
of the project are to clarify how this predisposition might be implemented, and to exam- 
ine more generally the kinds of representations that underlie naive preferences that interact 
with and facilitate, rather than replace, learning. These particular simulations show that 
the model exhibits the same qualitative behavior as chicks under three important sets of 
conditions. 
The rest of the paper first describes the architecture of the current model (in general terms 
and then in more detail). It goes on to describe the particular simulations, and then com- 
pares the results of those simulations with the data gathered from chicks. 
2 Architecture 
The neural network model's architecture is shown in Figure 1. The input layer is a 6x6 
pixel retina to which binary pictures are presented. The next layer is a feature detector. 
The predisposition serves as the home of the network's naive preference, while the IMLL 
(intermediate learning layer) is intended to correspond to a chick's IMHV, and is where the 
network stores its learned representations. The output layer consists of two units which are 
taken to represent different action patterns (following Bateson and Horn (1994)): an ap- 
proach unit and a withdraw unit. These are the two chick behaviors which researchers 
use to assess a chick's degree of preference for a particular stimulus. The feature detec- 
tor provides input to the predisposition and IMLL layers; they in turn provide input to the 
output layer. Where there are connections, layers (and subparts) are fully interconnected. 
The feature detector uses a linear activation function; the rest of the network has a hyper- 
bolic tangent activation function. All activations and all connections can be either positive 
Model of Predispositions in Chicks 33 
Input 
Fe._.e Predisp. Output 
box head/neck 
'- -- Fixed Weights 
........... = Modifiable Weights 
Figure 1: The network architec- 
ture sketched. All connections 
are feedforward (from input to- 
ward output) only. 
cylinder 
Figure 2: The three input patterns used by the 
network. They have between 16 and 18 pixels 
each, and the central moment of each image 
is the same. 
or negative; the connections are limited to +0.9. Most of the learning takes place via a 
covariant Hebb rule, because it is considered to be plausible neurally. 
The lowest level of the network is a feature-detecting preprocessor. The current implemen- 
tation of this network takes crude 6x6 binary pictures (examples of which can be seen in 
Fig. 2), and produces a 15-place floating-point vector. The output units of the feature de- 
tector are clustered into five groups of three units each; each group of three units operates 
under a winner-take-all rule, in order to increase the difference between preprocessed pat- 
terns for the relevant pictures. The feature detector was trained on random inputs for 400 
cycles with a learning rate of .01, and its weights were then frozen. Training on random 
input was motivated by the finding that the lower levels of the visual system require some 
kind of input in order to organize; Miller et al. (1989) suggest that, at least in cats, the 
random firing of retinal neurons is sufficient. 
The predisposition layer was trained via backprop using the outputs of the feature detector 
as its inputs. The pattern produced by the head-neck picture in the feature detector was 
trained to excite the approach output unit and inhibit the withdraw unit; other patterns 
were trained to a neutral value on both output units. These weights were stored, and treated 
as fixed in the larger network. (In fact, these weights were scaled down by a constant 
factor (.8) before being used in the larger network.) Since this is a naive preference, or 
predisposition, these weights are assumed to be fixed evolutionarily. Thus, the method of 
setting them is irrelevant; they could also have been found by a genetic algorithm. 
The IMLL layer is a winner-take-all network of three units. Its connections with the feature 
detector's outputs are learned by a Hebb rule with learning rate .01 and a weight decay (to 
0) term of .0005. For these simulations, its initial weights were fixed by hand, in a pattern 
which insured that each IMLL unit received a substantially different value for the same 
input pattern. This pattern of initial weights also increased the likelihood that the three 
patterns of interest in the simulations maximally affected different IMLL units. 
As previously mentioned, the output layer consists of an approach and a withdraw 
unit. It also learns via a Hebb rule, with the same learning rate and decay term as IMLL. 
Its connections with IMLL are learned; those with the predisposition layer are fixed. Initial 
weights between IMLL and the output layer are random, and vary from -0.3 to 0.3. The 
bias to the approach unit is 0; that to the withdraw unit is 0.05. 
34 L E. Hadden 
2.1 Training 
In the animal experiments on which this model is based, chicks are kept in the dark (and in 
isolation) except for training and testing periods. Training periods involve visual exposure 
to an object (usually a red box); testing involves allowing the chick to choose between 
approaching the training object and some other object (usually either a stuffed hen or a blue 
cylinder) (Horn, 1985). The percentage of time the chick approaches the training object (or 
other object of interest) is its preference score for that object (Horn, 1985). A preference 
score of 50% indicates indifference; scores above 50% indicate a preference for the target 
object, and those below indicate a preference for the other object. For the purposes of
