A Bound on the Error of Cross Validation Using 
the Approximation and Estimation Rates, with 
Consequences for the Training-Test Split 
Michael Kearns 
AT&T Research 
ABSTRACT
1 INTRODUCTION 
We analyze the performance of cross validation  in the context of model selection and 
complexity regularization. We work in a setting in which we must choose the right number 
of parameters for a hypothesis function in response to a finite training sample, with the goal 
of minimizing the resulting generalization error. There is a large and interesting literature 
on cross validation methods, which often emphasizes asymptotic statistical properties, or 
the exact calculation of the generalization error for simple models. Our approach here is 
somewhat different, and is primarily inspired by two sources. The first is the work of Barron 
and Cover [2], who introduced the idea of bounding the error of a model selection method 
(in their case, the Minimum Description Length Principle) in terms of a quantity known as 
the index of resolvability. The second is the work of Vapnik [5], who provided extremely 
powerful and general tools for uniformly bounding the deviations between training and 
generalization errors. 
We combine these methods to give a new and general analysis of cross validation perfor- 
mance. In the first and more formal part of the paper, we give a rigorous bound on the error 
of cross validation in terms of two parameters of the underlying model selection problem: 
the approximation rate and the estimation rate. In the second and more experimental part 
of the paper, we investigate the implications of our bound for choosing 7, the fraction of 
data withheld for testing in cross validation. The most interesting aspect of this analysis is 
the identification of several qualitative properties of the optimal 7 that appear to be invariant 
over a wide class of model selection problems: 
When the target function complexity is small compared to the sample size, the 
performance of cross validation is relatively insensitive to the choice of 7. 
The importance of choosing 7 optimally increases, and the optimal value for 7 
decreases, as the target function becomes more complex relative to the sample 
size. 
There is nevertheless a single fixed value for 7 that works nearly optimally for a 
wide range of target function complexity. 
2 THE FORMALISM 
We consider model selection as a two-part problem: choosing the appropriate number of 
parameters for the hypothesis function, and tuning these parameters. The training sample 
is used in both steps of this process. In many settings, the tuning of the parameters is 
determined by a fixed learning algorithm such as backpropagation, and then model selection 
reduces to the problem of choosing the architecture. Here we adopt an idealized version of 
this division of labor. We assume a nested sequence of function classes H C ... C Ha..., 
called the structure [5], where Ha is a class of boolean functions of d parameters, each 
Perhaps in conflict with accepted usage in statistics, here we use the term cross validation to 
mean the simple method of saving out an independent test set to perform model selection. Precise 
definitions will be stated shortly. 
184 M. KEARNS 
function being a mapping from some input space X into {0, 1}. For simplicity, in this 
paper we assume that the Vapnik-Chervonenkis (VC) dimension [6, 5] of the class Ha is 
O(d). To remove this assumption, one simply replaces all occurrences of d in our bounds by 
the VC dimension of Ha. We assume that we have in our possession a learning algorithm 
L that on input any training sample $ and any value d will output a hypothesis function 
ha  Ha that minimizes the training error over Ha that is, et(ha) = min,E, 
where c(h) is the fraction of the examples in $ on which h disagrees with the given label. 
In many situations, training error minimization is known to be computationally intractable, 
leading researchers to investigate heuristics such as backpropagation. The extent to which 
the theory presented here applies to such heuristics will depend in part on the extent to 
which they approximate training error minimization for the problem under consideration. 
Model selection is thus the problem of choosing the best value of d. More precisely, we 
assume an arbitrary target function f (which may or may not reside in one of the function 
classes in the structure H C .-. C Ha...), and an input distribution P; f and P together 
define the generalization error function %(h) = PrE,[h(z)  f(z)]. We are given a 
training sample S of f, consisting of m random examples drawn according to P and labeled 
by f (with the labels possibly corrupted by a noise process that randomly complements each 
label independently with probability r/< 1/2). The goal is to minimize the generalization 
error of the hypothesis selected. 
In this paper, we will make the rather mild but very useful assumption that the structure has 
the property that for any sample size m, there is a value d,,, (m) such that el(ha,,, (,,,)) = 
0 for any labeled sample $ of m examples. We call the function d,,,,,(rn.) the fitting 
number of the structure. The fitting number formalizes the simple notion that with enough 
parameters, we can always fit the training data perfectly, a property held by most sufficiently 
powerful function classes (including multilayer neural networks). We typically expect the 
fitting number to be a linear function of m, or at worst a polynomial in m. The significance 
of the fitting number for us is that no reasonable model selection method should choose ha 
for d _> d,,,,,(rn.), since doing so simply adds complexity without reducing the training 
error. 
In this paper we concentrate on the simplest version of cross validation. We choose a 
parameter 7  [0, 1], which determines the split between training and test data. Given the 
input sample $ of m examples, let $ be the subsample consisting of the first (1 - 7)rn. 
examples in $, and $ the subsample consisting of the last 7mexamples. In cross validation, 
rather than giving the entire sample $ to L, we give only the smaller sample $, resulting in 
the sequence h,..., h,,,(( _.fi,,,) of increasingly complex hypotheses. Each hypothesis 
is now obtained by training on only (1 - 7)m examples, which implies that we will only 
consider values of d smaller than the corresponding fitting number d,,,,((1 - 7)m); let 
us introduce the shorthand d't,, for d,,,,((1 - 7)m). Cross validation chooses the ha 
satisfying ha = minle{,...,ag,,, } {e'(hi)} where e'(hl) is the error of hi on the subsample 
S. Notice that we are not considering multifold cross validation, or other variants that 
make more efficient use of the sample, because our analyses will require the independence 
of the test set. However, we believe that many of the themes that emerge here may apply to 
these more sophisticated variants as well. 
We use c,, (m) to denote the generalization error %(ha) of the hypothesis ha chosen by cross 
validation when given as input a sample $ of m random examples of the target function. 
Obviously, c,, (m) depends on $, the structure, f, P, and the noise rate. When bounding 
cv (m), we will use the expression with high probability to mean with probability 1 - 6 
over the sample $, for some small fixed constant 6 > 0. All of our results can also be 
stated with 6 as a parameter at the cost of a log(1/6) factor in the bounds, or in terms of the 
expected value of ,, (m). 
3 THE APPROXIMATION RATE 
It is apparent that any nontrivial bound on e,, (m) must take account of some measure of the 
complexity of the unknown target function f. The correct measure of this complexity is 
less obvious. Following the example of Barron and Cover's analysis of MDL performance 
A Bound on the Error of Cross Validation 185 
in the context of density estimation [2], we propose the approximation rate as a natural 
measure of the complexity of f and P in relation to the chosen structure H1 C ... C Ha.... 
Thus we define the approximation rate function %(d) to be %(d) = minhEH .[%(h)}. The 
function %(d) tells us the best generalization error that can be achieved in the class Ha, 
and it is a nonincreasing function of d. If %(s) = 0 for some sufficiently large s, this 
means that the target function f, at least with respect to the input distribution, is realizable 
in the class Ho, and thus s is a coarse measure of how complex f is. More generally, even 
if %(d) > 0 for all d, the rate of decay of %(d) still gives a nice indication of how much 
representational power we gain with respect to f and P by increasing the complexity of 
our models. Still missing, of course, is some means of determining the extent to which this 
representational power can be realized by training on a finite sample of a given size, but 
this will be added shortly. First we give examples of the approximation rate that we will 
examine following the general bound on ec,, (m). 
The Intervals Problem. In this problem, the input space X is the real interval [0, 1], and 
the class Ha of the structure consists of all boolean step functions over [0, 1] of at most 
d steps; thus, each function partitions the interval [0, 1] into at most d disjoint segments 
(not necessarily of equal width). and assigns alternating positive and negative labels to 
these segments. The input space is one-dimensional, but the structure contains arbitrarily 
complex functions over [0, 1]. It is easily verified that our assumption that the VC dimension 
of Ha is O(d) holds here, and that the fitting number obeys d,,,,,(m) <_ m. Now suppose 
that the input density P is uniform, and suppose that the target function f is the function 
of s alternating segments of equal width l/s. for some s (thus. f lies in the class Ho). 
We will refer to these settings as the intervals problem. Then the approximation rate is 
%(d) = (1/2)(1 -
