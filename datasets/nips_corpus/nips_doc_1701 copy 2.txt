A Multi-class Linear Learning Algorithm 
Related to Winnow 
Chris Mesterharm* 
Rutgers Computer Science Department 
110 Frelinghuysen Road 
Piscataway, NJ 08854 
mesterha@ paul .rutgers.edu 
Abstract 
In this paper, we present Committee, a new multi-class learning algo- 
rithm related to the Winnow family of algorithms. Committee is an al- 
gorithm for combining the predictions of a set of sub-experts in the on- 
line mistake-bounded model of learning. A sub-expert is a special type of 
attribute that predicts with a distribution over a finite number of classes. 
Committee learns a linear function of sub-experts and uses this function 
to make class predictions. We provide bounds for Committee that show 
it performs well when the target can be represented by a few relevant 
sub-experts. We also show how Committee can be used to solve more 
traditional problems composed of attributes. This leads to a natural ex- 
tension that learns on multi-class problems that contain both traditional 
attributes and sub-experts. 
1 Introduction 
In this paper, we present a new multi-class learning algorithm called Committee. Committee 
learns a k class target function by combining information from a large set of sub-experts. 
A sub-expert is a special type of attribute that predicts with a distribution over the target 
classes. The target space of functions are linear-max functions. We define these as functions 
that take a linear combination of sub-expert predictions and return the class with maximum 
value. It may be useful to think of the sub-experts as individual classifying functions that 
are attempting to predict the target function. Even though the individual sub-experts may 
not be perfect, Committee attempts to learn a linear-max function that represents the target 
function. In truth, this picture is not quite accurate. The reason we call them sub-experts 
and not experts is because even though a individual sub-expert might be poor at prediction, 
it may be useful when used in a linear-max function. For example, some sub-experts might 
be used to add constant weights to the linear-max function. 
The algorithm is analyzed for the on-line mistake-bounded model of learning [Lit89]. This 
is a useful model for a type of incremental learning where an algorithm can use feedback 
about its current hypothesis to improve its performance. In this model, the algorithm goes 
through a series of learning trials. A trial is composed of three steps. First, the algorithm 
*Part of this work was supported by NEC Research Institute, Princeton, NJ. 
520 C. Mesterharm 
receives an instance, in this case, the predictions of the sub-experts. Second, the algorithm 
predicts a label for the instance; this is the global prediction of Committee. And last, the 
algorithm receives the true label of the instance; Committee uses this information to up- 
date its estimate of the target. The goal of the algorithm is to minimize the total number of 
prediction mistakes the algorithm makes while learning the target. 
The analysis and performance of Committee is similar to another learning algorithm, Win- 
now [Lit89]. Winnow is an algorithm for learning a linear-threshold function that maps 
attributes in [0, 1] to a binary target. It is an algorithm that is effective when the concept 
can be represented with a few relevant attributes, irrespective of the behavior of the other 
attributes. Committee is similar but deals with learning a target that contains only a few 
relevant sub-experts. While learning with sub-experts is interesting in it's own right, it 
turns out the distinction between the two tasks is not significant. We will show in section 5 
how to transform attributes from [0, 1] into sub-experts. Using particular transformations, 
Committee is identical to the Winnow algorithms, Balanced and WMA [Lit89]. Further- 
more, we can generalize these transformations to handle attribute problems with multi-class 
targets. These transformations naturally lead to a hybrid algorithm that allows a combina- 
tion of sub-experts and attributes for multi-class learning problems. This opens up a range of 
new practical problems that did not easily fit into the previous framework of [0, 1] attributes 
and binary classification. 
2 Previous work 
Many people have successfully tried the Winnow algorithms on real-world tasks. In the 
course of their work, they have made modifications to the algorithms to fit certain aspects 
of their problem. These modifications include multi-class extensions. 
For example, [DKR97] use Winnow algorithms on text classification problems. This multi- 
class problem has a special form; a document can belong to more than one class. Because 
of this property, it makes sense to learn a different binary classifier for each class. The linear 
functions are allowed, even desired, to overlap. However, this paper is concerned with cases 
where this is not possible. For example, in [GR96] the correct spelling of a word must be 
selected from a set of many possibilities. In this setting, it is more desirable to have the 
algorithm select a single word. 
The work in [GR96] presents many interesting ideas and modifications of the Winnow al- 
gorithms. At a minimum, these modification are useful for improving the performance of 
Winnow on those particular problems. Part of that work also extends the Winnow algorithm 
to general multi-class problems. While the results are favorable, the contribution of this pa- 
per is to give a different algorithm that has a stronger theoretical foundation for customizing 
a particular multi-class problem. 
Blum also works with multi-class Winnow algorithms on the calendar scheduling problem 
of [MCF+94]. In [Blu95], a modified Winnow is given with theoretical arguments for good 
performance on certain types of multi-class disjunctions. In this paper, these results are ex- 
tended, with the new algorithm Committee, to cover a wider range of multi-class linear func- 
tions. 
Other related theoretical work on multi-class problems includes the regression algorithm 
EG +-. In [KW97], Kivinen and Warmuth introduce EG +-, an algorithm related to Winnow 
but used on regression problems. In general, while regression is a useful framework for 
many multi-class problems, it is not straightforward how to extend regression to the con- 
cepts learned by Committee. A particular problem is the inability of current regression tech- 
niques to handle 0-1 loss. 
A Multi-class Linear Learning Algorithm Related to Winnow 521 
3 Algorithm 
This section of the paper describes the details of Committee. 
we will give a formal statement of the algorithm. 
Near the end of the section, 
3.1 Prediction scheme 
Assume there are n sub-experts. Each sub-expert has a positive weight that is used to vote 
for k different classes; let wi be the weight of sub-expert i. A sub-expert can vote for sev- 
eral classes by spreading its weight with a prediction distribution. For example, if k - 3, a 
sub-expert may give 3/5 of its weight to class 1, 1/5 of its weight to class 2, and 1/5 of its 
weight to class 3. Let xi represent this prediction distribution, where xï¿½ is the fraction of the 
n J Committee predicts 
weight sub-expert i gives to class j. The vote for class j is 5-= 1 wix. 
the class that has the highest vote. (On ties, the algorithm picks one of the classes involved 
in the tie.) We call the function computed by this prediction scheme a linear-max function, 
since it is the maximum class value taken from a linear combination of the sub-expert pre- 
dictions. 
3.2 Target function 
The goal of Committee is to minimize the number of mistakes by quickly learning 
sub-expert weights that correctly classify the target function. Assume there exists it, a vec- 
tor of nonnegative weights that correctly classifies the target. Notice that tz can be multiplied 
by any constant without changing the target. To remove this confusion, we will normalize 
the weights to sum to 1, i.e., n 
Y']4= tzi = 1. Let (j) be the target's vote for class j. 
= bti i 
Part of the difficulty of the learning problem is hidden in the target weights. Intuitively, a 
target function will be more difficult to learn if there is a small difference between the  votes 
of the correct and incorrect classes. We measure this difficulty by looking at the minimum 
difference, over all trials, of the vote of the correct label and the vote of the other labels. 
Assume for trial t that Pt is the correct label. 
5= min 
tETrials 
Because these are the weights of the target, and the target always makes the correct predic- 
tion, 5 > 0. 
One problem with the above assumptions is that they do not allow noise (cases where 5 _ 0). 
However, there are variations of the analysis that allow for limited amounts of noise [Lit89, 
Lit91]. Also experimental work [Lit9$, LM] shows the family of Winnow algorithms to be 
much more robust to noise than the theory would predict. Based on the similarity of the 
algorithm and analysis, and some preliminary experiments, Committee should be able to 
tolerate some noise. 
3.3 Updates 
Committee only updates on mistakes using multiplicative updates. The algorithm starts by 
initializing all weights to 1In. During the trials, let p be the correct label and  be the pre- 
dicted label of Committee. When   p the weight of each sub-expert i is multiplied by 
ct:-x This corresponds to increasing the weights of the sub-experts who predicted the 
522 C. Mesterharm 
correct label instead of the label Committee predicted. The value of c is initialized at the 
start of the algorithm. The optimal value of c for the bounds depends on 6. Often 6 is not 
known in advance, but experiments on Winnow algorithms suggest that these algorithms 
are more flexible, often performing well with a wider range of c values [LM]. Last, the 
weights are renormalize to sum to 1. While this is not strictly necessary, normalizing has 
several advantages including reducing 
