Inference for the Generalization Error 
Claude Nadeau 
CIRANO 
2020, University, 
Montreal, Qc, Canada, H3A 2A5 
j cnadeauOaltavista. net 
Yoshua Bengio 
CIRANO and Dept. IRO 
Universitd de Montrdal 
Montreal, Qc, Canada, H3C 3J7 
bengioyOiro. umontreal. ca 
Abstract 
In order to to compare learning algorithms, experimental results reported 
in the machine learning litterature often use statistical tests of signifi- 
cance. Unfortunately, most of these tests do not take into account the 
variability due to the choice of training set. We perform a theoretical 
investigation of the variance of the cross-validation estimate of the gen- 
eralization error that takes into account the variability due to the choice 
of training sets. This allows us to propose two new ways to estimate 
this variance. We show, via simulations, that these new statistics perform 
well relative to the statistics considered by Dietterich (Dietterich, 1998). 
1 Introduction 
When applying a learning algorithm (or comparing several algorithms), one is typically 
interested in estimating its generalization error. Its point estimation is rather trivial through 
cross-validation. Providing a variance estimate of that estimation, so that hypothesis test- 
ing and/or confidence intervals are possible, is more difficult, especially, as pointed out in 
(Hinton et al., 1995), if one wants to take into account the variability due to the choice of 
the training sets (Breiman, 1996). A notable effort in that direction is Dietterich's work (Di- 
etterich, 1998). Careful investigation of the variance to be estimated allows us to provide 
new variance estimates, which turn out to perform well. 
Let us first lay out the framework in which we shall work. We assume that data are avail- 
able in the form Z = {Z,..., Zn}. For example, in the case of supervised learning, 
Zi = (Xi, Yi)  Z C_ R p+q, where p and q denote the dimensions of the Xi's (inputs) 
and the Y/'s (outputs). We also assume that the Zi's are independent with Zi ,',, P(Z). 
Let �(D; Z), where D represents a subset of size nl < n taken from Z, be a function 
,Z TM x ,Z -->  For instance, this function could be the loss incurred by the decision 
that a learning algorithm trained on D makes on a new example Z. We are interested in 
estimating ntz -- E[�(Z; Zn+)] where Zn+ ' P(Z) is independent of Z. Subscript 
n stands for the size of the training set (Z here). The above expectation is taken over Z 
and Zn+, meaning that we are interested in the performance of an algorithm rather than 
the performance of the specific decision function it yields on the data at hand. According to 
Dietterich's taxonomy (Dietterich, 1998), we deal with problems of type 5 through 8, (eval- 
uating learning algorithms) rather then type 1 through 4 (evaluating decision functions). We 
call n/z the generalization error even though it can also represent an error difference: 
� Generalization error 
We may take 
�(D; Z) = �(D; (X, Y)) = Q(F(D)(X), Y), (1) 
308 C. Nadeau and Y. Bengio 
where F(D) (F(D) : lRv  lRq) is the decision function obtained when training an 
algorithm on D, and Q is a loss function measuring the inaccuracy of a decision. For 
instance, we could have Q(0, y) = I[0  y], where I[ ] i the indicator function, for 
classification problems and Q(O, y) -II - u II 2, where is II' I is the Euclidean norm, for 
regression problems. In that case ,it is what most people call the generalization error. 
� Comparison of generalization errors 
Sometimes, we are not interested in the performance of algorithms per se, but instead in 
how two algorithms compare with each other. In that case we may want to consider 
�(D;Z) = �(D;(X,Y)) =Q(FA(D)(X),Y) -Q(FB(D)(X),Y), (2) 
where FA(D) and FB(D) are decision functions obtained when training two algorithms 
(A and B) on D, and Q is a loss function. In this case ,/z would be a difference of 
generalization errors as outlined in the previous example. 
The generalization error is often estimated via some form of cross-validation. Since there 
are various versions of the latter, we lay out the specific form we use in this paper. 
� Let Sj be a random set of nx distinct integers from {1,...,n}(n < n). Here nx 
represents the size of the training set and we shall let n2 - n - nx be the size of the test 
set. 
� Let $x,... $j be independent such random sets, and let $ = {1,..., n} \ $j denote the 
complement of Sj. 
� Let Z$ s = {Zli 6 $j} be the training set obtained by subsampling Z according to the 
random index set Sj. The corresponding test set is Z$ = {Zili  $ }. 
� Let L(j, i) = �(Z$s; Zi). According to (1), this could be the error an algorithm trained 
on the training set Z$ makes on example Zi. According to (2), this could be the difference 
of such errors for two different algorithms. 
� Let j = K Y'k=x L(j, i) where if,..., i3c are randomly and independently drawn 
from $. Here we draw K examples from the test set Z$ with replacement and compute 
the average error committed. The notation does not convey the fact that j depends on K, 
n and n2. 
^ -- Y'4$? L(j, i) denote what bj becomes as K increases 
� Let ttf = limx-}oo j = = , 
without bounds. Indeed, when sampling infinitely often from Zs, each Zi (i  S) is 
x , yielding the usual average test egor. The use of K is 
chosen with relative frequency  
just a mathematical device to make the test exmples sampled independently from S. 
Then the cross-validation estimate of the generalization egor consider in this paper is 
n2gK 1   
nJ =  j' 
=1 
We note that this an unbiased estimator of n = E[E(Z  , Zn+)] (not the same as n). 
This paper is about the estimation of the vance of n=  
m a � We first study theoretically 
this vmiance in section 2, leading to two new vmiance estimators developped in section 3. 
Stion 4 shows p of a simulation study we peffomed to see how the proposed statistics 
behave compared to statistics already in use. 
2 Analysis of Vat[ 
n/J J 
Here we study Vat[ n r,c This is important to understand why some inference proce- 
n tJ J' 
dures about m tt presently in use are inadequate, as we shall underline in section 4. This 
investigation also enables us to develop estimators of Vat[ n/j ] in section 3. Before we 
proceed, we state the following useful lemma, proved in (Nadeau and Bengio, 1999). 
Inference for the Generalization Error 309 
Lemma 1 Let U1,..., Uk be random variables with common mean fi, common variance 
6 and Cov[Ui, Uj] = % Vi  j. Let rr =  be the correlation between Ui and Uj (i  j). 
Let  = k -1 I I 
Ei=l Ui a S = 1 
- i=i (Ui - 0): be the sample mean and sample 
variance respectively. Then E[Sb] = 6 - 7 and Var[O] = 7 + (-) - 6 ( + ) 
To study Vat[ n;',K1 
m J  we need to define the following covariances. 
� Let a0 = ao(nl) = Var[L(j, i)] when i is randomly drawn from S. 
� Let o- = rr(n,n2) = Cov[L(j,i),L(j,i')] for i and i' randomly and indepen- 
dently drawn from $. 
� Let rr2 = a2(n,n2) = C[L(j,i),L(j',i')], with j  j', i and i' randomly and 
independently drawn from S and S, respectively. 
* Letaa = aa(n) = C[L(j,i),L(j,i')] fori, i'  S and/ i'. is is not the 
same as a. In fact, it may be shown that 
a = Cov[L(j,i) L(j,i')] ao (n2- 1)a a0 
, = + = as + (3) 
2 2 
Let us look at the mean and viance of  and  . Concerning expectations, we 
obviously have E[] = n and thus E[ = . From Lemma 1, we have 
o- which implies 
Var[fiF] = Vat[ lim fi2] = lim Var[fij] = 
It can also be shown that Cov[fi2, fly] = a2, j  f, and therefore (using Lemma 1) 
mJ l = 2 + j = 2 + K -- 
j (4) 
We shall often encounter a0, a, a2, aa in the future, so some knowlge about those quan- 
tities is valuable. Here's what we can say about them. 
Proposition 1 For given n and n2, we have 0  a2'  al  ao and 0 
Proof See (Nadeau and Bengio, 1999). 
A natural question about the estimator n2 ?,K is how hi, n2, K and J affect its viance. 
Proposition 2 The variance of n r,K is non-increasing in J, K and 
nJ 
Proof See (Nadeau and Bengio, 1999). 
Clearly, increasing K leads to smaller variance because the noise introduced by sampling 
with replacement from the test set disappears when this is done over and over again. Also, 
averaging over many train/test (increasing J) improves the estimation of m/. Finally, all 
things equal elsewhere (r fixed among other things), the larger the size of the test sets, the 
better the estimation of m/. 
^K 
The behavior of Vat[ n2j ] with respect to rl is unclear, but we conjecture that in most 
n 
situations it should decrease in n. Our argument goes like this. The variability in n2 fi 
comes from two sources: sampling decision rules (training process) and sampling testing 
examples. Holding r2, J and K fixed freezes the second source of variation as it solely de- 
pends on those three quantities, not r. The problem to solve becomes: how does n affect 
the first source of variation? It is not unreasonable to say that the decision function yielded 
by a learning algorithm is less variable when the training set is large. We conclude that the 
first source of variation, and thus the total variation (that is Vat[ n2r, Kll is decreasing in 
nJ 
r. We advocate the use of the estimator 
310 C. Nadeau and Y. Bengio 
as it is easier to compute and has smaller variance than 
Var[ 
nllj ] = lim Vat[ n2f ] = 0'2 + 
K-- oo 
where p =  = Cr[, ]. 
3 Estimation of Vat[ 
1 'J (J' '1, '2 held constant). 
0'1-0'2 (l-p) (6) 
j =0' p+  ' 
We are interested in estimating n2,,2 Vat[ 2b] where ,:b is as defined in (5). We 
nl 'J - 1 1 
provide two different estimators of Vat[ n ^ oo 
m/J ]' The first is simple but may have a positive 
or negative bias for the actual variance. The second is meant to be conservative, that is, 
if our conjecture of the previous section is correct, its expected value exceeds the actual 
variance. 
1st Method:
