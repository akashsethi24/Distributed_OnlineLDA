Exploiting Tractable Substructures 
in Intractable Networks 
Lawrence K. Saul and Michael I. Jordan 
{lksaul, j ordar}*psyche. mir. edu 
Center for Biological and Computational Learning 
Massachusetts Institute of Technology 
79 Amherst Street, E10-243 
Cambridge, MA 02139 
Abstract 
We develop a refined mean field approximation for inference and 
learning in probabilistic neural networks. Our mean field theory, 
unlike most, does not assume that the units behave as independent 
degrees of freedom; instead, it exploits in a principled way the 
existence of large substructures that are computationally tractable. 
To illustrate the advantages of this framework, we show how to 
incorporate weak higher order interactions into a first-order hidden 
Markov model, treating the corrections (but not the first order 
structure) within mean field theory. 
1 INTRODUCTION 
Learning the parameters in a probabilistic neural network may be viewed as a 
problem in statistical estimation. In networks with sparse connectivity (e.g. trees 
and chains), there exist efficient algorithms for the exact probabilistic calculations 
that support inference and learning. In general, however, these calculations are 
intractable, and approximations are required. 
Mean field theory provides a framework for approximation in probabilistic neural 
networks (Peterson & Anderson, 1987). Most applications of mean field theory, 
however, have made a rather drastic probabilistic assumption--namely, that the 
units in the network behave as independent degrees of freedom. In this paper we 
show how to go beyond this assumption. We describe a self-consistent approxi- 
mation in which tractable substructures are handled by exact computations and 
only the remaining, intractable parts of the network are handled within mean field 
theory. For simplicity we focus on networks with binary units; the extension to 
discrete-valued (Potts) units is straightforward. 
Exploiting Tractable Substructures in Intractable Networks 487 
We apply these ideas to hidden Markov modeling (Rabiner & Juang, 1991). The 
first order probabilistic structure of hidden Markov models (HMMs) leads to net- 
works with chained architectures for which efficient, exact algorithms are available. 
More elaborate networks are obtained by introducing couplings between multiple 
HMMs (Williams & Hinton, 1990) and/or long-range couplings within a single HMM 
(Stolorz, 1994). Both sorts of extensions have interesting applications; in speech, 
for example, multiple HMMs can provide a distributed representation of the artic- 
ulatory state, while long-range couplings can model the effects of coarticulation. In 
general, however, such extensions lead to networks for which exact probabilistic cal- 
culations are not feasible. One would like to develop a mean field approximation for 
these networks that exploits the tractability of first-order HMMs. This is possible 
within the more sophisticated mean field theory described here. 
2 MEAN FIELD THEORY 
We briefly review the basic methodology of mean field theory for networks of binary 
(4-1) stochastic units (Parisi, 1988). For each configuration {$} = {$1, $,..., $N }, 
we define an energy E{S} and a probability P{$} via the Boltzmann distribution: 
P{S} = Z ' (1) 
where /3 is the inverse temperature and Z is the partition function. When it is 
intractable to compute averages over P{S}, we are motivated to look for an ap- 
proximating distribution Q{$}. Mean field theory posits a particular parametrized 
form for Q{$}, then chooses parameters to minimize the Kullback-Liebler (KL) 
divergence: 
KL(QllP) = y] Q{$} In [P--J' (2) 
{s} 
Why are mean field approximations valuable for learning? Suppose that P{$} 
represents the posterior distribution over hidden variables, as in the E-step of an 
EM algorithm (Dempster, Laird, & Rubin, 1977). Then we obtain a mean field 
approximation to this E-step by replacing the statistics of P{S} (which may be 
quite difficult to compute) with those of Q{$} (which may be much simpler). If, in 
addition, Z represents the likelihood of observed data (as is the case for the example 
of section 3), then the mean field approximation yields a lower bound on the log- 
likelihood. This can be seen by noting that for any approximating distribution 
Q{$}, we can form the lower bound: 
lnZ = lnEe-z{s} (3) 
{s} 
[e-E{s} 
= lnEQ{$}- [ Q{$} (4) 
{s} 
_ EQ{$}[-E{$}-lnQ{$}], (5) 
{s} 
where the last line follows from Jensen's inequality. The difference between the left 
and right-hand side of eq. (5) is exactly KL(QIIP); thus the better the approximation 
to P{$}, the tighter the bound on In Z. Once a lower bound is available, a learning 
procedure can maximize the lower bound. This is useful when the true likelihood 
itself cannot be efficiently computed. 
488 L.K. SAUL, M. I. JORDAN 
2.1 Complete Factorizability 
The simplest mean field theory involves assuming marginal independence for the 
units $i. Consider, for example, a quadratic energy function 
- = + 
i<j i 
and the factorized approximation: 
. (7) 
i 
The expectations under this mean field approximation are (Si) = rni and (SiSj) = 
mired for i  j. The best approximation of this form is found by minimizing the 
KL-divergence, 
KL(Q,,P) '-z[(l+mi)ln(l+mi) (1-mi) 
ï¿½ 2 2 +  In  (8) 
$ 
- Z Jijmirnj - Z himi + In Z, 
i<j i 
with respect to the mean field parameters mi. Setting the gradients of eq. (8) equal 
to zero, we obtain the (classical) mean field equations: 
tanh-X(mi) = Z Jijmj + hi. (9) 
2.2 Partial Factorizability 
We now consider a more structured model in which the network consists of interact- 
ing modules that, taken in isolation, define tractable substructures. One example 
of this would be a network of weakly coupled HMMs, in which each HMM, taken 
by itself, defines a chain-like substructure that supports efficient probabilistic cal- 
culations. We denote the interactions between these modules by parameters K?, 
where the superscripts/ and y range over modules and the subscripts i and j index 
units within modules. An appropriate energy function for this network is: 
- E{S} =  ... K..S.   
ij 
The first term in this energy function contains the intra-modular interactions; the 
lt term, the inter-modular ones. 
We now consider a mean field approximation that maintains the first sum over 
modules but dispenses with the inter-modular corrections: 
i 
The parameters of this mean field approximation are H; they will be chosen to 
provide a self-consistent model of the inter-modular interactions. We eily obtain 
the following expectations under the mean field approximation, where   : 
Exploiting Tractable Substructures in Intractable Networks 489 
Note that units in the same module are statistically correlated and that these cor- 
relations are assumed to be taken into account in calculating the expectations. We 
assume that an efficient algorithm is available for handling these intra-modular cor- 
relations. For example, if the factorized modules are chains (e.g. obtained from 
a coupled set of HMMs), then computing these expectations requires a forward- 
backward pass through each chain. 
The best approximation of the form, eq. (11), is found by minimizing the KL- 
divergence, 
KL(QIIP)=ln(Z/ZQ)+E(H-h)($I- EK'($$I, (14) 
3 
with respect to the mean field parameters H. To compute the appropriate gradi- 
ents, we use the fact that derivatives of expectations under a Boltzmann distribu- 
tion (e.g. O(Sf)/OH) yield cumulants (e.g. (SfS) -(Sf)(S)). The conditions 
for stationarity are then: 
O= (H-h)[(SS)- (S)(S)]- KJ  [(SSS)- (5S)(3)] . (15) 
Substituting the expectations from eqs. (12) and (13), we find that KL(QIIP) is 
minimized when 
O=VH-h-VKi(S)}[(S)-(S)(S) ]. (16) 
The resulting mean field equations are: 
v j 
These equations may be solved by iteration, in which the (assumed) tractable algo- 
rithms for averaging over Q{$} are invoked as subroutines to compute the expecta- 
tions ($f/on the right hand side. Because these expectations depend on HI, these 
equations may be viewed as a self-consistent model of the inter-modular interac- 
tions. Note that the mean field parameter H plays a role analogous to-tanh-X(mi) 
in eq. (9) of the fully factorized case. 
2.3 Inducing Partial Factorizability 
Many interesting networks do not have strictly modular architectures and can only 
be approximately decomposed into tractable core structures. Techniques are needed 
in such cases to induce partial factorizability. Suppose for example that we are given 
an energy function 
i<j i 
(18) 
for which the first two terms represent tractable interactions and the last term, 
intractable ones. Thus the weights Jij by themselves define a tractable skeleton 
network, but the weights Kij spoil this tractability. Mimicking the steps of the 
previous section, we obtain the mean field equations: 
i 
(19) 
490 L.K. SAUL, M. I. JORDAN 
In this case, however, the weights Kij couple units in the same core structure. Be- 
cause these units are not assumed to be independent, the triple correlator 
does not factorize, and we no longer obtain the decoupled update rules of eq. (17). 
Rather, for these mean field equations, each iteration requires computing triple 
correlators and solving a large set of coupled linear equations. 
To avoid this heavy computational load, we instead manipulate the energy function 
into one that can be partially factorized. This is done by introducing extra hidden 
variables Wij = 4-1 on the intractable links of the network. In particular, consider 
the energy function 
i<j i i<j 
(20) 
The hidden variables Wij in eq. (20) serve to decouple the units connected by 
the intractable weights Kij. However, we can always choose the new interactions, 
KJ ) and K ), so that 
e -tz{s} = y]. e -z{s'w}. (21) 
Eq. (21) states that the marginal distribution over {S} in the new network is iden- 
tical to the j
