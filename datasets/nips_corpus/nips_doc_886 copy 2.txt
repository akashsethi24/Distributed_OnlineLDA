A Rigorous Analysis Of 
Linsker-type Hebbian Learning 
J. Feng 
Mathematical Department 
University of Rome La Sapienza 
P. le A. Moro, 00185 Rome, Italy 
f engmat. uniromal. it 
H. Pan V.P. Roychowdhury 
School of Electrical Engineering 
Purdue University 
West Lafayette, IN 47907 
hpanecn. purdue. edu 
vwanidrum. ecn. purdue. edu 
Abstract 
We propose a novel rigorous approach for the analysis of Linsker's 
unsupervised Hebbian learning network. The behavior of this 
model is determined by the underlying nonlinear dynamics which 
are parameterized by a set of parameters originating from the Heb- 
bian rule and the arbor density of the synapses. These parameters 
determine the presence or absence of a specific receptive field (also 
referred to as a 'connection pattern') as a saturated fixed point 
attractor of the model. In this paper, we perform a qualitative 
analysis of the underlying nonlinear dynamics over the parameter 
space, determine the effects of the system parameters on the emer- 
gence of various receptive fields, and predict precisely within which 
parameter regime the network will have the potential to develop 
a specially designated connection pattern. In particular, this ap- 
proach exposes, for the first time, the crucial role played by the 
synaptic density functions, and provides a complete precise picture 
of the parameter space that defines the relationships among the 
different receptive fields. Our theoretical predictions are confirmed 
by numerical simulations. 
320 Jianfeng Feng, H. Pan, V. P. Roychowdhury 
I Introduction 
For the purpose of understanding the self-organization mechanism of primary vi- 
sual system, Linsker has proposed a multilayered unsupervised Hebbian learning 
network with random uncorrelated inputs and localized arborization of synapses 
between adjacent layers (Linsker, 1986 & 1988). His simulations have shown that 
for appropriate parameter regimes, several structured connection patterns (e.g., 
centre-surround and oriented afferent receptive fields (aRFs)) occur progressively 
as the Hebbian evolution of the weights is carried out layer by layer. The behavior 
of Linsker's model is determined by the underlying nonlinear dynamics which are 
parameterized by a set of parameters originating from the Hebbian rule and the 
arbor density of the synapses. For a nonlinear system, usually, there coexist several 
attractors for the same set of system parameters. That is, for a given set of the 
parameters, the state space comprises several attractive basins, each corresponding 
to a steady state respectively. The initial condition determines which attractor will 
be eventually reached. At the same time, a nonlinear system could have a different 
group of coexisting attractors for a different set of system parameters. That is, 
one could make the presence or absence of a specific state as a fixed point attrac- 
tor by varying the set of the parameters. For a development model like Linsker's 
network, what is expected to be observed is that the different aRFs could emerge 
under different sets of parameters but should be relatively not sensitive to the initial 
conditions. In other words, the dynamics should avoid the coexistence of several 
attractors in an appropriate way. The purpose of this paper is to gain more insights 
into the dynamical mechanism of this self-organization model by performing a rig- 
orous analysis on its parameter space without any approximation. That is, our goal 
is to reveal the effects of the system parameters on the stability of aRFs, and to 
predict precisely within which parameter regime the network will have the poten- 
tial to develop a specially designated aRF. The novel rigorous approach presented 
here applies not only to the Linsker-type Hebbian learning but also to other related 
self-organization models about neural development. 
In Linsker's network, each cell in the present layer J4 receives synaptic inputs from 
a number of cells in the preceding layer/2. The density of these synaptic connections 
decreases monotonically with distance rr from the point underlying the Jd-cell's 
position. Since the synaptic weights change on a long time scale compared to the 
variation of random inputs, by averaging the Hebb rule over the ensemble of inputs 
in layer /2, the dynamical equation for the development of the synaptic strength 
Wr (i) between a J4-cell and i-th/2-cell at time r is 
N� 
3w_[_1(i ) -' f {3-( i) --[- k 1 --[- E[Qi  .-[-/c2]r(j)dw(j) } 
j--1 
(1) 
where kl, k2 are system parameters which are particular combinations of the con- 
stants of the Hebb rule, r(-) is a non-negative normalized synaptic density function 
(SDF) 1, and 'ier r(i) = 1, and f(.) is a limiter function defined by f(x) = 
if x > w,,; = x, if lxl _ w,,; and = -w,,, if x < -w,,. The covariance 
1The SDF is explicitly incorporated into the dynamics (1) which is equivalent to 
Linsker's formulation. A rigorous explanation for this equivalence is given in MacKay 
Miller, 1990. 
A Rigorous Analysis of Linsker-type Hebbian Learning 321 
matrix {Qij } of the layer � describes the correlation of activities of the i-th and the 
j-th �-cells. Actually, the covariance matrix of each layer is determined by SDFs 
r(.) of all layers preceding the layer under consideration. 
The idea of this paper is the following. It is well known that in general it is in- 
tractable to characterize the behavior of a nonlinear dynamics, since the nonlinearity 
is the cause of the coexistence of many attractors. And one has the difficulty in 
obtaining the complete characteristics of attractive basins in the state space. But 
usually for some cases, it is relatively easy to derive a necessary and sufficient con- 
dition to check whether a given state is a fixed point of the dynamics. In terms of 
this condition, the whole parameter regime for the emergence of a fixed point of the 
dynamics may be obtained in the parameter space. If we are further able to prove 
the stability of the fixed point, which implies that this fixed point is a steady state 
if the initial condition is in a nonempty vicinity in the state space, we can assert 
the occurrence of this fixed point attractor in that parameter regime. For Linsker's 
network, fortunately, the above idea can be carried out because of the specific form 
of the nonlinear function f(.). Due to space limitations, the rigorous proofs are in 
(Feng, Pan, & Roychowdhury, 1995). 
2 
The Set Of Saturated Fixed Point Attractors And The 
Criterion For The Division Of Parameter Regimes 
In fact, Linsker's model is a system of first-order nonlinear difference equations, 
taking the form 
W+l(i) = f[w(i) + hi(w.,kl, k2)], w = {w(j),j = 1, ..., Nr}, (2) 
Na 
where hi(w,kl,k2) = k + y.j=[Q/ + k2]r(j)w(j). And the aRFs observed in 
Linsker's simulation are the saturated fixed point attractors of this nonlinear system 
(2). Since the limiter function f(.)is defined on 
in weight state space within which the dynamics is dominated by the linear system 
w+l (i) = w(i) + hi(w, k l, k2), the short-time behaviors of evolution dynamics of 
connection patterns can be fully characterized in terms of the properties of eigen- 
vectors and their eigenvalues. But this method of stability analysis will not be 
suitable for the long-time evolution of equation (1) or (2), provided the hypercube 
constraint is reached as the first largest component of w reaches saturation. How- 
ever, it is well-known that a fixed point or an equilibrium state of dynamics (2) 
satisfies 
Wr(i) = f[w(i) + hi(wr,kl,k2) ]. (3) 
Because of the special form of the nonlinear function f(.), the fixed point equation 
(3) implies that 3T, such that for r > T, 
l w(i) + h(w,kl,kS) l _> 
So a saturated fixed point wr(i) must have the same sign as 
if hi(w, kl, ks)  O. 
hi(w-, kl, ks), i.e. 
w.(i)hi(w.,kl,ks) > O. 
By using the above idea, our Theorems 1 & 2 (proven in Feng, Pan, & Roychowd- 
hury, 1995) state that the set of saturated fixed point attractors of the dynamics in 
322 Jianfeng Feng, tt. Pan, V. P. Roychowdhury 
equation (1) is given by 
FP ---- {W [ co(i)hi(co-, kl, k2) > O, 1 _< i _< N�}, 
and co � f,, is stable, where the weight vector co belongs to the set of all extreme 
points of the hypercube f (we assume coma = 1 without loss of generality). 
We next derive an explicit necessary and sufficient condition for the emergence of 
structured aRFs, i.e., we derive conditions to determine whether a given co belongs 
to fire. Define J+(co) = {i I co(i) = 1} as the index set of cells at the preceding layer 
� with excitatory weight for a connection pattern co, and J-(co) = {ilco(i ) = -1} 
as the index set of �-cells with inhibitory weight for w. Note from the property of 
fixed point attractors that a connection pattern co is an attractor of the dynamics 
(1) if and only if for i � J+(co), we have 
co(i){]gl q- E[Q/5 q- ]g2I/'(J)co(j)) '-- 
co(i){kl q- EjeJ+(w)[Qj q- k2]r(j)co(j) + Ejes-()[Qi5 + k2]r(j)co(j)} > O. 
By the definition of J+(co) and J-(co), we deduce from the above inequality that 
+ [o5 + [o5 + > 0 
jea+() jes-() 
namely 
kl+ k2[ E 
jes+() 
j e s-() j e s-() j e s+() 
Inequality above is satisfied for M1 i in J+(w), and the left hand is independent of 
i. Hence, 
k+k,[  r(j)-  r(j)]> max [ P Qr(j)-  Qr(j)]. 
On the other hand, for i  J-(w), we can similarly deduce that 
k+k2[ E r(j) E r(j)]< min [ E � ' 
- %r(3) � ' 
- QirO)]- 
ies-() 3 
 +()  -() '  -() +() 
We introduce the slope function: 
C(co) de-:'f E r(j)-- E 
jes+(,) 
which is the difference of sums of the SDF r(.) over J+(co) and J-(co), and two 
k-intercept functions: 
der{ 
a(co) = 
and 
def{ 
a=(co) = 
max/e.+(w)(5'jeg_(w) QiSr(j) - .+() QiSr(j)), 
min/es-()(5'jes-() QiS r(j) - 5's+() Qjr(j)), 
if S+(co)  0 
if J+(co) = 0 
if J-(co) k 0 
if J-(co) = 0 
A Rigorous Analysis of Linsker-type Hebbian Learning 323 
C k, C E
