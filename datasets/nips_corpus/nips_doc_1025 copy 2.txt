How Perception Guides Production in 
Birdsong Learning 
Christopher L. Fry 
cfry@cogsci.ucsd.edu 
Department of Cognitive Science 
University of California at San Diego 
La Jolla, CA 92093-0515 
Abstract 
A .computational model of song learning in the song sparrow 
(Melospiza melodia) learns to categorize the different syllables of 
a song sparrow song and uses this categorization to train itself to 
reproduce song. The model fills a crucial gap in the computational 
explanation of birdsong learning by exploring the organization of 
perception in songbirds. It shows how competitive learning may 
lead to the organization of a specific nucleus in the bird brain, 
replicates the song production results of a previous model (Doya 
and Sejnowski, 1995), and demonstrates how perceptual learning 
can guide production through reinforcement learning. 
1 INTRODUCTION 
The passeriformes or songbirds make up more han half of all bird species and 
are divided into two groups: the oscines which learn their songs and sub-oscines 
which do not. Oscines raised in isolation sing degraded species typical songs similar 
to wild song. Deafened oscines sing completely degraded songs (Konishi, 1965) , 
while deafened sub-oscines develop normal songs (Kroodsma and Konishi, 1991) 
indicating that auditory feedback is crucial in oscine song learning. 
Innate structures in the bird brain regulate song learning. For example, song spar- 
rows show innate preferences for their own species' songs and song structure (Mar- 
ler, 1991). Innate preferences are thought to be encoded in an auditory template 
which limits the sounds young birds may copy. According to the auditory tem- 
plate hypothesis birds go through two phases during song learning, a memo- 
rization phase and a motor phase. In the memorization phase, which lasts 
from approximately 20 to 50 days after birth in the song sparrow, the bird selects 
which sounds to copy based on an innate template and refines the template based 
How Perception Guides Production in Birdsong Learning 111 
POSTERI OR ANTERIOR 
toll'aGhea&8)t'inx - -  � 
Figure 1: A simplified sketch of a saggital section of the songbird brain. Field L (Field 
L) receives auditory input and projects to the production pathway: HVc (formerly the 
caudal nucleus of the hyperstriat,um), RA (robust nucleus of archistriatum), nXIIts (hy- 
poglossal nerve), the syrinx (vocal organ) and the learning pathway: X (area X), DLM 
(roedial nucleus of the dorsolateral thalamus), LMAN (lateral magnocellular nucleus of 
the anterior neostriatum), RA (Konishi, 1989; Vicario, 1994). V is the lateral ventricle. 
on the sounds it hears. In the motor phase (from approximately 272 to 334 days 
after birth) the template provides feedback during singing. Learning to sing the 
memorized, template song is a gradual process of refining the produced song to 
match memory (Marler, 1991). 
A song is made up of phrases, phrases of syllables and syllables of notes. Syllables, 
usually separated by periods of silence, are the main units of analysis. Notes typ- 
ically last from 10-100 msecs and are used to construct syllables (100-200 msecs) 
which are reused to produce trills and other phrases. 
2 NEUROBIOLOGY OF SONG 
The two main neural pathways that govern song are the motor and learning path- 
ways seen in figure 1 (Konishi, 1989). Lesions to the motor pathway interrupt 
singing throughout life while lesions to the learning pathway disrupt early song 
learning. Although these pathways seem to have segregated functions, recordings 
of neurons during song playback have shown that cells throughout the song system 
respond to song (Konishi, 1989). 
Studies of song perception have shown the best auditory stimulus that will evoke a 
response in the song system is the bird's own song (Margoliash, 1986). The song 
specific neurons in HVc of the white-crowned sparrow often require a sequence of 
two syllables to respond (Margoliash, 1986; Margoliash and Fortune, 1992) and are 
made up of two main types in HVc. One type is sensitive to temporal combinations 
of stimuli while the other is sensitive to harmonic characteristics (Margoliash and 
Fortune, 1992). 
3 COMPUTATION 
Previous computational work on birdsong learning predicted individual neural re- 
sponses using back-propagation (Margoliash and Bankes, 1993) and modelled motor 
mappings for song production (Doya and Sejnowski, 1995). The current work de- 
112 C.L. FRY 
O0 --- 
1000 - 
 o- 
- 1000 - 
time 
� 011 8ec 
Kohonen Neuron 
Input Layer 
Figure 2: Perceptual network input encoding. The song is converted into frequency bins 
which are presented to the Kohonen layer over four time steps. 
velops a model of birdsong syllable perception which extends Doya and Sejnowski's 
(1995) model of birdsong learning. Birdsong syllable segmentation is accomplished 
using an unsupervised system and this system is used to train the network to re- 
produce its input using reinforcement learning. 
The model implements the two phases of the auditory template hypothesis, mem- 
orization and motor. In the first phase the template song is segmented into 
syllables by an unsupervised Kohonen network (Kohonen, 1984). In the second 
phase the syllables are reproduced using a reinforcement learning paradigm based 
on Doya and Sejnowski (1995). 
The model extends previous work in three ways: 1) a self-organizing network picks 
out syllables in the song; 2) the self-organizing network provides feedback during 
song production; and 3) a more biologically plausible model of the syrinx is used to 
generate song. 
3.1 Perception 
Recognizing a syllable involves identifying a short sequence of notes. Kohonen 
networks use an unsupervised learning method to categorize an input space based 
on similar neural responses. Thus a Kohonen network is a natural candidate for 
identifying the syllables in a song. 
One song from the repertoire of a song sparrow was chosen as the training song 
for the network. The song was encoded by passing a sliding window across the 
training waveform (sampled at 22.255 kHz) of the selected song. At each time step, 
a non-overlapping 256 point (m .011 sec) fast fourier transform (FFT) was used to 
generate a power spectrum (figure 2). The power spectrum was divided into 8 bins. 
Each bin was mapped to a real number using a gaussian summation procedure with 
the peak of the gaussian at the center of each frequency bin. Four time-steps were 
passed to each Kohonen neuron. 
The network's task was to identify similar syllables in the input song. The input 
song was broken down into syllables by looking for points where the power at all 
How Perception Guides Production in Birdsong Learning 113 
10- 
kHz 
l 
S 0.0 
n2 
n3 
n4-- 
nS 
n6 
n7 
n8 
Figure 3: Categorization of song syllables by a Kohonen network. The power-spectrum of 
the training song is at the top. The responses of the Kohonen neurons are at the bottom. 
For each time-step the winning neuron is shown with a vertical bar. The shaded areas 
indicate the neuron that fired the most during the presentation of the syllable. 
frequencies dropped below a threshold. A syllable was defined as sound of duration 
greater than .011 seconds bounded by two low-power points. The network was not 
trained on the noise between syllables. The song was played for the network ten 
times (1050 training vectors), long enough for a stable response pattern to emerge. 
The activation of a neuron was: Netj = Eziwij. Where: Netj = output of neuron 
j, wij = the weight connecting inputi to neurorj, xi ---- irputi. The Kohonen net- 
work was trained by initializing the connection weights to 1/v/number of reurors 
+ small random component (r _< .01), normalizing the inputs, and updating the 
weights to the winning neuron by the following rule: wn,w = wod + c(x - wod) 
where: c = trairirg rate = .20. If the same neuron won twice in a row the train- 
ing rate was decreased by 1/2. Only the winning neuron was reinforced resulting 
in a non-localized feature map. 
3.1.1 Perceptual Results 
The Kohonen network was able to assign a unique neuron to each type of syllable 
(figure 3). Of the eight neurons in the network, the one that fired the most frequently 
during the presentation of a syllable uniquely identified the type of syllable. The 
first four syllables of the input song sound alike, contain similar frequencies, and 
are coded by the first neuron (N1). The last three syllables sound alike, contain 
similar frequencies, and are coded by the fourth neuron (N4). Syllable five was 
coded by neuron six (N6), syllable six by neuron two (N2) and syllable seven by 
neuron eight (N8). 
Figure 4 shows the frequency sensitivity of each neuron (1-8, figure 3) plotted against 
each time step (1-4). This plot shows the harmonic and temporally sensitive neu- 
rons that developed during the learning phase of the Kohonen network. Neuron 2 
is sensitive to only one frequency at approximately 6-7 kHz, indicated by the solid 
white band across the 6-7 kHz frequency range in figure 4. Neuron 4 is sensitive 
to mid-range frequencies of short duration. Note that in figure 4 N4 responds 
114 C.L. FRY 
N1 N2 N3 N4 
0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 
N5 N6 N7 N8 
0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 
Time Step 
Figure 4: The values of the weights mapping frequency bins and time steps to Kohonen 
neurons. White is maximum, Black is minimum. 
maximally to mid-range frequencies only in the first two time steps. It uses this 
temporal sensitivity to distinguish between the last three syllables and the fifth syl- 
lable (figure 3) by keying off the length of time mid-range frequencies are present. 
Contrast this early response sensitivity with neuron 6, which is sensitive to mid- 
range frequencies of long duration, but responds only after one time step. It uses 
this temporal sensitivity to respond to the long sustained frequency of syllable four. 
Considered together, neurons 2
