206 
APPLICATIONS OF 
]RROR BACK-PROPAGATION 
TO PHONETIC CLASSIFICATION 
Hong C. Leung & Victor W. Zue 
Spoken Language Systems Group 
Laboratory for Computer Science 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
ABSTRACT 
This paper is concerced with the use of error back-propagation 
in phonetic classification. Our objective is to investigate the ba- 
sic characteristics of back-propagation, and study how the frame- 
work of multi-layer perceptrons can be exploited in phonetic recog- 
nition. We explore issues such as integration of heterogeneous 
sources of information, conditioq that can affect performance of 
phonetic classification, internal representations, comparisons with 
traditional pattern classification techniques, comparisons of differ- 
ent error metrics, and initialization of the network. Our investiga- 
tion is performed within a set of experiments that attempts to rec- 
ognize the 16 vowels in American English independent of speaker. 
Our results are comparable to human performance. 
Early approaches in phonetic recognition fall into two major extremes: heuristic 
and algorithmic. Both approaches have their own merits and shortcomings. The 
heuristic approach has the intuitive appeal that it focuses on the linguistic informa- 
tion in the speech signal and exploits acoustic-phonetic knowledge. However, the 
weak control strategy used for utilizing our knowledge has been grossly inadequate. 
At the other extreme, the algorithmic approach relies primarily on the powerful con- 
trol strategy offered by well-formulated pattern recognition techniques. However, 
relatively little is known about how our speech knowledge accumulated over the 
past few decades can be incorporated into the well-formulated algorithms. We feel 
that artificial neural networks (ANN) have some characteristics that can potentially 
enable them to bridge the gap between these two extremes. On the one hand, our 
speech knowledge can provide guidance to the structure and design of the network. 
On the other hand, the self-organizing mechanism of ANN can provide a control 
strategy for utilizing our knowledge. 
In this paper, we extend our earlier work on the use of artificial neural networks 
for phonetic recognition [2]. Specifically, we focus our investigation on the following 
sets of issues. First, we describe the use of the network to integrate heterogeneous 
sources of information. We will see how classification performance improves as more 
Error Back-Propagation to Phonetic Classification 207 
information is available. Second, we discuss several important factors that can sub- 
stantially affect the performance of phonetic classification. Third, we examine the 
internal representation of the network. Fourth, we compare the network with two 
traditional classification techniques: K-nearest neighbor and Gaussian classifica- 
tion. Finally, we discuss our specific implementations of back-propagation that 
yield improved performance and more efficient learning time. 
EXPERIMENTS 
Our investigation is performed within the context of a set of experiments that 
attempts to recognize the 16 vowels in American English independent of speaker. 
The vowels are excised from continuous speech and they can be preceded and fol- 
lowed by any phonemes, thus providing a rich environment to study contextual 
influence. We assume that the locations of the vowels have been detected. Given a 
time region, the network determines which one of the 16 vowels was spoken. 
CORPUS 
As Table 1 shows, our training set consists of 20,000 vowel tokens, excised from 
2,500 continuous sentences spoken by 500 male and female speakers. The test set 
consists of about 2,000 vowel tokenS, excised from 250 sentences spoken by 50 dif- 
ferent speakers. All the data are extracted from the TIMIT database, which has a 
wide range of American dialectical variations [1]. The speech signal is represented 
by spectral vectors obtained from an auditory model [4]. Speaker and energy nor- 
malization are also performed [5]. 
Tokens Sentences Speakers (M/F) 
Training 20,000 2500 500 (350/150) 
Testing 2,000 250 50 (33/17) 
Table 1: Corpus extracted from the TIMIT database. 
NETWORK STRUCTURE 
The structure of the network we have examined most extensively has i hidden 
layer as shown in Figure 1. It has 16 output units, with one unit for each of the 16 
vowels. In order to capture dynamic information, the vowel region is divided into 
three equal subregions. An average spectrum is then computed in each subregion. 
These 3 average spectra are then applied to the first 3 sets of input units. Additional 
sources of information, such as duration and local phonetic contexts, can also be 
made available to the network. While spectral and durational inputs are continuous 
and numerical, the contextual inputs are discrete and symbolic. 
208 Leung and Zue 
eft context 
Jright context 
output from auditory model 
(synchrony spectrogram) 
duration 
Figure 1: Basic structure of the network. 
HETEROGENEOUS INFORMATION INTEGRATION 
In our earlier study, we have examined the integration of the Synchrony En- 
velopes and the phonetic contexts [2]. The Synchrony Envelopes, an output of 
the auditory model, have been shown to enhance the formant information. In this 
study, we add additional sources of information. Figure 2 shows the performance 
as heterogeneous sources of information are made available to the network. The 
performance is about 60% when only the Synchrony Envelopes are available. The 
performance improves to 64% when the Mean Rate Response, a different output of 
the auditory model which has been shown to enhance the temporal aspects of the 
speech signal, is also available. We can also see that the performance improves con- 
sistently to 77% as durational and contextual inputs are provided to the network. 
This experiment suggests that the network is able to make use of heterogeneous 
sources of information, which can be numerical and/or symbolic. 
Error Back-Propagation to Phonetic Classification 209 
One may ask how well human listeners can recognize the vowels. Experiments 
have been performed to study how well human listeners agree with each other when 
they can only listen to sequences of 3 phonemes, i.e. the phoneme before the vowel, 
the vowel itself, and the phoneme after the vowel [3]. Results indicate that the 
average agreement among the listeners on the identities of the vowels is between 
65% and 70%. 
80' 
70' 
5O 
Synchrony Add Add Add 
Envelopes Mean Rate Duration Phonetic 
Response Context 
Sources of Information 
Figure 2: Integration of heterogeneous sources of information. 
PERFORMANCE RESULTS 
We have seen that one of the important factors for the network performance 
is the amount of information available to the network. To gain additional insights 
about how the network performs under different conditions, several experiments 
were conducted using different databases. In these and the subsequent experiments 
we describe in this paper, only the Synchrony Envelopes are available to the network. 
Table 2 shows the performance results for several recognition tasks. In each 
of these tasks, the network is trained and tested with independent sets of speech 
data. The first task recognizes vowels spoken by one speaker and excised from the 
/b/-vowel-/t/environment, spoken in isolation. This recognition task is relatively 
straightforward, resulting in perfect performance. In the second experiment, vowel 
tokens are extracted from the same phonetic context, but spoken by 17 male and 
female speakers. Due to inter-speaker variability, the accuracy degrades to 86%. 
The third task recognizes vowels spoken by one speaker and excised from an un- 
restricted context, spoken continuously. We can see that the accuracy decreases 
further to 70%. Finally, data from the TIMIT database are used, spoken by multi- 
ple speakers. The accuracy drops to 60%. These results indicate that a substantial 
difference in performance can be expected under different conditions, depending on 
whether the task is speaker-independent, what is the restriction on the phonetic 
210 Leung and Zue 
Speakers(M/F) Context Training Percent Remark 
Tokens Correct 
1(1/0) b t 64 100 isolated 
17(8/9) b __ t 256 86 isolated 
1 (1/0) * __ * 3,000 70 continuous 
500(350/150) * __ * 20,000 60 continuous 
Table 2: Performance for different tasks, using only the synchrony spectral infor- 
mation. * stands for any phonetic contexts. 
contexts, whether the speech material is spoken continuously, and how much data 
are used to train the network. 
INTERNAL REPRESENTATION 
To understand how the network makes use of the input information, we exam- 
ined the connection weights of the network. A vector is formed by extracting the 
connections from all the hidden units to one output unit as shown in Figure 3a. The 
same process is repeated for all output units to obtain a total of 16 vectors. The 
correlations among these vectors are then examined by measuring the inner prod- 
ucts or the angles between them. Figure 3b shows the distribution of the angles 
after the network is trained, as a function of the number of hidden units. The circles 
represent the mean of the distribution and the vertical bars stand for one standard 
deviation away from the mean. As the number of hidden units increases, the distri- 
bution becomes more and more concentrated and the vectors become increasingly 
orthogonal to each other. 
The correlations of the connection weights before training were also examined, 
as shown in Figure 3c. Comparing parts (b) and (c) of Figure 3, we can see that 
the distributions before and after training overlap more and more as the number of 
hidden units increases. With 128 hidden units, the two distributions are actually 
quite similar. This leads us to suspect that perhaps the connection weights between 
the hidden and the output layer need not be trained if we have a suffi
