Learning in large linear percepttons and 
why the thermodynamic limit is relevant 
to the real world 
Peter Sollich 
Department of Physics, University of Edinburgh 
Edinburgh EH9 3JZ, U.K. 
P. Solliched. ac .uk 
Abstract 
We present a new method for obtaining the response function 6 
and its average G from which most of the properties of learning 
and generalization in linear perceptrons can be derived. We first 
rederive the known results for the 'thermodynamic limit' of infinite 
perceptron size N and show explicitly that 6 is self-averaging in 
this limit. We then discuss extensions of our method to more gen- 
eral learning scenarios with anisotropic teacher space priors, input 
distributions, and weight decay terms. Finally, we use our method 
to calculate the finite N corrections of order 1IN to G and discuss 
the corresponding finite size effects on generalization and learning 
dynamics. An important spin-off is the observation that results 
obtained in the thermodynamic limit are often directly relevant to 
systems of fairly modest, 'real-world' sizes. 
1 INTRODUCTION 
One of the main areas of research within the Neural Networks community is the issue 
of learning and generalization. Starting from a set of training examples (normally 
assumed to be input-output pairs) generated by some unknown 'teacher' rule l], one 
wants to find, using a suitable learning or training algorithm, a student A/' (read 
'Neural Network') which generalizes from the training set, i.e., predicts the outputs 
corresponding to inputs not contained in the training set as accurately as possible. 
208 Peter Sollich 
If the inputs are N-dimensional vectors x C 7 N and the outputs are scalars y C 7, 
then one of the simplest functional forms that can be assumed for the student iV' is 
the linear perceptron, which is parametrized in terms of a weight vector wv  7 N 
and implements the linear input-output mapping 
wvx. (1) 
A commonly used learning algorithm for the linear perceptron is gradient descent 
on the training error, i.e., the error that the student  makes on the training set. 
Using the standard squared output deviation error meure, the training error for a 
given set ofp training examples { (x , y), y = 1... p} is Et =   (y - y(x ))a = 
Z ,(y _ wx,/)a. To prevent the student from fitting noise in the training 
1 2 
data, a quadratic weight decay term Aw is normally added to the training error, 
with the value of the weight decay parameter A determining how strongly large 
weight vectors are penalized. Gradient descent is thus performed on the function 
1 2 
E = Et + Aw, and the corresponding learning dynamics is, in a continuous time 
approximation, dw/dt = -VE. As discussed in detail by Krogh and Hertz 
(1992), this results in an exponential approach of w to its ymptotic value, with 
decay constants given by the eigenvalues of the matrix M, defined by (1 denotes 
the N x N identity matrix) 
M=AI+A, A=E.x(x) T' 
To examine what generalization performance is achieved by the above learning 
algorithm, one h to make an sumption about the functional form of the teacher. 
The simplest such sumption is that the problem is learnable, i.e., that the teacher, 
like the student, is a linear perceptron. A teacher  is then specified by a weight 
x ouou = 
that the test inputs for which the student is ked to predict the corresponding 
outputs are drawn from an isotropic Gaussian distribution P(x)  exp(-  2 
, x ). The 
generalization error, i.e., the average error that a student  makes on a random 
input when compared to teacher , is given by 
Inserting the learning dynamics w: w(t), the generalization acquires a time 
dependence, which in its exact form depends on the specific training set, teacher, and 
initial value of the student weight vector, w(t = 0). We shall confine our attention 
to the average of this time-dependent generalization error over all possible training 
sets and teachers; to avoid clutter, we write this average simply as eg(t). We sume 
that the inputs x  in the training set are chosen independently and randomly from 
the same distribution as the test inputs, and that the corresponding training outputs 
are the teacher outputs corrupted by additive noise, y = yv(x ) + , where the  
have zero mean and variance . If we further sume an isotropic Gaussian prior on 
1 
the teacher weight vectors, P(wv)  exp(-w), then the average generalization 
error for t   is (Krogh and Hertz, 1992) 
�g(t ) =  a+A(a --A) , (3) 
where G is the average of the scalled response function over the training inputs: 
G = (6)p({,), 6 = trM . (4) 
Learning in Large Linear Perceptrons 209 
The time dependence of the average generalization error for finite but large t is an 
exponential approach to the asymptotic value (3) with decay constant  + amin, 
where amin is the lowest eigenvalue occurring in the average eigenvalue spectrum of 
the input correlation matrix A (Krogh and Hertz, 1992). This average eigenvalue 
spectrum, which we denote by p(a), can be calculated from the average response 
function according to (Krogh, 1992) 
p(a) = i lim ImGIx=-,-i, (5) 
71' e-- O+ 
where we have assumed p(a) to be normalized, fda p(a)= 1. 
Eqs. (3,5) show that the key quantity determining learning and generalization in 
the linear perceptton is the average response function G defined in (4). This 
function has previously been calculated in the 'thermodynamic limit', N -- o<> 
at a = pin = const., using a diagrammatic expansion (Hertz et al., 1989) and the 
replica method (Opper, 1989, Kinzel and Opper, 1991). In Section 2, we present 
what we believe to be a much simpler method for calculating G, based only on 
simple matrix identities. We also show explicitly that 6 is self-averaging in the 
thermodynamic limit, which means that the fluctuations of 6 around its average G 
become vanishingly small as N - cx>. This implies, for example, that the gener- 
alization error is also self-averaging. In Section 3 we extend the method to more 
general cases such as anisotropic teacher space priors and input distributions, and 
general quadratic penalty terms. Finite size effects are considered in Section 4, 
where we calculate the O(1/N) corrections to G, g(t - o<>) and p(a). We discuss 
the resulting effects on generalization and learning dynamics and derive explicit con- 
ditions on the perceptton size N for results obtained in the thermodynamic limit 
to be valid. We conclude in Section 5 with a brief summary and discussion of our 
results. 
2 THE BASIC METHOD 
Our method for calculating the average response function G is based on a recursion 
relation relating the values of the (unaveraged) response function 6 for p and p + 1 
training examples. Assume that we are given a set of p training examples with 
corresponding matrix M. By adding a new training example with input x, we 
obtain the matrix M+ M  T 
= + NXX . It is straightforward to show that the 
inverse of M+ can be expressed as 
(M)-i = Mr 1 - 
 Mrl xxTMrl 
1 + xTMlx ' 
(One way of proving this identity is to multiply both sides by M+ and exploit the 
1 T -1 
fact that M+M} 1 = I + Nxx M,r .) Taking the trace, we obtain the following 
recursion relation for 6: 
6(p+ 1) = 6(p)- 1 xTM2x (6) 
i ,,rT llr- 1 
Ni+N 
i T 
Now denote zi = Nx lvt x (i = 1,2). With x drawn randomly from the assumed 
x2), the zi can readily be shown to be random 
input distribution P(x) cr exp(- 
210 Peter $ollich 
variables with means and (co-)variances 
(zi) = trM i (AziAzj) = 2 Mri-j 
, N,tr � 
Combining this with the fact that tr M k < NA -k = O(N), we have that the 
fluctuations Azi of the zi around their average values are O(1/v/-); inserting this 
i tr M. 2 q- O(N_a/2) 
= 6(p)- Nl+trM  
i o6(p) 
into (6), we obtain 
6(p+ 1) 
(7) 
Starting from 6(0) = l/A, we can apply this recursion p - aN times to obtain 
6(p) up to terms which add up to at most O(pN -a/a) = O(1/V). This shows 
that 6 is self-averaging in the thermodynamic limit: whatever the training set, the 
value of 6 will always be the same up to fluctuations of O(1/v). In fact, we shall 
show in Section 4 that the fluctuations of 6 are only O(1/N). This means that the 
O(N -3/) fluctuations from each iteration of (7) are only weakly correlated, so that 
they add up like independent random variables to give a total fluctuation for 6(p) 
of O((p/Na) /2) = O(1/N). 
We have seen that, in the thermodynamic limit, 6 is identical to its average G 
because its fluctuations are vanishingly small. To calculate the value of G in the 
thermodynamic limit as a function of a and A, we insert the relation 6(p+1)-6(p) = 
06(ot)/0o +O(1/N ) into eq. (7) (with 6 replaced by G) and neglect all finite N 
corrections. This yields the partial differential equation 
OG OG 1 
0a 0A i+G =0' (8) 
which can readily be solved using the method of characteristic curves (see, e.g., 
John, 1978). Using the initial condition G[,=0 = 1/, gives a/(1 + G) = 1/G- A, 
which leads to the well-known result (see, e.g., Hertz et al., 1989) 
1( 
G= 2A 1-ct-A+V/(1-a-A) a+4A . (9) 
In the complex A plane, G has a pole at A = 0 and a branch cut arising from 
the root; according to eq. (5), these singularities determine the average eigenvalue 
spectrum p(a) of A, with the result (Krogh, 1992) 
1 
p(a) = (1 - a)O(1 - a)5(a) + a X/(a+ - a)(a - a_), (10) 
where O(z) is the Heaviside step function, O(z) = 1 for z > 0 and 0 otherwise. 
The root in eq. (10) only contributes when its argument is non-negative, i.e., for a 
between the 'spectral limits' a_ and a+, which have the values a+ -- (1 q- v/-) a. 
3 
EXTENSIONS TO MORE GENERAL LEARNING 
SCENARIOS 
We now discuss some extensions of our method to more general learning sce- 
narios. First, consider the case of an anisotropic teacher space prior, P(wv) cr 
Learning in Large Linear Perceptrons 2 1 1 
1 T Wv) , with symmetric positive definite Sl. v.
