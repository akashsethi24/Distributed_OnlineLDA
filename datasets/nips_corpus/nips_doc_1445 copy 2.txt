Unsupervised Classification with 
Non-Gaussian Mixture Models using ICA 
Te-Won Lee, Michael S. Lewicki and Terrence Sejnowski 
Howard Hughes Medical Institute 
Computational Neurobiology Laboratory 
The Salk Institute 
10010 N. Torrey Pines Road 
La Jolla, California 92037, USA 
{tewon, lewicki, terry}�salk. edu 
Abstract 
We present an unsupervised classification algorithm based on an 
ICA mixture model. The ICA mixture model assumes that the 
observed data can be categorized into several mutually exclusive 
data classes in which the components in each class are generated 
by a linear mixture of independent sources. The algorithm finds 
the independent sources, the mixing matrix for each class and also 
computes the class membership probability for each data point. 
This approach extends the Gaussian mixture model so that the 
classes can have non-Gaussian structure. We demonstrate that 
this method can learn efficient codes to represent images of natural 
scenes and text. The learned classes of basis functions yield a better 
approximation of the underlying distributions of the data, and thus 
can provide greater coding efficiency. We believe that this method 
is well suited to modeling structure in high-dimensional data and 
has many potential applications. 
I Introduction 
Recently, Blind Source Separation (BSS) by Independent Component Analysis 
(ICA) has shown promise in signal processing applications including speech en- 
hancement systems, telecommunications and medical signal processing. ICA is a 
technique for finding a linear non-orthogonal coordinate system in multivariate data. 
The directions of the axes of this coordinate system are determined by the data's 
second- and higher-order statistics. The goal of the ICA is to linearly transform the 
data such that the transformed variables are as statistically independent from each 
Unsupervised Classification with Non-Gaussian Mixture Models Using ICA 509 
other as possible (Bell and Sejnowski, 1995; Cardoso and Laheld, 1996; Lee et al., 
1999a). ICA generalizes the technique of Principal Component Analysis (PCA) 
and, like PCA, has proven a useful tool for finding structure in data. 
One limitation of ICA is the assumption that the sources are independent. Here, 
we present an approach for relaxing this assumption using mixture models. In a 
mixture model (Duda and Hart, 1973), the observed data can be categorized into 
several mutually exclusive classes. When the class variables are modeled as mul- 
tivariate Gaussian densities, it is called a Gaussian mixture model. We generalize 
the Gaussian mixture model by modeling each class with independent variables 
(ICA mixture model). This allows modeling of classes with non-Gaussian (e.g., 
platykurtic or leptokurtic) structure. An algorithm for learning the parameters is 
derived using the expectation maximization (EM) algorithm. In Lee et al. (1999c), 
we demonstrated that this approach showed improved performance in data clas- 
siftcation problems. Here, we apply the algorithm to learning efficient codes for 
representing different types of images. 
2 The ICA Mixture Model 
We assume that the data were generated by a mixture density (Duda and Hart, 
1973): 
K 
P(XIO)-- -'P(xICk,Ok)p(Ck), (1) 
where 6) = (01,... , OK) are the unknown parameters for each p(x[C, 0k), called 
the component densities. We further assume that the number of classes, K, and 
the a priori probability, p(Ck), for each class are known. In the case of a Gaussian 
mixture model, p(xlCk,Ok) or N(tk, Zk). Here we assume that the form of the 
component densities is non-Gaussian and the data within each class are described 
by an ICA model. 
xk = Aks + bk, (2) 
where Ak is a N x M scalar matrix (called the basis or mixing matrix) and bk 
is the bias vector for class k. The vector sk is called the source vector (these 
are also the coefficients for each basis vector). It is assumed that the individual 
sources si within each class are mutually independent across a data ensemble. For 
simplicity, we consider the case where Ak is full rank, i.e. the number of sources 
(M) is equal to the number of mixtures (N). Figure I shows a simple example of 
a dataset that can be described by ICA mixture model. Each class was generated 
from eq.2 using a different A and b. Class (o) was generated by two uniform 
distributed sources, whereas class (+) was generated by two Laplacian distributed 
sources (p(s) c exp(-Isl) ). The task is to model the unlabeled data points and 
to determine the parameters for each class, Ak,bk and the probability of each 
class p(Ck Ix, 01:K) for each data point. A learning algorithm can be derived by an 
expectation maximization approach (Ghahramani, 1994) and implemented in the 
following steps: 
� Compute the log-likelihood of the data for each class: 
logp(x[Ck,Ok) = logp(sk) - log(det 
where 0k = {Ate, bk, sk}. 
� Compute the probability for each class given the data vector x 
p(xl0k, Ck )p(Ck ) 
p(Cklx, O:K) = Y'k p(xlOk,Ck)p(Ck)' 
(3) 
(4) 
510 T.-W. Lee, M. S. Lewicki and T. d. Sejnowsla' 
-5 
Figure 1: A simple example for classifying an ICA mixture model. There are 
two classes (+) and (o); each class was generated by two independent variables, 
two bias terms and two basis vectors. Class (o) was generated by two uniform 
distributed sources as indicated next to the data class. Class (-t-) was generated by 
two Laplacian distributed sources with a sharp peak at the bias and heavy tails. 
The inset graphs show the distributions of the source variables, si,k, for each basis 
vector. 
� Adapt the basis functions A and the bias terms b for each class. The basis 
functions are adapted using gradient ascent 
AAk 
o 
oc OA 1�gp(x16h:K) 
0 
= p(C&lx, logp(xlC&,&). 
(5) 
Note that this simply weights any standard ICA algorithm gradient by 
p(Ct Ix, 01:K). The gradient can also be summed over multiple data points. 
The bias term is updated according to 
bk __ Yt xtp(Cklxt, Ol:K) 
5-t P(CkIxt,O1:K) ' (6) 
where t is the data index (t - 1,..., T). 
The three steps in the learning algorithm perform gradient ascent on the total 
likelihood of the data in eq. 1. 
The extended infomax ICA learning rule is able to blindly separate mixed sources 
with sub- and super-Gaussian distributions. This is achieved by using a simple 
type of learning rule first derived by Girolami (1998). The learning rule in Lee 
et al. (1999b) uses the stability analysis of Cardoso and Laheld (1996) to switch 
between sub- and super-Gaussian regimes. The learning rule expressed in terms of 
W -- A -1, called the filter matrix is: 
AW oc [I -- K tanh(u)u T - uu T] W, (7) 
Unsupervised Classification with Non-Gaussian Mixture Models Using ICA 511 
logp(s) cr- Z log cosh s. - -- 
where ki are elements of the N-dimensional diagonal matrix K and u = Wx. The 
unmixed sources u are the source estimate s (Bell and Sejnowski, 1995). The ki's 
are (Lee et al., 1999b) 
ki = sign (E[sech 2ui]E[u] - E[ui tanh ui]). (8) 
The source distribution is super-Gaussian when ki - 1 and sub-Gaussian when ki - 
-1. For the log-likelihood estimation in eq.3 the term log p(s) can be approximated 
as follows 
Sn super-Gaussian 
2 
2 
logp(s) o+ Zlogcoshs, - s_ sub-Gaussian (9) 
2 
Super-Gaussian densities, are approximated by a density model with heavier tail 
than the Gaussian density; Sub-Gaussian densities are approximated by a bimodal 
density (Girolami, 1998). Although the source density approximation is crude it 
has been demonstrated that they are sufficient for standard ICA problems (Lee 
et al., 1999b). When learning sparse representations only, a Laplacian prior (p(s) c 
exp(-Isl)) can be used for the weight update which simplifies the infomax learning 
rule to 
AW cr [I-sign(u)u T] W, (10) 
logp(s) cr - Z Isnl Laplacian prior 
n 
3 Learning efficient codes for images 
Recently, several approaches have been proposed to learn image codes that utilize a 
set of linear basis functions. Olshausen and Field (1996) used a sparseness criterion 
and found codes that were similar to localized and oriented receptive fields. Similar 
results were presented by Bell and Sejnowski (1997) using the infomax algorithm 
and by Lewicki and Olshausen (1998) using a Bayesian approach. By applying the 
ICA mixture model we present results which show a higher degree of flexibility in 
encoding the images. We used images of natural scenes obtained from Olshausen 
and Field (1996) and text images of scanned newspaper articles. The training 
set consisted of 12 by 12 pixel patches selected randomly from both image types. 
Figure 2 illustrates examples of those image patches. Two complete basis vectors 
A and A2 were randomly initialized. Then, for each gradient in eq.5 a stepsize 
was computed as a function of the amplitude of the basis vectors and the number 
of iterations. The algorithm converged after 100,000 iterations and learned two 
classes of basis functions as shown in figure 3. Figure 3 (top) shows basis functions 
corresponding to natural images. The basis functions show Gaborl-like structure 
as previously reported in (Olshausen and Field, 1996; Bell and Sejnowski, 1997; 
Lewicki and Olshausen, 1998). However, figure 3 (bottom) shows basis functions 
corresponding to text images. These basis functions resemble bars with different 
lengths and widths that capture the high-frequency structure present in the text 
images. 
3.1 Comparing coding efficiency 
We have compared the coding efficiency between the ICA mixture model and similar 
models using Shannon's theorem to obtain a lower bound on the number of bits 
Gaussian modulated siusoidal 
512 T.-W. Lee, M. S. Lewicki and T. J.. Sejnowski 
Figure 2: Example of natural scene and text image. The 12 by 12 pixel image 
patches were randomly sampled from the images and used as inputs to the ICA 
mixture model. 
required to encode the pattern. 
bits :> - log 2 P(x[A) - Nlog2(a), (11) 
where N is the dimensionality of the
