715 
A COMPUTER SIMULATION OF CEREBRAL NEOCORTEX: 
COMPUTATIONAL CAPABILITIES OF NONLINEAR NEURAL NETWORKS 
Alexander Singer* and John P. Donoghue** 
*Department of Biophysics, Johns Hopkins University, 
Baltimore, MD 21218 (to whom all correspondence should 
be addressed) 
**Center for Neural Science, Brown University, 
Providence, RI 02912 
@ American Institute of Physics 1988 
716 
ABSTRACT
A synthetic neural network simulation of cerebral neocortex was 
developed based on detailed anatomy and ph. ysiology. Processing elements 
possess temporal nonlinearities and connecuon patterns similar to those of 
cortical neurons. The network was able to replicate spatial and temporal 
integration properties found experimentally in neocortex. A certain level of 
randomness was found to be crucial for the robustness of at least some of 
the network's computational capabilities. Emphasis was placed on how 
synthetic simulations can be of use to the study of both artificial and 
biological neural networks. 
A variety of fields have benefited from the use of computer simulations. This is 
true in spite of the fact that general theories and conceptual models are lacking in many 
fields and conlzasts with the use of simulations to explore existing theoretical structures that 
are extremely complex (cf. MacGregor and Lewis, 1977). When theoretical 
superstructures are missing, simulations can be used to synthesize empirical findings into a 
system which can then be studied analytically in and of itself. The vast compendium of 
neuroanatomical and neurophysiological data that has been collected and the concomitant 
absence of theories of brain function (Crick, 1979; Lewin, 1982) makes neuroscience an 
ideal candidate for the application of synthetic simulations. Furthermore, in keeping with 
the spirit of this meeting, neural network simulations which synthesize biological data can 
make contributions to the study of artificial neural systems as general information 
processing machines as well as to the study of the brain. A synthetic simulation of cerebral 
neocortex is presented here and is intended to be an example of how traffic might flow on 
the two-way street which this conference is trying to build between artificial neural network 
modelers and neuroscientists. 
The fact that cerebral neocortex is involved in some of the highest forms of 
information processing and the fact that a wide variety of neurophysiological and 
neuroanatomical data are amenable to simulation motivated the present development of a 
synthetic simulation of neocortex. The simulation itself is comparatively simple; 
nevertheless it is more realistic in terms of its structure and elemental processing units than 
most artificial neural networks. 
The neurons from which our simulation is constructed go beyond the simple 
sigmoid or hard-saturation nonlinearities of most artificial neural systems. For example, 
717 
because inputs to actual neurons are mediated by ion currents whose driving force depends 
on the membrane potential of the neuron, the amplitude of a cell's response to an input, i.e. 
the amplitude of the post-synaptic potential (PSP), depends not only on the strength of the 
synapse at which the input arrives, but also on the state of the neuron at the time of the 
input's arrival. This aspect of classical neuron electrophysiology has been implemented in 
our simulation (figure 1A), and leads to another important nonlinearity of neurons: 
namely, current shunting. Primarily effective as shunting inhibition, excitatory current can 
be shunted out an inhibitory synapse so that the sum of an inhibitory postsynaptic potential 
and an excitatory postsynaptic potential of equal amplitude does not result in mutual 
cancellation. Instead, interactions between the ion reversal potentials, conductance values, 
relative timing of inputs, and spatial locations of synapses determine the amplitude of the 
response in a nonlinear fashion (figure lB) (see Koch, Poggio, and Torre, 1983 for a 
quantitative analysis). These properties of actual neurons have been ignored by most 
artificial neural network designers, though detailed knowledge of them has existed for 
decades and in spite of the fact that they can be used to implement complex computations 
(e.g. Torre and Poggio, 1978; Houchin, 1975). 
The development of action potentials and spatial interactions within the model 
neurons have been simplified in our simulation. Action potentials involve preprogrmm'ned 
fluctuations in the membrane potential of our neurons and result in an absolute and a 
relative refractory period. Thus, during the time a cell is fh'ing a spike synaptic inputs are 
ignored, and immediately following an action potential the neuron is hyperpolarized. The 
modeling of spatial interactions is also limited since neurons are modeled primarily as 
spheres. Though the spheres can be deformed through control of a synaptic weight which 
modulates the amplitudes of ion conductances, detailed dendritic interactions are not 
simulated. Nonetheless, the fact that inhibition is generally closer to a cortical neuron's 
soma while excitation is more distal in a celrs dendritic tree is simulated through the use of 
sUronger inhibitory synapses and relatively weaker excitatory synapses. 
The relative strengths of synapses in a neural network define its connectivity. 
Though initial connectivity is random in many artificial networks, brains can be thought to 
contain a combination of randomness and fixed structure at distinct levels (Szentagothai, 
1978). From a macroscopic perspective, all of cerebral neocortex might be structured in a 
modular fashion analogous to the way the barrel field of mouse somatosensory cortex is 
structured (Woolsey and Van der Loos, 1970). Though speculative, arguments for the 
existence of some sort of anatomical modularity over the entire cortex are gaining ground 
718 
(Mountcastle, 1978; Szentagothai, 1979; Shepherd, in press). Thus, inspired by the 
ban'els of mice and by growing interest in functional units of 50 to 100 microns with on the 
order of 1000 neurons, our simulation is built up of five modules (60 cells each) with more 
dense local interconnections and fewer intermodular contacts. Furthermore, a wide variety 
of neuronal classification schemes have led us to subdivide the gross structure of each 
module so as to contain four classes of neurons: cortico-cortical pyramids, output 
pyramids, spiny stellate or local excitatory cells, and GABAergic or inhibirtory cells. 
At this level of analysis, the impressed structure allows for control over a variety of 
pathways. In our simulation each class of neurons within a module is connected to every 
other class and intermodular connections are provided along pathways from cortico-cortical 
pyramids to inhibitory cells, output pyramids, and cortico-cortical pyramids in immediately 
adjacent modules. A general sense of how strong a pathway is can be inferred from the 
product of the number of synapses a neuron receives from a particular class and the 
strength of each of those synapses. The broad architecture of the simulation is further 
structured to emphasize a three step path: Inputs to the network impact most su:ongly on 
the spiny stellate cells of the module receiving the input; these cells in turn project to 
cortico-cortical pyramidal cells more strongly than they do to other cell types; and finally, 
the pathway from the cortico-cortical pyramids to the output pyramidal cells of the same 
module is also particularly strong. This general architecture (figure 2) has received 
empirical support in many regions of cortex (Jones, 1986). 
In distinction to this synaptic architecture, a fine-grain connectivity is defined in our 
simulated network as well. At a more microscopic level, connectivity in the network is 
random. Thus, within the confines of the architecture described above, the determination 
of which neuron of a particular class is connected to which other cell in a target class is 
done at random. Two distinct levels of connectivity have, therefore, been established 
(figure 3). Together they provide a middle ground between the completely arbitrary 
connectivity of many artificial neural networks and the problem specific connectivities of 
other artificial systems. This distinction between gross synaptic architecture and fine-grain 
connectivity also has intuitive appeal for theories of brain development and, as we shall 
see, has non-trivial effects on the computational capabilities of the network as a whole. 
With defintions for input integration within the local processors, that is within the 
neurons, and with the establishment of connectivity patterns, the network is complete and 
ready to perform as a computational unit. In order to judge the simulation's capabilities in 
some rough way, a qualitative analysis of its response to an input will suffice. Figure 4 
719 
shows the response of the network to an input composed of a small burst of action 
potentials arriving at a single module. The data is displayed as a raster in which time is 
mapped along the abscissa and all the cells of the network are arranged by module and cell 
class along the ordinate. Each marker on the graph represents a single action potential f'n'ed 
by the appropriate neuron at the indicated time. Qualitatively, what is of importance is the 
fact that the network does not remain unresponsive, saturate with activity in all neurons, or 
oscillate in any way. Of course, that the network behave this way was predetermined by 
the combination of the properties of the neurons with a judicious selection of synaptic 
weights and path strengths. The properties of the neurons were fixed from physiological 
data, and once a synaptic architecture was found which produced the results in figure 4, 
that too was fixed. A more detailed analysis of the temporal firing pattern and of the 
distribution of activity over the differen
