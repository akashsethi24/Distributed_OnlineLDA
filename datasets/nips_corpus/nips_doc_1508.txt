Convergence Rates of Algorithms for 
Visual Search: Detecting Visual Contours 
A.L. Yuille 
Smith-Kettlewell Inst. 
San Francisco, CA 94115 
James M. Coughlan 
Smith-Kettlewell Inst. 
San Francisco, CA 94115 
Abstract 
This paper formulates the problem of visual search as Bayesian 
inference and defines a Bayesian ensemble of problem instances. 
In particular, we address the problem of the detection of visual 
contours in noise/clutter by optimizing a global criterion which 
combines local intensity and geometry information. We analyze 
the convergence rates of A* search algorithms using results from 
information theory to bound the probability of rare events within 
the Bayesian ensemble. This analysis determines characteristics of 
the domain, which we call order parameters, that determine the 
convergence rates. In particular, we present a specific admissible 
A* algorithm with pruning which converges, with high probability, 
with expected time O(N) in the size of the problem. In addi- 
tion, we briefly summarize extensions of this work which address 
fundamental limits of target contour detectability (i.e. algorithm 
independent results) and the use of non-admissible heuristics. 
I Introduction 
Many problems in vision, such as the detection of edges and object boundaries in 
noise/clutter, see figure (1), require the use of search algorithms. Though many 
algorithms have been proposed, see Yuille and Coughlan (1997) for a review, none 
of them are clearly optimal and it is difficult to judge their relative effectiveness. One 
approach has been to compare the results of algorithms on a representative dataset 
of images. This is clearly highly desirable though determining a representative 
dataset is often rather subjective. 
In this paper we are specifically interested in the convergence rates of A* algorithms 
(Pearl 1984). It can be shown (Yuille and Coughlan 1997) that many algorithms 
proposed to detect visual contours are special cases of A* We would like to 
understand what characteristics of the problem domain determine the convergence 
642 A. L. Yuille and J.. M. Coughlan 
Figure 1: The difficulty of detecting the target path in clutter depends, by our 
theory (Yuille and Coughlan 1998), on the order parameter K. The larger K the 
less computation required. Left, an easy detection task with K - 3.1. Middle, a 
hard detection task K - 1.6. Right, an impossible task with K - -0.7. 
rates. 
We formulate the problem of detecting object curves in images to be one of statistical 
estimation. This assumes statistical knowledge of the images and the curves, see 
section (2). Such statistical knowledge has often been used in computer vision for 
determining optimization criteria to be minimized. We want to go one step further 
and use this statistical knowledge to determine good search strategies by defining 
a Bayesian ensemble of problem instances. For this ensemble, we can prove certain 
curve and boundary detection algorithms, with high probability, achieve expected 
time convergence in time linear with the size of the problem. Our analysis helps 
determine important characteristics of the problem, which we call order parameters, 
which quantify the difficulty of the problem. 
The next section (2) of this paper describes the basic statistical assumptions we 
make about the domain and describes the mathematical tools used in the remaining 
sections. In section (3) we specify our search algorithm and establish convergence 
rates. We conclude by placing this work in a larger context and summarizing recent 
extensions. 
2 Statistical Background 
Our approach assumes that both the intensity properties and the geometrical shapes 
of the target path (i.e. the edge contour) can be determined statistically. This path 
can be considered to be a set of elementary path segments joined together. We first 
consider the intensity properties along the edge and then the geometric properties. 
The set of all possible paths can be represented by a tree structure, see figure (2). 
The image properties at segments lying on the path are assumed to differ, in a 
statistical sense, from those off the path. More precisely, we can design a filter 6(.) 
with output {Yx - b(I(x))} for a segment at point x so that: 
P(Yx) = Po(Yx), if x lies on the true path 
P(Yx) = Po//(Yx), if x lies off the true path. (1) 
For example, we can think of the {Yx} as being values of the edge strength at point 
x and Port, Poyy being the probability distributions of the response of 6(-) on and 
off an edge. The set of possible values of the random variable Yx is the alphabet 
with alphabet size M (i.e. yx can take any of M possible values). See (Geman and 
Jedynak 1996) for examples of distributions for Port, Poyy used in computer vision 
applications. 
We now consider the geometry of the target contour. We require the path to be 
made up of connected segments x, x2,..., xN. There will be a Markov probability 
distribution P9 (xi+ Ixi) which specifies prior probabilistic knowledge of the target. 
Convergence Rates of Algorithms for 14sual Search: Detecting 14sual Contours 643 
It is convenient, in terms of the graph search algorithms we will use, to consider that 
each point x has a set of Q neighbours. Following terminology from graph theory, 
we refer to Q as the branching factor. We will assume that the distribution Pg 
depends only on the relative positions of xi+l and xi. In other words, Pg(xi+ Ixi) - 
PA(xi+ -xi). An important special case is when the probability distribution 
is uniform for all branches (i.e. PA(Ax) -- U(Ax) -- l/Q, VAx). The joint 
distribution P(X, �) of the road geometry X and filter responses � determines the 
Bayesian Ensemble. 
By standard Bayesian analysis, the optimal path X* = (x,..., Xv) maximizes the 
sum of the log posterior: 
-xi) (2) 
E(X) = Elog Von(Y(x,)) + Elog U(xi+ -xi) ' 
i Poff(Y(x,)) i 
where the sum i is taken over all points on the target. U(xi+ - xi) is the uniform 
distribution and its presence merely changes the log posterior E(X) by a constant 
value. It is included to make the form of the intensity and geometric terms similar, 
which simplifies our later analysis. 
We will refer to E(X) as the reward of the path X which is the sum of the intensity 
rewards log lon(y()) and the geometric rewards log l-(+-) 
Poll (Y(i)) U(xi+ -xi ) 
It is important to emphasize that our results can be extended to higher-order 
Markov chain models (provided they are shift-invariant). We can, for example, 
define the x variable to represent spatial orientation and position of a small edge 
segment. This will allow our theory to apply to models, such as snakes, used in 
recent successful vision applications (Geman and Jedynak 1996). (It is straightfor- 
ward to transform the standard energy function formulation of snakes into a Markov 
chain by discretizing and replacing the derivatives by differences. The smoothness 
constraints, such as membranes and thin plate terms, will transform into first and 
second order Markov chain connections respectively). Recent work by Zhu (1998) 
shows that Markov chain models of this type can be learnt using Minimax Entropy 
Learning theory from a representative set of examples. Indeed Zhu goes further by 
demonstrating that other Gestalt grouping laws can be expressed in this framework 
and learnt from representative data. 
Most Bayesian vision theories have stopped at this point. The statistics of the prob- 
lem domain are used only to determine the optimization criterion to be minimized 
and are not exploited to analyze the complexity of algorithms for performing the op- 
timization. In this paper, we go a stage further. We use the statistics of the problem 
domain to define a Bayesian ensemble and hence to determine the effectiveness of 
algorithms for optimizing criteria such as (2). To do this requires the use of Sanov's 
theorem for calculating the probability of rare events (Cover and Thomas 1991). 
For the road tracking problem this can be re-expressed as the following theorem, 
derived in (Yuille and Coughlan 1998): 
Theorem 1. The probabilities that the spatially averaged log-likelihoods on, and 
off, the true curve are above, or below, threshold T are bounded above as follows: 
Pr( 1 (log Pon(Y(,)) }on ( T} ( (n q- 1)M2 -nD(Pv[IP�') (3) 
n i---1 Pofl(Y(z,)) - 
(4) 
644 A. L. Yuille and.].. M. Coughlan 
where the subscripts on and off mean that the data is generated by Pon, Poff, 
PT(Y) = on [Y)off /Z(T) where 0 _< A(T)  I is a scalar which depends 
on the threshold T and Z(T) is a noalization actor. The value o A(T) is deter- 
mined by the constraint y PT (Y) log Po  T. 
In the next section, we will use Theorem 1 to determine a criterion for pruning 
the search based on comparing the intensity reward to a threshold T (pruning will 
also be done using the geometric reward). The choice of T involves a trade-off. If 
T is large (i.e. close to D(PonllPo.f.f)) then we will rapidly reject false paths but 
we might also prune out the target (true) path. Conversely, if T is small (close 
to -D(Po.f.fl]Pon)) then it is unlikely we will prune out the target path but we 
may waste a lot of time exploring false paths. In this paper we choose T large and 
write the fall-off factors (i.e. the exponents in the bounds of equations (3,4)) as 
D(PTI[Pon) = e (T), D(PT[[PoL) = D(Pon[lPoII) - e2(T)where e (T),e2(T) are 
positive and (el(T),e2(T)) }-+ (0,0) as T }-+ D(PonllPo). We perform a similar 
analysis for the geometric rewards by substituting P/xa, U for Port, PofI. We choose 
a threshold  satisfying -D(SlIPxa) < < D(Pxal[S). The results of Theorem 
1 apply with the obvious substitutions. In particular, the alphabet factor becomes 
Q (the branching factor). Once again, in this paper, we choose P to be large and 
obtain fall-off factors D(P7, llP, x9) =  (P), D(P7, llU ) = D(P, xg[IU) - 2(). 
3 Tree Search: A*, heuristics, and block pruning 
We 
