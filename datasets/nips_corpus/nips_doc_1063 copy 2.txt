Stochastic Hillclimbing as a Baseline 
Method for Evaluating Genetic 
Algorithms 
Ari Juels 
Department of Computer Science 
University of California at Berkeley* 
Martin Wattenberg 
Department of Mathematics 
University of California at Berkeley t 
Abstract 
We investigate the effectiveness of stochastic hillclimbing as a baseline for 
evaluating the performance of genetic algorithms (GAs) as combinato- 
rial function optimizers. In particular, we address two problems to which 
GAs have been applied in the literature: Koza's 11-multiplexer problem 
and the jobshop problem. We demonstrate that simple stochastic hill- 
climbing methods are able to achieve results comparable or superior to 
those obtained by the GAs designed to address these two problems. We 
further illustrate, in the case of the jobshop problem, how insights ob- 
tained in the formulation of a stochastic hillclimbing algorithm can lead 
to improvements in the encoding used by a GA. 
1 Introduction 
Genetic algorithms (GAs) are a class of randomized optimization heuristics based 
loosely on the biological paradigm of natural selection. Among other proposed ap- 
plications, they have been widely advocated in recent years as a general method 
for obtaining approximate solutions to hard combinatorial optimization problems 
using a minimum of information about the mathematical structure of these prob- 
lems. By means of a general evolutionary strategy, GAs aim to maximize an 
objective or fitness function f: S -- R over a combinatorial space S, i.e., to find 
some state s 6 S for which f(s) is as large as possible. (The case in which f is to 
be minimized is clearly symmetrical.) For a detailed description of the algorithm 
see, for example, [7], which constitutes a standard text on the subject. 
In this paper, we investigate the effectiveness of the GA in comparison with that 
of stochastic hillclimbing (SH), a probabilistic variant of hillclimbing. As the term 
*Supported in part by NSF Grant CCR-9505448. E-mail: juels@cs.berkeley. edu 
rE-mail: wattenbe@math.berkeley. edu 
Stochastic Hillclimbing as a Baseline Method for Evaluating Genetic Algorithms 431 
hillclimbing suggests, if we view an optimization problem as a landscape in 
which each point corresponds to a solution s and the height of the point corre- 
sponds to the fitness of the solution, f(s), then hillclimbing aims to ascend to a 
peak by repeatedly moving to an adjacent state with a higher fitness. 
A number of researchers in the GA community have already addressed the issue 
of how various versions of hillclimbing on the space of bitstrings, {0, 1) , compare 
with GAs [1] [4] [9] [18] [15]. Our investigations in this paper differ in two important 
respects from these previous ones. First, we address more sophisticated problems 
than the majority of these studies, which make use of test functions developed for 
the purpose of exploring certain landscape characteristics. Second, we consider hill- 
climbing algorithms based on operators in some way natural to the combinatorial 
structures of the problems to which we are seeking solutions, very much as GA de- 
signers attempt to do. In one of the two problems in this paper, our SH algorithm 
employs an encoding exactly identical to that in the proposed GA. Consequently, 
the hillclimbing algorithms we consider operate on structures other than bitstrings. 
Constraints in space have required the omission of a great deal of material found 
in the full version of this paper. This material includes the treatment of two addi- 
tional problems: the NP-complete Maximum Cut Problem [11] and an NP-complete 
problem known as the multiprocessor document allocation problem (MDAP). Also 
in the full version of this paper is a substantially more thorough exposition of the 
material presented here. The reader is encouraged to refer to [10], available on the 
World Wide Web at http://www.cs.berkeley. edu/ojuels/. 
2 Stochastic Hillclimbing 
The SH algorithm employed in this paper searches a discrete space $ with the aim 
of finding a state whose fitness is as high (or as low) as possible. The algorithm 
does this by making successive improvements to some current state a C $. As is 
the case with genetic algorithms, the form of the states in S depends upon how the 
designer of the SH algorithm chooses to encode the solutions to the problems to be 
solved: as bitstrings, permutations, or in some other form. The local improvements 
effected by the SH algorithm are determined by the neighborhood structure and the 
fitness function f imposed on S in the design of the algorithm. We can consider the 
neighborhood structure as an undirected graph G on vertex set $. The algorithm 
attempts to improve its current state a by making a transition to one of the neigh- 
bors of a in G. In particular, the algorithm chooses a state - according to some 
suitable probability distribution on the neighbors of or. If the fitness of - is as least 
as good as that of a then - becomes the new current state, otherwise a is retained. 
This process is then repeated 
3 GP and Jobshop 
3.1 The Experiments 
In this section, we compare the performance of SH algorithms with thai of GAs 
proposed for two problems: the jobshop problem and Koza's 11-multiplexer prob- 
lem. We gauge the performance of the GA and SH algorithms according to the 
fitness of the best solution achieved after a fixed number of function evaluations, 
rather than the running time of the algorithms. This is because evaluation of the 
fitness function generally constitutes the most substantial portion of the execution 
time of the optimization algorithm, and accords with standard practice in the GA 
community. 
432 A. JUELS, M. WATFENBERG 
3.2 Genetic Programming 
Genetic programming (GP) is a method of enabling a genetic algorithm to search 
a potentially infinite space of computer programs, rather than a space of fixed- 
length solutions to a combinatorial optimization problem. These programs take the 
form of Lisp symbolic expressions, called S-expressions. The S-expressions in GP 
correspond to programs which a user seeks to adapt to perform some pre-specified 
task. Details on GP, an increasingly common GA application, and on the 11- 
multiplexer problem which we address in this section, may be found, for example, 
in [13] [12] [14]. 
The boolean l 1-multiplexer problem entails the generation of a program to per- 
form the following task. A set of 11 distinct inputs is provided, with la- 
bels a0, al, a., do, dl,..., d7, where a stands for address and d for data. Each 
input takes the value 0 or 1. The task is to output the value din, where m = 
ao + 2al + 4a.. In other words, for any 11-bit string, the input to the address 
variables is to be interpreted as an index to a specific data variable, which the 
program then yields as output. For example, on input al - 1,ao - a. = 0, 
and d. = 1, do = dl = ds = ... = d7 = 0, a correct program will output a '1', since 
the input to the 'a' variables specifies address 2, and variable d2 is given input 1. 
The GA Koza's GP involves the use of a GA to generate an S-expression corre- 
sponding to a correct l 1-multiplexer program. An S-expression comprises a tree of 
LISP operators and operands, operands being the set of data to be processed -- the 
leaves of the tree -- and operators being the functions applied to these data and 
internally in the tree. The nature of the operators and operands will depend on the 
problem at hand, since different problems will involve different sets of inputs and 
will require different functions to be applied to these inputs. For the l 1-multiplexer 
problem in particular, where the goal is to create a specific boolean function, the 
operands are the input bits a0, al, a., do, d,..., dz, and the operators are AND, 
OR, NOT, and IF. These operators behave as expected: the subtree (AND a a.), 
for instance, yields the value al A a.. The subtree (IF al d4 d3) yields the value d4 
if al = 0 and d3 if al - 1 (and thus can be regarded as a 3-multiplexer). NOT 
and OR work similarly. An S-expression constitutes a tree of such operators, with 
operands at the leaves. Given an assignment to the operands, this tree is evaluated 
from bottom to top in the obvious way, yielding a 0 or I output at the root. 
Koza makes use of a mating operation in his GA which swaps subexpressions 
between two such S-expressions. The subexpressions to be swapped are chosen 
uniformly at random from the set of all subexpressions in the tree. For details 
on selection in this GA, see [13]. The fitness of an S-expression is computed by 
evaluating it on all 2048 possible inputs, and counting the number of correct outputs. 
Koza does not employ a mutation operator in his GA. 
The SH Algorithm For this problem, the initial state in the SH algorithm is 
an S-expression consisting of a single operand chosen uniformly at random from 
{a0, a, a., do,..., dz}. A transition in the search space involves the random replace- 
ment of an arbitrary node in the S-expression. In particular, to select a neighboring 
state, we chose a node uniformly at random from the current tree and replace it 
with a node selected randomly from the set of all possible operands and operators. 
 the replacement node is drawn uniformly at random from the 
With probability  
set of operands {ao,a, a.,do,..., dz}, otherwise it is drawn uniformly at random 
from the set of operators, (AND, OR, NOT, IF}. In modifying the nodes of the 
S-expression in this way, we may change the number of inputs they require. By 
changing an AND node to a NOT node, for instance, we reduce the number of in- 
puts taken by the node from 2 to 1. In order to accommodate such changes, we do 
Stochastic Hillclimbing as a Baseline Method for Evaluating Genetic Algorithms 433 
the following. Where a replacement reduces the number of inputs taken by a node, 
we remove the required number
