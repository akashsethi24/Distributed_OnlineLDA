Optimization Principles for the Neural 
Code 
Michael DeWeese 
Sloan Center, Salk Institute 
La Jolla, CA 92037 
deweese@salk.edu 
Abstract 
Recent experiments show that the neural codes at work in a wide 
range of creatures share some common features. At first sight, these 
observations seem unrelated. However, we show that these features 
arise naturally in a linear filtered threshold crossing (LFTC) model 
when we set the threshold to maximize the transmitted information. 
This maximization process requires neural adaptation to not only 
the DC signal level, as in conventional light and dark adaptation, 
but also to the statistical structure of the signal and noise distribu- 
tions. We also present a new approach for calculating the mutual 
information between a neuron's output spike train and any aspect 
of its input signal which does not require reconstruction of the in- 
put signal. This formulation is valid provided the correlations in 
the spike train are small, and we provide a procedure for checking 
this assumption. This paper is based on joint work (DeWeese [1], 
1995). Preliminary results from the LFTC model appeared in a 
previous proceedings (DeWeese [2], 1995), and the conclusions we 
reached at that time have been reaffirmed by further analysis of the 
model. 
I Introduction 
Most sensory receptor cells produce analog voltages and currents which are smoothly 
related to analog signals in the outside world. Before being transmitted to the brain, 
however, these signals are encoded in sequences of identical pulses called action 
potentials or spikes. We would like to know if there is a universal principle at work 
in the choice of these coding strategies. The existence of such a potentially powerful 
theoretical tool in biology is an appealing notion, but it may not turn out to be 
useful. Perhaps the function of biological systems is best seen as a complicated 
compromise among constraints imposed by the properties of biological materials, 
the need to build the system according to a simple set of development rules, and 
282 M. DEWEESE 
the fact that current systems must arise from their ancestors by evolution through 
random change and selection. In this view, biology is history, and the search for 
principles (except for evolution itself) is likely to be futile. Obviously, we hope that 
this view is wrong, and that at least some of biology is understandable in terms of the 
same sort of universal principles that have emerged in the physics of the inanimate 
world. 
Adrian noticed in the 1920's that every peripheral neuron he checked produced dis- 
crete, identical pulses no matter what input he administered (Adrian, 1928). From 
the work of Hodgkin and Huxley we know that these pulses are stable non-linear 
waves which emerge from the non-linear dynamics describing the electrical proper- 
ties of the nerve cell membrane These dynamics in turn derive from the molecular 
dynamics of specific ion channels in the cell membrane. By analogy with other non- 
linear wave problems, we thus understand that these signals have propagated over a 
long distance -- e.g.  one meter from touch receptors in a finger to their targets 
in the spinal cord -- so that every spike has the same shape. This is an important 
observation since it implies that all information carried by a spike train is encoded 
in the arrival times of the spikes. Since a creature's brain is connected to all of its 
sensory systems by such axons, all the creature knows about the outside world must 
be encoded in spike arrival times. 
Until recently, neural codes have been studied primarily by measuring changes in the 
rate of spike production by different input signals. Recently it has become possible 
to characterize the codes in information-theoretic terms, and this has led to the 
discovery of some potentially universal features of the code (Bialek, 1996) (or see 
(Bialek, 1993) for a brief summary). They are: 
o 
Very high information rates. The record so far is 300 bits per second in a 
cricket mechanical sensor. 
High coding eOciency. In cricket and frog vibration sensors, the information 
rate is within a factor of 2 of the entropy per unit time of the spike train. 
Linear decoding. Despite evident non-linearities of the nervous system, spike 
trains can be decoded by simple linear filters. Thus we can write an estimate 
of the analog input signal s(t) as sest(t) = --i K(t-ti), with Ki chosen to 
minimize the mean-squared errors (X 2) in the estimate. Adding non-linear 
K9. (t - ti, t - t j) terms does not significantly reduce X 9'. 
Moderate signal-to-noise ratios (SNR). The SNR in these experiments was 
defined as the ratio of power spectra of the input signal to the noise referred 
back to the input; the power spectrum of the noise was approximated by X 9' 
defined above. All these examples of high information transmission rates 
have SNR of order unity over a broad bandwidth, rather than high SNR in 
a narrow band. 
We will try to tie all of these observations together by elevating the first to a principle: 
The neural code is chosen to maximize information transmission where information 
is quantified following Shannon. We apply this principle in the context of a simple 
model neuron which converts analog signals into spike trains. Before we consider 
a specific model, we will present a procedure for expanding the information rate of 
any point process encoding of an analog signal about the limit where the spikes are 
uncorrelated. We will briefly discuss how this can be used to measure information 
rates in real neurons. 
Optimization Principles for the Neural Code 2 83 
This work will also appear in Network. 
2 Information Theory 
In the 1940's, Shannon proposed a quantitative definition for information (Shan- 
non, 1949). He argued first that the average amount of information gained by 
observing some event x is the entropy of the distribution from which x is chosen, 
and then showed that this is the only definition consistent with several plausible 
requirements. This definition implies that the amount of information one signal can 
provide about some other signal is the difference between the entropy of the first 
signal's a priori distribution and the entropy of its conditional distribution. The 
average of this quantity is called the mutual (or transmitted) information. Thus, 
we can write the amount of information that the spike train, {ti), tells us about the 
time dependent signal, s(t), as 
I '- -/ti 
]P[(ti)]log9P[(ti)]-(-/Dti ),s()]) (1) 
. P[(ti}ls()] log. P[{ti , 
where f Dti is shorthand for integration over all arrival times (ti) from 0 to T 
and summation over the total number of spikes, N (we have divided the integration 
measure by N! to prevent over counting due to equivalent permutations of the spikes, 
rather than absorb this factor into the probability distribution as we did in (DeWeese 
[1], 1995)). <... >s = f VsP[sO]... denotes integration over the space of functions 
s(t) weighted by the signal's a priori distribution, P[(ti)lsO] is the probability 
distribution for the spike train when the signal is fixed and P[(/i)] is the spike 
train's average distribution. 
3 Arbitrary Point Process Encoding of an Analog Signal 
In order to derive a useful expression for the information given by Eq. (1), we need 
an explicit representation for the conditional distribution of the spike train. If we 
choose to represent each spike as a Dirac delta function, then the spike train can be 
defined as 
N 
p(t) -- - (2) 
i=1 
This is the output spike train for our cell, so it must be a functional of both the 
input signal, s(t), and all the noise sources in the cell which we will lump together 
and call r/(t). Choosing to represent the spikes as delta functions allows us to think 
of p(t) as the probability of finding a spike at time t when both the signal and noise 
are specified. In other words, if the noise were not present, p would be the cell's 
firing rate, singular though it is. This implies that in the presence of noise the cell's 
observed firing rate, r(t), is the noise average of p(/): 
r(t) = ] = 
Notice that by averaging over the conditional distribution for the noise rather than its 
a priori distribution as we did in (DeWeese [1], 1995), we ensure that this expression 
is still valid if the noise is signal dependent, as is the case in many real neurons. 
For any particular realization of the noise, the spike train is completely specified 
which means that the distribution for the spike train when both the signal and 
284 M. DEWEESE 
noise are fixed is a modulated Poisson process with a singular firing rate, p(t). We 
emphasize that this is true even though we have assumed nothing about the encoding 
of the signal in the spike train when the noise is not fixed. One might then assume 
that the conditional distribution for the spike train for fixed signal would be the 
noise average of the familiar formula for a modulated Poisson process: 
f; . 
i=1 / 
(4) 
However, this is only approximately true due to subtleties arising from the singular 
nature of p(t). One can derive the correct expression (DeWeese [1], 1995) by care- 
fully taking the continuum limit of an approximation to this distribution defined for 
discrete time. The result is the same sum of noise averages over products of p's 
produced by expanding the exponential in Eq. (4) in powers of f dtp(t) except that 
all terms containing more than one factor of p(t) at equal times are not present. 
The exact answer is: 
(5) 
where the superscripted minus sign reminds us to remove all terms containing 
products of coincident p's after expanding everything in the noise average in powers 
of p. 
4 Expanding About the Poisson Limit 
An exact solution for the mutual information between the input signal and spike 
train would be hopeless for all but a few coding schemes. However, the success 
of linear decoding coupled with the high informat
