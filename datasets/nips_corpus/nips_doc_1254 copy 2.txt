Statistically Efficient Estimation Using 
Cortical Lateral Connections 
Alexandre Pouget 
alex@salk.edu 
Kechen Zhang 
zhang@salk.edu 
Abstract 
Coarse codes are widely used throughout the brain to encode sen- 
sory and motor variables. Methods designed to interpret these 
codes, such as population vector analysis, are either inefficient, i.e., 
the variance of the estimate is much larger than the smallest possi- 
ble variance, or biologically implausible, like maximum likelihood. 
Moreover, these methods attempt to compute a scalar or vector 
estimate of the encoded variable. Neurons are faced with a simi- 
lar estimation problem. They must read out the responses of the 
presynaptic neurons, but, by contrast, they typically encode the 
variable with a further population code rather than as a scalar. 
We show how a non-linear recurrent network can be used to per- 
form these estimation in an optimal way while keeping the estimate 
in a coarse code format. This work suggests that lateral connec- 
tions in the cortex may be involved in cleaning up uncorrelated 
noise among neurons representing similar variables. 
I Introduction 
Most sensory and motor variables in the brain are encoded with coarse codes, i.e., 
through the activity of large populations of neurons with broad tuning to the vari- 
ables. For instance, direction of visual motion is believed to be encoded in visual 
area MT by the responses of a large number of cells with bell-shaped tuning, as 
illustrated in figure 1-A. 
Neurophysiological recordings have shown that, in response to an object moving 
along a particular direction, the pattern of activity across such a population would 
look like a noisy hill of activity (figure l-B). On the basis of this activity, _, the 
best that can be done is to recover the conditional probability of the direction of 
motion, 0, given the activity, p(01, ). A slightly less ambitious goal is to come up 
with a good guess, or estimate, , of the direction, 0, given the activity. Because 
of the stochastic nature of the noise, the estimator is a random variable, i.e, for 
�AP is at the Institute for Computational and Cognitive Sciences, Georgetown Univer- 
sity, Washington, DC 20007 and KZ is at The Salk Institute, La Jolla, CA 92037 . This 
work was funded by McDonnell-Pew and Howard Hughes Medical Institute. 
98 A. PougetandK. Zhang 
A 
B 
3 
2.5 
-' 2 
 1o$ 
1 
0.5 
0 
100 200 300 
Direction (deg) 
100 200 300 
Preferred Direction (deg) 
Figure 1: A- Tuning curves for 16 direction tuned neurons. B- Noisy pattern of 
activity (o) from 64 neurons when presented with a direction of 180 �. The ML 
estimate is found by moving an expected hill of activity (dotted line) until the 
squared distance with the data is minimized (solid line) 
the same image, t will vary from trial to trial. A good estimator should have 
the smallest possible variance across those trials because the variance determines 
how well two similar directions can be discriminated using this estimator. The 
Cram6r-Rao bound provides an analytical lower bound for this variance given the 
noise in the system and the unit tuning curves [5] Typically, computationally 
simple estimators, such as optimum linear estimator (OLE), are very inefficient; 
their variances are several times the bound. By contrast, Bayesian or maximum 
likelihood (ML) estimat0rs (which are equivalent for the case under consideration 
in this paper) can reach this bound but require more complex calculations [5]. 
These decoding technics are valuable for a neurophysiologist interested in reading 
out the population code but they are not directly relevant for understanding how 
neural circuits perform estimation. In particular, they all provide the estimate in a 
format which is incompatible with what we know of sensory representations in the 
cortex. For example, cells in V4 are estimating orientation from the noisy responses 
of orientation tuned V1 cells, but, unlike ML or OLE which provide a scalar esti- 
mate, V4 neurons retain orientation in a coarse code format, as demonstrated by 
the fact that V4 cells are just as broadly tuned to orientation as V1 neurons. 
Therefore, it seems that a theory of estimation in biological networks should have 
two critical characteristics: 1- it should preserve the estimate in a coarse code and 
2- it should be efficient, i.e., the variance should be close to the Cramdr-Rao bound. 
We explore in this paper various network architectures for performing estimations 
with coarse code using lateral connections. We start by briefly describing several 
classical estimators such as OLE or ML. Then, we consider linear and non-linear 
recurrent networks and compare their performances with the classical estimators. 
2 Classical Methods 
The simplest estimators are linear of the form tOLr = lgrT. Better performance 
can be obtained with a center of mass estimator (COM), tCOM = Y-i Oiai/Y-i ai; 
however, in the case of a periodic variable, such as direction of motion, the best 
one-shot method known is the complex estimator (COMP), tCOMP = phase(z) 
N 
where z = Y-=I aei�k [5]. This estimator consists in fitting a cosine through 
the pattern of activity, like the one shown in figure l-B, and using the phase of 
Statistically Efficient Estimations Using Cortical Lateral Connections 99 
A B 
40. 
20, 
1 oo 
Activity over Time 
5( 
0 100 200 300 
preferred Direction (,deg} 
Figure 2: A- Circular network of 64 units. Only the connections originating from 
one unit are shown. B- Activity over time in the non-linear network when initialized 
with a random pattern at t = 0. The activity of the units are plotted as a function 
of their position along the circle which is equivalent to their preferred direction of 
motion with appropriate choice of weights. 
the best cosine fit as the estimate of direction. This method is suboptimal if the 
data were not generated by cosine tuning functions as in the case illustrated in 
figure 1-A. It is possible to obtain optimum performance by fitting the curve that 
was actually used to generate the data, i.e, the actual tuning curves of the units. 
A maximum likelihood estimate, defined as being the direction maximizing p(ff]0), 
involves exactly this type of curve fitting, a process illustrated in figure 1-B [5]. The 
estimate is computed by finding first the expected hill- the hill that would be 
obtained in a noise free system- minimizing the distance with the data. In the case 
of gaussian noise, the appropriate distance measure to minimize is the euclidian 
squared distance. The final position of the peak of the hill corresponds to the 
maximum likelihood estimate, M�. 
3 Recurrent Networks 
Consider a circular network of 64 units fully connected like the one depicted in 
figure 2-A. With an appropriate choice of weights and activation function, this 
network will develop a hill-shaped pattern of activity in response to a transient 
input as illustrated in figure 2-B. If we initialize this networks with activity patterns 
- = {ai} corresponding to the responses of 64 direction tuned units (figure 1), we 
can use the final position of the hill across the neuronal array after relaxation as 
an estimate of the direction, 0. The variance of this estimator will depend on the 
exact choice of activation function and weights. 
3.1 Linear Network 
We first consider a network of 64 units whose dynamics is governed by the following 
difference equation: 
( ) 
o,(t + ,t) = o(t) + ,t + wjoj(t) 
The dynamics of such networks is well understood [3]. If each unit receives the 
same weight vector , then the weight matrix W is symmetric. In this case, the 
100 A. Pouget and K. Zhang 
network dynamics amplifies or suppresses the Fourier component of the initial input 
pattern, {ai}, independently by a factors equal to the corresponding component of 
the Fourier transform, 5, of . For example, if the first component of v is more 
than one (resp. less than one) the first Fourier component of the initial pattern of 
activity will be amplified (resp. suppressed). 
Thus, we can choose W such that the network amplifies selectively the first Fourier 
component of the data while suppressing the others. The network would be unstable 
but if we stop after a large, yet fixed, number of iterations, the activity pattern would 
look like a cosine function of direction with a phase corresponding to the phase of 
the first Fourier components of the data. In other words, the network would end 
up fitting a cosine function in the data which is equivalent to the COMP method 
described above. A network for orientation selectivity proposed by Ben-Yishai et 
al [1] is closely related to this linear network. 
Although this method keeps the estimate in a coarse code format, it suffers two 
problems: it is unclear how it could be extended to non periodic variables, such as 
disparity, and it is suboptimal since it is equivalent to the COMP estimator. 
3.2 Non-Linear Network 
We consider next a network of 64 units fully connected whose dynamics is governed 
by the following difference equations: 
oi(t) = g(ui(t))= 6.3 (log (1 + e 5+'�'(t)) )0.s (2) 
( 
.,(t + = .,(t) + -.,(t) + w,joj(t) (3) 
Zhng (1996) has demonstrated that with pproprite symmetric weights, 
this network develops  stable hill of ctivity in response to n rbitmry transient 
input pttern (h)(figure 2-B). The shape of the hill is fully specified by the weights 
nd ctiwtion function where,, by controt, the final position of the hill on the 
neuronl rray depends only on the initial input. Therefore, like ML, the network 
fits n expected function through the dt. We first present  set of simulations 
in which we investigated whether ML nd the network place the hill t the sme 
loction. 
Methods: The simulations consisted estimating the value of the direction of a 
moving bar based on the activity, _ = {ai}, of 64 input units with hill-shaped 
tuning to direc
