Variational Inference for Bayesian 
Mixtures of Factor Analysers 
Zoubin Ghahramani and Matthew J. Beal 
Gatsby Computational Neuroscience Unit 
University College London 
17 Queen Square, London WCIN 3AR, England 
{zoubin,m. beale�gat sby. ucl. ac. uk 
Abstract 
We present an algorithm that infers the model structure of a mix- 
ture of factor analysers using an efficient and deterministic varia- 
tional approximation to full Bayesian integration over model pa- 
rameters. This procedure can automatically determine the opti- 
mal number of components and the local dimensionality of each 
component (i.e. the number of factors in each factor analyser). 
Alternatively it can be used to infer posterior distributions over 
number of components and dimensionalities. Since all parameters 
are integrated out the method is not prone to overfitting. Using a 
stochastic procedure for adding components it is possible to per- 
form the variational optimisation incrementally and to avoid local 
maxima. Results show that the method works very well in practice 
and correctly infers the number and dimensionality of nontrivial 
synthetic examples. 
By importance sampling from the variational approximation we 
show how to obtain unbiased estimates of the true evidence, the 
exact predictive density, and the KL divergence between the varia- 
tional posterior and the true posterior, not only in this model but 
for variational approximations in general. 
1 Introduction 
Factor analysis (FA) is a method for modelling correlations in multidimensional 
data. The model assumes that each p-dimensional data vector y was generated by 
first linearly transforming a k < p dimensional vector of unobserved independent 
zero-mean unit-variance Gaussian sources, x, and then adding a p-dimensional zero- 
mean Gaussian noise vector, n, with diagonal covariance matrix : i.e. y - Ax + n. 
Integrating out x and n, the marginal density of y is Gaussian with zero mean 
and covariance AA r + . The matrix A is known as the factor loading matrix. 
Given data with a sample covariance matrix E, factor analysis finds the A and � 
that optimally fit E in the maximum likelihood sense. Since k < p, a single factor 
analyser can be seen as a reduced parametrisation of a full-covariance Gaussian.  
Factor analysis and its relationship to principal components analysis (PCA) and mix- 
ture models is reviewed in [10]. 
450 Z. Ghahramani and M. J. Beal 
A mixture of factor analysers (MFA) models the density for y as a weighted average 
of factor analyser densities 
s 
P(ylA, = P(sl)P(yls, A s, (1) 
s----1 
where r is the vector of mixing proportions, s is a discrete indicator variable, and 
A s is the factor loading matrix for factor analyser s which includes a mean vector 
for y. 
By exploiting the factor analysis parameterisation of covariance matrices, a mix- 
ture of factor analysers can be used to fit a mixture of Gaussians to correlated high 
dimensional data without requiring O(p 2) parameters or undesirable compromises 
such as axis-aligned covariance matrices. In an MFA each Gaussian cluster has in- 
trinsic dimensionality k (or ks if the dimensions are allowed to vary across clusters). 
Consequently, the mixture of factor analysers simultaneously addresses the prob- 
lems of clustering and local dimensionality reduction. When � is a multiple of the 
identity the model becomes a mixture of probabilistic PCAs. Tractable maximum 
likelihood procedure for fitting MFA and MPCA models can be derived from the 
Expectation Maximisation algorithm [4, 11]. 
The maximum likelihood (ML) approach to MFA can easily get caught in local 
maxima. 2 Ueda et al. [12] provide an effective deterministic procedure for avoiding 
local maxima by considering splitting a factor analyser in one part of space and 
merging two in a another part. But splits and merges have to be considered simul- 
taneously because the number of factor analysers has to stay the same since adding 
a factor analyser is always expected to increase the training likelihood. 
A fundamental problem with maximum likelihood approaches is that they fail to 
take into account model complexity (i.e. the cost of coding the model parameter- 
s). So more complex models are not penalised, which leads to overfitting and the 
inability to determine the best model size and structure (or distributions thereof) 
without resorting to costly cross-validation procedures. Bayesian approaches over- 
come these problems by treating the parameters 0 as unknown random variables 
and averaging over the ensemble of models they define: 
P(�) = f ). (2) 
P(Y) is the evidence for a data set Y = {y,... ,yN}. Integrating out parameters 
penalises models with more degrees of freedom since these models can a priori 
model a larger range of data sets. All information inferred from the data about the 
parameters is captured by the posterior distribution P(OIY ) rather than the ML 
point estimate 
While Bayesian theory deals with the problems of overfitting and model selec- 
tion/averaging, in practice it is often computationally and analytically intractable to 
perform the required integrals. For Gaussian mixture models Markov chain Monte 
Carlo (MCMC) methods have been developed to approximate these integrals by 
sampling [8, 7]. The main criticism of MCMC methods is that they are slow and 
2Technically, the log likelihood is not bounded above if no constraints are put on the 
determinant of the component covariances. So the real ML objective for MFA is to find 
the highest finite local maximum of the likelihood. 
awe sometimes use 0 to refer to the parameters and sometimes to all the unknown 
quantities (parameters and hidden variables). Formally the only difference between the two 
is that the number of hidden variables grows with N, whereas the number of parameters 
usually does not. 
Variational Inference for Bayesian Mixtures of Factor Analysers 451 
it is usually difficult to assess convergence. Furthermore, the posterior density over 
parameters is stored as a set of samples, which can be inefficient. 
Another approach to Bayesian integration for Gaussian mixtures [9] is the Laplace 
approximation which makes a local Gaussian approximation around a maximum a 
posteriori parameter estimate. These approximations are based on large data limits 
and can be poor, particularly for small data sets (for which, in principle, the advan- 
tages of Bayesian integration over ML are largest). Local Gaussian approximations 
are also poorly suited to bounded or positive parameters such as the mixing pro- 
portions of the mixture model. Finally, it is difficult to see how this approach can 
be applied to online incremental changes to model structure. 
In this paper we employ a third approach to Bayesian inference: variational ap- 
proximation. We form a lower bound on the log evidence using Jensen's inequality: 
f f P(Y,O) 
/2 -- in P(Y) = In dO P(Y, O) _> dO Q(O) In Q(O) - 
(3) 
which we seek to maximise. Maximising 5 r is equivalent to minimising the KL- 
divergence between Q(O) and P(OIY), so a tractable Q can be used as an approx- 
imation to the intractable posterior. This approach draws its roots from one way 
of deriving mean field approximations in physics, and has been used recently for 
Bayesian inference [13, 5, 1]. 
The variational method has several advantages over MCMC and Laplace approxi- 
mations. Unlike MCMC, convergence can be assessed easily by monitoring 5 r. The 
approximate posterior is encoded efficiently in Q(O). Unlike Laplace approxima- 
tions, the form of Q can be tailored to each parameter (in fact the optimal form 
of Q for each parameter falls out of the optimisation), the approximation is global, 
and Q optimises an objective function. Variational methods are generally fast, 5 r 
is guaranteed to increase monotonically and transparently incorporates model com- 
plexity. To our knowledge, no one has done a full Bayesian analysis of mixtures of 
factor analysers. 
Of course, vis-a-vis MCMC, the main disadvantage of variational approximations 
is that they are not guaranteed to find the exact posterior in the limit. However, 
with a straightforward application of sampling, it is possible to take the result of 
the variational optimisation and use it to sample from the exact posterior and exact 
predictive density. This is described in section 5. 
In the remainder of this paper we first describe the mixture of factor analysers in 
more detail (section 2). We then derive the variational approximation (section 3). 
We show empirically that the model can infer both the number of components and 
their intrinsic dimensionalities, and is not prone to overfitting (section 6). Finally, 
we conclude in section 7. 
2 The Model 
Starting from (1), the evidence for the Bayesian MFA is obtained by averaging the 
likelihood under priors for the parameters (which have their own hyperparameters): 
P(Y) 
(4) 
452 Z. Ghahramani and M. . Beal 
Here {a, a, b, ) are hyperparameters 4, v are precision parameters (i.e. inverse vari- 
ances) for the columns of A. The conditional independence relations between the 
variables in this model are shown 
tation in Figure 1. 
Figure 1: Generative model for 
variational Bayesian mixture of 
factor analysers. Circles denote 
random variables, solid rectangles 
denote hyperparameters, and the 
dashed rectangle shows the plate 
(i.e. repetitions) over the data. 
graphically in the usual belief network represen- 
While arbitrary choices could be made for the 
priors on the first line of (4), choosing priors that 
are conjugate to the likelihood terms on the sec- 
ond line of (4) greatly simplifies inference and 
interpretability. 5 So we choose P(rla ) to be 
symmetric Dirichlet, which is conjugate to the 
multinomial P(slr ). 
The prior for the factor loading matrix plays a 
key role in this model. Each component of the 
mixture has a Gaussian prior P(A81vs), where 
each element of the vector 8 is
