Connectionist Optimisation of Tied Mixture 
Hidden Markov Models 
Steve Renals 
Nelson Morgan 
ICSI 
Berkeley CA 94704 
USA 
Herv Bourlard 
L&H Speechproducts 
Ieper B-9800 
Belgium 
Horacio Franco 
Michael Cohen 
SRI International 
Menlo Park CA 94025 
USA 
Abstract 
Issues relating to the estimation of hidden Markov model (HMM) local 
probabilities are discussed. In particular we note the isomorphism of ra- 
dial basis functions (RBF) networks to tied mixture density modelling; 
additionally we highlight the differences between these methods arising 
from the different training criteria employed. We present a method in 
which connectionist training can be modified to resolve these differences 
and discuss some preliminary experiments. Finally, we discuss some out- 
standing problems with discriminative training. 
I INTRODUCTION 
In a statistical approach to continuous speech recognition the desired quantity is 
the posterior probability P(WwlX r, O) of a word sequence W w = wl, ..., ww given 
the acoustic evidence X r = xl,..., xT and the parameters of the speech model used 
O. Typically a set of models is used, to separately model different units of speech. 
This probability may be re-expressed using Bayes' rule: 
(1) 
P(W71XLe) = 
P(XltlWi w, )P(WW[) 
P(XT[) 
P(XlWi w, )P(WW[) 
'w' e(xltl w', o)e(w'[o) ' 
P(XiW w,O)/P(xlr[O) is the acoustic model. This is the ratio of the likelihood 
of the acoustic evidence given the sequence of word models, to the probability of 
167 
168 Renals, Morgan, Bourlard, Franco, and Cohen 
the acoustic data being generated by the complete set of models. P(XrlO) may be 
regarded as a normalising term that is constant (across models) at recognition time. 
However at training time the parameters O are being adapted, thus P(XrlO) is no 
longer constant. The prior, P(WWlO), is obtained from a language model. 
The basic unit of speech, typically smaller than a word (here we use phones), is 
modelled by a hidden Markov model (HMM). Word models consist of concate- 
nations of phone HMMs (constrained by pronunciations stored in a lexicon), and 
sentence models consist of concatenations of word HMMs (constrained by a gram- 
mar). The lexicon and grammar together make up a language model, specifying 
prior probabilities for sentences, words and phones. 
A HMM is a stochastic automaton defined by a set of states qi, a topology specify- 
ing allowed state transitions and a set of local probability density functions (PDFs) 
P(xt, qilqj, Xt-). Making the further assumptions that the output at time t is inde- 
pendent of previous outputs and depends only on the current state, we may separate 
the local probabilities into state transition probabilities P(qilqj) and output PDFs 
P(xtlqi). A set of initial state probabilities must also be specified. 
The parameters of a HMM are usually set via a maximum likelihood procedure that 
optimally estimates the joint density P(q, xlO). The forward-backward algorithm, a 
provably convergent algorithm for this task, is extremely efficient in practice. How- 
ever, in speech recognition we do not wish to make the best model of the data ix, q) 
given the model parameters; we want to make the optimal discrimination between 
classes at each time. This can be better achieved by computing a discriminant 
P(qlx, ). Note that in this case we do not model the input density P(xIO). 
We may estimate P(qlx,)) using a feed-forward network trained to an entropy 
criterion (Bourlard &: Wellekens, 1989). However, we require likelihoods of the form 
P(xlq, ), as HMM output probabilities. We may convert posterior probabilities to 
scaled likelihoods P(xlq, O)/P(xlO), by dividing the network outputs by the relative 
frequencies of each class . Note that we are not using connectionist training to 
obtain density estimates here; we are obtaining a ratio and not modelling P(xl)). 
This ratio is the quantity that we wish to maximise: this corresponds to maximising 
P(xlq�, 9) and minimising P(xlqi, 9), i  c, where q� is the correct class. We have 
used discriminatively trained networks to estimate the output PDFs (Bourlard & 
Morgan, 1991; Renals et al., 1991, 1992), and have obtained superior results to 
maximum likelihood training on continuous speech recognition tasks. 
In this paper, we are mainly concerned with radial basis function (RBF) networks. 
A RBF network generally has a single hidden layer, whose units may be regarded 
as computing local (or approximately local) densities, rather than global decision 
surfaces. The resultant posteriors are obtained by output units that combine these 
local densities. We are interested in using RBF networks for various reasons: 
� A RBF network is isomorphic to a tied mixture density model, although the 
training criterion is typically different. The relationship between the two is 
explored in this paper. 
� The locality of RBFs makes them suitable for situations in which the input 
XThese axe the estimates of P(qi) implicifiy used during classifier gaining. 
Connectionist Optimisation of Tied Mixture Hidden Markov Models 169 
distribution may change (e.g. speaker adaptation). Surplus RBFs in a region 
of the input space where data no longer occurs will not effect the final classi- 
fication. This is not so for sigmoidal hidden units in a multi-layer perceptron 
(MLP), which have a global effect. 
RBFs are potentially more computationally efficient than MLPs at both train- 
ing and recognition time. 
2 TIED MIXTURE HMM 
Tied mixtures of Gaussians have proven to be powerful PDF estimators in HMM 
speech recognition systems (Huang & Jack, 1989; Bellegarda & Nahamoo, 1990). 
The resulting systems are also known as semi-continuous HMMs. Tied mixture 
density estimation may be regarded as an interpolation between discrete and con- 
tinuous density modelling Essentially, tied mixture modelling has a single code- 
book of Gaussians shared by all output PDFs. Each of these PDFs has its own 
set of mixture coefficients used to combine the individual Gaussians. If f:(xlq:) is 
the output PDF of state q:, and Nj(xlyi,Ej) are the component Gaussians, then: 
(2) fk(xlqk, O)=  atjNj(xlYj, Yj) 
 a j = 1 0 _< a  _< 1, 
J 
where aq is an element of the matrix of mixture coefficients (which may be inter- 
preted as the prior probability P(gj, I;jlq,)) defining how much component density 
Nj(xlgi, Ej) contributes to output PDF f(xlq, O). Alternatively this may be re- 
garded as fuzzy vector quantisation. 
3 RADIAL BASIS FUNCTIONS 
The radial basis functions (RBF) network was originally introduced as a means 
of function interpolation (Powell, 1985; Broomhead 2z Lowe, 1988). A set of K 
approximating functions, f:(x) is constructed from a set of J basis functions )(x): 
J 
(3) f(x) =  ai)/(x) 1 _< k _< K 
This equation defines a RBF network with J RBFs (hidden units) and K outputs. 
The output units here are linear, with weights aj. The RBFs are typically Gaus- 
sians, with means #j and covariance matrices 
(4) 0j(x) = R exp -(x - j)rzfl(x - j) , 
where R is a normalising constant. The covariance matrix is frequently sumed to 
be diagonal 2. 
h is often reonable for speech applications, since reel or PLP cepsal coecien 
oogonal. 
170 Renals, Morgan, Bourlard, Franco, and Cohen 
Such a network has been used for HMM output probability estimation in contin- 
uous speech recognition (Renan et al., 1991) and an isomorphism to tied-mixture 
HMMs was noted. However, there is a mismatch between the posterior probabilities 
estimated by the network and the likelihoods required for the HMM decoding. Pre- 
viously this was resolved by dividing the outputs by the relative frequencies of each 
state. It would be desirable, though, to retain the isomorphism to tied mixtures: 
specifically we wish to interpret the hidden-to-output weights of an RBF network as 
the mixture coefficients of a tied mixture likelihood function. This can be achieved 
by defining the transfer units of the output units to implement Bayes' rule, which 
relates the posterior gk(x) to the likelihood fk(x): 
(5) g(x) 
f(x)P(q) 
Such a transfer function ensures the output units sum to 1; if f(x) is guaranteed 
non-negative, then the outputs are formally probabilities. The output of such a 
network is a probability distribution and we are using 'l-from-K' training: thus the 
relative entropy E is simply: 
(6) 
E = - log g�(x), 
where q, is the desired output class (HMM distribution). Bridle (1990) has demon- 
strated that minimising this error function is equivalent to maximising the mutual 
information between the acoustic evidence and HMM state sequence. 
If we wish to interpret the weights as mixture coefficients, then we must ensure 
that they are non-negative and sum to 1. This may be achieved using a normalised 
exponential (softmax) transformation: 
exp(w) 
(7) a = 5''., exp(w) ' 
The mixture coefficients a are used to compute the likelihood estimates, but it is 
the derived variables wq that are used in the unconstrained optimisation. 
3.1 TRAINING 
Steepest descent training specifies that: 
(8) aw=_ 
Here E is the relative entropy objective function (6). We may decompose the right 
hand side of this by a careful application of the chain rule of differentiation: 
0E 
(9) ow'-'- = 
t=-I h=l 
Connectionist Optimisation of Tied Mixture Hidden Markov Models 171 
We may write down expressions for each of these partials (where , is the Kronecker 
delta and q� is the desired state): 
aE $�t 
(10) ogt(x) _ 
(11) 3gdx) gk(x____) ($ _ 
ark(x) f(x) 
(12) a/(x) _ ,(x) 
(13) aa - a($] - aW). 
Substituting (10), (11), (12) and (13) into (9) we obtain: 
aE 1 
(14) oNv'. - ft(x--5 (gt(x) - $)ao ()y(x) - ft(x)) . 
Apart from the added terms due to the normalisation of the weights, the major dif- 
ference in the gradient compared with using a sigmoid or softmax transfer function 
is the 1/f(x) factor. To some extent w
