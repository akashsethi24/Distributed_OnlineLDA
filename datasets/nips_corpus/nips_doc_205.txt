498 Barhen, Toomarian and Gulati 
A djoint 
Learning 
Operator Algorithms for Faster 
in Dynamical Neural Networks 
Jacob Barhen 
Nikzad Toomarian 
Sandeep Gulati 
Center for Space Microelectronics Technology 
Jet Propulsion Laboratory 
California Institute of Technology 
Pasadena, CA 91109 
ABSTRACT 
A methodology for faster supervised learning in dynamical nonlin- 
ear neural networks is presented. It exploits the concept of adjoint 
operators to enable computation of changes in the network's re- 
sponse due to perturbations in all system parameters, using the so- 
lution of a single set of appropriately constructed linear equations. 
The lower bound on speedup per learning iteration over conven- 
tional methods for calculating the neuromorphic energy gradient is 
O(N2), where N is the number of neurons in the network. 
I INTRODUCTION 
The biggest promise of artifcial neural networks as computational tools lies in the 
hope that they will enable fast processing and synthesis of complex information 
patterns. In particular, considerable efforts have recently been devoted to the for- 
mulation of efficent methodologies for learning (e.g., Rumelhart et al., 1986; Pineda, 
1988; Pearlmutter, 1989; Williams and Zipser, 1989; Barhen, Gulati and Zak, 1989). 
The development of learning algorithms is generally based upon the minimization 
of a neuromorphic energy function. The fundamental requirement of such an ap- 
proach is the computation of the gradient of this objective function with respect 
to the various parameters of the neural architecture, e.g., synaptic weights, neural 
Adjoint Operator Algorithms 499 
gains, etc. The paramount contribution to the often excessive cost of learning us- 
ing dynamical neural networks arises from the necessity to solve, at each learning 
iteration, one set of equations for each parameter of the neural system, since those 
parameters affect both directly and indirectly the network's energy. 
In this paper we show that the concept of adjoint operators, when applied to dynam- 
ical neural networks, not only yields a considerable algorithmic speedup, but also 
puts on a firm mathematical basis prior results for recurrent networks, the deriva- 
tions of which sometimes involved much heuristic reasoning. We have already used 
adjoint operators in some of our earlier work in the fields of energy-economy mod- 
eling (Alsmiller and Barhen, 1984) and nuclear reactor thermal hydraulics (Barhen 
et al., 1982; Toomarian et al., 1987) at the Oak Ridge National Laboratory, where 
the concept flourished during the past decade (Oblow, 1977; Cacuci et al., 1980). 
In the sequel we first motivate and construct, in the most elementary fashion, a 
computational framework based on adjoint operators. We then apply our results 
to the Cohen-Grossberg-Hopfield (CGH) additive model, enhanced with terminal 
attractor (Barhen, Gulati and Zak, 1989) capabilities. We conclude by presenting 
the results of a few typical simulations. 
2 ADJOINT OPERATORS 
Consider, for the sake of simplicity, that a problem of interest is represented by the 
following system of N coupled nonlinear equations 
�(a,p) = 0 (2.1) 
where � denotes a nonlinear operator  . Let fi and i5 represent the N-vector of 
dependent state variables and the M-vector of system parameters, respectively. We 
will assume that generally M >> N and that elements of  are, in principle, inde- 
pendent. Furthermore, we will also assume that, for a specific choice of parameters, 
a unique solution of Eq. (2.1) exists. Hence,  is an implicit function of i5. A 
system response, R, represents any result of the calculations that is of interest. 
Specifically 
R = R(,) (2.2) 
i.e., R is a known nonlinear function ofp and  and may be calculated from Eq. (2.2) 
when the solution fi in Eq. (2.1) has been obtained for a given p. The problem of 
interest is to compute the sensitivities of R, i.e., the derivatives of R with respect 
to parameters p, it = 1,. -., M. By definition 
dR OR OR O 
= + (2.3) 
dp, Opt, O Opt, 
lff differential operators appear in Eq. (2.1), then a corresponding set of boundary and/or 
initial conditions to specify the domain of qo must also be provided. In general an inhomogeneous 
source term can also be present. The learning model discussed in this paper focuses on the 
adiabatic approximation only. Nonadiabatic learning algorithms, wherein the response is defined 
as a functional, will be discussed in a forthcoming article. 
500 Barhen, Toomarian and Gulati 
Since the response R is known analytically, the computation of OR/Opt, and OR/O 
is straightforward. The quantity that needs to be determined is the vector O/Op,. 
Differentiating the state equations (2.1), we obtain a set of equations to be referred 
to as forward sensitivity equations 
0� 0a 
= (2.4) 
c9 fi O p . c9 p , 
To simplify the notations, we are omitting the transposed sign and denoting the 
N by N forward sensitivity matrix 0�/0 by A, the N-vector O/Opt, by  and 
the source N-vector -O�/Opt, by g. Thus 
(2.5) 
Since the source term in Eq. (2.5) explicitly depends on /, computing dR/dpt, , 
requires solving the above system of N algebraic equations for each parameter pu. 
This difficulty is circumvented by introducing adjoint operators. Let A* denote the 
formal adjoint 2 of the operator A. The adjoint sensitivity equations can then be 
expressed as 
A* '* = '*. (2.6) 
By definition, for algebraic operators 
.q* (Aq) = .q* = (A* q*) = us* (2.7) 
Since Eq. (2.3), can be rewritten as 
dR OR OR 
= + -- (2.8) 
dp, c9p, 
if we identify 
OR 
= 'S* = * (2.9) 
0 - - 
we observe that the source term for the adjoint equations is independent of the 
specific parameter p,. Hence, the solution of a single set of adjoint equations will 
provide all the information required to compute the gradient of R with respect to all 
parameters. To underscore that fact we shall denote ' as 0. Thus 
dR OR 
dp, = Opt, o Opt, (2.10) 
We will now apply this computational framework to a CGH network enhanced with 
terminal attractor dynamics. The model developed in the sequel differs from our 
2 Adjoint operatom can only be considered for densely defined linear operatom on Banach spaces 
(see e.g., Cacuci, 1980). For the neural application under consideration we will limit ourselves to 
real Hilbert spaces. Such spaces are self-dual. Furthermore, the domain of an adjoint operator is 
determined by selecting appropriate adjoint boundary conditions . The associated bilinear form 
evaluated on the domain boundary must thus be also generally included. 
Adjoint Operator Algorithms 501 
earlier formulations (Barhen, Gulati and Zak, 1989; Barhen, Zak and Gulati, 1989) 
in avoiding the use of constraints in the neuromorphic energy timetlon, thereby 
eliminating the need for differential equations to evolve the concomitant Lagrange 
multipliers. Also, the usual activation dynamics is transformed into a set of equiv- 
alent equations which exhibit more congenial numerical properties, such as con- 
traction. 
3 APPLICATIONS TO NEURAL LEARNING 
We formalize a neural network as an adaptive dynamical system whose temporal 
evolution is governed by the following set of coupled nonlinear differential equations 
+ = > r,.. + (3.1) 
where z, represents the mean soma potential of the nth neuron and , denotes the 
synaptic coupling from the m-th to the n-th neuron. The weighting factor w, 
enforces topological considerations. The constant ,, characterizes the decay of neu- 
ron activity. The sigmoidM function g,(.) modulates the neural response, with gain 
given by 7; typically, g,(z) = tanh(7z). The source term I,, which includes 
dimensional considerations, encodes contribution in terms of attractor coordinates 
of the k-th training sample via the following expression 
{ [an] - [an - gv(zn) ] if n  Sx (3.2) 
I = 0 ifnSUSv 
The topographic input, output and hidden network partitions Sx, Sv and St are 
architectural requirements related to the encoding of mapping-type problems for 
which a number of possibilities exist (Barhen, Gulati and Zak, 1989; Barhen, Zak 
and Gulati, 1989). In previous articles (ibid; Zak, 1989) we have demonstrated that 
in general, for fi = (2i+ 1) - and i a strictly positive integer, such attractors have 
infinite local stability and provide opportunity for learning in real-time. Typically, 
 can be set to 1/3. Assuming an adiabatic framework, the fixed point equations 
at equilibrium, i.e., as i  0, yield 
g u,, + L (3.3) 
where u, = g,(z,) represents the neural response. The superscript  denotes 
quantities evaluated at steady state. Operational network dynamics is then given 
by 
- -- w. u,, I. (3.4) 
n  un -- gv n m n 
To proceed formally with the development of a supervised learning algorithm, we 
consider an approach based upon the minimization of a constrained neuromorphic 
energy function E given by the following expression 
1 
E(fi,p) =   [fin - an ]2 Vn e Sx VSy (3.5) 
k n 
502 Barhen, Toomarian and Gulati 
We relate adjoint theory to neural learning by identifying the neuromorphic energy 
function, E in Eq. (3.5), with the system response R. Also, let p denote the following 
system parameters: 
The proposed objective function enforces convergence of every neuron in $x and 
$- to attractor coordinates corresponding to the components in the input-output 
training patterns, thereby prompting the network to learn the embedded invari- 
ances. Lyapunov stability requires an energy-like function to be monotonically de- 
creasing in time. Since in our model the internal dynamical parameters of interest 
are the synaptic strengths T,m of the interconnection topology, the characteristic 
decay constants tn and the gain parameters 7n this implies that 
For each adaptive system parameter, p,, Lyapunov stability will be satisfied by the 
following choice of equations of motion 
Examples include 
dE 
p 
