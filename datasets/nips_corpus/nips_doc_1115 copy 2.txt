Estimating the Bayes Risk from Sample Data 
Robert R. Snapp* and Tong Xu 
Computer Science and Electrical Engineering Department 
University of Vermont 
Burlington, VT 05405 
Abstract 
A new nearest-neighbor method is described for estimating the Bayes risk 
of a multiclass pattern classification problem from sample data (e.g., a 
classified training set). Although it is assumed that the classification prob- 
lem can be accurately described by sufficiently smooth class-conditional 
distributions, neither these distributions, nor the corresponding prior prob- 
abilities of the classes are required. Thus this method can be applied to 
practical problems where the underlying probabilities are not known. This 
method is illustrated using two different pattern recognition problems. 
1 INTRODUCTION 
An important application of artificial neural networks is to obtain accurate solutions to 
pattern classification problems. In this setting, each pattern, represented as an n-dimensional 
feature vector, is associated with a discrete pattern class, or state of nature (Duda and Hart, 
1973). Using available information, (e.g., a statistically representative set of labeled feature 
vectors {(xi, �i)}, where xi E [' denotes a feature vector and ti 
its correct pattern class), one desires a function (e.g., a neural network classifier) that assigns 
new feature vectors to pattern classes with the smallest possible misclassification cost. 
If the classification problem is stationary, such that the patterns from each class are generated 
according to known probability distributions, then it is possible to construct an optimal 
classifier that assigns each pattern to a class with minimal expected risk. Although our 
method can be generalized to problems in which different types of classification errors 
incur different costs, we shall simplify our discussion by assuming that all errors are equal. 
In this case, a Bayes classifier assigns each feature vector to a class with maximum posterior 
probability. The expected risk of this classifier, or Bayes risk then reduces to the probability 
of error 
t:lB = fs [1- sup P(glx)] f(x)dx , (1) 
* E-mail:snapp�emba. uvm. edu 
Estimating the Bayes Risk from Sample Data 233 
(Duda and Hart, 1973). Here, P(elx) denotes the posterior probability of class t conditioned 
on observing the feature vector x, f(x) denotes the unconditional mixture density of the 
feature vector x, and $ C R n denotes the probability-one support of f. 
Knowing how to estimate the value of the Bayes risk of a given classification problem with 
a specific input representation, may facilitate the design of more accurate classifiers. For 
example, since the value of RB depends upon the set of features chosen to represent each 
pattern (e.g., the significance of the input units in a neural network classifier), one might 
compare estimates of the Bayes risk for a number of different feature sets, and then select 
the representation that yields the smallest value. Unfortunately, it is necessary to know the 
explicit probability distributions to evaluate (1). Thus with the possible exception of trivial 
examples, the Bayes risk cannot be determined exactly for practical classification problems. 
Lacking the means to evaluate the B ayes risk exactly, motivates the development of statistical 
estimators of RB. In this paper, we use a recent asymptotic analysis of the finite-sample 
risk of the k-nearest-neighbor classifier to obtain a new procedure for estimating the Bayes 
risk from sample data. Section 2 describes the k-nearest-neighbor algorithm, and briefly 
describes how estimates of its finite-sample risk have been used to estimate RB. Section 3 
describes how a recent asymptotic analysis of the finite-sample risk can be applied to obtain 
new statistical estimators of the Bayes risk. In Section 4 the k-nearest-neighbor algorithm 
is used to estimate the Bayes risk of two example problems. Section 5 contains some 
concluding remarks. 
2 THE k-NEAREST-NEIGHBOR CLASSIFIER 
Due to its analytic tractability, and its nearly optimal performance in the large sample limit, 
the k-nearest-neighbor classifier has served as a useful framework for estimating the Bayes 
risk from classified samples. Recall, that the k-nearest-neighbor algorithm (Fix and Hodges, 
1951) classifies an n-dimensional feature vector x by consulting a reference sample of re 
correctly classified feature vectors A', = {(x,g): i = 1,...re}. First, the algorithm 
identifies the k nearest neighbors of x, i.e., the k feature vectors within A', that lie closest 
to x with respect to a given metric. Then, the classifier assigns x to the most frequent class 
label represented by the k nearest neighbors. (A variety of procedures can be used to resolve 
ties.) In the following, (7 denotes the number of pattern classes. 
The finite-sample risk of this algorithm, Rm, equals the probability that the k-nearest- 
neighbor classifier assigns x to an incorrect class, averaged over all input vectors x, and 
all m-samples, A',. The following properties have been shown to be true under weak 
assumptions: 
Property 1 
with 
(Cover and Hart, 1967): For fixed k, 
R,  Rod(k), as 
RB _< Roo(1) _< R (2 
C Rs) 
C-1 ' 
(2) 
Property 2 (Devroye, 1981): If k >_ 5, and C = 2, then there exist universal constants 
a = 0.3399..., and/3 = 0.9749... such that Rod(k) is bounded by 
R < Roo(k) < (l + ak)R, where ak - k _ 3.25 ' 
More generally, if C = 2, then 
234 R.R. SNAPP, T. XU 
By the latter property, this algorithm is said to be Bayes consistent in that for any e > 0, it 
is possible to construct a k-nearest-neighbor classifier such that IR, - Rsl < e if m and 
k are sufficiently large. Bayes consistency is also evident in other nonparametric pattern 
classifiers. 
Several methods for estimating RB from sample data have previously been proposed, e.g., 
(Devijver, 1985), (Fukunaga, 1985), (Fukunaga and Hummels, 1987), (Garnett and Yau, 
1977), and (Loizou and Maybank, 1987). Typically, these methods involve constructing 
sequences of k-nearest neighbor classifiers, with increasing values of k and m. The mis- 
classification rates are estimated using an independent test sample, from which upper and 
lower bounds to Rs are obtained. Because these experiments are necessarily performed 
with finite reference samples, these bounds are often imprecise. This is especially true for 
problems in which Rm converges to Ro(k) at a slow rate. In order to remedy this deficiency, 
it is necessary to understand the manner in which the limit in Property 1 is achieved. In the 
next section we describe how this information can be used to construct new estimators for 
the Bayes risk of sufficiently smooth classification problems. 
3 NEW ESTIMATORS OF THE BAYES RISK 
For a subset of multiclass classification problems that can be described by probability 
densities with uniformly bounded partial derivatives up through order N + 1 (with N > 2), 
the finite-sample risk of a k-nearest-neighbor classifier that uses a weighted L v metric can 
be represented by the truncated asymptotic expansion 
N 
Rm = Ro(k)+ E cjm-J/n + 0 (m-(N+l)/n) , (4) 
j=2 
(Psaltis, Snapp, and Venkatesh, 1994), and (Snapp and Venkatesh, 1995). In the above, 
n equals the dimensionality of the feature vectors, and Rod(k), c2,..., cv, are the ex- 
pansion coefficients that depend upon the probability distributions that define the pattern 
classification problem. 
This asymptotic expansion provides a parametric description of how the finite-sample risk 
Rm converges to its infinite sample limit Ro(k). Using a large sample of classified data, 
one can obtain statistical estimates of the finite-sample risk/m for different values of 
m. Specifically, let {rn} denote a sequence of M different sample sizes, and select fixed 
values for k and N. For each value of rn, construct an ensemble of k-nearest-neighbor 
classifiers, i.e., for each classifier construct a random reference sample 3Jm by selecting 
rn patterns with replacement from the original large sample. Estimate the empirical risk 
of each classifier in the ensemble with an independently drawn set of test vectors. Let 
tm, denote the average empirical risk of the i-th ensemble. Then, using the resulting set 
of data points {(m, ,)}, find the values of the coefficients Ro(k), and c2 through cv, 
that minimizes the sum of the squares: 
E lm, - Rm(k)- E cjrn-J/n (5) 
i=l 
Several inequalities can then be used obtain approximations of Rs from the estimated value 
of Ro(k). For example, if k = 1, then Cover and Hart's inequality in Property I implies 
that 
Rod(l) 
_< Rs _< Rod(l). 
2 
To enable an estimate of Rs with precision e, choose k > 2/e , and estimate Rod(k) by the 
above method. Then Devroye's inequality (3) implies 
Ro(k)- < Rod(k)(1 -) < Rs < Rod(k). 
Estimating the Bayes Risk from Sample Data 235 
4 EXPERIMENTAL RESULTS 
The above procedure for estimating ];lB was applied to two pattern recognition problems. 
First consider the synthetic, two-class problem with prior probabilities P = P2 = 1/2, and 
normally distributed, class-conditional densities 
1  ((x +(-1]]+? * x?) 
fdx) = (2r),/2 e - ' ' ' '=  , 
for f = 1 and 2. Pseudorandom labeled feature vectors (x, f) were numerically generated in 
accordance with the above for dimensions r = 1 and r, = 5. Twelve sample sizes between 
10 and 3000 were examined. For each dimension and sample size the risks R, of many 
independent k-nearest-neighbor classifiers with k = 1, 7, and 63 were empirically estimated. 
(Because the asymptotic expansion (4) does not accurately describe the very small sample 
behavior of the k-nearest-neighbor classifier, sample sizes smaller than 2k were not included 
in the fit.) 
Estimates of the coefficients in (5) for six different fits appear in the first equation of each cell 
in the third and fourth columns of Table 1. For reference, the second column c
