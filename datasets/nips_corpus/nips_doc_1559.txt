Linear Hinge Loss and Average Margin 
Claudio Gentile 
DSI, Universita' di Milano, 
Via Comelico 39, 
20135 Milano. Italy 
gentile@dsi .unimi. it 
Manfred K. Warmuth* 
Computer Science Department, 
University of California, 
95064 Santa Cruz, USA 
manfred@cse.ucsc. edu 
Abstract 
We describe a unifying method for proving relative loss bounds for on- 
line linear threshold classification algorithms, such as the Perceptron and 
the Winnow algorithms. For classification problems the discrete loss is 
used, i.e., the total number of prediction mistakes. We introduce a con- 
tinuous loss function, called the linear hinge loss, that can be employed 
to derive the updates of the algorithms. We first prove bounds w.r.t. the 
linear hinge loss and then convert them to the discrete loss. We intro- 
duce a notion of average margin of a set of examples. We show how 
relative loss bounds based on the linear hinge loss can be converted to 
relative loss bounds i.t.o. the discrete loss using the average margin. 
1 Introduction 
Consider the classical Perceptron algorithm. The hypothesis of this algorithm at trial t 
is a linear threshold function determined by a weight vector tot  7 '. For an instance 
a:t  7 ' the linear activation at -- tot � a:t is passed through a threshold function ar 
which is -1 on arguments less than the threshold r and +1 otherwise. Thus the prediction 
of the algorithm is binary and -1, +1 denote the two classes. The Perceptron algorithm 
is aimed at learning a classification problem where the examples have the form (mr, l/t)  
x 
After seeing T examples (xt, Yt)i<t<r, the algorithm predicts with r+ = cr,.(Wr+ � 
xr+i) on the next instance xr+. If the algorithm's prediction r+l agrees with the label 
YT+ on the instance xr+, then its loss is zero. If the prediction and the label disagree, 
then the loss is one. We call this loss the discrete loss. 
The convergence of the Perceptron algorithm is established in the Perceptron convergence 
theorem. There is a second by now classical algorithm for learning with linear threshold 
functions: the Winnow algorithm of Nick Littlestone [Lit88]. This algorithm also maintains 
a weight vector and predicts with the same linear threshold function defined by the current 
weight vector tot. However, the update of the weight vector tot = (wt.i, ..., 
* Supported by NSF grant CCR-970020 I. 
226 C. Gentile and M. K. V/arrnuth 
performed by the two algorithms is radically different: 
Perceptron: 
Winnow: In Wt+l,i 
The Perceptron algorithm performs a simple additive update. The parameter r/is a positive 
learning rate and (t equals (t - lt)/2, which lies in {-1, 0, +1}. When (t -' 0 the pre- 
diction of the algorithm is correct and no update occurs. Both the Perceptron algorithm and 
Winnow update conservatively, i.e., they update only when the prediction of the algorithm 
is wrong. Ift -' +1 and//t -- -1 then the algorithm overshot and (t = +1. This causes 
the Perceptron to subtract r/a:t from the current weight wt. Similarly if t -- -1 and 
//t - +1 then the algorithm undershot and (t = -1. Now the Perceptron adds r :rt to the 
current weight wt. We will later interpret (t a:t as a gradient of a loss function. Winnow 
uses the same gradient but the update is done through the componentwise logarithm of the 
weight vector. One can also rewrite Winnow's update as 
wt+,i := wt,i exp (-r/ 
so that the gradient appears in the exponents of factors that multiply the old weights. The 
factors are now used to correct the weights in the right direction when the algorithm under 
or overshot. 
The algorithms are good for different purposes and, generally speaking, incomparable (see 
[KWA97] for a discussion). In [KW97] a framework was introduced for deriving simple 
on-line learning updates. This framework has been applied to a variety of different learning 
algorithms and differentiable loss functions [HKW95, KW98]. The updates are always 
derived by approximately solving the following minimization problem 
Wt+l :=argminwS(w), whereS(w)=d(w, wt)+rlloss(yt, ar(w.xt)). (1) 
Here loss denotes the chosen loss function. In our setting this would be the discrete loss. 
What is different now is that the prediction of the algorithm t = rrr(wt � xt) and the 
discrete loss are discontinuous in the weight vector wt. We will return to this point later 
after discussing the other parts of the above minimization problem. The parameter r is the 
learning rate mentioned above and, most importantly, d(w, wt) is a divergence measuring 
how far w is from wt. The divergence function has two purposes. It motivates the update 
and it becomes the potential function in the amortized analysis used to prove loss bounds 
for the corresponding algorithm. 
The use of an amortized analysis in the context of learning essentially goes back to [Lit89] 
and the method for deriving updates based on the divergence was introduced in [KW97]. 
The divergence may be seen as a regularization term and may also serve as a barrier func- 
tion in the optimization problem (1) for the purpose of keeping the weights in a particular 
region. The additive algorithms, such as gradient descent and the Perceptron algorithm, use 
d(w, w,) - IIw - will2/2 as the divergence. This can be used as a potential function for 
the proof of the Perceptron convergence theorem. Multiplicative update algorithms such as 
Winnow and various exponentiated gradient algorithms use entropy-based divergences as 
potential functions [HKW95, KW98]. The function U in (1) is minimized by differentiat- 
ing w.r.t.w. This works very well when the loss function is convex and differentiable. For 
example for linear regression, when the loss function is the square loss (wt � xt - gtt)2/2, 
then minimizing U(w) with the divergence [[w - wt[[2/2 gives the Widrow-Hoff update: 
Various exponentiated gradient algorithms [KW97] can be derived in the same way when 
entropic divergences are used instead. However, in our case we cannot differentiate the 
discrete loss since it is discontinuous. 
We asked ourselves which loss function motivates the Perceptron and Winnow algorithms 
in this framework. We will see that the loss function that achieves this is continuous and 
Linear Hinge Loss and Average Margin 227 
its gradient w.r.t. wt is 5tXt, where 5t  {-1, O, +1}. We call this loss the (linear) hinge 
loss (HL) and we believe this is the key tool for understanding linear threshold algorithms 
such as the Perceptron and Winnow. However, in the process of changing the discrete 
loss to the HL we also changed our learning problem from a classification to a regression 
problem. There are now two versions of each algorithm, a classification version and a 
regression version. The classification version predicts with a binary label using its linearly 
thresholded prediction. The loss function is the discrete loss. The regression version, on 
the other hand, predicts on the next instance xt with its linear activation at = wt .xt. In the 
classification problem the labels Yt of the examples are -1 and +1, while in the regression 
problem the labels at are -c and +c. We will see that both versions of each algorithm 
use the same rule to update the weight vector wt. 
Another strong hint that the HL is related to Perceptron and Winnow comes from the fact 
that this loss may be seen as a limiting case of the entropic loss used in logistic regression. 
In logistic regression the threshold function a,. is replaced by the smooth t:anh function. 
There is a technical way of associating a matching loss function with a given increasing 
transfer function [HKW95]. The matching loss for the t:anh transfer function is the en- 
tropic loss. We will show that by making this transfer function steeper and by taking the 
right viewpoint of the matching loss, the entropic loss converges to the HL. In the limiting 
case the slope of the transfer function is infinite, i.e., it becomes the threshold function 
The question is whether this introduction of the HL buys us anything. We believe so. 
We can prove a unifying meta-theorem for the whole class of general additive algorithms 
[GLS97, KW98], when defined w.r.t. the HL. The bounds for the regression versions of the 
Perceptron and Winnow are simple special cases. These loss bounds can then be converted 
to loss bounds for the corresponding classification problems w.r.t. the discrete loss. This 
conversion is carried out through working with the average margin of a set of examples 
relative to a linear threshold classifier. The conversion of the HL described in this paper 
can then be considered a principled way of deriving average margin-based mistake bounds. 
The average margin reveals the inner structure of mistake bound results that have been 
proven thus far for conservative on-line algorithms. Previously used definitions, such as 
the deviation [FS98] and the attribute error [Lit91], can easily be related to the average 
margin or reinterpreted in terms of the HL and the average margin. 
2 Preliminaries and the linear hinge loss 
We define two subsets of 7gr: the weight domain W and the instance domain X. The 
weights w maintained by the algorithms always lie in the weight domain and the instances 
x of the examples always lie in the instance domain. We require I/V be convex. 
A general additive algorithm and a divergence are defined in terms of a link function f. 
Such a function is a vector valued function from the interior int W of the weight domain 
I/V onto 7g n, with the property that its Jacobian is strictly positive definite everywhere in 
int W. A link function f has a unique inverse f-1 : 7gn _ int 142. We assume that f 
is the gradient of a (potential) function Pf from int 142 to 7g, i.e., f(w) = VPf(w) for 
w  int 142. It is easy to extend the domain of Pf such that it includes the boundary of 142. 
For any link function f, a (Bregman) divergence function df : 142 X int 142 --> [0, c
