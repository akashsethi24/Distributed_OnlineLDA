Some Estimates of Necessary Number of 
Connections and Hidden Units for 
Feed-Forward Networks 
Adam Kowalczyk 
Telecom Australia, Research Laboratories 
770 Blackburn Road, Clayton, Vic. 3168, Australia 
(a.kowalczyk@trl.oz.au) 
Abstract 
The feed-forward networks with fixed hidden units (FHU-networks) 
are compared against the category of remaining feed-forward net- 
works with variable hidden units (VttU-networks). Two broad 
classes of tasks on a finite domain X C R n are considered: ap- 
proximation of every function from an open subset of functions on 
X and representation of every dichotomy of X. For the first task 
it is found that both network categories require the same minimal 
number of synaptic weights. For the second task and X in gen- 
eral position it is shown that VHU-networks with threshold logic 
hidden units can have approximately 1In times fewer hidden units 
than any FHU-network must have. 
1 Introduction 
A good candidate artificial neural network for short term memory needs to be: (i) 
easy to train, (ii) able to support a broad range of tasks in a domain of interest and 
(iii) simple to implement. The class of feed-forward networks with fixed hidden 
units (HU) and adjustable synaptic weights at the top layer only (shortly: FHU- 
networks) is an obvious candidate to consider in this context. This class covers a 
wide range of networks considered in the past, including the classical perceptron, 
higher order networks and non-linear associative mapping. Also a number of train- 
ing algorithms were specifically devoted to this category (e.g. perceptron, madaline 
639 
640 Kowalczyk 
or pseudoinverse) and a number of hardware solutions were investigated for their 
implementation (e.g. optical devices [8]). 
Leaving aside the non-trivial tasks of constructing the domain specific HU for a 
FHU-network [9] and then optimal loading of specific tasks, in this paper we con- 
centrate on assessing the abilities of such structures to support a wide range of tasks 
in comparison to more complex feedforward networks with multiple layers of variable 
HU (VHU-networks). More precisely, on a finite domain X two benchmark tests 
are considered: approximation of every function from an open subset of functions 
on X and representation of every dichotomy of X. Some necessary and sufficient 
estimates of minimal necessary numbers of adaptable synaptic weights and of HU 
are obtained and then combined with some sufficient estimates in [10] to provide 
the final results. In Appendix we present an outline some of our recent results on 
the extension of the classical Function-Counting Theorem [2] to the multilayer case 
and discuss some of its implications to assessing network capacities. 
2 Statement of the main results 
In this paper X will denote a subset of R ' of N points. Of interest to us are 
multilayer feed-forward networks (shortly FF-networks), Fw: X - R, depending 
on the k-tuple w: (w, ..., w) C R  of adjustable synaptic weights to be selected on 
loading to the network desired tasks. The FF-networks are split into two categories 
defined above: 
� FHU-network with fixed hidden units qSi: X - R 
k 
Fw(x) eff (x C X), 
i----1 
� VHU-networks with variable hidden units bw,,,i : X - R depending on 
__-- R/d ' 
some adjustable synaptic weights w, where w (w , w) G R ' x -- 
R  
Fw(x) (x e x). (2) 
i=1 
Of special interest are situations where hidden units are built from one or more layers 
of artificial neurons, which, for simplicity, can be thought of as devices computing 
simple functions of the form 
(y, ...,y)  R TM -* cr(wiy + wiy2 + ' + wi,,yrn), 
where cr � R - R is a non-decreasing squashing function. Two particular examples 
of squashing functions are (i) infinitely differentiable sigmoid function t  (1 + 
exp(-t)) - and (ii) the step function O(t) defined as 1 for t _> 0 and = O, otherwise. 
In the latter case the artificial neuron is called a threshold logic neuron (ThL- 
In the formulation of results below all biases are treated as synaptic weights attached 
to links from special constant HUs (-- 1). 
Estimates of Necessary Connections & Hidden Units for Feed-Forward Networks 641 
2.1 Function approximation 
The space R x of all real functions on X has the natural structure of a vector space 
isomorphic with R v. We introduce the euclidean norm 
on R X and denote by L/C R X an open, non-empty subset. We say that the FF- 
network Fw can approximate a function f on X with accuracy e 
for a weight vector w C R k. 
Theorem 1 Assume the FF-network Fw is continuously differenttable with respect 
to the adjustable synaptic weights w  R  and k ( N. If it can approximate 
any function in Lt with any accuracy then for almost every function f  Lt, if 
lin_o IIFw(,) - ill-- 0, where w(1), w(2), ... C R , then lirra_oo IIw(i)[I-- . 
In the above theorem almost every means with the exception of a subset of the 
Lebesgue measure 0 on R X  R N. The proof of this theorem relies on use of 
Sard's theorem from differential topology (c.f. Section 3). Note that the above the- 
orem is applicable in particular to the popular back-propagation network which 
is typically built from artificial neurons with the continuously differenttable sigmoid 
squashing function. 
The proof of the following theorem uses a different approach, since the network is 
not differentiably dependent on its synaptic weights to HUs. This theorem applies 
in particular to the classical FP-networks built from ThL-neurons. 
Theorem 2 A FF-network Fw must have _ N HU in the top hidden layer if all 
units o� this layer have a finite number of activation levels and the network can 
approximate any �unction in Lt with any accuracy. 
The above theorems mean in particular that if we want to achieve an arbitrarily good 
approximation of any function in// ae__ {f: X - R; If(x)l  A), where A  0, 
and we can use one of VHU-networks of the above type with synaptic weights of a 
restricted magnitude only, then we have to have at least N such weights. However 
that many weights are necessary and sufficient to achieve the same, with a FHU- 
network (1) if the functions bi are linearly independent on X. So variable hidden 
units give no advantage in this case. 
2.2 Implementation of dichotomy 
We say that the FF-network Fw can implement a dichotomy (X_, X+) of X if 
there exists w ff R  such that Fw ( 0 on X_ and Fw ) 0 on X+. 
Proposition 3 A FHU-network Fw can implement every dichotomy of X if and 
only i� it can exactly compute every function on X. In such a case it must have 
_ N HU in the top hidden layer. 
The non-trivial part of the above theorem is necessity in the first part of it, i.e. that 
being able to implement every dichotomy on X requires N (fixed) hidden units. In 
Section 3.3 we obtain this proposition from a stronger result. Note that the above 
642 Kowalczyk 
proposition can be deduced from the classical Function-Counting Theorem [2] and 
also that an equivalent result is proved directly in [3, Theorem 7.2]. 
We say that the points of a subdomain X C R  are in general position if every 
in R  contains no more than n points of X. Note that points of every finite 
subdomain of R n are in general position after a sufficiently small perturbation and 
that the property of being in general position is preserved under sufficiently small 
perturbations. Note also that the points of a typical N-point subdomain X C R n 
are in general position, where typical means with the exception of subdomains X 
corresponding to a certain subset of Lebesgue measure 0 in the space (Rn) N of all 
N-tuples of points from R '. 
It is proved in [10] that for a subdomain set X C R  of N points in general position 
a VHU-network having [(N- 1)/n] (adjustable) ThL-neurons in the first (and the 
only) hidden layer can implement every dichotomy of X, where the notation 
denotes the smallest integer > t. Furthermore, examples are given showing that the 
above bound is tight. (Note that this paper corrects and gives rigorous proofs of 
some early results in [1, Lemma 1 and Theorem 1] and also improves [6, Theorem 4].) 
Combining these results with Proposition 3 we get the following result. 
Theorem 4 Assume that all N points of X C R  are in general position. In the 
class of all FF-networks which can implement every dichotomy on X there exists a 
VHU-network with threshold logic HU having a fraction 1/n+O(1/N) of the number 
of the HU that any FHU-network in this class must have. There are examples of 
X in genera/position of any even cardinality N > 0 showing that this estimate is 
tight. 
3 Proofs 
Below we identify functions f � X -* R with N-tuples of their values at N-points 
of X (ordered in a unique manner). Under this identification the FF-networks Fw 
can be regarded as a transformation 
w 6 R  - Fw 6 R S (3) 
with the range T(Fw) aeg {Fw; w 6 R k) C R N. 
3.1 Proof of Theorem 1. 
In this case the transformation (3) is continuously differentiable. Every value of it 
is singular since k < N, thus according to Sard's Theorem [5], 7(Fw) C R N has 
Lebesgue measure 0. It is enough to show now that if 
f 6 L/- 7(Fw) 
(4) 
and 
lira [[Fw(i)-fl[=0 and [[w(i)[[ <M, 
for some M > 0, then a contradiction follows. Actually from (5) it follows that 
f belongs to the topological closure cl(7M) of 7M ae__f {Fw ; w 6 R k & [[w[I < 
Estimates of Necessary Connections & Hidden Units for Feed-Forward Networks 643 
}. However, TM is a compact set as a continuous image of a closed ball {w C 
; ]]w]l _< M}, so cI(TM) -- TM. Consequently f C Td.M C T(Fw) which 
contradicts (4). Q.E.D. 
3.2 Proof of Theorem 2. 
We consider the FF-network (1) for which there exists a finite set V C R of s points 
R a k' 
such that bw,,,i(x)  V for every w  , 1 < i < and x  X. It is sufficient 
to show that the set T(Fw) of all functions computable by Fw is not dense in L/if 
k  < N . Actually, we can write T(Fw) as a union 
IJ Lw
