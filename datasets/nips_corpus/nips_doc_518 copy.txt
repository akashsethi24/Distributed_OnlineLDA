Structural Risk Minimization 
for Character Recognition 
I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla 
AT&T Bell Laboratories 
Holmdel, NJ 07733, USA 
Abstract 
The method of Structural Risk Minimization refers to tuning the capacity 
of the classifier to the available amount of training data. This capac- 
ity is influenced by several factors, including: (1) properties of the input 
space, (2) nature and structure of the classifier, and (3) learning algorithm. 
Actions based on these three factors are combined here to control the ca- 
pacity of linear classifiers and improve generalization on the problem of 
handwritten digit recognition. 
I RISK MINIMIZATION AND CAPACITY 
1.1 EMPIRICAL RISK MINIMIZATION 
A common way of training a given classifier is to adjust the parameters w in the 
classification function F(x, w) to minimize the Irathing error Etrain, i.e. the fre- 
quency of errors on a set of p training examples. Etrain estimates the expected risk 
based on the empirical data provided by the p available examples. The method is 
thus called Empirical Risk Minimization. But the classification function F(x, w*) 
which minimizes the empirical risk does not necessarily minimize the generalization 
error, i.e. the expected value of the risk over the full distribution of possible inputs 
and their corresponding outputs. Such generalization error Egene cannot in general 
be computed, but it can be estimated on a separate test set (Etest). Other ways of 
471 
472 
Guyon, Vapnik, Boser, Bottou, and Solla 
estimating Eoe.e include the leave-one-ou! or moving control method [Vap82] (for 
a review, see [Moo92]). 
1.2 CAPACITY AND GUARANTEED RISK 
Any family of classification functions {F(x, w)} can be characterized by its capacity. 
The Vapnik-Chervonenkis dimension (or VC-dimension) [Vap82] is such a capacity, 
defined as the maximum number h of training examples which can be learnt without 
error, for all possible binary labelings. The VC-dimension is in some cases simply 
given by the number of free parameters of the classifier, but in most practical cases 
it is quite difficult to determine it analytically. 
The VC-theory provides bounds. Let {F(x, w)} be a set of classification functions 
of capacity h. With probability (1 - r/), for a number of training examples p > h, 
simultaneously for all classification functions F(x, w), the generalization error Eoene 
is lower than a guaranteed risk defined by: 
Eoaarant = Etrain + e(p, h, Etrain, rl) , 
(1) 
where e(p,h, Etr,,i,, 1)is proportional to e0 = [h(ln2p/h + 1)-rl]/p for small Etr,,i,, 
and to v/ for Etrain close to one [Vap82,Vap92]. 
For a fixed number of training examples p, the training error decreases monoton- 
ically as the capacity h increases, while both guaranteed risk and generalization 
error go through a minimum. Before the minimum, the problem is overdetermined: 
the capacity is too small for the amount of training data. Beyond the minimum 
the problem is underdetermined. The key issue is therefore to match the capacity 
of the classifier to the amount of training data in order to get best generalization 
performance. The method of Structural Risk Minimization (SRM) [Vap82,Vap92] 
provides a way of achieving this goal. 
1.3 STRUCTURAL RISK MINIMIZATION 
Let us choose a family of classifiers {F(x,w)}, and define a structure consisting of 
nested subsets of elements of the family: $1 C $2 C ... C $ C .... By defining 
such a structure, we ensure that the capacity h of the subset of classifiers $ is less 
than h+ of subset $+. The method of SRM amounts to finding the subset S �pt 
for which the classifier F(x,w*) which minimizes the empirical risk within such 
subset yields the best overall generalization performance. 
Two problems arise in implementing SRM: (I) How to select $opt? (II) How to find 
a good structure? Problem (I) arises because we have no direct access to 
In our experiments, we will use the minimum of either Et,t or Ea,,a,-a,t to select 
S �pt, and show that these two minima are very close. A good structure reflects the 
a priori knowledge of the designer, and only few guidelines can be provided from the 
theory to solve problem (II). The designer must find the best compromise between 
two competing terms: Et,-.i. and . Reducing h causes  to decrease, but 
to increase. A good structure should be such that decreasing the VC-dimension 
happens at the expense of the smallest possible increase in training error. We now 
examine several ways in which such a structure can be built. 
Structural Risk Minimization �or Character Recognition 473 
2 
PRINCIPAL COMPONENT ANALYSIS, OPTIMAL 
BRAIN DAMAGE, AND WEIGHT DECAY 
Consider three apparently different methods of improving generalization perfor- 
mance: Principal Component Analysis (a preprocessing transformation of input 
space) [The89], Optimal Brain Damage (an architectural modification through 
weight pruning) [LDS90], and a regularization method, Weight Decay (a modifi- 
cation of the learning algorithm) [Vap82]. For the case of a linear classifier, these 
three approaches are shown here to control the capacity of the learning system 
through the same underlying mechanism: a reduction of the effective dimension of 
weight space, based on the curvature properties of the Mean Squared Error (MSE) 
cost function used for training. 
2.1 LINEAR CLASSIFIER AND MSE TRAINING 
Consider a binary linear classifier F(x, w) = 00(wTx), where w T is the transpose of 
w and the function 00 takes two values 0 and 1 indicating to which class x belongs. 
The VC-dimension of such classifier is equal to the dimension of input space I (or 
the number of weights): h = dim(w) = dim(x) = n. 
The empirical risk is given by: 
p 
= (yk_ 00(wTxk))2 
P k=l 
(2) 
where x  is the k th example, and y is the corresponding desired output. The 
problem of minimizing Etrain as a function of w can be approached in different 
ways [DH73], but it is often replaced by the problem of minimizing a Mean Square 
Error (MSE) cost function, which differs from (2) in that the nonlinear function 00 
has been removed. 
2.2 CURVATURE PROPERTIES OF THE MSE COST FUNCTION 
The three structures that we investigate rely on curvature properties of the MSE 
cost function. Consider the dependence of MSE on one of the parameters wi. 
Training leads to the optimal value w for this parameter. One way of reducing 
the capacity is to set wi to zero. For the linear classifier, this reduces the VC- 
dimension by one: h' = dim(w) - 1 = n - 1. The MSE increase resulting from 
setting wi = 0 is to lowest order proportional to the curvature of the MSE at wt. 
Since the decrease in capacity should be achieved at the smallest possible expense in 
MSE increase, directions in weight space corresponding to small MSE curvature 
are good candidates for elimination. 
The curvature of the MSE is specified by the Hessian matrix H of second derivatives 
of the MSE with respect to the weights. For a linear classifier, the Hessian matrix is 
given by twice the correlation matrix of the training inputs, H = (2/p) --=1 xxT' 
The Hessian matrix is symmetric, and can be diagonalized to get rid of cross terms, 
1We assume, for simplicity, that the first component of vector x is constant and set to 
1, so that the corresponding weight introduces the bias value. 
474 Guyon, Vapnik, Boser, Bottou, and Solla 
to facilitate decisions about the simultaneous elimination of several directions in 
weight space. The elements of the Hessian matrix after diagonalization are the 
eigenvalues ,ki; the corresponding eigenvectors give the principal directions w of 
' 0 takes a 
the MSE. In the rotated axis, the increase AMSE due to setting w i = 
simple form: 
ZXMSE  ,(w?) 2 (3) 
The quadratic approximation becomes an exact equality for the linear classifier. 
' corresponding to small eigenvalues ,ki of H are good candi- 
Principal directions w i 
dates for elimination. 
2.3 PRINCIPAL COMPONENT ANALYSIS 
One common way of reducing the capacity of a classifier is to reduce the dimension 
of the input space and thereby reduce the number of necessary free parameters 
(or weights). Principal Component Analysis (PCA) is a feature extraction method 
based on eigenvalue analysis. Input vectors x of dimension n are approximated by a 
linear combination of m _< n vectors forming an ortho-normal basis. The coefficients 
of this linear combination form a vector x t of dimension m. The optimal basis in 
the least square sense is given by the rn eigenvectors corresponding to the rn largest 
eigenvalues of the correlation matrix of the training inputs (this matrix is 1/2 of H). 
A structure is obtained by ranking the classifiers according to m. The VC-dimension 
of the classifier is reduced to: h' = dim(x') = m. 
2.4 OPTIMAL BRAIN DAMAGE 
For a linear classifier, pruning can be implemented in two different but equivalent 
ways: (i) change input coordinates to a principal axis representation, prune the 
components corresponding to small eigenvalues according to PCA, and then train 
with the MSE cost function; (ii) change coordinates to a principal axis represen- 
tation, train with MSE first, and then prune the weights, to get a weight vector 
w' of dimension rn < n. Procedure (i) can be understood as a preprocessing, 
whereas procedure (ii) involves an a posterJori modification of the structure of the 
classifier (network architecture). The two procedures become identical if the weight 
elimination in (ii) is based on a 'smallest eigenvalue' criterion. 
Procedure (ii) is very reminiscent of Optimal Brain Damage (OBD), a weight prun- 
ing procedure applied after training. In OBD, the best candidates for pruning are 
those weights which minimize the increase AMSE defined in equation (3). The rn 
weights that are kept do not necessarily correspond to the largest rn eigenvalues, 
due to the extra factor of (w?) 2 in equation (3). In either implementation, the 
VC-dimension 
