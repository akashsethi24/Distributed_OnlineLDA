573 
BIT - SERIAL NEURAL NETWORKS 
Alan F. Murray, Anthony V. W. Smith and Zoe F. Buffer. 
Department of Electrical Engineering, University of Edinburgh, 
The King's Buildings, Mayfield Road, Edinburgh, 
Scoff and, EH9 3JL. 
ABSTRACT 
A bit - serial VLSI neural network is described from an initial architecture for a 
synapse array through to silicon layout and board design. The issues surrounding bit 
- serial computation, and analog/digital arithmetic are discussed and the parallel 
development of a hybrid analog/digital neural network is outlined. Learning and 
recall capabilities are reported for the bit - serial network along with a projected 
specification for a 64 - neuron, bit - serial board operating at 20 MHz. This tech- 
nique is extended to a 256 (2562 synapses) network with an update time of 3ms, 
using a paging technique to time - multiplex calculations through the synapse 
array. 
1. INTRODUCTION 
The functions a synthetic neural network may aspire to mimic are the ability to con- 
sider many solutions simultaneously, an ability to work with corrupted data and a 
natural fault tolerance. This arises from the parallelism and distributed knowledge 
representation which gives rise to gentle degradation as faults appear. These func- 
tions are attractive to implementation in VLSI and WSI. For example, the natural 
fault - tolerance could be useful in silicon wafers with imperfect yield, where the 
network degradation is approximately proportional to the non-functioning silicon 
area. 
To cast neural networks in engineering language, a neuron is a state machine that is 
either on or off, which in general assumes intermediate states as it switches 
smoothly between these extrema. The synapses weighting the signals from a 
transmitting neuron such that it is more or less excitatory or inhibitory to the receiv- 
ing neuron. The set of synaptic weights determines the stable states and represents 
the learned information in a system. 
The neural state, Vi, is related to the total neural activity stimulated by inputs to 
the neuron through an activation function, F. Neural activity is the level of excita- 
tion of the neuron and the activation is the way it reacts in a response to a change 
in activation. The neural output state at time t, V[, is related to x! by 
V[ = F(x/) (1) 
The activation function is a squashing function ensuring that (say) Vi is 1 when 
xi is large and -1 when xi is small. The neural update function is therefore straight- 
forward: 
j =n -1 
x: *f= x[ ..... +8  T o V.[ (2) 
where 8 represents the rate of change of neural activity, Tq is the synaptic weight 
and n is the number of terms giving an n - neuron array [1]. 
Although the neural function is simple enough, in a totally interconnected n - neu- 
ron network there are n 2 synapses requiring n 2 multiplications and summations and 
ï¿½ American Institute of Physics 1988 
574 
a large number of interconnects. The challenge in VLSI is therefore to design a sim- 
ple, compact synapse that can be repeated to build a VLSI neural network with 
manageable interconnect. In a network with fixed functionality, this is relatively 
straightforward. If the network is to be able to learn, however, the synaptic weights 
must be programmable, and therefore more complicated. 
2. DESIGNING A NEURAL NETWORK IN VLSI 
There are fundamentally two approaches to implementing any function in silicon - 
digital and analog. Each technique has its advantages and disadvantages, and these 
are listed below, along with the merits and demerits of bit - serial architectures in 
digital (synchronous) systems. 
Digital rs. analog: The primary advantage of digital design for a synapse array is 
that digital memory is well understood, and can be incorporated easily. Learning 
networks are therefore possible without recourse to unusual techniques or technolo- 
gies. Other strengths of a digital approach are that design techniques are advanced, 
automated and well understood and noise immunity and computational speed can 
be high. Unattractive features are that digital circuits of this complexity need to be 
synchronous and all states and activities are quantised, while real neural networks 
are asynchronous and unquantised. Furthermore, digital multipliers occupy a large 
silicon area, giving a low synapse count on a single chip. 
The advantages of analog circuitry are that asynchronous behaviour and smooth 
neural activation are automatic. Circuit elements can be small, but noise immunity 
is relatively low and arbitrarily high precision is not possible. Most importantly, no 
reliable analog, non - volatile memory technology is as yet readily available. For 
this reason, learning networks lend themselves more naturally to digital design and 
implementation. 
Several groups are developing neural chips and boards, and the following listing 
does not pretend to be exhaustive. It is included, rather, to indicate the spread of 
activity in this field. Analog techniques have been used to build resistor / opera- 
tional amplifier networks [2, 3] similar to those proposed by Hopfield and Tank [4]. 
A large group at Caltech is developing networks implementing early vision and 
auditory processing functions using the intrinsic nonlinearities of MOS transistors in 
the subthreshold regime [5, 6]. The problem of implementing analog networks with 
electrically programmable synapses has been addressed using CCD/MNOS technol- 
ogy [7]. Finally, Garth [8] is developing a digital neural accelerator board ('let- 
sire) that is effectively a fast SIMD processor with supporting memory and com- 
munications chips. 
Bit - serial rs. bit - parallel: Bit - serial arithmetic and communication is efficient 
for computational processes, allowing good communication within and between 
VLSI chips and tightly pipelined arithmetic structures. It is ideal for neural net- 
works as it minimises the interconnect requirement by eliminating multi - wire 
busses. Although a bit - parallel design would be free from computational latency 
(delay between input and output), pipelining makes optimal use of the high bit - 
rates possible in serial systernx, and makes for efficient circuit usage. 
2.1 An asynchronous pulse stream VLSI neural network: 
In addition to the digital system that forms the substance of this paper, we are 
developing a hybrid analog/digital network family. This work is outlined here, and 
has been reported in greater detail elsewhere [9, 10, 11]. The generic (logical and 
layout) architecture of a single network of n totally interconnected neurons is shown 
575 
schematically in figure 1. Neurons are represented by circles, which signal their 
states, Vi upward into a matrix of synaptic operators. The state signals are con- 
neeted to a n - bit horizontal bus running through the synaptic array, with a con- 
nection to each synaptic operator in every column. All columns have n operators 
(denoted by squares) and each operator adds its synaptic contribution, T 0 Vj, to the 
running total of activity for the neuron i at the foot of the column. The synaptic 
function is therefore to multiply the signalling neuron state, Vi, by the synaptic 
weight, T0, and to add this product to the running total. This architecture is com- 
mon to both the bit - serial and pulse - stream networks. 
I I _1 I 
Synapse 
States { Vj } 
Neurons 
Figure 1. Generic architecture for a network of n totally interconnected neurons. 
This type of architecture has many attractions for implementation in 2 - dimensional 
j =n -1 
silicon as the summation  To V  is distributed in space. The interconnect 
requirement (n inputs to each neuron) is therefore distributed through a column, 
reducing the need for long - range wiring. The architecture is modular, regular and 
can be easily expanded. 
In the hybrid analog/digital system, the circuitry uses a pulse stream signalling 
method similar to that in a natural neural system. Neurons indicate their state by 
the presence or absence of pulses on their outputs, and synapfic weighting is 
achieved by time - chopping the presynaptic pulse stream prior to adding it to the 
postsynaptic activity summation. It is therefore asynchronous and imposes no fun- 
damental limitations on the activation or neural state. Figure 2 shows the pulse 
stream mechanism in more detail. The synaptic weight is stored in digital memory 
local to the operator. Each synaptic operator has an excitatory and inhibitory pulse 
stream input and output. The resultant product of a synaptic operation, To V, is 
added to the running total propagating down either the excitatory or inhibitory 
channel. One binary bit (the MSBit) of the stored T 0 determines whether the con- 
tribution is excitatory or inhibitory. 
The incoming excitatory and inhibitory pulse stream inputs to a neuron are 
integrated to give a neural activation potential that varies smoothly from 0 to 5 V. 
This potential controls a feedback loop with an odd number of logic inversions and 
576 
EXe. Znh. EXe. 
.. ! ,.,,,, 
Yi 
Figure 2. Pulse stream arithmetic. Neurons are denoted by C) and synaptic operators 
byE. 
thus forms a switched ring - oscillator. If the inhibitory input dominates, the feed- 
back loop is broken. If excitatory spikes subsequently dominate at the input, the 
neural activity rises to 5V and the feedback loop oscillates with a period determined 
by a delay around the loop. The resultant periodic waveform is then converted to a 
series of voltage spikes, whose pulse rate represents the neural state, Vi. Interest- 
ingly, a not dissimilar technique is reported elsewhere in this volume, although the 
synapse function is executed differently [12]. 
3. A 5 - STATE BIT - SERIAL NEURAL NETWORK 
The overall architecture of the 5 - state bit - serial neural network is identical to 
that of the pulse stream network. It is an array of n 2 interconnected synchronous 
synaptic operators, and whereas the pulse stream met
