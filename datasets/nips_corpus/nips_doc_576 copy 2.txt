Holographic Recurrent Networks 
Tony A. Plate 
Department of Computer Science 
University of Toronto 
Toronto, M5S 1A4 Canada 
Abstract 
Holographic Recurrent Networks (HRNs) are recurrent networks 
which incorporate associative memory techniques for storing se- 
quential structure. HRNs can be easily and quickly trained using 
gradient descent techniques to generate sequences of discrete out- 
puts and trajectories through continuous space. The performance 
of HRNs is found to be superior to that of ordinary recurrent net- 
works on these sequence generation tasks. 
I INTRODUCTION 
The representation and processing of data with complex structure in neural networks 
remains a challenge. In a previous paper [Plate, 1991b] I described Holographic Re- 
duced Representations (HRRs) which use circular-convolution associative-memory 
to embody sequential and recursire structure in fixed-width distributed represen- 
tations. This paper introduces Holographic Recurrent Networks (HRNs), which 
are recurrent nets that incorporate these techniques for generating sequences of 
symbols or trajectories through continuous space. The recurrent component of 
these networks uses convolution operations rather than the logistic-of-matrix-vector- 
product traditionally used in simple recurrent networks (SRNs) [Elman, 1991, 
Cleeremans el al., 1991]. 
The goals of this work are threefold: (1) to investigate the use of circular-convolution 
associative memory techniques in networks trained by gradient descent; (2) to see 
whether adapting representations can improve the capacity of HRRs; and (3) to 
compare performance of HRNs with SRNs. 
34 
Holographic Recurrent Networks 35 
1.1 RECURRENT NETWORKS & SEQUENTIAL PROCESSING 
SRNs have been used successfully to process sequential input and induce finite 
state grammars [Elman, 1991, Cleeremans et al., 1991]. However, training times 
were extremely long, even for very simple grammars. This appeared to be due to the 
difficulty of findi a recurrent operation that preserved sufficient context [Maskara 
and Noetzel, 1992]. In the work reported in this paper the task is reversed to be 
one of generating sequential output. Furthermore, in order to focus on the context 
retention aspect, no grammar induction is required. 
1.2 CIRCULAR CONVOLUTION 
Circular convolution is an associative memory operator. The role of convolution 
in holographic memories is analogous to the role of the outer product operation in 
matrix style associative memories (e.g., Hopfield nets). Circular convolution can be 
viewed as a vector multiplication operator which maps pairs of vectors to a vector 
(just as matrix multiplication maps pairs of matrices to a matrix). It is defined as 
z ---- x(y ' zj ---- -k=0 ykxj-k, where � denotes circular convolution, x, y, and z 
are vectors of dimension n , xi etc. are their elements, and subscripts are modulo-n 
(so that x-2 -= x,_2). Circular convolution can be computed in O(nlogn) using 
Fast Fourier Transforms (FFTs). Algebraically, convolution behaves like scalar 
multiplication: it is commutative, associative, and distributes over addition. The 
identity vector for convolution (I) is the impulse vector: its zero'th element is 1 
and all other elements are zero. Most vectors have an inverse under convolution, 
i.e., for most vectors x there exists a unique vector y (-x -1) such that x�y - I. 
For vectors with identically and independently distributed zero mean elements and 
an expected Euclidean length of 1 there is a numerically stable and. simply derived 
approximate inverse. The approximate inverse of x is denoted by x and is defined 
by the relation xj - x,_j. 
Vector pairs can be associated by circular convolution. Multiple associations can 
be summed. The result can be decoded by convolving with the exact inverse or 
approximate inverse, though the latter generally gives more stable results. 
Holographic Reduced Representations [Plate, 1991a, Plate, 1991b] use circular con- 
volution for associating elements of a structure in a way that can embody hierar- 
chical structure. The key property of circular convolution that makes it useful for 
representing hierarchical structure is that the circular convolution of two vectors is 
another vector of the same dimension, which can be used in further associations. 
Among associative memories, holographic memories have been regarded as inferior 
because they produce very noisy results and have poor error correcting properties. 
However, when used in Holographic Reduced Representations the noisy results can 
be cleaned up with conventional error correcting associative memories. This gives 
the best of both worlds - the ability to represent sequential and recursive structure 
and clean output vectors. 
2 TRAJECTORY-ASSOCIATION 
A simple method for storing sequences using circular convolution is to associate 
elements of the sequence with points along a predetermined trajectory. This is akin 
36 Plate 
to the memory aid called the method of loci which instructs us to remember a list 
of items by associating each term with a distinctive location along a familiar path. 
2.1 STORING SEQUENCES BY TRAJECTORY-ASSOCIATION 
Elements of the seqnence and loci (points) on the trajectory are all represented by 
n-dimensional vectors. The loci are derived from a single vector k - they are its 
successive convolutive powers: k �, k , k 2, etc. The convolutive power is defined in 
the obvious way: k � is the identity vector and k i+ - ki�k. 
The vector k must be chosen so that it does not blow up or disappear when raised 
to high powers, i.e., so that IIkPl] - 1 v p. The class of vectors which satisfy this 
constraint is easily identified in the frequency domain (the range of the discrete 
Fourier transform). They are the vectors for which the magnitude of the power of 
each frequency component is equal to one. This class of vectors is identical to the 
class for which the approximate inverse is equal to the exact inverse. 
Thus, the trajectory-association representation for the sequence abc is 
s.o. = a - b�k - c�k  . 
2.2 DECODING TRAJECTORY-ASSOCIATED SEQUENCES 
Trajectory-associated sequences can be decoded by repeatedly convolving with the 
inverse of the vector that generated the encoding loci. The results of decoding 
summed convolution products are very noisy. Consequently, to decode trajectory 
associated sequences, we must have all the possible sequence elements stored in an 
error correcting associative memory. I call this memory the clean up memory. 
For example, to retrieve the third element of the sequence s,bc we convolve twice 
with k -1 , which expands to a�k -2 + b�k -1 + c. The two terms involving powers 
of k are unlikely to be correlated with anything in the clean up memory. The most 
similar item in clean up memory will probably be c. The clean up memory should 
recognize this and output the clean version of c. 
2.3 CAPACITY OF TRAJECTORY-ASSOCIATION 
In [Plate, 1991a] the capacity of circular-convolution based associative memory was 
calculated. It was asstimed that the elements of all vectors (dimension n) were 
chosen randomly from a gaussian distribution with mean zero and variance 1/n 
(giving an expected Euclidean length of 1.0). Quite high dimensional vectors were 
required to ensure a low probability of error in decoding. For example, with 512 
element vectors and 1000 items in the clean tip memory, 5 pairs can be stored with 
a 1% chance of an error in decoding. The scaling is nearly linear in n: with 1024 
element vectors l0 pairs can be stored with about a 1% chance of error. This works 
out to a information capacity of about 0.1 bits per element. The elements are real 
nmnbers, but high precision is not required. 
These capacity calculations are roughly applicable to the trajectory-association 
method. They slightly underestimate its capacity because the restriction that the 
encoding loci have unity power in all frequencies restIlts in lower decoding noise. 
Nonetheless this figure provides a usefill benchmark against which to compare the 
capacity of HRNs which adapt vectors using gradient descent. 
Holographic Recurrent Networks 37 
3 TRAJECTORY ASSOCIATION & RECURRENT NETS 
HRNs incorporate the trajectory-association scheme in recurrent networks. HRNs 
are very similar to SRNs, such as those used by tElman, 1991] and [Cleeremans et 
al., 1991]. However, the task used in this paper is different: the generation of target 
sequences at the output units, with inputs that do not vary in time. 
In order to understand the relationship between HRNs and SRNs both were tested 
on the sequence generation task. Several different unit activation functions were 
tried for the SRN: symmetric (tanh) and non-symmetric sigmoid (1/(1 + e-z)) for 
the hidden tinits, and softmax and normalized RBF for the output tinits. The best 
combination was symmetric sigmoid with softmax outputs. 
3.1 ARCHITECTURE 
The HRN and the SRN used in the experiments described here are shown in Fig- 
ure 1. In the ItRN the key layer y contains the generator for the inverse loci 
(corresponding to k -1 in Section 2). The hidden to output nodes implement the 
clean-up memory: the output representation is local and the weights on the links 
to an output unit form the vector that represents the symbol corresponding to that 
tinit. The softmax filnction serves to give maximtim activation to the output tinit 
whose weights are most similar to the activation at the hidden layer. 
The input representation is also local, and input activations do not change during 
the generation of one sequence. Thus the weights from a single input unit determine 
the activations at the code layer. Nets are reset at the beginning of each sequence. 
The HRN computes the following functions. Time superscripts are omitted where 
all are the same. See Figure 1 for symbols. The parameter g is an adaptable input 
gain shared by all output units. 
Code units: 

