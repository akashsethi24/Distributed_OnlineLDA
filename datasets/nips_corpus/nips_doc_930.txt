Dynamic Cell Structures 
Jiirg Bruske and Gerald Sommer 
Department of Cognitive Systems 
Christian Albrechts University at Kiel 
24105 Kiel - Germany 
Abstract 
Dynamic Cell Structures (DCS) represent a family of artificial neural 
architectures suited both for unsupervised and supervised learning. 
They belong to the recently [Martinetz94] introduced class of Topology 
Representing Networks (TRN) which build perfectly topology pre- 
serving feature maps. DCS employ a modified K0honen learning rule 
in conjunction with competitive Hebbian learning. The Kohonen type 
learning rule serves to adjust the synaptic weight vectors while Hebbian 
learning establishes a dynamic lateral connection structure between 
the units reflecting the topology of the feature manifold. In case of super- 
vised learning, i.e. function approximation, each neural unit implements 
a Radial Basis Function, and an additional layer of linear output units 
adjusts according to a delta-rule. DCS is the first RBF-based approxima- 
tion scheme attempting to concurrently learn and utilize a perfectly to- 
pology preserving map for improved performance. 
Simulations on a selection of CMU-Benchmarks indicate that the DCS 
idea applied to the Growing Cell Structure algorithm [Fritzke93] leads 
to an efficient and elegant algorithm that can beat conventional models 
on similar tasks. 
1 Introduction 
The quest for smallest topology preserving maps motivated the introduction of growing 
feature maps like Fritzke's Growing Cell Structures (GCS). In GCS, see [Fritzke93] for de- 
tails, one starts with a k-dimensional simplex of N = k+l neural units and (k + 1) � k?2 
lateral connections (edges). Growing of the network is performed such that after insertion 
498 Jtrg Bruske, Gerald Sommer 
of a new unit the network consists solely of k dimensional simplices again. Thus, like Ko- 
honen's SOM, GCS can only learn a perfectly topology preserving feature map I if k 
meets the actual dimension of the feature manifold. Assuming that the lateral connections 
do reflect the actual topology the connections serve to define a neighborhood for a Kohonen 
like adaptation of the synaptic vectors w i and guide the insertion of new units. Insertion 
happens incrementally and does not necessitate a retraining of the network. The principle 
is to insert new neurons in such a way that the expected value of a certain local error mea- 
sure, which Fritzke calls the resource, becomes equal for all neurons. For instance, the 
number of times a neuron wins the competition, the sum of distances to stimuli for which 
the neuron wins or the sum of errors in the neuron's output can all serve as a resource and 
dramatically change the behavior of GCS. Using different error measures and guiding in- 
sertion by the lateral connections contributes much to the success of GCS. 
The principle of DCS is to avoid any restriction of the topology of the network (lateral con- 
nection scheme between the neural units) but to concurrently learn and utilize a perfectly 
topology preserving ..map. This.is achieved by adapting the lateral connection structure ac- 
cording to a competitive Hebban learning rule: 
Cu(t + 1) 
rnax {Yi . Yj, Cij(t) } : yi . Yj> yk . yt V ( l < k, l < N) 
0 : Cij(t ) <0  (1) 
: otherwise,  
OC/j (t) 
where c, 0 < c < 1 is a forgetting constant, 0, 0 < 0 < 1 serves as a threshold for deleting 
lateral connections, and Yi = R (11v - will) is the activation of the i-th unit with w i as the 
centre of its receptive field on pre'entati6n of stimulus v. R(.) can be any positive continu- 
ously monotonically decreasing function. For batch learning with a training set T of fixed 
size IT1, o = Irlf is a good choice. 
Since the isomorphic representation of the topology of the feature manifold M in the lateral 
connection structure is central to performance, in many situations a DCS algorithm may be 
the right choice. These situations are characterized by missing a priori knowledge of the 
topology of the feature manifold M or a topology of M which cannot be readily mapped to 
the existing models. Of course, if such a priori knowledge is available then models like 
GCS or Kohonen's SOM allowing to incorporate such knowledge have an advantage, es- 
pecially if training data are sparse. 
Note that DCS algorithms can also aid in cluster analysis: In a perfectly topology preserv- 
ing map clusters which are bounded by regions of P(v) = 0 can be identified simply by a 
connected component analysis. However, without prior knowledge about the feature man- 
ifold M it is in principal impossible to check for perfect topology preservation of $. Noise 
in the input data may render perfect topology learning even more difficult. So what can per- 
fect topology learning be used for? The answer is simply that for every set S of reference 
vectors perfect topology learning yields maximum topology preservation 3 with respect to 
this set. And connected components with respect to the lateral connection structure C may 
well serve as an initialization for postprocessing by hierarchical cluster algorithms. 
1. We use the term perfectly topology preserving feature map in accordance with its rigorous 
definition in [Martinetz93]. 
2. In his very recent and recommendable article [Martinetz94] the term Topology Representing 
Network (TRN) is coined for any network employing competitive Hebbian learning for topolo- 
gy learning. 
3. if topology preservation is measured by the topographic function as defined in [Viiimann94]. 
Dynamic Cell Structures 499 
The first neural algorithm attempting to learn perfectly topology preserving feature maps 
is the Neural Gas algorithm ofT. Martinetz [Martinetz92]. However, unlike DCS the Neu- 
ral Gas does not further exploit this information: In every step the Neural Gas computes 
the k nearest neighbors to a given stimulus and, in the supervised case, employs all of them 
for function approximation. DCS avoids this computational burden by utilizing the lateral 
connection structure (topology) learned so far, and it restricts interpolation between acti- 
vated units to the submanifold of the current stimulus. 
Applying the principle of DCS to Fritzke's GCS yields our DCS-GCS algorithm. This al- 
gorithm sticks very closely to the basic structure of its ancestor GCS except the predefined 
k-dimensional simplex connection structure being replaced by perfect topology learning. 
Besides the conceptual advantage of perfect topology learning, DCS-GCS does decrease 
overhead (Fritzke has to handle quite sophisticated data structures in order to maintain the 
k-dimensional simplex structure after insertion/deletion of units) and can be readily imple- 
mented on any serial computer. 
2 Unsupervised DCS-GCS 
The unsupervised DCS-GCS algorithm starts with initializing the network (graph) to two 
neural units (vertices) n I and n 2. Their weight vectors w l, w 1 (centres of receptive fields) 
are set to points v l, v 2  M which are drawn from M according to P(v). They are connected 
by a lateral connection of weight C = C = 1 Note that lateral connections in DCS 
� 2 21 . � 
are always bidirectional and have symmetric weights. 
Now the algorithm enters its outer loop which is repeated until some stopping criterium is 
fulfilled. This stopping criterium could for instance be a test whether the quantization error 
has already dropped below a predefined accuracy. 
The inner loop is repeated for ) times. In off-line learning ) can be set to the number ex- 
amples in the training set T. In this case, the inner loop just represents an epoch of training. 
Within the inner loop, the algorithm first draws an input stimulus v  M from M according 
to P(v) and then proceeds to calculate the two neural units which weight vectors are first 
and second closest to v. 
In the next step, the lateral connections between the neural units are modified according to 
eq. (1), the competitive Hebbian learning rule. As has already been mentioned, in off-line 
learning it is a good idea to set o = . 
Now the weight vectors w i of the best matching unit and its neighbors are adjusted in a 
Kohonen like fashion: 
AWbmu - �B (12 -- Wbmu) and Awj - �Nh (12 - wj) , (2) 
where the neighborhood Nh (j) of a unit j is defined by 
Sh(j) - {il(CjiO,l <i<S)} . 
The inner loop ends with updating the resource value of the best matching unit. The re- 
source of a neuron is a local error measure attached to each neural unit. As has been pointed 
out, one can choose alternative update functions corresponding to different error measures. 
For our experiments (section 2.1 and section 3.1) we used the accumulated squared distance 
to the stimulus, i.e. A'gbm u - IIv- Wbmull ' 
The outer loop now proceeds by adding a new neural unit r to the network. This unit is lo- 
cated in-between the unit I with largest resource value and its neighbor n with second larg- 
est resource value: 4 
500 JOrg Bruske, Gerald Sommer 
The exact location of its centre of receptive field w r is calculated according to the ratio of 
the resource values x l, x n , and the resource values of units n and I are redistributed among 
r, n and h 
1 1 
� '- 'lJn/('lJ n q- 'll) , AX l '- , ( 1 -- t) ,1 l and AX n -' ,t n (3) 
w r '- w I q- �(w n - Wl) , X r '- A n q- AX t , X t = X l - AX t and '1 n '- X n -- A n . (4) 
This gives an estimate of the resource values if the new unit had been in the network right 
from the start. Finally the lateral connections are changed, 
Crl '- Clr '- 1, Crn '- Crn --- 1 and Cnl ' Cln -' 0, (5) 
connecting unit r to unit l and disconnecting n and 1. 
This heuristic guided by the lateral connection structure and the resource values promises 
insertion of new units at good initial positions. It is responsible for the better performance 
of DCS-GCS and GCS compared to algorithms which do not exploit the neighborhood re- 
lation between existing units. 
The outer loop closes by decrementin
