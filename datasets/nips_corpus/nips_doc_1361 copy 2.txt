On the infeasibility of training neural 
networks with small squared errors 
Van H. Vu 
Department of Mathematics, Yale University 
vuha@math.yale.edu 
Abstract 
We demonstrate that the problem of training neural networks with 
small (average) squared error is computationally intractable. Con- 
sider a data set of M points (Xi, Y/), i - 1, 2,..., M, where Xi are 
input vectors from R d, Y/ are real outputs (Y/ & R). For a net- 
M 
work f0 in some class : of neural networks, (/4) Ei_-(f0(Xi)- 
)2)/.._ in f fe:(1/M) -.iM__(f(Xi )_ yi)2)1[2 is the (avarage) rel- 
ative error occurs when one tries to fit the data set by f0. We will 
prove for several classes Y of neural networks that achieving a rela- 
tive error smaller than some fixed positive threshold (independent 
from the size of the data set) is NP-hard. 
1 Introduction 
Given a data set (Xi, ), i = 1, 2,..., M, Xi are input vectors from R d, Y/are real 
outputs (Y  R). We call the points (Xi, Y) data points. The training problem 
for neural networks is to find a network from some class (usually with fixed number 
of nodes and layers), which fits the data set with small error. In the following we 
describe the problem with more details. 
Let Y be a class (set) of neural networks, and c be a metric norm in /M. To 
each f  , associate an error vector Ef - (]f(Xi) - ])/M__ (E: depends on the 
data set, of course, though we prefer this notation to avoid difficulty of having too 
many subindices). The norm of Ef in c shows how well the network f fits the data 
regarding to this particular norm. Furthermore, let e,: denote the smallest error 
achieved by a network in , namely: 
In this context, the training problem we consider here is to find f   such that 
372 V. H. Vu 
[]ES]] - e,: _< e:, where �: is a positive number given in advance, and does not 
depend on the size M of the data set. We will call (: relative error. The norm a 
is chosen by the nature of the training process, the most common norms are: 
l norm: [Iv[] = maz[vi (interpolation problem) 
l: norm: IIvll = vY) whe=e v = (Xet square error prob- 
lem). 
The qutity I IEy][: is usually referred to as the emperical error of the training 
process. The first goal of this paper is to show that achieving small emperical error 
is NP-hard. From now on, we work with l: norm, if not otherwise specified. 
A question of great importance is: given the data set,  and ey in advance, could 
one find an ecient algorithm to solve the training problem formulated above. By 
eciency we mean an algorithm terminating in polynomial time (polynomial in the 
size of the input). This question is closely related to the problem of learning neural 
networks in polynomial time (see [3]). The input in the algorithm is the data set, 
by its size we means the number of bits required to write down all (Xi, ). 
Question 1. Given  and e y and a dala set. Could one find an ecienl algorithm 
which produces a funelton f   such that IlEsll < e + e 
Question i is very dicult o answer in general. In his paper we will investigate 
he following importan sub-question: 
Question 2. Can one achieve arbitrary small relative error using polynomial algo- 
rithms ? 
Our purpose is to give a negative answer for Question 2. This question was posed 
by L. Jones in his seminar at Yale (1996). The crucial point here is that we are 
dealing with 12 norm, which is very important from statistical point of view. Our 
investigation is also inspired by former works done in [2], [6], [7], etc, which show 
negative results in the l norm case. 
Definition. A positive number e is a threshold of a class ' of neural networks if 
the training problem by networks from .T with relative error less than � is NP-hard 
(i.e., computationally infeasible). 
In order to provide a negative answer to Question 2, we are going to show the 
existence of thresholds (which is independent from the size of the data set) for the 
following classes of networks. 
� : {flf(x) - (1/n)(Zi step(aix -- Oi)} 
� ': {fiX(x) - (i=x cistep(aix - hi)} 
� 6n -- {gig(x) = -i ci(i(aix - hi)) 
where n is a positive integer, step(x) = i if x is positive and zero otherwise, ai and 
x are vectors from R d, bi are real numbers, and ci are positive numbecs. It is clear 
that the class Y contains Y; the reason why we distinguish these two cases is that 
the proof for  is relatively easy to present, while contains the most important 
ideas. In the third class, the functions bi are sigmoid functions which satisfy certain 
Lipchitzian conditions (for more details see [9]) 
Main Theorem 
(i) The classes 'l, '2, ' and ( have absolute constant (positive) thresholds 
On the Infeasibility of Training Neural Networks with Small Squared Errors 373 
(it) For every class .Tr,+2, n > O, there is a threshold of form 
(iii) For every .T'+2, n > 0, there is a threshold of form 
(iv) For every class +2, n > 0, there is a threshold of form (n-5/2d -/2. 
In the last three statements, ( is an absolute positive constant. 
Here is the key argument of the proof. Assume that there is an algorithm A which 
solves the training problem in some class (say  ) with relative error . From 
some (properly chosen) NP-hard problem. we will construct a data set so that if e 
is sufficiently small, then the solution found by A (given the constructed data set 
as input) in ., implies a solution for the original NP-hard problem. This will give 
a lower bound on e, if we assume that the algorithm A is polynomial. In all proofs 
the leading parameter is d (the dimension of data inputs). So by polynomial we 
mean a polynomial with d as variable. All the input (data) sets constructed will 
have polynomial size in d. 
The paper is organized as follow. In the next Section, we discuss earlier results 
concerning the l norm. In Section 3, we display the NP-hard results we will use in 
the reduction. In Section 4, we prove the main Theorem for class '2 and mention 
the method to handle more general cases. We conclude with some remarks and 
open questions in Section 5. 
To end this Section, let us mention one important corollary. The Main Theorem 
implies that learning ., .t and  (with respect to l norm) is hard. For more 
about the connection between the complexity of training and learning problems, we 
refer to [3], [5]. 
Notation: Through the paper Ud denotes the unit hypercube in R d. For any 
number z, zd denotes the vector (z, x, .., z) of length d. In particular, 0d denotes 
the origin of R . For any half space H, r is the complement of H. For any set A, 
is the number of elements in A. A function y(d) is said to have order of magnitude 
O(F(d)), if there are c < C positive constants such that c < y(d)/F(d) < C for all 
d. 
2 Previous works in the loo case 
The case ct = loo (interpolation problem) was considered by several authors for 
many different classes of (usually) 2-layer networks (see [6],[2], [7], [8]). Most of the 
authors investigate the case when there is a perfect fit, i.e., �l,x = 0. In [2], the 
authors proved that training 2-layer networks containing 3 step function nodes with 
zero relative error is NP-hard. Their proof can be extended for networks with more 
inner nodes and various logistic output nodes. This generalized a former result of 
Maggido [8] on data set with rational inputs. Combining the techniques used in 
[2] with analysis arguments, Lee Jones [6] showed that the training problem with 
relative error 1/10 by networks with two monotone Lipschitzian Sigmoid inner nodes 
and linear output node, is also NP-hard (NP-complete under certain circumstances). 
This implies a threshold (in the sense of our definition) (1/10)M -/2 for the class 
examined. However, this threshold is rather weak, since it is decreasing in M. This 
result was also extended for the n inner nodes case [6]. 
It is also interesting to compare our results with Judd's. In [7] he considered the 
following problem Given a network and a set of training examples (a data set), 
does there exist a set of weights so that the network gives correct output for all 
training examples ? He proved that this problem is NP-hard even if the network is 
374 V.H. Vu 
required to produce the correct output for two-third of the traing examples. In fact, 
it was shown that there is a class of networks and a data sets so that any algorithm 
will produce poorly on some networks and data sets in the class. However, from 
this result one could not tell if there is a network which is hard to train for all 
algorithms. Moreover, the number of nodes in the networks grows with the size of 
the data set. Therefore, in some sense, the result is not independent from the size 
of the data set. 
In our proofs we will exploit many techniques provided in these former works. The 
crucial one is the reduction used by A. Blum and R. Rivest, which involves the 
NP-hardness of the Hypergraph 2-Coloring problem. 
3 Some NP hard problems 
Definition Let B be a CNF formula, where each clause has at most k literals. 
Let max(B) be the maximum number of clauses which can be satisfied by a truth 
assignment. The APP MAX k-SAT problem is to find a truth assignment which 
satisfies (1 - e)max(B) clauses. 
The following Theorem says that this approximation problem is NP -hard, for some 
small e. 
Theorem 3.1.1 Fix k _> 2. There is e > O, such that finding a truth assignment. 
which satisfies at least (1- e)max(B) clauses is NP-hard. 
The problem is still hard, when every literal in B appears in only few clauses, and 
every clause contains only few literals. Let Ba(5) denote the class of CNFs with at 
most 3 literals in a clause and every literal appears in at most 5 clauses (see [1]). 
Theorem 3.1.2 There is e2 > 0 such that finding a truth assignment, which satisfies 
at least (1 - e)max(B) clauses in a formula B  B3(5) is NP-hard. 
The optimal thresholds in these theorems can be computed, due to recent resu
