Oriented Non-Radial Basis Functions for Image 
Coding and Analysis 
Avijit Saha 1 Jim Christian D.S. Tang 
Microelectronics and Computer Technology Corporation 
3500 West Balcones Center Drive 
Austin, TX 78759 
Chuan-Lin Wu 
Department of Electrical and Computer Engineering 
University of Texas at Austin, 
Austin, TX 78712 
ABSTRACT 
We introduce oriented non-radial basis function networks (ONRBF) 
as a generalization of Radial Basis Function networks (RBF)- wherein 
the Euclidean distance metric in the exponent of the Gaussian is re- 
placed by a more general polynomial. This permits the definition of 
more general regions and in particular- hyper-ellipses with orienta- 
tions. In the case of hyper-surface estimation this scheme requires a 
smaller number of hidden units and alleviates the curse of dimen- 
sionality associated kernel type approximators.In the case of an im- 
age, the hidden units correspond to features in the image and the 
parameters associated with each unit correspond to the rotation, scal- 
ing and translation properties of that particular feature. In the con- 
text of the ONBF scheme, this means that an image can be 
represented by a small number of features. Since, transformation of an 
image by rotation, scaling and translation correspond to identical 
transformations of the individual features, the ONBF scheme can be 
used to considerable advantage for the purposes of image recognition 
and analysis. 
1 INTRODUCTION 
Most, neural network or connectionist models have evolved primarily as adaptive 
function approximators. Given a set of input-output pairs <x,y> (x from an underlying 
function f (i.e. y = fix)), a feed forward, time-independent neural network estimates a 
1. Alternate address: Dept. of ECE, Univ. of Texas at Austin, Austin, TX 78712 
728 
Oriented Non-Radial Basis Functions for Image Coding and Analysis 729 
function y' = g(p,x) such that E= p(y - y') is arbitrarily small over all <x,y> pairs. Here, p 
is the set of parameters associated with the network model and p is a metric that measures 
the quality of approximation, usually the Euclidean norm. In this paper, we shall restrict 
our discussion to approximation of real valued functions of the form f:R n -> R. For a net- 
work of fixed structure (determined by g), all or part of the constituent parameter set p, 
that minimize E are determined adaptively by modifying the set of parameters. The prob- 
lem of approximation or hypersurface reconstruction is then one of determining what class 
of g to use, and then the choice of a suitable algorithm for determining the parameters p- 
given a set of samples {<x,y>).By far the most popular method for determining network 
parameters has been the gradient descent method. If the error surface is quadratic or con- 
vex, gradient descent methods will yield an optimal value for the network parameter- 
s.However, the burning problem in still remains the determination of network parameters 
when the error function is infested with local minimas. One way of obviating the problem 
of local minimas is to match a network architecture with an objective function such that 
the error surface is free of local minimas. However, this might limit the power of the net- 
work architecture such as in the case of linear perceptrons[1]. Another approach is to ob- 
tain algebraic transformations of the objective functions such that algorithms can be 
readily designed around the transformed functions to avoid local minimas. Random opti- 
mization method of Matyas and its variations have been studied recently [2], as alternate 
avenues for detennining the parameter set p. Perhaps the most probable reason for the BP 
algorithms popularity is that the error surface is relatively smooth [1],[3] 
The problem of local minimas is circumvented somewhat differently in local or kernel 
type estimators. The input space in such a method is partitioned into a number of local re- 
gions and if the number of regions defined is sufficiently large, then the output response in 
each local region is sufficiently uniform or smooth and the error will remain bounded i.e. a 
local minima will be close to the global minima. The problem with kernel type of esthna- 
tors is that the number of bins, kemels or regions that need to be defined increases 
exponentially with the dimension of the input space. An improvement such as the one con- 
sidered by [4] is to define the kemels only in regions of the input space where there is data. 
However, our experiments indicate that even this may not be sufficient to lift the curse of 
dimensionality. If instead of limiting the shape of the kemels to be boxes or even hyper- 
spheres we select the kernels to be shapes defined by a second order polynomials then a 
larger class of shapes or regions can be defined resulting in significant reductions in the 
number of kernels required. This was the principal motivation behind our generalization 
of ordinary RBF networks. Also, we have determined that radial basis function networks 
will, given sufficiently large widths, linearize the output response between two hidden 
units. This gives rise to hyperacuity or coarse coding, whereby a high resolution of stimuli 
can be observed at the signal level despite poor resolution in the sensor array. In the con- 
text of function approximation this means that if the hyper-surface being approximated 
varies linearly in a certain region, the output behavior can be captured by suitably placing 
a single widely tuned receptive field in that region. Therefore, it is advantageous to choose 
the regions with proper knowledge of the output response in that region as opposed to 
choosing the bins based on the inputs alone. These were some of the principal motivations 
for our generalization. 
In addition to the architectural and learning issues, we have been concerned with approx- 
imation schemes in which the optimal parameter values have readily interpretable forms 
that may allow other useful processing elsewhere. In the following section we present 
ONBF as a generalization to RBF [4] and GRBF [5]. We show how rotation, scaling and 
730 Saha, Christian, 2mg, and Wu 
translation (center) information of these regions can be readily extracted from the parame- 
ter values associated with each hidden unit. In subsequent sections we present experimen- 
tal results illustrating the performance of ONRBF as a function approximator and 
feasibility of ONRBF for the purposes of image coding and analysis. 
2 ORIENTED NON-RADIAL BASIS FUNCTION NETWORKS 
Radial Basis Function networks can be described by the formula: 
k 
f(x) =  wcRc(x) 
Ct=0 
where fix) is the output of the network, k is the number of hidden units, w is the weight 
associated with hidden unit at, and P(x) is the response of unit at, The response P(x) of 
unit at is given by 
R(x=c 
Poggio and Girosi [5] have considered the generalization where a different width parame- 
ter o, is associated with each input dimension i. The response function P is then defined 
as 
i=l ai 
R(x(x) = � 
Now each o, s can influence the response of the atth unit and the effect is that widths associ- 
ated with irrelevant or correlated inputs will tend to be increased. It has been shown that if 
one of the input components has a random input and a constant width (constant for that 
particular dimension) is used for each receptive field, then the width for that particular re- 
ceptive field is maximum [6]. 
The generalization we consider in this paper is a further shaping of the response P by 
composing it with a rotation function S designed to rotate the unit about its center in d- 
space, where d is the input dimension. This composition can be represented compactly by 
a response function of the form: 
-II M[x, ..... x d,1 2 
Rct=e 
where M is a d by d+l matrix. The matrix transforms the input vectors and these transfor- 
mations correspond to translation (center information), scaling and rotation of the input 
vectors. The response function presented above is the restricted form of a more general re- 
sponse function of the form: 
Rot = e-[P(x)] 
where the exponent is a general polynomial in the input variables. In the following sec- 
tions we present the learning rules and we show how center, rotation and scaling informa- 
tion can be extracted from the matrix elements. We do this for the case when the input 
dimension is 2 (as is the case for 2-dimensional images) but the results are generalized 
easily. 
Oriented Non-Radial Basis Functions for Image Coding and Analysis 731 
2.1 LEARNING RULES 
Consider the n-dimensional case where <x,...x.> represents the input vector and 
represents the matrix element of the j' row and k' column of the matrix M associated 
with the c' unit. Then the response of the c' unit is given by: 
Rct(x.y) = �'l,i-, maxidd 
The total stun square error over b patterns is given by: 
TE  [f(xl)- F (xl)] 2 
Then the derivative of the error due to the I h pattern with respect to the matrix element 
mmj of the W h unit is given by: 
i) (Ei) 2[f( F(x) ]_ i) f = 2[L] i) f 
'Om.. = x)- Om.. 
lj lj Ij 
and: 
 .f = -2met ' 1)TxjRa(xl) 
3m..  (xl' 
ij 
where, 
nk6' is the i th row of the matrix corresponding to the c th unit 
x' is the input vector 
xj' is the jth variable in the input space. 
Then the update role for the matrix elements with learning rate 1 is given by: 
t+l t 
mij = mij-q'..(EiQ 
ij 
and the leaming role for the weights wc is given by: 
t+l t 
wc = wc-LRc(xi) 
2.2 EXTRACTING ROTATION, SCALE AND CENTER VALUES 
In this section we present the equations for extracting the rotation, translation and scal- 
ing values (widths) of the cz th receptive field from its associated matrix elements. We 
present these for the special case when n the input dimension is equal to 2, since that is 
the case for images. The input vector x is represented by <x,y> and the rules for con- 
vertin
