31 
AN ARTIFICIAL NEURAL NETWORK FOR SPATIO- 
TEMPORAL BIPOLAR PATTERNS: APPLICATION TO 
PHONEME CLASSIFICATION 
Toshiteru Homma 
Les E. Atlas 
Robert J. Marks H 
Interactive Systems Design Laboratory 
Department of Electrical Engineering, FT-10 
University of Washington 
Seattle, Washington 98195 
ABSTRACT 
An artificial neural network is developed to recognize spatioqemporal 
bipolar patterns associatively. The function of a formal neuron is generalized by 
replacing multiplication with convolution, weights with transfer functions, and 
thresholding with nonlinear transform following adaptation. The Hebbian learn- 
ing rule and the delta learning rule are generalized accordingly, resulting in the 
learning of weights and delays. The neural network which was first developed 
for spatial patterns was thus generalized for spario-temporal patterns. It was 
tested using a set of bipolar input patterns derived from speech signals, showing 
robust classification of 30 model phonemes. 
1. INTRODUCTION 
Learning spatio-temporal (or dynamic) patterns is of prominent importance in biological 
systems and in artificial neural network systems as well. In biological systems, it relates to such 
issues as classical and operant conditioning, temporal coordination of sensorimotor systems and 
temporal reasoning. In artificial systems, it addresses such real-world tasks as robot control, 
speech recognition, dynamic image processing, moving target detection by sonars or radars, EEG 
diagnosis, and seismic signal processing. 
Most of the processing elements used in neural network models for practical applications 
have been the formal neuron 1 or its variations. These elements lack a memory flexible to tem- 
poral patterns, thus limiting most of the neural network models previously proposed to problems 
of spatial (or static) patterns. Some past solutions have been to convert the dynamic problems to 
static ones using buffer (or storage) neurons, or using a layered network with/without feedback. 
We propose in this paper to use a dynamic formal neuron as a processing element for 
learning dynamic patterns. The operation of the dynamic neuron is a temporal generalization of 
the formal neuron. As shown in the paper, the generalization is straightforward when the activa- 
tion part of neuron operation is expressed in the frequency domain. Many of the existing learn- 
ing rules for static patterns can be easily generalized for dynamic patterns accordingly. We show 
some examples of applying these neural networks to classifying 30 model phonemes. 
American Institute of Physics 1988 
32 
2. FORMAL NEURON AND DYNAMIC FORMAL NEURON 
The formal neuron is schematically drawn in Fig. l(a), where 
Input 
Activation 
Output 
Transmittance 
Node operator 
Neuron operation 
= [x x2 � � � xt.] r 
Yi, i = 1,2 ..... N 
zi, i = 1,2 ..... N 
= [wi wi2''' w.] r 
where 1(') is a nonlinear memoryless transform 
(2.1) 
Note that a threshold can be implicitly included as a transmittance from a constant input. 
In its original form of formal neuron, xi  {0,1} and '1(') is a unit step function u (-). A 
variation of it is a bipolar formal neuron where xi  {-1,1} and 11(. ) is the sign function sgn('). 
When the inputs and output are converted to frequency of spikes, it may be expressed as 
xi  R and 11(') is a rectifying function r('). Other node operators such as a sigmoidal function 
may be used. 
We generalize the notion of formal neuron so that the input and output are functions of 
time. In doing so, weights are replaced with transfer functions, multiplication with convolution, 
and the node operator with a nonlinear transform following adaptation as often observed in bio- 
logical systems. 
Fig. l(b) shows a schematic diagram of a dynamic formal neuron where 
Input 
Activation 
Output 
Transfer function 
Adaptation 
Node operator 
Neuron operation 
(t) = [Xl(t ) x2(t ) � � � xr(t)] ? 
yi(t), i = 1,2 ..... N 
zi (t), i = 1,2 ..... N 
v(t) = [Wil(t ) Wi2(t ) ''' w.(t)] r 
ai (t) 
11 where '1(') is a nonlinear memoryless transform 
z i (t) = 11 (a i (-t), q (t)T, 2(t )) 
(2.2) 
For convenience, we denote 
with b(0 is equivalent to correlating a(-0 with b(t). 
If the Fourier transforms (f ) = F {(t)}, 
ai Or ) = F {ai (t )} exist, then 
yi O c ) = ai O c) [qOc) cr 
where q (f)cr is the conjugate transpose of /(t). 
� X Wi L Yl = W ZI 
 (a) 
/(f) = F {/(t)}, Yi(f) = F{yi(t)}, 
. as correlation instead of convolution. Note that convolving a(t) 
and 
x,(I) 
X2(I) W (I) w.(t) 
.,,,,,'''WL() 
x,lt) 
Fig. 1. Formal Neuron and Dynamic Formal Neuron. 
(2.3) 
z(i) 
33 
3. LEARNING FOR FORMAL NEURON AND DYNAMIC FORMAL NEURON 
A number of learning rules for formal neurons has been proposed in the past. In the fol- 
lowing paragraphs, we formulate a learning problem and describe two of the existing learning 
rules, namely, Hebbian learning and delta learning, as examples. 
Present to the neural network M pairs of input and desired output samples 
{k), k)}, k = 1,2 ..... M , in order. Let W () = [t ) 2 () � .. )]r where ?) is the 
transmittance vector at the k-th step of learning. Likewise, let 
_Z () = [z 40 z 4-) � ' ' z4)], and D () = [0) (2) ... ()], 
where 
k) = W(� ), z ) = 1'()), and q(.) = [q(Y0 q(Y2) ' ' ' al(Yv)] r. 
The Hebbian learning rule 2 is described as follows*: 
W (k) = W(-0 + eu() 
The delta learning (or LMS learning) rule 3, 4 is described as follows: 
_w() = _w(k-o _ a{_w(-�) _ 
(3.1) 
(3.2) 
The leaming rules described in the previous section are generalized for the dynamic formal 
neuron by replacing multiplication with correlation. First, the problem is reformulated and then 
the generalized rules are described as follows. 
Present to the neural network M pairs of time-varing input and output samples 
{k)(t), )(t)}, k = 1,2 ..... M , in order. Let W()(t) = [g(t)()(t) ?)(t). � � k)(t)] r 
where ,Vi()(t) is the vector whose elements w?)(t) are transfer functions connecting the input j 
to the neuron i at the k-th step of learning. The Hebbian learning rule for the dynamic neuron is 
then 
W()(t ) = W(-O(t ) + a(-t ), k)(t ), )(t ) r . (3.3) 
The delta learning rule for dynamic neuron is then 
W()(t ) = W(-O(t ) - a(-t ), {W(-0(t ), k)(t ) - )(t )}, )(t )r . (3.4) 
This generalization procedure can be applied to other learning rules in some linear discrim- 
inant systems 5 , the serf-organizing mapping system by Kohonen 6 , the perceptton 7, the back- 
propagation model 3 , etc. When a system includes a nonlinear operation, more careful analysis 
is necesssay as pointed out in the Discussion section. 
4. DELTA LEARNING, PSEUDO INVERSE AND REGULARIZATION 
This section reviews the relation of the delta learning rule to the pseudo-inverse and the 
technique known as regularization. 4, 6, 8, 9, lo 
Consider a minimization problem as described below: Find __W which minimizes 
k 
subject to y(k) = 
A solution by the delta rule is, using a gradient descent method, 
_w() = __w( - ) _ a-R () 
* This interpretation assumes a strong supervising signal at the output while learning. 
(4.1) 
(4.2) 
34 
where R (t') = II t') - ')l[ � The minimum norm solution to the problem, _W*, is unique and 
can Ie expressed as 
IV* = DX t (4.3) 
where/t is the Moore-Penrose pseudo-inverse of/ , i.e., 
X t = lim(XrX + o'2I)-lX T = IinlXT(XX T + o21) -1. (4.4) 
2 
On the condition that 0 < ct <  where  is the maximum eigenvalue of XrX, ') and 
S t') am independent, and IVO,) is uncorrelated with '), 
E {_w* 3 = � {w(')3 (4.5) 
where � {x } denotes the expected value of x. One way to make use of this relation is to calcu- 
late W* for known standard data and refine it by (4.2), thereby saving time in the early stage of 
learning, 
However, this solution results in an ill-conditioned IV often in practice. When the prob- 
lem is ill-posed as such, the technique known as regularization can alleviate the ill-conditioning 
of IV. The problem is reformulated by finding IV which minimizes 
R (o) = llY '�') - ')ll  + o2y'.ll ,ll  (4.6) 
subject to ') = __WX ') where IV = [2 ' t] r � 
This reformulation regularizes (4.3) to 
Iv(o) = D_Xr(X_X_ r + o2/) - (4.7) 
which is statistically equivalent to W(') when the input has an additive noise of variance o a 
uncorrelated with '). Interestingly, the leaky LMS algorithm II leads to a statistically 
equivalent solution 
Ivtk) = _ _ (4.8) 
2 
where 0 < 15 < 1 and 0 < ct <  These solutions am related as 
if o a = 1-- when W (t') is uncorrelated with ') .li 
(4.9) 
Equation (4.8) can be generalized for a network using dynamic formal neurons, resulting in 
a equation similar to (3.4). Making use of (4.9), (4.7) can be generalized for a dynamic neuron 
network as 
Iv(t ;(X) = F- {D (f )X_ (f )Cr (x (f )X (f ) cr + (x2/) 43 (4.10) 
where F - denotes the inverse Fourier transform. 
5. SYNTHESIS OF BIPOLAR PHONEME PATTERNS 
This section illustrates the scheme used to synthesize bipolar phoneme patterns and to 
form prototype and test patterns. 
The fundamental and first three formant frequencies, along with their bandwidths, of 
phoneroes provided by Klat0 2 were taken as parameters to synthesize 30 prototype phoneme pat- 
terns. The phoneroes were labeled as shown in Table 1. An array of L (=100) input neurons 
Covered the range of 100 to 4000 Hz. Each neuron had a bipolar state which was +1 only when 
one of the frequency bands in the phoneme presented to the network was within the critical band 
35 
of the neuron and -1 otherwise. The center frequencies (fc) of critical bands were obtained by 
dividing the 100 to 4000 Hz range into a log scale by L. The critical bandwidth was a constant 
100 Hz up to the center frequency fc = 500 Hz and 0.2fc Hz when f >500 Hz. 13 
The parameters shown in Table 1 were used to construct Table 1. Labels of Phoneroes 
30 prototype phoneme patterns. For O, it was constructed as a 
combination of t and 0. 
