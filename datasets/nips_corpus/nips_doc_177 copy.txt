3O5 
ALVINN: 
AN AUTONOMOUS LAND VEHICLE IN A 
NEURAL NETWORK 
Dean A. Pomerleau 
Computer Science Department 
Carnegie Mellon University 
Pittsburgh, PA 15213 
ABSTRACT 
ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer 
back-propagation network designed for the task of road following. Cur- 
ten fly ALVINN takes images from a camera and a laser range finder as input 
and produces as output the direction the vehicle should travel in order to 
follow the road. Training has been conducted using simulated road images. 
Successful tests on the Carnegie Mellon autonomous navigation test vehicle 
indicate that the network can effectively follow real roads under certain field 
conditions. The tepresentation developed to perform the task differs dra- 
matically when the network is trained under various conditions, suggesting 
the possibility of a novel adaptive autonomous navigation system capable of 
tailoring its processing to the conditions at hand. 
INTRODUCTION 
Autonomous navigation has been a difficult problem for traditional vision and robotic 
techniques, primarily because of the noise and variability associated with real world 
scenes. Autonomous navigation systems based on traditional image processing and pat- 
tern recognition techniques often perform well under certain conditions but have problems 
with others. Part of the difficulty stems from the fact that the processing performed by 
these systems remains fixed across various driving situations. 
Artificial neural networks have displayed promising performance and flexibility in other 
domains characterized by high degrees of noise and variability, such as handwritten 
character recognition [Jackel et al., 1988] [Pawlicki et al., 1988] and speech recognition 
[Waibel et al., 1988]. ALVINN (Autonomous Land Vehicle In a Neural Network) is a 
connectionist approach to the navigational task of road following. Specifically, ALVINN 
is an artificial neural network designed to control the NAVLAB, the Carnegie Mellon 
autonomous navigation test vehicle. 
NETWORK ARCI-HTECTURE 
ALVINN's current architecture consists of a single hidden layer back-propagation network 
306 Pomerleau 
Road Intensity 
Feedback Unit 
45 Direction 
Output Units 
29 
Hidden 
Units 
8x32 Range Finder 
Input Retina 
30x32 Video 
Input Retina 
Figure 1: ALVINN Architecture 
(See Figure 1). The inpm layer is divided into three sets of units: two retinas and a 
single intensity feedback unit. The two retinas correspond to the two forms of sensory 
input available on the NAVLAB vehicle; video and range information. The first retina, 
consisting of 30x32 units, receives video camera input' from a road scene. The activation 
level of each unit in this retina is proportional to the intensity in the blue color band of 
the corresponding patch of the image. The blue band of the color image is used because 
it provides the highest conWast between the road and the non-road. The second retina, 
consisting of 8x32 units, receives input from a laser range finder. The activation level of 
each unit in this retina is proportional to the proximity of the corresponding area in the 
image. The road intensity feedback unit indicates whether the road is lighter or darker 
than the non-road in the previous image. Each of these 1217 input units is fully connected 
to the hidden layer of 29 units, which is in turn fully connected to the output layer. 
The output layer consists of 46 units, divided into two groups. The first set of 45 units 
is a linear representation of the turn curvature along which the vehicle should travel in 
order to head towards the road center. The middle unit represents the travel straight 
ahead condition while units to the left and right of the center represent successively 
sharper left and right tums. The network is trained with a desired output vector of all 
zeros except for a hill of activation centered on the unit representing the correct mm 
curvature, which is the curvature which would bring the vehicle to the road center 7 
meters ahead of its currein position. More specifically, the desired activation levels for 
ALVINN: An Autonomous Land Vehicle in a Neural Network 307 
Real Road Image Simulated Road Image 
Figure 2: Real and simulated road images 
the nine units centered around the correct turn curvature unit are 0.10, 0.32, 0.61, 0.89, 
1.00, 0.89, 0.61, 0.32 and 0.10. During testing, the turn curvature dictated by the network 
is taken to be the curvature represented by the output unit with the highest activation 
level. 
The final output unit is a road intensity feedback unit which indicates whether the road 
is lighter or darker than the non-road in the current image. During testing, the activation 
of the output road intensity feedback unit is recirculated to the input layer in the style 
of Jordan [Jordan, 1988] to aid the network's processing by providing rudimentary in- 
formation concerning the relative intensities of the road and the non-road in the previous 
image. 
TRAINING AND PERFORMANCE 
Training on actual road images is logistically difficult, because in order to develop a 
general representation, the network must be presented with a large number of training 
exemplars depicting roads under a wide variety of conditions. Collection of such a 
data set would be difficult, and changes in parameters such as camera orientation would 
requffe collecting an entirely new set of road images. To avoid these difficulties we 
have developed a simulated road generator which creates road images to be used as 
training exemplars for the network. Figure 2 depicts the video images of one real and 
one artificial road. Although not shown in Figure 2, the road generator also creates 
corresponding simulated range finder images. At the relatively low resolution being used 
it is difficult to distinguish between real and simulated roads. 
Network training is performed using these artificial road snapshots and the Warp back- 
'308 Pomerleau 
Figure 3: NAYLAB, the CMU autonomous navigation test vehicle. 
propagation simulator described in [Pomerleau et al., 1988]. Training involves first cre- 
ating a set of 1200 road snapshots depicting roads with a wide variety of retinal orienta- 
tions and positions, under a variety of lighting conditions and with realistic noise levels. 
Back-propagation is then conducted using this set of exemplars until only asymptotic 
performance improvements appear likely. During the early stages of training, the input 
road intensity unit is given a random activation level. This is done to prevent the net- 
work from merely learning to copy the activation level of the input road intensity unit 
to the output road intensity unit, since their activation levels should almost always be 
identical because the relative intensity of the road and the non-road does not often change 
between two successive images. Once the network has developed a representation that 
uses image characteristics to determine the activation level for the output road intensity 
unit, the network is given as input whether the road would have been darker or lighter 
than the non-road in the previous image. Using this extra information concerning the 
relative brightness of the road and the non-road, the network is better able to determine 
the correct direction for the vehicle to travel. 
After 40 epochs of training on the 1200 simulated road snapshots, the network correctly 
dictates a turn curvature within two units of the correct answer approximately 90% 
of the time on novel simulated road images. The primary testing of the ALVINN's 
performance has been conducted on the NAVLAB (See Figure 3). The NAVLAB is 
a modified Chevy van equipped with 3 Sun computers, a Warp, a video camera, and 
a laser range finder, which serves as a testbed for the CMU autonomous land vehicle 
project [Thorpe et al., 1987]. Performance of the network to date is comparable to that 
achieved by the best traditional vision-based autonomous navigation algorithm at CMU 
under the limited conditions tested. Specifically, the network can accurately drive the 
NAVLAB at a speed of 1/2 meter per second along a 400 meter path through a wooded 
ALVINN: An Autonomous Land Vehicle in a Neural Network 309 
Weights to Direction Output Units 
&Ill/' -I} I 
Weight to Output 
Feedback Unit 
Weight from Input 
Feedback Unit 
Weight from 
Bias Unit 
Weights from Video Camera Retina 
Weights from 
Range Finder Retina 
Road 1 Road 2 
Excitatory 
Periphery Connections, 
I [.1 I._l ! [ Ill l1 ] ! II rT 
i] ' 
lilii 
,, ': Inhibitory 
Central Connections 
Figure 4: Diagram of weights projecting to and from a typical hidden unit in a network 
trained on roads with a fixed width. The schematics on the right are aids for interpretation. 
area of/.he CMU campus under sunny fall conditions. Under similar conditions on the 
same course, the ALV group at CMU has recently achieved similar driving accuracy at 
a speed of one meter per second by implementing their image processing autonomous 
navigation algorithm on the Warp computer. In contrast, the ALVINN network is currently 
simulated using only an on-board Sun computer, and dramatic speedups are expected 
when tests are performed using the Warp. 
NETWORK REPRESENTATION 
The representation developed by the network to perform the road following task depends 
dramatically on the characteristics of the training set. When trained on examples of roads 
with a fixed width, the network develops a representations consisting of overlapping road 
filters. Figure 4 is a diagram of the weights projecting to and from a single hidden unit 
in such a network. 
As indicated by the weights to and from the feedback units, this hidden unit expects the 
road to be lighter than the non-road in the previous image and supports the road being 
lighter than the non-road in the current image. More specifically, the weights from the 
310 Pomerleau 
Weights to Direction Output Units 

