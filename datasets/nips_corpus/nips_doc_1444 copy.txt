Classification on Pairwise Proximity Data 
Thore Graepel t, Ralf Herbrich t, 
Peter Bollmann-Sdorra t, Klaus Obermayer* 
Technical University of Berlin, 
 Statistics Research Group, Sekr. FR 6-9, 
I Neural Information Processing Group, Sekr. FR 2-1, 
Franklinstr. 28/29, 10587 Berlin, Germany 
Abstract 
We investigate the problem of learning a classification task on data 
represented in terms of their pairwise proximities. This representa- 
tion does not refer to an explicit feature representation of the data 
items and is thus more general than the standard approach of us- 
ing Euclidean feature vectors, from which pairwise proximities can 
always be calculated. Our first approach is based on a combined 
linear embedding and classification procedure resulting in an ex- 
tension of the Optimal Hyperplane algorithm to pseudo-Euclidean 
data. As an alternative we present another approach based on a 
linear threshold model in the proximity values themselves, which is 
optimized using Structural Risk Minimization. We show that prior 
knowledge about the problem can be incorporated by the choice of 
distance measures and examine different metrics w.r.t. their gener- 
alization. Finally, the algorithms are successfully applied to protein 
structure data and to data from the cat's cerebral cortex. They 
show better performance than K-nearest-neighbor classification. 
1 Introduction 
In most areas of pattern recognition, machine learning, and neural computation it 
has become common practice to represent data as feature vectors in a Euclidean 
vector space. This kind of representation is very convenient because the Euclidean 
vector space offers powerful analytical tools for data analysis not available in other 
representations. However, such a representation incorporates assumptions about 
the data that may not hold and of which the practitioner may not even be aware. 
And - an even more severe restriction - no domain-independent procedures for the 
construction of features are known [3]. 
A more general approach to the characterization of a set of data items is to de- 
Classification on Pairwise Proximity Data 439 
fine a proximity or distance measure between data items - not necessarily given as 
feature vectors - and to provide a learning algorithm with a proximity matrix of 
a set of training data. Since pairwise proximity measures can be defined on struc- 
tured objects like graphs this procedure provides a bridge between the classical and 
the structural approaches to pattern recognition [3]. Additionally, pairwise data 
occur frequently in empirical sciences like psychology, psychophysics, economics, 
biochemistry etc., and most of the algorithms developed for this kind of data - pre- 
dominantly clustering [5, 4] and multidimensional scaling [8, 6]- fall into the realm 
of unsupervised learning. 
In contrast to nearest-neighbor classification schemes [10] we suggest algorithms 
which operate on the given proximity data via linear models. After a brief discus- 
sion of different kinds of proximity data in terms of possible embedclings, we suggest 
how the Optimal Hyperplane (OHC) algorithm for classification [2, 9] can be applied 
to distance data from both Euclidean and pseudo-Euclidean spaces. Subsequently, 
a more general model is introduced which is formulated as a linear threshold model 
on the proximities, and is optimized using the principle of Structural Risk Mini- 
mization [9]. We demonstrate how the choice of proximity measure influences the 
generalization behavior of the algorithm and apply both algorithms to real-world 
data from biochemistry and neuroanatomy. 
2 The Nature of Proximity Data 
When faced with proximity data in the form of a matrix P = {Pij} of pairwise 
proximity values between data items, one idea is to embed the data in a suitable 
space for visualization and analysis. This is referred to as multidimensional scaling, 
and Torgerson [8] suggested a procedure for the linear embedding of proximity data. 
Interpreting the proximities as Euclidean distances in some unknown Euclidean 
space one can calculate an inner product matrix H = xTx w.r.t. to the center of 
mass of the data from the proximities according to [8] 
m:l n:l m,n=l 
Let us perform a spectral decomposition H : UDU  = XrX and choose D 
and U such that their columns are sorted in decreasing order of magnitude of 
the eigenvalues hi of H. The embedding in an n-dimensional space is achieved 
by calculating the first n rows of X = DU r. In order to embed a new data 
item characterized by a vector p consisting of its pairwise proximities Pi w.r.t. to 
the previously known data items, one calculates the corresponding inner product 
vector h using (1) with ($)ij, Pij, and Pmj replaced by hi, pi, and p respectively, 
and then obtains the embedding x = D-Urh. 
The matrix H has negative eigenvalues if the distance data P were not Eu- 
clidean. Then the daa can be isometrically embedded only in a pseudo-Euclidean 
or Minkowski space (+'-), equipped with a bilinear form , which is not 
positive definite. In this case the distance measure takes the form p(xi, xj) = 
(xi - xj) = (xi - xj)TM(xi- xj), where M is any n x n symmetric matrix 
assumed to have full rank, but not necessarily positive definite. However, we can 
always find a basis such that the matrix M assumes the form M = diag(I+, -I-) 
with n = n + + n-, where the pair (n +, n-) is called the signature of the pseudo- 
Euclidean space [3]. Also in this case (1) serves to reconstruct the symmetric bilinear 
form, and the embedding proceeds as above with D replaced by D, whose diagonal 
contains the modules of the eigenvalues of H. 
440 T. Graepel, R. Herbrich, P Bollmann-Sdorra and K. Obermayer 
From the eigenvalue spectrum of H the effective dimensionality of the proximity 
preserving embedding can be obtained. (i) If there is only a small number of large 
positive eigenvalues, the data items can be reasonably embedded in a Euclidean 
space. (ii) If there is a small number of positive and negative eigenvalues of large 
absolute value, then an embedding in a pseudo-Euclidean space is possible. (iii) If 
the spectrum is continuous and relatively fiat, then no linear embedding is possible 
in less than - I dimensions. 
3 Classification in Euclidean and Pseudo-Euclidean Space 
Let the training set $ be given by an f x f matrix P of pairwise distances of unknown 
data vectors x in a Euclidean space, and a target class yi C {-1, +l} for each data 
item. Assuming that the data are linearly separable, we follow the OHC algorithm 
[2] and set up a linear model for the classification in data space, 
y(x): sign(xTw + b). (2) 
Then we can always find a weight vector w and threshold b such that 
yi(x/Tw q-b) _ 1 i= 1,... ,L (3) 
Now the optimal hyperplane with maximal margin is found by minimizing Ilw]] 2 
under the constraints (3). This is equivalent to maximizing the Wolfe dual W(c) 
w.r.t. o, 
1 
w(c) = cl - -crYXrXYc, (4) 
2 
with Y = diag(y), and the g-vector 1. The constraints are ci _> 0, Vi, and 1Tyc * = 
0. Since the optimal weight vector w* can be expressed as a linear combination of 
training examples 
w* = XYa*, (5) 
and the optimal threshold b* is obtained by evaluating b* = yi - xw* for any 
training example xi with 5, y 0, the decision function (2) can be fully evaluated 
using inner products between data vectors only. This formulation allows us to learn 
on the distance data directly. 
In the Euclidean case we can apply (1) to the distance matrix P of the training 
data, obtain the inner product matrix H = X;rX, and introduce it directly - 
without explicit embedding of the data - into the Wolfe dual (4). The same is true 
for the test phase, where only the inner products of the test vector with the training 
examples are needed. 
In the case of pseudo-Euclidean distance data the inner product matrix H obtained 
from the distance matrix P via (1) has negative eigenvalues. This means that 
the corresponding data vectors can only be embedded in a pseudo-Euclidean space 
R (n+'n-) as explained in the previous section. Also H cannot serve as the Hessian 
in the quadratic programming (QP) problem (4). It turns out, however, that the 
indefiniteness of the bilinear form in pseudo-Euclidean spaces does not forestall 
linear classification [3]. A decision plane is characterized by the equation xTMw = 
0, as illustrated in Fig. 1. However, Fig. 1 also shows that the same plane can just 
as well be described by xT = 0 - as if the space were Euclidean - where r = Mw 
is simply the mirror image of w w.r.t. the axes of negative signature. For the 
OHC algorithm this means, that if we can reconstruct the Euclidean inner product 
matrix XTX from the distance data, we can proceed with the OHC algorithm as 
usual. ft = X;rX is calculated by flipping the axes of negative signature, i.e., 
with f) = diag(I,X I,..., I'X]), we can calculate ft according to 
I: UDU r (6) 
Classification on Pairwise Proximity Data 441 
/ 
/ 
xTMw = 0 
/ /xTMx = 0 
Figure 1: Plot of a decision line (thick) 
in a 2D pseudo-Euclidean space with sig- 
nature (1, 1), i.e., M = diag(1,-1). The 
decision line is described by xTMw = 0. 
When interpreted as Euclidean it is at right 
angles with , which is the mirror image 
of w w.r.t. the axis x- of negative signa- 
ture. In physics this plot is referred to as a 
Minkowski space-time diagram, where x + 
corresponds to the space axis and x- to the 
time axis. The dashed diagonal lines indi- 
cate the points xTMx: 0 of zero length, 
the light cone. 
which serves now as the Hessian matrix for normal OHC classification. Note, that 
I is positive semi-definite, which ensures a unique solution for the QP problem (4). 
4 Learning a Linear Decision Function in Proximity Space 
In order to cope with general proximity data (case (iii) of Section 2) let the training 
set S be given by an f X t? proximity matrix P whose elements p,j = p(.r,
