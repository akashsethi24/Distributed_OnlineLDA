Weight Space Probability Densities 
in Stochastic Learning: 
II. Transients and Basin Hopping Times 
Genevieve B. Orr and Todd K. Leen 
Department of Computer Science and Engineering 
Oregon Graduate Institute of Science & Technology 
19600 N.W. von Neumann Drive 
Beaverton, OR 97006-1999 
Abstract 
In stochastic learning, weights are random variables whose time 
evolution is governed by a Markov process. At each time-step, 
n, the weights can be described by a probability density function 
P(w, n). We summarize the theory of the time evolution of P, and 
give graphical examples of the time evolution that contrast the 
behavior of stochastic learning with true gradient descent (batch 
learning). Finally, we use the formalism to obtain predictions of the 
time required for noise-induced hopping between basins of different 
optima. We compare the theoretical predictions with simulations 
of large ensembles of networks for simple problems in supervised 
and unsupcrviscd learning. 
I Weight-Space Probability Densities 
Despite the recent application of convergence theorems from stochastic approxima- 
tion theory to neural network learning (Oja 1982, White 1989) there remain out- 
standing questions about the search dynamics in stochastic learning. For example, 
the convergence theorems do not tell us to which of several optima the algorithm 
507 
508 Orr and Leen 
is likely to converge . Also, while it is widely recognized that the intrinsic noise 
in the weight update can move the system out of sub-optimal local minima (for a 
graphical example, see Darken and Moody 1991), there have been no theoretical 
predictions of the time required to escape from local optima, or of its dependence 
on learning rates. 
In order to more fully understand the dynamics of stochastic search, we study the 
weight-space probability density and its time evolution. In this paper we summarize 
a theoretical framework that describes this time evolution. We graphically portray 
the motion of the density for examples that contrast stochastic and batch learning. 
Finally we use the theory to predict the statistical distribution of times required for 
escape from local optima. We compare the theoretical results with simulations for 
simple examples in supervised and unsupervised learning. 
2 Stochastic Learning and Noisy Maps 
2.1 Motion of the Probability Density 
We consider stochastic learning algorithms of the form 
co(n+ 1) = co(n) + /H[co(n),z(n)] (1) 
where co(n)  m is the weight, x(n) is the data exemplar input to the algorithm at 
time-step n, tt is the learning rate, and HI... ]  m is the weight update function. 
The exemplars x(n) can be either inputs or, in the case of supervised learning, 
input/target pairs. We assume that the x(n) are i.i.d. with density p(x). Angled 
brackets {... )z denote averaging over this density. In what follows, the learning 
rate will be held constant. 
The learning algorithm (1) is a noisy map on co. The weights are thus random 
variables described by the probability density function P(co, n). The time evolution 
of this density is given by the Kolmogorov equation 
P(co, n + 1) = / dco' P(co',n) W(co' --+ co) (2) 
where the single time-step transition probability is given by (Leen and Orr 1992, 
Leen and Moody 1993) 
W(co'-+co) = ( 3(co-co'-ttH[co',:] ) }z (3) 
and 3(... ) is the Dirac delta function. 
The Kolmogorov equation can be recast as a differential-difference equation by 
expanding the transition probability (3) as a power series in it. This gives a Kramers- 
Moyal expansion (Leen and Orr 1992, Leen and Moody 1993) 
1However Kushner (1987) has proved convergence to global optima for stochastic 
approximation algorithms with added Gaussian noise subject to logarithmic annealing 
schedules. 
Weight Space Probability Densities in Stochastic Learning 509 
P(w,n + 1) - P(w,n) = 
 (-) 0  
i! ( (4) 
i=l Jt .... Ji =1 
where w5 and Hj are the ja component of weight, and weight update, respectively. 
uncating (4) to second order in p leaves a Fokker-Planck equation  that is valid for 
small [oH [. The drift coefficient { H ) is simply the average update. It is important 
to note that the diffusion coefficients, (H; H; , c be stronol dependent on 
location in the weight-space. This spaiffldee'ndence inn,n both equilibria 
and trsient phenomena. In section 3.1 we will use both the Kolmogorov equation 
(2), d the Fokker-Planck equation to track the time evolution of network ensemble 
densities. 
2.2 First Passage Times 
Our discussion of basin hopping will use the notion of the first passage time (Gar- 
diner, 1990); the time required for a network initialized at coo to first pass into an 
e-neighborhood D of a global or local optimum w. (see Figure 1). The first passage 
time is a random variable. Its distribution function 7>(n; w0) is the probability that 
a network initialized at w0 makes its first passage into D at the n th iteration of the 
learning rule. ob 
igure 1: Sample search path. 
To arrive at an expression for 7>(n; coo), we first examine the probability of passing 
from the initial weight coo to the weight w after n iterations. This probability can 
be expressed as 
P(,n I o;0,0) = fd' P(o;,n I d, 1 ) W(o;0 -d). (5) 
Substituting the single time-step transition probability (3) into the above expres- 
sion, integrating over o;', and making use of the time-shift invariance of the system a 
we find 
P(o;,-I o;0, 0) = ( P(o;,-- 1 I o;0 + H(o;0, x), 0) L (6) 
Next, let G(n; wo) denote the probability that a network initialized at o;0 has not 
passed into the region D by the r th iteration. We obtain G(n; wo) by integrating 
P(w, n I w0,0) over weights o; not in D; 
a(n;o;0) = fo do; P(o;,n I o;0,0) (7) 
2See (Ritter and Schulten 1988) and (Radons et al. 1990) for independent derivations. 
aWith our assumptions of a constant learning rate /t and stationary sam- 
ple density p(:r), the system is time-shift invariant. Mathematically stated, 
P(w,n I w', m) -- P(w,n - 1 I :',m -- 1) 
510 Orr and Leen 
where D c is the complement of D. Substituting equation (6) into (7) and integrating 
over w we obtain the recursion 
G(n;wo) = ( G(n- 1;Wo 4- pH[wo,x]) ) (8) 
Before any learning takes place, none of the networks in the ensemble have entered 
D. Thus the initial condition for G is 
G(0;wo) = 1, wo6D  
(9) 
Networks that have entered D are removed from the ensemble (i.e. OD is an ab- 
sorbing boundary). Thus G satisfies the boundary condition 
a(n;wo) = o, woD . (10) 
Finally, the probability that the network has not passed into the region D on or 
before iteration n- i minus the probability the network has not passed into D 
on or before iteration n is simply the probability that the network has passed into 
D exactly at iteration n. This is just the probability for first passage into D at 
time-step n. Thus 
7>(n;w0) -- a(n-1;w0) - a(n;wo) (11) 
Finally the recursion (8) for G can be expanded in a power series in p to obtain the 
backward Kramers-Moyal equation 
G(n;w) - G(n- 1;w) = 
i=l Jl .... ji----1 
a(n-1;w) . (12) 
Truncation to second order in p results in the backward Fokker-Planck equation. In 
section 3.2 we will use both the full recursion (8) and the Fokker-Planck approxi- 
mation to (12) to predict basin hopping times in stochastic learning. 
3 Backpropagation and Competitive Nets 
We apply the above formalism to study the time evolution of the probability density 
for simple backpropagation and competitive learning problems. We give graphical 
examples of the time evolution of the weight space density, and calculate times for 
passage from local to global optima. 
3.1 Densities for the XOR Problem 
Feed-forward networks trained to solve the XOR problem provide an example of 
supervised learning with well-characterized local optima (Lisboa and Pcrantonis, 
1991). We use a 2-input, 2-hidden, 1-output network (9 weights) trained by stochas- 
tic gradient descent on the cross-entropy error function in Lisboa and Pcrantonis 
(1991). For computational tractability, we reduce the state space dimension by 
Weight Space Probability Densities in Stochastic Learning $11 
constraining the search to one- or two-dimensional subspaces of the weight space. 
To provide global optima at finite weight values, the output targets are set to J and 
1-J, with $ << 1. 
Figure 2a shows the cost function evaluated along a line in the weight space. This 
line, parameterized by v, is d.osen to pass through a global optimum at v = 0, 
and a local optimum at v = 1.0 . In this one-dimensional slice, another local 
optimum occurs at v = 1.24. Figure 2b shows the evolution of P(v, n) obtained by 
numerical integration of the Fokker-Planck equation. Figure 2c shows the evolution 
of P(v, n) estimated by simulation of 10,000 networks, each receiving a different 
random sequence of the four input/target patterns. Initially the density is peaked 
up about the local optimum at v = 1.24. At intermediate times, there is a spike of 
density at the local optimum at v = 1.0. This spike is narrow since the diffusion 
coefficient is small there. At late times the density collects at the global optimum. 
We note that for the learning rate used here, the local optimum at v = 1.24 is 
asymptotically stable under true gradient descent, and no escape would occur. 
a) -0.5 0.0 0.5 1.0 1.5 2.0 
P(v,t) 
P(v,t) 
I 
0 
e  ( 0.! 
b) v 1 11 O0 c) v 
2 
time 
Figure 2: a) XOR cost function. b) Predicted density. c) Simulated density. 
Figure 3 shows a series of snapshots of the density superimposed on the cost function 
for a 2-D slice through the XOR weight space. The first frame shows the weight 
evolution under true gradient descent. The weights are initialized at the upper 
right-hand corner of the frame, travel down the gradient and settle into a local 
optimum. The remaining frames show the evolution of the density calculated by 
direct integration of the Kolmogorov equation (2). Here one sees a
