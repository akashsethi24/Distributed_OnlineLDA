Neuronal Regulation Implements 
Efficient $ynaptic Pruning 
Gal Chechik and Isaac Meilijson 
School of Mathematical Sciences 
Tel Aviv University, Tel Aviv 69978, Israel 
ggal@math.tau.ac.il isaco@math.tau.ac.il 
Eytan Ruppin 
Schools of Medicine and Mathematical Sciences 
Tel Aviv University, Tel Aviv 69978, Israel 
ruppin@math.tau.ac.il 
Abstract 
Human and animal studies show that mammalian brain undergoes 
massive synaptic pruning during childhood, removing about half of 
the synapses until puberty. We have previously shown that main- 
taining network memory performance while synapses are deleted, 
requires that synapses are properly modified and pruned, remov- 
ing the weaker synapses. We now show that neuronal regulation, a 
mechanism recently observed to maintain the average neuronal in- 
put field, results in weight-dependent synaptic modification. Under 
the correct range of the degradation dimension and synaptic up- 
per bound, neuronal regulation removes the weaker synapses and 
judiciously modifies the remaining synapses. It implements near 
optimal synaptic modification, and maintains the memory perfor- 
mance of a network undergoing massive synaptic pruning. Thus, 
this paper shows that in addition to the known effects of Hebbian 
changes, neuronal regulation may play an important role in the 
self-organization of brain networks during development. 
I Introduction 
This paper studies one of the fundamental puzzles in brain development: the mas- 
sive synaptic pruning observed in mammals during childhood, removing more than 
half of the synapses until puberty (see [1] for review). This phenomenon is ob- 
served in various areas of the brain both in animal studies and human studies. How 
can the brain function after such massive synaptic elimination? what could be the 
computational advantage of such a seemingly wasteful developmental strategy? In 
98 G. Chechik I. Meilijson and E. Ruppin 
previous work [2], we have shown that synaptic overgrowth followed by judicial 
pruning along development improves the performance of an associative memory 
network with limited synaptic resources, thus suggesting a new computational ex- 
planation for synaptic pruning in childhood. The optimal pruning strategy was 
found to require that synapses are deleted according to their efficacy, removing the 
weaker synapses first. 
But is there a mechanism that can implement these theoretically-derived synaptic 
pruning strategies in a biologically plausible manner ? To answer this question, we 
focus here on studying the role of neuronal regulation (NR), a mechanism operating 
to maintain the homeostasis of the neuron's membrane potential. NR has been re- 
cently identified experimentally by [3], who showed that neurons both up-regulate 
and down-regulate the efficacy of their incoming excitatory synapses in a multi- 
plicative manner, maintaining their membrane potential around a baseline level. 
Independently, [4] have studied NR theoretically, showing that it can efficiently 
maintain the memory performance of networks undergoing synaptic degradation. 
Both [3] and [4] have hypothesized that NR may lead to synaptic pruning during 
development. 
In this paper we show that this hypothesis is both computationally feasible and 
biologically plausible by studying the modification of synaptic values resulting from 
the operation of NR. Our work thus gives a possible account for the way brain 
networks maintain their performance while undergoing massive synaptic pruning. 
2 The Model 
NR-driven synaptic modification (NRSM) results from two concomitant processes: 
synaptic degradation (which is the inevitable consequence of synaptic turnover 
[5]), and neuronal regulation (NR) operating to compensate for the degradation. 
We therefore model NRSM by a sequence of degradation-strengthening steps. At 
each time step, synaptic degradation stochastically reduces the synaptic strength 
W t (W t > 0) to W 't+l by 
W 't+ = W t- (wt)rlt; rl ,-- N(lr (1) 
where r/is noise term with positive mean and the power c defines the degradation 
dimension parameter chosen in the range [0, 1]. Neuronal regulation is modeled by 
letting the post-synaptic neuron multiplicatively strengthen all its synapses by a 
common factor to restore its original input field 
wt+l -- w't+l f/ 
ft (2) 
where f/t is the input field of neuron i at time t. The excitatory synaptic efficacies are 
assumed to have a viability lower bound B- below which a synapse degenerates and 
vanishes, and a soft upper bound B + beyond which a synapse is strongly degraded 
reflecting their maximal efficacy. To study of the above process in a network, a 
model incorporating a segregation between inhibitory and excitatory neurons (i.e. 
obeying Dale's law) is required. To generate this essential segregation, we modify 
the standard low-activity associative memory model proposed by [6] by adding a 
small positive term to the synaptic learning rule. In this model, M memories 
are stored in an excitatory N-neuron network forming attractors of the network 
dynamics. The synaptic efficacy Wij between the jth (pre-synaptic) neuron and 
the ith (post-synaptic) neuron is 
M 
Wij=[(-p)(J'-p)+a] ,l_i:j_N ; Wii=O (3) 
Neuronal Regulation Implements Efficient Synaptic Pruning 99 
where t j,= are {0, 1} memory patterns with coding level p (fraction of firing 
neurons), and a is some positive constant . The updating rule for the state X[ of 
the ith neuron at time t is 
1 N 27  l+sign(f) (4) 
= = - r, 0(s) = 
j----1 j--1 
where T is the neuronal threshold, and 27 is the inhibition strength. g is a general 
modification function over the excitatory synapses, which is either derived explicitly 
(See Section 4), or determined implicitly by the operation of NRSM. If g is linear 
and 27: Ma the model reduces to the original model described by [6]. The overlap 
rn (or similarity) between the network's activity pattern X and the memory  
serves to measure memory performance (retrieval acuity), and is defined as rn  - 
N ' 
3 Neuronally Regulated Synaptic Modification 
NRSM was studied by simulating the degradation-strengthening sequence in a net- 
work in which memory patterns were stored according to Eq.3. Figure la plots a 
typical distribution of synaptic values as traced along a sequence of degradation- 
strengthening steps (Eq. 1,2). As evident, the synaptic values diverge: some of the 
weights are strengthened and lie close to the upper synaptic bounds, while the other 
synapses degenerate and vanish. Using probabilistic considerations, it can be shown 
that the synaptic distribution converge to a meta-stable state where it remains for 
long waiting times. Figure lb describes the metastable synaptic distribution as 
calculated for different a values. 
Evolving distribution of synaptic efficacies 
a. Simulation results b. Numerical results 
10000 
1.0 
1/7000 
11000 0.8 
Initial 
synaptic efficacy 
.... Alpha=frO 
--- Alpha=0.5 
-- Alpha=0.9 
5 10 .- 15 
synaptic etticacy 
20 
Figure 1: Distribution of synaptic strengths following a degradation-strengthening 
process. a) Synaptic distribution after 0,200,400, 1000 and 5000 degradation- 
strengthening steps of a 400 neurons network with 1000 stored memory patterns. 
a=0.8, p = 0.1, B- = 10 -s, B + = 18 and r/ N(0.05, 0.05). Qualitatively similar 
results were obtained for a wide range of simulation parameters. b) The synaptic 
distribution of the remaining synapses at the meta-stable state was calculated as 
the main eigen vector of the transition probability matrix. 
As the weights are normally distributed with expectation Ma > 0 and standard devi- 
ation O(x/), the probability of a negative synapse vanishes as M goes to infinity (and 
is negligible already for several dozens of memories in the parameters' range used here). 
100 G. Chechik, I. Meil7son and E. Ruppin 
a. NRSM functions at the 
Metastable state 
b. NRSM and 
random deletion 
120 
 8o 
. 4.0 
0.0 
1.0 
4.0 8.0 12.0 
Original synaptic strength 
o 
0.0 
NR modification 
Random deletion 
0.6 0.5 04 
Network's Connectivity 
0.0 0.3 
Figure 2: a) NRSM functions at the metastable state for different a values. Re- 
sults were obtained in a 400-neurons network after performing 5000 degradation- 
strengthening steps. Parameter values are as in Figure 1, except B + - 12. b) 
Performance of NR modification and random deletion. The retrieval acuity of 200 
memories stored in a network of 800 neurons is portrayed as a function of network 
connectivity, as the network undergoes continuous pruning until NR reaches the 
metastable state. c - 0, B + : 7.5, p = 0.1, rn0 = 0.80, a - 0.01, T = 0.35, 
B- - 10 -s and r/ N(0.01, 0.01). 
To further investigate which synapses are strengthened and which are pruned, we 
study the resulting synaptic modification function. Figure 2a plots the value of 
synaptic efficacy at the metastable state as a function of the initial synaptic effi- 
cacy, for different values of the degradation dimension c. As observed, a sigmoidal 
dependency is obtained, where the slope of the sigmoid s.trongly depends on the 
degradation dimension. In the two limit cases, additive degradation (c = 0) results 
in a step function at the metastable state, while multiplicative degradation (c = 1) 
results in random diffusion of the synaptic weights toward a memoryless mean value. 
Different values of c and B + result in different levels of synaptic pruning: When 
the synaptic upper bound B + is high, the surviving synapses assume high values, 
leading to massive pruning to maintain the neuronal input field, which in turn re- 
duces network's performance. Low B + values lead to high connectivity, but limit 
synapses to a small set of possible values, again reducing memory performance. Our 
simulations show that optimal memory retrieval is obtained for B + values that lead 
to deletion levels of 40% - 60%, in which NR indeed maintains the network perfor- 
mance. 
