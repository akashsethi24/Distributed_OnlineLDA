Learning Human-like Knowledge by Singular 
Value Decomposition: A Progress Report 
Thomas K. Landauer Darrell Laham 
Department of Psychology & Institute of Cognitive Science 
University of Colorado at Boulder Boulder, CO 80309-0345 
{ landauer, dlaham} psych. colorado. edu 
Peter Foltz 
Department of Psychology 
New Mexico State University Las Cruces, NM 88003-8001 
pfoltzcrl. nmsu. edu 
Abstract 
Singular value decomposition (SVD) can be viewed as a method for 
unsupervised training of a network that associates two classes of events 
reciprocally by linear connections through a single hidden layer. SVD 
was used to learn and represent relations among very large numbers of 
words (20k-60k) and very large numbers of natural text passages (lk- 
70k) in which they occurred. The result was 100-350 dimensional 
semantic spaces in which any trained or newly added word or passage 
could be represented as a vector, and similarities were measured by the 
cosine of the contained angle between vectors. Good accuracy in 
simulating human judgments and behaviors has been demonstrated by 
performance on multiple-choice vocabulary and domain knowledge 
tests, emulation of expert essay evaluations, and in several other ways. 
Examples are also given of how the kind of knowledge extracted by this 
method can be applied. 
1 INTRODUCTION 
Traditionally, imbuing machines with human-like knowledge has relied primarily on 
explicit coding of symbolic facts into computer data structures and algorithms. A serious 
limitation of this approach is people's inability to access and express the vast reaches of 
unconscious knowledge on which they rely, knowledge based on masses of implicit 
inference and irreversibly melded data. A more important deficiency of this state of affairs 
is that by coding the knowledge ourselves, (as we also do when we assign subjectively 
hypothesized rather than objectively identified features to input or output nodes in a neural 
net) we beg important questions of how humans acquire and represent the coded 
knowledge in the first place. 
46 T. K. Landauer, D. Laham and P Foltz 
Thus, from both engineering and scientific perspectives, there are reasons to try to design 
learning machines that can acquire human-like quantities of human-like knowledge from 
the same sources as humans. The success of such techniques would not prove that the 
same mechanisms are used by humans, but because we presently do not know how the 
problem can be solved in principle, successful simulation may offer theoretical insights 
as well as practical applications. In the work reported here we have found a way to induce 
significant amounts of knowledge about the meanings of passages and of their constituent 
vocab.!aries of words by training on large bodies of natural text. In general terms, the 
method simultaneously extracts the similarity between words (the likelihood of being 
used in passages that convey similar ideas) and the similarity of passages (the likelihood 
of containing words of similar meaning). The conjoint estimation of similarity is 
accomplished by a fundamentally simple representational technique that exploits mutual 
constraints implicit in the occurrences of very many words in very many contexts. We 
view the resultant system both as a means for automatically learning much of the 
semantic content of words and passages, and as a potential computational model for the 
process that underlies the corresponding human ability. 
While the method starts with data about the natural contextual co-occurrences of words, it 
uses them in a different manner than has previously been applied. A long-standing 
objection to co-occurrence statistics as a source of linguistic knowledge (Chomsky's 
1957) is that many grammatically acceptable expressions, for example sentences with 
potentially unlimited embedding structures, cannot be produced by a finite Markov 
pn:rmss whose elements are transition probabilities from word to word. If word-word 
probabilities are insufficient to generate language, then, it is argued, acquiring estimates 
of such probabilities cannot be a way that language can be leamed. However, our 
approach to statistical knowledge learning differs from those considered in the past in two 
ways. First, the basic associafional data from which knowledge is induced are not 
transition frequencies between successive individual words or phrases, but rather the 
frequencies with which particular words appear as components of relatively large nanual 
passages, utterances of the kind that humans use to convey complete ideas. The result of 
this move is that the statistical regularifies reflected are relations among unitary 
expressions of meaning, rather than syntactic constraints on word order that may serve 
additional purposes such as output and input processing efficiencies, error protection, or 
esthetic style. Second, the mutual constraints inherent in a multitude of such local co- 
occurrence relations are jointly satisfied by being forced into a global representation of 
lower dimensionality. This constraint satisfaction, a form of induction, was accomplished 
by singular value decomposition, a linear factorization technique that produces a 
representational structure equivalent to a three layer neural network model with linear 
activation functions. 
2 THE TEXT ANALYSIS MODEL AND METHOD 
The text analysis process that we have explored is called Latent Semantic Analysis (LSA) 
(Deemester etal., 1990; Landauer and Dumais, 1997). It comprises four steps: 
(1) A large body of text is represented as a matrix [ij], in which rows stand for individual 
word types, columns for meaning-g passages such as sentences or paragraphs, and 
cells contain the frequency with which a word occurs in a passage. 
(2) Cell entries ffreq;) are Wansformed to: 
!og(freqo + 1) 
freqi ï¿½ freq. 
-'. 1o  
a measure of the first order association of a word and its context. 
Learning Human Knowledge by Singular Value Decomposition: A Progress Report 47 
(3) The matrix is then subjected to singular value decomposition (Berry, 1992): 
[ij] = [ik] [kk] [jk]' 
in which [ik] and [jk] have orthonormal columns, [kk] is a diagonal matrix of singular 
values, and k <= max (i,j). 
(4) Finally, all but the d largest singular values are set to zero. Pre-multiplication of the 
right-hand matrices produces a least-squares best approximation to the original matrix 
given the number of dimensions, d, (hidden units in a corresponding neural net model 
representation) that are retained. The SVD with dimension reduction constitutes a 
constraint-satisfaction induction process in that it predicts the original observations on the 
basis of linear relations among the abstracted representations of the data that it has 
retained. By hypothesis, the analysis induces human-like relationships among passages 
and words because humans also make inferences about semantic relationships from 
abstracted representations based on limited data, and do so by an analogous process. 
In the result, each word and passage is represented as a vector of length d. Performance 
depends strongly on the choice of number of dimensions. The optimal number is 
typically around 300. The similarity of any two words, any two text passages, or any 
word and any text passage, are computed by measures on their vectors. We have most 
often used the cosine (of the contained angle between the vectors in semantic d-space) -- 
which we interpret as the degree of qualitative similarity of meaning. The length of 
vectors is also useful and interpretable. 
3 TESTS OF LSA'S PERFORMANCE 
LSA's ability to simulate human knowledge and meaning relations has been tested in a 
variety of ways. Here we describe two relatively direct sources of evidence and briefly list 
several others. 
3.1 VOCABULARY & DOMAIN KNOWLEDGE TESTS 
In all cases, LSA was first trained on a large text corpus intended to be representative of 
the text from which humans gain most of the semantic knowledge to be simulated. In a 
previously reported test (Landauer and Dumais, 1997), LSA was trained on approximately 
five million words of text sampled from a high-school level encyclopedia, then tested on 
multiple choice items from the Educational Testing Service Test of English as a Foreign 
Language (TOEFL). These test questions present a target word or short phrase and ask the 
student to choose the one of four alternative words or phrases that is most similar in 
meaning. LSA's answer was determined by computing the cosine between the derived 
vector for the target word or phrase and each of the alternatives and choosing the largest. 
LSA was correct on 64% of the 80 items, identical to the average of a large sample of 
students from non-English speaking countries who had applied for admission to U.S. 
colleges. When in error, LSA made choices positively correlated (product-moment r = .44) 
with those preferred by students. We have recently replicated this result with training on a 
similar sized sample from the Associated Press newswire 
In a new set of tests, LSA was trained on a popular introductory psychology textbook 
(Myers, 1995) and tested with the same four-alternative multiple choice tests used for 
students in two large classes. In these experiments, LSA's score was about 60% lower 
than the class averages but above passing level, and far above guessing probability. Its 
errors again resembled those of students; it got right about half as many of questions rated 
difficult by the test constructors as ones rated easy (Landauer, Foltz and Laham, 1997). 
3.2 ESSAY TESTS 
48 T. K. Landauer, D. Laham and P Foltz 
Word-word meaning similarities are a good test of knowledge indeed, vocabulary tests 
are the best single measure of human intelligence. However, they are not sufficient to 
assess the correspondence of LSA and human knowledge because people usually express 
knowledge via larger verbal strings, such as sentences
