340 Carter, Rudolph and Nucci 
Operational Fault Tolerance 
of CMAC Networks 
Michael J. Carter Franklin J. Rudolph Adam J. Nucci 
Intelligent Structures Group 
Department of Electrical and Computer Engineering 
University of New Hampshire 
Durham, NH 03824-3591 
ABSTRACT 
1 
The performance sensitivity of Albus' CMAC network was studied for 
the scenario in which faults are introduced into the adjustable weights 
after training has been accomplished. It was found that fault sensitivity 
was reduced with increased generalization when loss of weight faults 
were considered, but sensitivity was increased for saturated weight 
faults. 
INTRODUCTION 
Fault-tolerance is often cited as an inherent property of neural networks, and is thought by 
many to be a natural consequence of massively parallel computational architectures. 
Numerous anecdotal reports of fault-tolerance experiments, primarily in pattern 
classification tasks, abound in the literature. However, there has been surprisingly little 
rigorous investigation of the fault-tolerance properties of various network architectures in 
other application areas. In this paper we investigate the fault-tolerance of the CMAC 
(Cerebellar Model Arithmetic Computer) network [Albus 1975] in a systematic manner. 
CMAC networks have attracted much recent attention because of their successful 
application in robotic manipulator control [Ersu 1984, Miller 1986, Lane 1988]. Since 
fault-tolerance is a key concern in critical control tasks, there is added impetus to study 
Operational Fault Tolerance of CMAC Networks 341 
this aspect of CMAC performance. In particular, we examined the effect on network 
performance of faults introduced into the adjustable weight layer after training has been 
accomplished in a fault-free environment. The degradation of approximation error due to 
faults was studied for the task of learning simple real functions of a single variable. The 
influence of receptive field width and total CMAC memory size on the fault sensitivity of 
the network was evaluated by means of simulation. 
2 THE CMAC NETWORK ARCHITECTURE 
The CMAC network shown in Figure 1 implements a form of distributed table lookup. 
It consists of two parts: 1) an address generator module, and 2) a layer of adjustable 
weights. The address generator is a fixed algorithmic transformation from the input space 
to the space of weight addresses. This transformation has two important properties: 1) 
Only a fixed number C of weights are activated in response to any particular input, and 
more importantly, only these weights are adjusted during training; 2) It is locally 
generalizing, in the sense that any two input points separated by Euclidean distance less 
than some threshold produce activated weight subsets that are close in Hamming distance, 
i.e. the two weight subsets have many weights in common. Input points that are 
separated by more than the threshold distance produce non-overlapping activated weight 
subsets. The first property gives rise to the extremely fast training times noted by all 
CMAC investigators. The number of weights activated by any input is referred to as the 
generalization parameter, and is typically a small number ranging from 4 to 128 in 
practical applications [Miller 1986]. Only the activated weights are summed to form the 
response to the current input. A simple delta rule adjustment procedure is used to update 
the activated weights in response to the presentation of an input-desired output exemplar 
pair. Note that there is no adjustment of the address generator transformation during 
learning, and indeed, there are no weights available for adjustment in the address 
generator. It should also be noted that the hash-coded mapping is in general necessary 
because there are many more resolution cells in the input space than there are unique 
finite combinations of weights in the physical memory. As a result, the local 
generalization property will be disturbed because some distant inputs share common 
weight addresses in their activated weight subsets due to hashing collisions. 
While the CMAC network readily lends itself to the task of learning and mimicking 
multidimensional nonlinear transformations, the investigation of network fault-tolerance 
in this setting is daunting! For reasons discussed in the next section, we opted to study 
CMAC fault-tolerance for simple one-dimensional input and output spaces without the 
use of the hash-coded mapping. 
3 FAULT-TOLERANCE EXPERIMENTS 
We distinguish between two types of fault-tolerance in neural networks [Carter 1988]: 
operational fault-tolerance and learning fault-tolerance. Operational fault-tolerance deals 
with the sensitivity of network performance to faults introduced after learning has been 
342 Carter, Rudolph and Nucci 
accomplished in a fault-free environment. Learning fault-tolerance refers to the sensitivity 
of network performance to faults (either permanent or transient) which are present during 
training. It should be noted that the term fault-tolerance as used here applies only to 
faults that represent perturbations in network parameters or topology, and does not refer 
to noisy or censored input data. Indeed, we believe that the latter usage is both 
inappropriate and inconsistent with conventional usage in the computer fault-tolerance 
community. 
3.1 EXPERIMENT DESIGN PHILOSOPHY 
Since the CMAC network is widely used for learning nonlinear functions (e.g. the motor 
drive voltage to joint angle transformation for a multiple degree-of-freedom robotic 
manipulator), the obvious measure of network performance is function approximation 
error. The sensitivity of approximation error to faults is the subject of this paper. There 
are several types of faults that are of concern in the CMAC architecture. Faults that occur 
in the address generator module may ultimately have the most severe impact on 
approximation error since the selection of incorrect weight addresses will likely produce a 
bad response. On the other hand, since the address generator is an algorithm rather than a 
true network of simple computational units, the fault-tolerance of any serial processor 
implementation of the algorithm will be difficult to study. For this reason we initially 
elected to study the fault sensitivity of the adjustable weight layer only. 
The choice of fault types and fault placement strategies for neural network fault tolerance 
studies is not at all straightforward. Unlike classical fault-tolerance studies in digital 
systems which use stuck-at-zero and stuck-at-one faults, neural networks which use 
analog or mixed analog/digital implementations may suffer from a host of fault types. In 
order to make some progress, and to study the fault tolerance of the CMAC network at 
the architectural level rather than at the device level, we opted for a variation on the 
stuck-at fault model of digital systems. Since this study was concerned only with the 
adjustable weight layer, and since we assumed that weight storage is most likely to be 
digital (though this will certainly change as good analog memory technologies are 
developed), we considered two fault models which are admittedly severe. The first is a 
loss of weight fault which results in the selected weight being set to zero, while the 
second is a saturated weight fault which might correspond to the situation of a 
stuck-at-one fault in the most significant bit of a single weight register. 
The question of fault placement is also problematic. In the absence of a specific circuit 
level implementation of the network, it is difficult to postulate a model for fault 
distribution. We adopted a somewhat perverse outlook in the hope of characterizing the 
network's fault tolerance under a worst-case fault placement strategy. The insight gained 
will still prove to be valuable in more benign fault placement tests (e.g. random fault 
placement), and in addition, if one can devise network modifications which yield good 
fault-tolerance in this extreme case, there is hope of still better performance in more 
Operational Fault Tolerance of CMAC Networks 343 
typical instances of circuit failure. When placing loss of weight faults, we attacked 
large magnitude weight locations first, and continued to add more such faults to locations 
ranked in descending order of weight magnitude. Likewise, when placing saturated weight 
faults we attacked small magnitude weight locations first, and successive faults were 
placed in locations ordered by ascending weight magnitude. Since the activated weights 
are simply summed to form a response in CMAC, faults of both types create an error in 
the response which is equal to the weight change in the faulted location. Hence, our 
strategy was designed to produce the maximum output error for a given number of faults. 
In placing faults of either type, however, we did not place two faults within a single 
activated weight subset. Our strategy was thus not an absolute worst-case strategy, but 
was still more stressful than a purely random fault placement strategy. Finally, we did 
not mix fault types in any single experiment. 
The fault tolerance experiments presented in the next section all had the same general 
structure. The network under study was trained to reproduce (to a specified level of 
approximation error) a real function of a single variable, y=f(x), based upon presentation 
of (x,y) exemplar pairs. Faults of the types described previously were then introduced, 
and the resulting degradation in approximation error was logged versus the number of 
faults. Many such experiments were conducted with varying CMAC memory size and 
generalization parameter while learning the same exemplar function. We considered 
smoothly varying functions (sinusoids of varying spatial frequency) and discontinuous 
functions (step functions) on a bounded interval. 
3.2 EXPERIMENT RESULTS AND DISCUSSION 
In this secti
