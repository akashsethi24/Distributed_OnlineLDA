Batch and On-line Parameter Estimation of 
Gaussian Mixtures Based on the Joint Entropy 
Yoram Singer 
AT&T Labs 
singer @ research. att.com 
Manfred K. Warmuth 
University of California, Santa Cruz 
manfred @ cse.ucsc.edu 
Abstract 
We describe a new iterative method for parameter estimation of Gaus- 
sian mixtures. The new method is based on a framework developed by 
Kivinen and Warmuth for supervised on-line learning. In contrast to gra- 
dient descent and EM, which estimate the mixture's covariance matrices, 
the proposed method estimates the inverses of the covariance matrices. 
Furthermore, the new parameter estimation procedure can be applied in 
both on-line and batch settings. We show experimentally that it is typi- 
cally faster than EM, and usually requires about half as many iterations 
as EM. 
1 Introduction 
Mixture models, in particular mixtures of Gaussians, have been a popular tool for density 
estimation, clustering, and un-supervised learning with a wide range of applications (see 
for instance [5, 2] and the references therein). Mixture models are one of the most useful 
tools for handling incomplete data, in particular hidden variables. For Gaussian mixtures 
the hidden variables indicate for each data point the index of the Gaussian that generated it. 
Thus, the model is specified by a joint density between the observed and hidden variables. 
The common technique used for estimating the parameters of a stochastic source with hid- 
den variables is the EM algorithm. In this paper we describe a new technique for estimating 
the parameters of Gaussian mixtures. The new parameter estimation method is based on a 
framework developed by Kivinen and Warmuth [8] for supervised on-line learning. This 
framework was successfully used in a large number of supervised and un-supervised prob- 
lems (see for instance [7, 6, 9, 1]). 
Our goal is to find a local minimum of a loss function which, in our case, is the negative 
log likelihood induced by a mixture of Gaussians. However, rather than minimizing the 
Parameter Estimation of Gaussian Mixtures 579 
loss directly we add a term measuring the distance of the new parameters to the old ones. 
This distance is useful for iterative parameter estimation procedures. Its purpose is to keep 
the new parameters close to the old ones. The method for deriving iterative parameter 
estimation can be used in batch settings as well as on-line settings where the parameters 
are updated after each observation. The distance used for deriving the parameter estimation 
method in this paper is the relative entropy between the old and new joint density of the 
observed and hidden variables. For brevity we term the new iterative parameter estimation 
method the joint-entropy (JE) update. 
The JE update shares a common characteristic with the Expectation Maximization [4, 10] 
algorithm as it first calculates the same expectations. However, it replaces the maximization 
step with a different update of the parameters. For instance, it updates the inverse of the 
covariance matrix of each Gaussian in the mixture, rather than the covariance matrices 
themselves. We found in our experiments that the JE update often requires half as many 
iterations as EM. It is also straightforward to modify the proposed parameter estimation 
method for on-line setting where the parameters are updated after each new observation. 
As we demonstrate in our experiments with digit recognition, the on-line version of the 
JE update is especially useful in situations where the observations are generated by a non- 
stationary stochastic source. 
2 Notation and preliminaries 
Let S be a sequence of training examples (z,z2,...,zN} where each zi is a d- 
dimensional vector in IR d. To model the distribution of the examples we use m d- 
dimensional Gaussians. The parameters of the i-th Gaussian are denoted by �i and they 
include the mean-vector and the covariance matrix 
/i = E(xl�i) Oi = E((x-/i)(x- 
The density function of the ith Gaussian, denoted P(xl�i ), is 
We denote the entire set of parameters of a Gaussian mixture by � = {Oi}i= 1 -' 
C 'rr/, � '7 
{wi, Ii, i}i= where w = (w,. w,) is a non-negative vector of mixture coefficients 
such that Y'4= wi = 1. We denote by ?(xlO) = wiP(xl�i) the likelihood of an 
observation x according to a Gaussian mixture with parameters�. Let �i and �i be two 
Gaussian distributions. For brevity, we denote by Ei (Z) and Ei (Z) the expectation of a 
random variable Z with respect to �i and �i. Let f be a parametric function whose param- 
eters constitute a matrix A = (aij). We denote by Of/OA the matrix of partial derivatives 
of f with respect to the elements in A. That is, the ij element of Of/OA is Of/Oaij. 
Similarly, let B = (bij (z)) a matrix whose elements are functions of a scalar z. Then, we 
denote by dB/dz the matrix of derivatives of the elements in B with respect to z, namely, 
the ij element of dB/dz is dbis(z)/dz. 
3 The framework for deriving updates 
Kivinen and Warmuth [8] introduced a general framework for deriving on-line parameter 
updates. In this section we describe how to apply their framework for the problem of 
580 Y. Singer and M. K. Warrnuth 
parameter estimation of Gaussian mixtures in a batch setting. We later discuss how a 
simple modification gives the on-line updates. 
Given a set of data points ,9 in I a and a number m, the goal is to find a set of m 
Gaussians that minimize the loss on the data, denoted as loss(,9113). For density esti- 
mation the natural loss function is the negative log-likelihood of the data loss(,9113 ) - 
-( 1/IS[) In P(S113) de___f --(1 / 1`91) x sin P(x]O). The best parameters which minimize 
the above loss cannot be found analytically. The common approach is to use iterative meth- 
ods such as EM [4, 10] to find a local minimizer of the loss. 
In an iterative parameter estimation framework we are given the old set of parameters 
and we need to find a set of new parameters 13t+1 that induce smaller loss. The framework 
introduced by Kivinen and Warmuth [8] deviates from the common approaches as it also 
requires to the new parameter vector to stay close to the old set of parameters which 
incorporates all that was learned in the previous iterations. The distance of the new param- 
eter setting 13t+1 from the old setting 13t is measured by a non-negative distance function 
A(13 t+l , Ot). We now search for a new set of parameters 13t+1 that minimizes the distance 
summed with the loss multiplied by r/. Here r/is a non-negative number measuring the rel- 
ative importance of the distance versus the loss. This parameter r/will become the leaming 
rate of the update. More formally, the update is found by setting 13t+ 1 _ arg min U t (13) 
where Ut()): A(), 13t) + ,/loss(`91)) + x(Eii ,7i - 1). (We use a Lagrange multi- 
plier ,X to enforce the constraint that the mixture coefficients sum to one.) By choosing the 
apropriate distance function and r/-- 1 one can show that EM becomes the above update. 
For most distance functions and learning rates the minimizer of the function Ut(13) can- 
not be found analytically as both the distance function and the log-likelihood are usually 
non-linear in 13. Instead, we expand the log-likelihood using a first order Taylor expan- 
sion around the old parameter setting. This approximation degrades the further the new 
parameter values are from the old ones, which further motivates the use of the distance 
function A(13, 13t) (see also the discussion in [7]). We now seek a new set of parameters 
13t+1 .__ arg min V t () where 
rrt 
V*()) = a(), O*) + r/(loss(SiS*) + () - O*). V�1oss(SlO*)) + ,X(y] , - 1). (1) 
Here Voloss(,9113 t) denotes the gradient of the loss at 13t. We use the above method 
Eq. (1) to derive the updates of this paper. For density estimation, it is natural to use the 
relative entropy between the new and old density as a distance. In this paper we use the 
joint density between the observed (data points) and hidden variables (the indices of the 
Gaussians). This motivates the name joint-entropy update. 
4 Entropy based distance functions 
We first consider the relative entropy between the new and old parameter parameters of a 
single Gaussian. Using the notation introduced in Sec. 2, the relative entropy between two 
Gaussian distributions denoted by )i, 13i is 
Parameter Estimation of Gaussian Mixtures 581 
Using standard (though tedious) algebra we can rewrite the expectations as follows: 
11n = 
A(0i, Oi) =  ICl 2 
ltr(C-li) q_ 1 (i-/-ti) � (2) 
+  (g- ,)rC;- 
The relative entropy between the new and the old mixture models is the following 
P(xlO) , 
/x(), ) a. p(xlZ3) In p(-lo)ax =  W,P(xlZ3, ) In = W,P(xl),) 
= ,= , p(-o-dx . (3) 
Ideally, we would like to use the above distance function in V t to give us an update of 
O in terms of O. However, there isn't a closed form expression for Eq. (3). Although the 
relative entropy between two Gaussians is a convex function in their parameters, the relative 
entropy between two Gaussian mixtures is non-convex. Thus, the loss function V t (O) may 
have multiple minima, making the problem of finding arg minV t (0) difficult. 
In order to sidestep this problem we use the log-sum inequality [3] to obtain an upper bound 
for the distance function A(O, O). We denote this upper bound as A(O, O). 
= w, ln w,  P(xlO') dx = w,ln 
- --+ ,a ,, o,). 
-- + w, P(x ,)In p(xlO, ) 
=1 =1 t=l *=1 
(4) 
We call the new distance function A(O, O) thejoint-entropy distance. Note that in this 
distance the parameters of i and wi are coupled i the sense that it is a convex combi- 
nation of the distances A(), O). In particular, A(O, O) as a function of the parameters 
i, i, 2i does not remain constant any more when the parameters of the individual Gaus- 
sians are permuted. Furthermore, A(O, O) is also is sufficiently convex so that finding the 
minimizer of V t is possi
