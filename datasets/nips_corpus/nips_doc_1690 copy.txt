A generative model for attractor dynamics 
Richard S. Zemel 
Department of Psychology 
University of Arizona 
Tucson, AZ 85721 
zemel@u. arizona. edu 
Michael C. Mozer 
Department of Computer Science 
University of Colorado 
Boulder, CO 80309-0430 
mozer@colorado. edu 
Abstract 
Attractor networks, which map an input space to a discrete out- 
put space, are useful for pattern completion. However, designing 
a net to have a given set of attractors is notoriously tricky; training 
procedures are CPU intensive and often produce spurious attrac- 
tors and ill-conditioned attractor basins. These difficulties occur 
because each connection in the network participates in the encod- 
ing of multiple attractors. We describe an alternative formulation 
of attractor networks in which the encoding of knowledge is local, 
not distributed. Although localist attractor networks have similar 
dynamics to their distributed counterparts, they are much easier 
to work with and interpret. We propose a statistical formulation of 
localist attractor net dynamics, which yields a convergence proof 
and a mathematical interpretation of model parameters. 
Attractor networks map an input space, usually continuous, to a sparse output 
space composed of a discrete set of alternatives. Attractor networks have a long 
history in neural network research. 
Attractor networks are often used for pattern completion, which involves filling in 
missing, noisy; or incorrect features in an input pattern. The initial state of the 
attractor net is typically determined by the input pattern. Over time, the state is 
drawn to one of a predefined set of statesrathe attractors. Attractor net dynam- 
ics can be described by a state trajectory (Figure la). An attractor net is generally 
implemented by a set of visible units whose activity represents the instantaneous 
state, and optionally, a set of hidden units that assist in the computation. Attractor 
dynamics arise from interactions among the units. In most formulations of attrac- 
tor nets, e,3 the dynamics can be characterized by gradient descent in an energy 
landscape, allowing one to partition the output space into attractor basins. Instead 
of homogeneous attractor basins, it is often desirable to sculpt basins that depend 
on the recent history of the network and the arrangement of attractors in the space. 
In psychological models of human cognition, for example, priming is fundamental: 
after the model visits an attractor, it should be faster to fall into the same attractor 
in the near future, i.e., the attractor basin should be broadened. l, 6 
Another property of attractor nets is key to explaining behavioral data in psycho- 
logical and neurobiological models: the gang effect, in which the strength of an 
attractor is influenced by other attractors in its neighborhood. Figure lb illustrates 
the gang effect: the proximity of the two rightmost attractors creates a deeper at- 
tractor basin, so that if the input starts at the origin it will get pulled to the right. 
Generaave Model for ,4ttractor Dynamics 81 
Figure 1: (a) A two-dimensional space can be carved into three regions (dashed 
lines) by an attractor net. The dynamics of the net cause an input pattern (the X) 
to be mapped to one of the attractors (the O's). The solid line shows the tempo- 
ral trajectory of the network state. (b) the actual energy landscape for a localist 
attractor net as a function of ,, when the input is fixed at the origin and there are 
three attractors, w = ((-1, 0), (1, 0), (1,-.4)), with a uniform prior. The shapes of 
attractor basins are influenced by the proximity of attractors to one another (the 
gang effect). The origin of the space (depicted by a point) is equidistant from the 
attractor on the left and the attractor on the upper right, yet the origid dearly lies 
in the basin of the right attractors. 
This effect is an emergent property of the distribution of attractors, and is the basis 
for interesting dynamics; it produces the mutuall reinforcing or inhibitory influ- 
Y 
ence of similar items in domains such as semantics, 9 memory, �' 2 and olfaction. 4 
Training an attractor net is notoriously tricky. Training procedures are CPU inten- 
sive and often produce spurious attractors and ill-conditioned attractor basins. 5.   
Indeed, we are aware of no existing procedure that can robustly translate an arbi- 
trary specification of an attractor landscape into a set of weights. These difficulties 
are due to the fact that each connection partialpates in the specification of multiple 
attractors; thus, knowledge in the net is distributed over connections. 
We describe an alternative attractor network model in which knowledge is local- 
ized, hence the name localist attractor network. The model has many virtues, includ- 
ing: a trivial procedure for wiring up the architechn given an attractor landscape; 
eliminating spurious attractors; achieving gang effects; providing a dear mathe- 
matical interpretation of the model parameters, which clarifies how the parameters 
control the qualitative behavior of the model (e.g., the magnitude of gang effects); 
and proofs of convergence and stability. 
A localist attractor net consists of a set of n state units and m attractor units. Pa- 
rameters associated with an attractor unit i encode the location of the attractor, 
denoted wi, and its pullor strength, denoted rri, which influence the shape of 
the attractor basin. Its activity at time t, qi (t), reflects the normalized distance from 
the attractor center to the current state, y(t), weighted by the attractor strength: 
qi(t) = ti#(y(t),wi,(t)) (1) 
y'j rd#(y(t), wj, r(t)) 
g(y,w,r) = exp(-ly-wl2/2r 2) (2) 
Thus, the attractors form a layer of normalized radial-basis-function units. 
The input to the net, �, serves as the initial value of the state, and thereafter the 
state is pulled toward attractors in proportion to their activity. A straightforward 
82 R. S. Zemel and M. C. Mozer 
expression of this behavior is: 
y(t + 1) = a(t)� + (1 - a(t)) y qi(t)wi. 
i 
(3) 
where a(1) = 1 on the first update and a(t) = 0 fort > 1. More generally, however, 
one might want to gradually reduce a over time, allowing for a persistent effect of 
the external input on the asymptotic state. The variables tr(t) and a(t) are not free 
parameters of the model, but can be derived from the formalism we present below. 
The localist attractor net is motivated by a generative model of the input based on 
the attractor distribution, and the network dynamics corresponds to a search for 
a maximum likelihood interpretation of the observation. In the following section, 
we derive this result, and then present simulation studies of the architecture. 
1 A MAXIMUM LIKELIHOOD FORMULATION 
The starting point for the statistical formulation of a localist attractor network is 
a mixture of Gaussians model. A standard mixture of Gaussians consists of m 
Gaussian density functions in n dimensions. Each Gaussian is parameterized by 
a mean, a covariance matrix, and a mixture coefficient. The mixture model is gen- 
erative, i.e., it is considered to have produced a set of observations. Each obser- 
vation is generated by selecting a Gaussian based on the mixture coefficients and 
then stochastically selecting a point from the corresponding density function. The 
model parameters are adjusted to maximize the likelihood of a set of observations. 
The Expectation-Maximization (EM) algorithm provides an efficient procedure for 
estimating the parameters.The Expectation step calculates the posterior probabil- 
ity qi of each Gaussian for each observation, and the Maximization step calculates 
the new parameters based on the previous values and the set of qi. 
The mixture of Gaussians model can provide an interpretation for a localist attrac- 
tor network, in an unorthodox way. Each Gaussian corresponds to an attractor, and 
an observation corresponds to the state. Now, however, instead of fixing the ob- 
servation and adjusting the Gaussians, we fix the Gaussians and adjust the obser- 
vation. If there is a single observation, and a = 0 and all Gaussians have uniform 
spread tr, then Equation 1 corresponds to the Expectation step, and Equation 3 to 
the Maximization step in this unusual mixture model. 
Unfortunately, this simple characterization of the localist attractor network does 
not produce the desired behavior. Many situations produce partial solutions, in 
which the observation does not end up at an attractor. For example, if two unidi- 
mensional Gaussians overlap significantly, the most likely value for the observa- 
tion is midway between them rather than at the mean of either Gaussian. 
We therefore extend this mixture-of-Gaussians formulation to better characterize 
the localist attractor network. As in the simple model, each of the m attractors is 
a Gaussian generator, the mean of which is a location in the n-dimensional state 
space. The input to the net, �, is considered to have been generated by a stochastic 
selection of one of the attractors, followed by the addition of zero-mean Gaussian 
noise with variance specified by the attractor. Given a particular observation �, 
the an attractor's posterior probability is the normalized Gaussian probability of 
�, weighted by its mixing proportion. This posterior distribution for the attractors 
corresponds to a distribution in state space that is a weighted sum of Gaussians. 
We then consider the attractor network as encoding this distribution over states 
implied by the attractor posterior probabilities. At any one time, however, the 
attractor network can only represent a single position in state space, rather than 
,4 Generative Model for ,4ttractor Dynamics 83 
the entire distribution over states. This restriction is appropriate when the state is 
an n-dimensional point represented by the pattern of activity over n state units. 
To accommodate this restriction, we change
