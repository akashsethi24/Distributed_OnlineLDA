Reinforcement Learning for Continuous 
Stochastic Control Problems 
Rmi Munos 
CEMAGREF, LISC, Parc de Tourvoie, 
BP 121, 92185 Antony Cedex, FRANCE. 
Remi. Munos@cemagref. fr 
Paul Bourgine 
Ecole Polytechnique, CREA, 
91128 Palaiseau Cedex, FRANCE. 
Bourgine@poly. polytechnique.fr 
Abstract 
This paper is concerned with the problem of Reinforcement Learn- 
ing (RL) for continuous state space and time stochastic control 
problems. We state the Hamilton-Jacobi-Bellman equation satis- 
fied by the value function and use a Finite-Difference method for 
designing a convergent approximation scheme. Then we propose a 
RL algorithm based on this scheme and prove its convergence to 
the optimal solution. 
Introduction to RL in the continuous, stochastic case 
The objective of RL is to find -thanks to a reinforcement signal- an optimal strategy 
for solving a dynamical control problem. Here we sudy the continuous time, con- 
tinuous state-space stochastic case, which covers a wide variety of control problems 
including target, viability, optimization problems (see [FS93], [KP95])_for which a 
formalism is the following. The evolution of the current state x(t) E 0 (the state- 
space, with O open subset of IRd), depends on the control u(t)  U (compact subset) 
by a stochastic differential equation, called the state dynamics: 
dx = f(x(t),u(t))dt + c(x(t),u(t))dw (1) 
where f is the local drift and c.dw (with w a brownian motion of dimension r and 
a a d x r-matrix) the stochastic part (which appears for several reasons such as lake 
of precision, noisy influence, random fluctuations) of the diffusion process. 
For initial state x and control u(t), (1) leads to an infinity of possible trajectories 
x(t). For some trajectory x(t) (see figure 1)_, let - be its exit time from O (with 
the convention that if x(t) always stays in O, then - = o). Then, we define the 
functional J of initial state x and control u(.) as the expectation for all trajectories 
of the discounted cumulative reinforcement: 
J(x; u(.) ) - Ex,u(.) { /o /t r(x(t), u(t) )dt + -/ R(x(-) ) ) 
1030 R. Munos and P. Bourgine 
where r(x, u) is the running reinforcement and R(x) the boundary reinforcement. 
? is the discount factor (0 _< /< 1). In the following, we assume that f, cr are of 
class C 2, r and R are Lipschitzian (with constants Lr and LR) and the boundary 
c90 is C 2. 
Figure 1: The state space, the discretized ES(the square dots) and its frontier 
(the round ones). A trajectory x(t) goes through the neighbourhood of state 
RL uses the method of Dynamic Programqing (DP) which generates an optimal 
(feed-back) control u*(x) by estimating the value function (VF), defined as the 
maximal value of the functional J as a function of initial state x: 
V(x) = sup J(x; u(.)). (2) 
In the IlL approach, the state dynamics is unknown from the system ; the only 
available information for learning the optimal control is the reinforcement obtained 
at the current state. Here we propose a model-based algorithm, i.e. that learns 
on-line a model of the dynamics and approximates the value function by successive 
iterations. 
Section 2 states the Hamilton-Jacobi-Bellman equation and use a Finite-Difference 
(FD) method derived from Kushner [Kus90] for generating a convergent approxi- 
mation scheme. In section 3, we propose a RL algorithm based on this scheme and 
prove its convergence to the VF in appendix A. 
2 A Finite Difference scheme 
Here, we state a second-order nonlinear differential equation (obtained from the DP 
principle, see [FS93]) satisfied by the value function, called the Hamilton-Jacobi- 
Bellman equation. 
Let the d x d matrix a = or.or  (with  the transpose of the matrix). We consider 
the uniformly parabolic case, i.e. we assume that there exists c > 0 such that 
Vx E , Vu E U, Vy G lRd,y4,j=l aij(x, u)yiyj _> cllyll 2. Then V is C 2 (see [Kry80]). 
Let Vx be the gradient of V and Vx,j its second-order partial derivatives. 
Theorem 1 (Hamilton-Jacobi-Bellman) The following HJB equation holds 
V(x) In'), + sup [r(x,u) + V(x).f(x,u) +  -.i,j=i aijVj(x)] =0 .for x  0 
uU 
Besides, V satisfies the .following boundary condition � V(x) = R(x) for x  00. 
Reinforcement Learning for Continuous Stochastic Control Problems 1031 
Remark 1 The challen9e of learnin9 the VF is motivated by the fact that from V, 
we can deduce the followin9 optimal feed-back control policy' 
u*(x) E arg sup [r(x,u) + Vx(x).f(x,u) + � -4,j= aijVxj(x)] 
uU 
In the following, we assume that O is bounded. Let el, ..., ed be a basis for ]R d. 
Let the positive and negative parts of a function b be � b + = max(b, 0) and 
b- - max(-b, 0). For any discretization step 6, let us consider the lattices � 6Z a - 
6. Y-i=l jiei where jl,..., ja are any integers, and E 5 - 6Z a f O. Let c0E 5 the 
frontier of E 5 denote the set of points { E 6Z a  O such that at least one adjacent 
point  4- 6ei G E 5 } (see figure 1). 
Let U 5 c U be a finite control set that approximates U in the sense: 
U 5' C U 5 and UsU 5 -- U. Besides, we assume that- Vi = 1..d, 
aii(x , it) -- Eji laij( x, /')l -- 0. 
By replacing the gradient V() by the forward and backward first-order finite- 
difference quotients: AV(): � [V( 4- 5ei) - V()] and V,j () by the second- 
order fmite-difference quotients' 
-  5ei) eel) 2v(e)] 
,%,,v() - r [v( + + v(,. - - 
Ax:kix jV() -- 1 [W( q- ei 4- ej) q- V( - e i : ej) 
-- 252 
-( + ,) - u( - ,) - u( + ,) - u( - ,) + 2u()] 
in the HJB equation, we obtain the following � for   E , 
V 6 ()   + SUpuev {r(, u) + il [f (, )-i V6 () - f (, )'i V6 () 
(e') A,,V()+ Ei ((e')5 + .V( 5(e') )]} 
Knowing that (At In 3') is an approximation of (.at _ 1) as At tends to 0, we deduce' 
V5(�) = supuev [?('u)ECep.P(�,u,()VS(() +r(�,u)r(�,u)] (4) 
2 
with r(,u) = E:2[,(,)+,,(,)_ E,,,(,) ] (5) 
wch appears  a DP equation for some te Markovian Decision Procs (see 
[Ber87]) whose state space is E* d probabti of tramition � 
p(, 71, 4- ei) 
p(, u,  +Sei 4- 
p(, u,  -- 5ei 4- 5ej) 
p(,,) 
- -( u) for i # j, 
-- 252 uijkq 
= a}(�,u) for ij, 
= 0 otherwise. 
(6) 
Thanks to a contraction property due to the discount factor 3', there exists a unique 
solution (the fixed-point) Vto equation (4) for   E 5 with the boundary condition 
Vs() = R() for   0E 5. The following theorem (see [Kus90] or [FS93]) insures 
that V 5 is a convergent approximation scheme. 
1032 R. Munos and P. Bourgine 
Theorem 2 (Convergence of the FD scheme) V  converges to V as 6  0 � 
lim,,, V  () = V(x) uniformly on  
--x 
Remark 2 Condition (3) insures that the p(, u, ) are positive. If this condition 
does not hold, several possibilities to overcome this are described in [Kus90]. 
3 The reinforcement learning algorithm 
Here we assume that f is bounded from below. As the state dynamics (f and a) 
is unknown from the system, we approximate it by building a model f and  from 
samples of trajectories x(t) ' we consider series of successive states x = x(t) 
and y = x(t + r) such that' 
- Vte [t, t + ], x(t) e N() neighbourhood of  whose diameter is inferior to 
kN.6 for some positive constant kN, 
- the control u is constant for t E [t, t + r], 
- r satisfies for some positive kl and k2, 
k16 _< rk _< k26. 
(7) 
Then incrementally update the model � 
and compute the approximated time (x, u) and the approximated probabilities of 
transition (, u, ) by replacing f and a by f and  in (5) and (6). 
We obtain the following updating rule of the VS-value of state ' 
V+(): supev [/r('') Y]� (, u, )V() + (x, u)F(, u)] 
(9) 
which can be used as an off-line (synchronous, Gauss-Seidel, asynchronous) or on- 
time (for example by updating V, (f) as soon as a trajectory exits from the neigh- 
bourood of f) DP algorithm (see [BBS95]). 
Besides, when a trajectory hits the boundary 00 at some exit point x(r) then 
update the closest state  E 0E  with- 
(e) = 
(10) 
Theorem 3 (Convergence of the algorithm) Suppose that the model as well 
as the V -value o.f every state   E and control u  U are regularly updated 
(respectively with (3) and (9)) and that every state  e OE 5 are updated with (10) 
at least once. Then Ve  O, 3A such that V5 _< A, 3N, Vn >_ N, 
supez IV2()- V()I _<  with probability I 
Reinforcement Learning for Continuous Stochastic Control Problems 1033 
4 Conclusion 
This paper presents a model-based RL algorithm for continuous stochastic control 
problems. A model of the dynamics is approximated by the mean and the covariance 
of successive states. Then, a RL updating rule based on a convergent FD scheme is 
deduced and in the hypothesis of an adequate exploration, the convergence to the 
optimal solution is proved as the discretization step 5 tends to 0 and the number 
of iteration tends to infinity. This result is to be compared to the model-free RL 
algorithm for the deterministic case in [Mun97]. An interesting possible future 
work should be to consider model-free algorithms in the stochastic case for which a 
Q-learning rule (see [Wat89]) could be relevant. 
A Appendix: proof of the convergence 
Let Mf, Ma, Mf:, and M.r be the upper bounds of f, a, fx and ax and my the lower 
bound of f. Let E  = super, IV()- V()] and E = super, IV()- V()]. 
A.1 Estimation error of the model f. and  and the probabilities 
Suppose that the trajectory xe(t) occured for some occurence we(t) of the brownJan 
motion' xe(t) = xe 4- ftt f(xe(t), u)dt 4- ftt er(xe(t), u)dwe. Then we consider a 
trajectory ze(t) starting from  at te and following the same brownian motion' 
Let ze = ze(te +re). Then (ye- xe)-(ze -) = ft [f(xe(t),u) - f(ze(t),u)]dt+ 
ftt.+- 
 [er(xe(t), u) - c(ze(t), u)] dwe. Thus, from the C  property of f and 
11(Ye - xe) - (ze - e)ll -< (M/. + M.).kN.re.& (11) 
The diffusion processes has the f
