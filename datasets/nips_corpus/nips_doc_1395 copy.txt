Local Dimensionality Reduction 
Stefan Schaal 1,2,4 
sschaalusc.edu 
http://www-slab.usc.edu/sschaal 
Sethu Vijayakumar 3,1 
sethucs.titech.ac.jp 
http://ogawa- 
www.cs.titech.ac.jp/~sethu 
Christopher G. Atkeson 4 
cgacc.gatech.edu 
http://www.cc.gatech.edu/ 
fac/Chris.Atkeson 
1ERATO Kawato Dynamic Brain Project (JST), 2-2 Hikaxidai, Seika-cho, Soraku-gun, 619-02 Kyoto 
2Dept. of Comp. Science & Neuroscience, Univ. of South. California HNB-103, Los Angeles CA 90089-2520 
3Department of Computer Science, Tokyo Institute of Technology, Meguro-ku, Tokyo-152 
4College of Computing, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280 
Abstract 
If globally high dimensional data has locally only low dimensional distribu- 
tions, it is advantageous to perform a local dimensionality reduction before 
further processing the data. In this paper we examine several techniques for 
local dimensionality reduction in the context of locally weighted linear re- 
gression. As possible candidates, we derive local versions of factor analysis 
regression, principle component regression, principle component regression 
on joint distributions, and partial least squares regression. After outlining the 
statistical bases of these methods, we perform Monte Carlo simulations to 
evaluate their robustness with respect to violations of their statistical as- 
sumptions. One surprising outcome is that locally weighted partial least 
squares regression offers the best average results, thus outperforming even 
factor analysis, the theoretically most appealing of our candidate techniques. 
1 TRODUCTION 
Regression tasks involve mapping a n-dimensional continuous input vector x  filn onto 
a m-dimensional output vector y  fil m. They form a ubiquitous class of problems found 
in fields including process control, sensorimotor control, coordinate transformations, and 
various stages of information processing in biological nervous systems. This paper will 
focus on spatially localized learning techniques, for example, kernel regression with 
Gaussian weighting functions. Local learning offer advantages for real-time incremental 
learning problems due to fast convergence, considerable robustness towards problems of 
negative interference, and large tolerance in model selection (Atkeson, Moore, & Schaal, 
1997; Schaal & Atkeson, in press). Local learning is usually based on interpolating data 
from a local neighborhood around the query point. For high dimensional learning prob- 
lems, however, it suffers from a bias/variance dilemma, caused by the nonintuitive fact 
that ... [in high dimensions] if neighborhoods are local, then they are almost surely 
empty, whereas if a neighborhood is not empty, then it is not local. (Scott, 1992, p. 198). 
Global learning methods, such as sigmoidal feedforward networks, do not face this 
634 S. SchaaI, S. Vijayakumar and C. G. Atkeson 
problem as they do not employ neighborhood relations, although they require strong 
prior knowledge about the problem at hand in order to be successful. 
Assuming that local learning in high dimensions is a hopeless, however, is not necessar- 
ily warranted: being globally high dimensional does not imply that data remains high di- 
mensional if viewed locally. For example, in the control of robot arms and biological 
arms we have shown that for estimating the inverse dynamics of an arm, a globally 21- 
dimensional space reduces on average to 4-6 dimensions locally (Vijayakumar & Schaal, 
1997). A local learning system that can robustly exploit such locally low dimensional 
distributions should be able to avoid the curse of dimensionality. 
In pursuit of the question of what, in the context of local regression, is the fight 
method to perform local dimensionality reduction, this paper will derive and compare 
several candidate techniques under i) perfectly fulfilled statistical prerequisites (e.g., 
Gaussian noise, Gaussian input distributions, perfectly linear data), and ii) less perfect 
conditions (e.g., non-Gaussian distributions, slightly quadratic data, incorrect guess of 
the dimensionality of the true data distribution). We will focus on nonlinear function ap- 
proximation with locally weighted linear regression (LWR), as it allows us to adapt a va- 
riety of global linear dimensionality reduction techniques, and as LWR has found wide- 
spread application in several local learning systems (Atkeson, Moore, & Schaal, 1997; 
Jordan & Jacobs, 1994; Xu, Jordan, & Hinton, 1996). In particular, we will derive and 
investigate locally weighted principal component regression (LWPCR), locally weighted 
joint data principal component analysis (LWPCA), locally weighted factor analysis 
(LWFA), and locally weighted partial least squares (LWPLS). Section 2 will briefly out- 
line these methods and their theoretical foundations, while Section 3 will empirically 
evaluate the robustness of these methods using synthetic data sets that increasingly vio- 
late some of the statistical assumptions of the techniques. 
2 METHODS OF DIMENSIONAI,ITY REDUCTION 
We assume that our regression data originate from a generating process with two sets of 
observables, the inputs i and the outputs . The characteristics of the process en- 
sure a functional relation  = f(i). Both i and  are obtained through some measure- 
ment device that adds independent mean zero noise of different magnitude in each ob- 
servable, such that x = i + c x and y = y + cy. For the sake of simplicity, we will only fo- 
cus on one-dimensional output data (rn=l) and functions f that are either linear or 
slightly quadratic, as these cases are the most common in nonlinear function approxima- 
tion with locally linear models. Locality of the regression is ensured by weighting the er- 
ror of each data point with a weight from a Gaussian kernel: 
x denotes the query point, and D a positive semi-definite distance metric which deter- 
rmnes the size and shape of the neighborhood contributing to the regression (Atkeson et 
al., 1997). The parameters xq and D can be determined in the framework of nonparamet- 
ric statistics (Schaal & Atkeson, in press) or parametric maximum likelihood estimations 
(Xu et al, 1995). for the present study they are determined manually since their origin is 
secondary to the results of this paper. Without loss of generality, all our data sets will set 
xq to the zero vector, compute the weights, and then translate the input data such that the 
lbcally weighted mean,  = E w, xi /E wi, is zero. The output data is equally translated to 
be mean zero. Mean zero data is necessary for most of techniques considered below. The 
(translated) input data is summarized in the rows of the matrix X, the corresponding 
(translated) outputs are the elements of the vector y, and the corresponding weights are in 
the diagonal matrix W. In some cases, we need the joint input and output data, denoted 
as Z=[X y]. 
Local DimensionaIity Reduction 635 
2.1 FACTOR ANALYSIS (LWFA) 
Factor analysis (Everitt, 1984) is a technique of dimensionality reduction which is the 
most appropriate given the generating process of our regression data. It assumes the ob- 
served data z was produced by a mean zero independently distributed k -dimensional 
vector of factors v, transformed by the matrix U, and contaminated by mean zero inde- 
pendent noise e with diagonal covariance matrix 
z=Uv+e, where z= xr,y and e= ex, (2) 
If both v and  are normally distributed, the parameters C and U can be obtained itera- 
tively by the Expectation-Maximization algorithm (EM) (Rubin & Thayer, 1982). For a 
linear regression problem, one assumes that z was generated with U=[I,/5 ]r and v = 
where/ denotes the vector of regression coefficients of the linear model y =/Yx, and I 
the identity matrix. After calculating C and U by EM in joint data space as formulated in 
(2), an estimate of / can be derived from the conditional probability P{yl x). As all 
distributions are assumed to be normal, the expected value ofy is the mean of this condi- 
tional distribution. The locally weighted version (LWFA) of/ can be obtained together 
with an estimate of the factors v from the joint weighted covariance matrix W of z and v: 
U r [2(=(m+k) xn) 22(=(m+k)x(m+k)).] 
(3) 
where E{.} denotes the expectation operator and B a matrix of coefficients involved in 
estimating the factors v. Note that unless the noise e is zero, the estimated/ is different 
from the true ]3 as it tries to average out the noise in the data. 
2.2 JOINT-SPACE PRINCIPAL COMPONENT ANALYSIS (LWPCA) 
An alternative way of determining the parameters/ in a reduced space employs locally 
weighted principal component analysis (LWPCA) in the joint data space. By defining the 
largest k+l principal components of the weighted covariance matrix of Z as U: 
U=[eigertvectors(Zwi(zi_Xzi_)T/Zwi)]max(l:k+l) (4) 
and noting that the eigenvectors in U are unit length, the matrix inversion theorem (Horn 
& Johnson, 1994) provides a means to derive an efficient estimate of/ 
[Ux(= n x k)] 
/ = U(Uy r- ( I)-' where [Uy(=mxk)J 
Uy T UyUy T - VyUyT 1 e-- 
In our one dimensional output case, Uy is just a (1 x k)-dimensional row vector and the 
evaluation of (5) does not require a matrix inversion anymore but rather a division. 
If one assumes normal distributions in all variables as in LWFA, LWPCA is the special 
case of LWFA where the noise covariance f is spherical, i.e., the same magnitude of 
noise in all observables. Under these circumstances, the subspaces spanned by U in both 
methods will be the same. However, the regression coefficients of LWPCA will be dif- 
ferent from those of LWFA unless the noise level is zero, as LWFA optimizes the coeffi- 
cients according to the noise in the data (Equation (3)). Thus, for normal distributions 
and a correct guess of k, LWPCA is always expected to perform worse than LWFA. 
636 $. $chaal, $. Vijayakumar and C. G. Atke
