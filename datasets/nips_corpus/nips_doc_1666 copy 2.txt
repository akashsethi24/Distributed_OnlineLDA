Scale Mixtures of Gaussians and the 
Statistics of Natural Images 
Martin J. Wainwright 
Stochastic Systems Group 
Electrical Engineering & CS 
MIT, Building 35-425 
Cambridge, MA 02139 
mjwain@mit. edu 
Eero P. Simoncelli 
Ctr. for Neural Science, and 
Courant Inst. of Mathematical Sciences 
New York University 
New York, NY 10012 
eero. simoncel li @nyu. edu 
Abstract 
The statistics of photographic images, when represented using 
multiscale (wavelet) bases, exhibit two striking types of non- 
Gaussian behavior. First, the marginal densities of the coefficients 
have extended heavy tails. Second, the joint densities exhibit vari- 
ance dependencies not captured by second-order models. We ex- 
amine properties of the class of Gaussian scale mixtures, and show 
that these densities can accurately characterize both the marginal 
and joint distributions of natural image wavelet coefficients. This 
class of model suggests a Markov structure, in which wavelet coeffi- 
cients are linked by hidden scaling variables corresponding to local 
image structure. We derive an estimator for these hidden variables, 
and show that a nonlinear normalization procedure can be used 
to Gaussianize the coefficients. 
Recent years have witnessed a surge of interest in modeling the statistics of natural 
images. Such models are important for applications in image processing and com- 
puter vision, where many techniques rely (either implicitly or explicitly) on a prior 
density. A number of empirical studies have demonstrated that the power spectra 
of natural images follow a 1If v law in radial frequency, where the exponent  is 
typically close to two [e.g., 1]. Such second-order characterization is inadequate, 
however, because images usually exhibit highly non-Gaussian behavior. For in- 
stance, the marginals of wavelet coefficients typically have much heavier tails than 
a Gaussian [2]. Furthermore, despite being approximately decorrelated (as sug- 
gested by theoretical analysis of 1If processes [3]), orthonormal wavelet coefficients 
exhibit striking forms of statistical dependency [4, 5]. In particular, the standard 
deviation of a wavelet coefficient typically scales with the absolute values of its 
neighbors [5]. 
A number of researchers have modeled the marginal distributions of wavelet coef- 
ficients with generalized Laplacians, p�(y) c exp(-ly/AI p) [e.g. 6, 7, 8]. Special 
cases include the Gaussian (p - 2) and the Laplacian (p - 1), but appropriate ex- 
Research supported by NSERC 1969 fellowship 160833 to MJW, and NSF CAREER grant 
MIP-9796040 to EPS. 
856 M. J. Wainwright and E. P Sirnoncelli 
Mixing density 
Positive, V/ - stable 
No explicit form 
GSM density GSM char. function 
symmetrized Gamma 
Student: 
, 1 
[1/(), + > 
a-stable 
generalized Laplacian: 
exp(-[y/XlP), pe (0,2] 
t2 
(l+2-X-  , >0 
No explicit form 
exp (-Itla), a e (0,2] 
No explicit form 
Table 1. Example densities from the class of Gaussian scale mixtures. Z(7 ) de- 
notes a positive gamma variable, with density p(z)= [1/F(y)]z *- exp(-z). 
The characteristic function of a random variable x is defined as 
opt(t) -- f_��oo p(x) exp (jxt) dx. 
ponents for natural images are typically less than one. Simoncelli [5, 9] has modeled 
the variance dependencies of pairs of wavelet coefficients. Romberg et al. [10] have 
modeled wavelet densities using two-component mixtures of Gaussians. Huang and 
Mumford [11] have modeled marginal densities and cross-sections of joint densities 
with multi-dimensional generalized Laplacians. 
In the following sections, we explore the semi-parametric class of Gaussian scale 
mixtures. We show that members of this class satisfy the dual requirements of 
being heavy-tailed, and exhibiting multiplicative scaling between coefficients. We 
also show that a particular member of this class, in which the multiplier variables 
are distributed according to a gamma density, captures the range of joint statistical 
behaviors seen in wavelet coefficients of natural images. We derive an estimator for 
the multipliers, and show that a nonlinear normalization procedure can be used 
to Gaussianize the wavelet coefficients. Lastly, we form random cascades by linking 
the multipliers on a multiresolution tree. 
I Scale Mixtures of Gaussians 
d d 
A random vector Y is a Gaussian scale mixture (GSM) if Y - zU, where = denotes 
equality in distribution; z > 0 is a scalar random variable; U ,- iV'(0, Q) is a 
Gaussian random vector; and z and U are independent. 
As a consequence, any GSM variable has a density given by an integral: 
/_ 1 ( YTo-Y'  
PY(Y) = oo Iz2QI1/2 exp  / c)(z)dz. 
where bz is the probability density of the mixing variable z (henceforth the mul- 
tiplier). A special case of a GSM is a finite mixture of Gaussians, where z is a 
discrete random variable. More generally, it is straightforward to provide condi- 
tions on either the density [12] or characteristic function of X that ensure it is a 
GSM, but these conditions do not necessarily provide an explicit form of bz. Nev- 
ertheless, a number of well-known distributions may be written as Gaussian scale 
mixtures. For the scalar case, a few of these densities, along with their associated 
characteristic functions, are listed in Table 1. Each variable is characterized by a 
scale parameter A, and a tail parameter. All of the GSM models listed in Table 1 
produce heavy-tailed marginal and variance-scaling joint densities. 
Scale Mixtures of Gaussians and the Statistics of Natural Images 857 
baboon boats flower frog 
-500 0 5430 
_2[ 
-500 0 500 1000 -1000 -500 0 500 
['7, ,'k2] = [0.97, 15.04] 
AH/H = 0.00079 
[0.45, 13.77] [0.78, 26.83] [0.80, 15.39] 
0.0030 0.0030 0.0076 
Figure 1. GSMs (dashed lines) fitted to empirical histograms (solid lines). Below 
each plot are the parameter values, and the relative entropy between the histogram 
(with 256 bins) and the model, as a fraction of the histogram entropy. 
2 Modeling Natural Images 
As mentioned in the introduction, natural images exhibit striking non-Gaussian 
behavior, both in their marginal and joint statistics. In this section, we show that 
this behavior is consistent with a GSM, using the first of the densities given in 
Table I for illustration. 
2.1 Marginal distributions 
We begin by examining the symmetrized Gamma class as a model for marginal 
distributions of wavelet coefficients. Figure I shows empirical histograms of a par- 
ticular wavelet subband 1 for four different natural images, along with the best fitting 
instance of the symmetrized Gamma distribution. Fitting was performed by min- 
imizing the relative entropy (i.e., the Kullback-Leibler divergence, denoted AH) 
between empirical and theoretical histograms. In general, the fits are quite good: 
the fourth plot shows one of the worst fits in our data set. 
2.2 Normalized components 
For a GSM random vector Y __.a zU, the normalized variable Y/z formed by 
component-wise division is Gaussian-distributed. In order to test this behavior 
empirically, we model a given wavelet coefficient Y0 and a collection of neighbors 
{Yl,.- � , YN) as a GSM vector. For our examples, we use a neighborhood of N = 11 
coefficients corresponding to basis functions at 4 adjacent positions, 5 orientations, 
and 2 scales. Although the multiplier z is unknown, we can estimate it by max- 
imizing the log likelihood of the observed coefficients:  _a argmaxz  logp(Y]z)). 
Under reasonable conditions, the normalized quantity Y/ should converge in dis- 
tribution to a Gaussian as the number of neighbors increases. The estimate  is 
simple to derive: 
 = argmax{logp(Y[z)} 
z 
= argmin {Nlog(z) + YrQ-1y/2z2} 
z 
= v/YrQ-1y/N, 
1We use the steerable pyramid, an overcomplete multiscale representation described 
in [13]. The marginal and joint statistics of other multiscale oriented representations are 
similar. 
858 M. d. Wainwright and E. P. Simoncelli 
baboon 
boats 
flowers frog 
o 
AH/H = 0.00035 
0.00041 
o 
0.00042 
0.00043 
Figure 2. Marginal log histograms (solid lines) of the normalized coefficient v for a 
single subband of four natural images. Each shape is close to an inverted parabola, 
in agreement with Gaussians (dashed lines) of equivalent empirical variance. Below 
each plot is the relative entropy between the histogram (with 256 bins) and a 
variance-matched Gaussian, as a fraction of the total histogram entropy. 
where Q __a E [UU T] is the positive definite covariance matrix of the underlying 
Gaussian vector U. 
Given the estimate , we then compute the normalized coefficient y  Yo/. This is 
a generalization of the variance normalization proposed by Ruderman and Bialek[1], 
and the weighted sum of squares normalization procedure used by Simoncelli [5, 14]. 
Figure 2 shows the marginal histograms (in the log domain) of this normalized 
coefficient for four natural images, along with Gaussians of equal empirical variance. 
In contrast to histograms of the raw coefficients (shown in Figure 1), the histograms 
of normalized coefficients are nearly Gaussian. 
The GSM model makes a stronger prediction: that normalized quantities corre- 
sponding to nearby wavelet pairs should be jointly Gaussian. Specifically, a pair 
of normalized coefficients should be either correlated or uncorrelated Gaussians, 
depending on whether the underlying Gaussians U = Jul u2] T are correlated or un- 
correlated. We examine this prediction by collecting joint conditional histograms of 
normalized coefficients. The top row of Figure 3 shows joint conditional histograms 
for raw wavelet coefficients (taken from the same four natural images as Figure 2). 
The first two columns correspond to adjacent spatial scales; though decorrelated, 
they exhibit the familiar form of multiplicative scaling. The latter two columns cor- 
respond to adjacent orientations; in addition to being correlated, they also exhibit 
the multiplicative form of dependency. 
The bottom row sho
