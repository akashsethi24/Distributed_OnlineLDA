72 
ANALYSIS AND COMPARISON OF DIFFERENT LEARNING 
ALGORITHMS FOR PATTERN ASSOCIATION PROBLEMS 
J. Bernasconi 
Brown Boveri Research Center 
CH-5405 Baden, Switzerland 
ABSTRACT 
We investigate the behavior of different learning algorithms 
for networks of neuron-like units. As test cases we use simple pat- 
tern association problems, such as the XOR-problem and symmetry de- 
tection problems. The algorithms considered are either versions of 
the Boltzmann machine learning rule or based on the backpropagation 
of errors. We also propose and analyze a generalized delta rule for 
linear threshold units. We find that the performance of a given 
learning algorithm depends strongly on the type of units used. In 
particular, we observe that networks with �1 units quite generally 
exhibit a significantly better learning behavior than the correspon- 
ding 0,1 versions. We also demonstrate that an adaption of the 
weight-structure to the symmetries of the problem can lead to a 
drastic increase in learning speed. 
INTRODUCTION 
In the past few years, a number of learning procedures for 
neural network models with hidden units have been proposed 1'2 They 
can all be considered as strategies to minimize a suitably chosen 
error measure. Most of these strategies represent local optimization 
procedures (e.g. gradient descent) and therefore suffer from all the 
problems with local minima or cycles. The corresponding learning 
rates, moreover, are usually very slow. 
The performance of a given learning scheme may depend critical- 
ly on a number of parameters and implementation details. General 
analytical results concerning these dependences, however, are prac- 
tically non-existent. As a first step, we have therefore attempted 
to study empirically the influence of some factors that could have a 
significant effect on the learning behavior of neural network sys- 
tems. 
Our preliminary investigations are restricted to very small 
networks and to a few simple examples. Nevertheless, we have made 
some interesting observations which appear to be rather general and 
which can thus be expected to remain valid also for much larger and 
more complex systems. 
NEURAL NETWORK MODELS FOR PATTERN ASSOCIATION 
An artificial neural network consists of a set of interconnec- 
ted units (formal neurons). The state of the i-th unit is described 
by a variable So which can be discrete (e.g. So = 0,1 or So = �1) or 
1 
continuous (e.g . 0 < So < 1 or -1 < So < +1) , and each connection 
j+i carries a weighW  hich can be oitive, zero, or negative 
x3 
American Institute of Physics 1988 
The dynamics of the network is determined by a local update 
rule, 
Si(t+l ) = f(l Wij So(t)) (1) 
j  ' 
where f is a nonlinear activation function, specifically a threshold 
function in the case of discrete units and a sigmoid-type function, 
e.g. 
f(x) = 1/(l+e -x) (2) 
or 
f(x) = (1-e-X)/(l+e -x) , (3) 
respectively, in the case of continuous units. The individual units 
can be given different thresholds by introducing an extra unit which 
always has a value of 1. 
If the network is supposed to perform a pattern association 
task, it is convenient to divide its units into input units, output 
units, and hidden units. Learning then consists in adjusting the 
weights in such a way that, for a given input pattern, the network 
relaxes (under the prescribed dynamics) to a state in which the 
output units represent the desired output pattern. 
Neural networks learn from examples (input/output pairs) which 
are presented many times, and a typical learning procedure can be 
viewed as a strategy to minimize a suitably defined error function 
F. In most cases, this strategy is a (stochastic) gradient descent 
method: To a clamped input pattern, randomly chosen from the lear- 
ning examples, the network produces an output pattern {Oi}. This is 
compared with the desired output, say {T.}, and the error F({Oi} , 
1 . 
{Ti}) is calculated. Subsequently, each wetght is changed by an 
amount proportional to the respective gradient of F, 
F 
awij = - ' (4) 
and the procedure is repeated for a new learning example until F is 
minimized to a satisfactory level. 
In our investigations, we shall consider two different types of 
learning schemes. The first is a deterministic version of the Boltz- 
mann machine learning rule 1 and has been proposed by Yann Le Cun . 
It applies to networks with symmetric weights, Wij = Wji , so that an 
energy 
E(S) = - W.. S.S. (5) 
- (i,j) x3 x 3 
can be associated with each state S = {S:}. If X refers to the net- 
work state when only the input units areXclampe and Y to the state 
when both the input and output units are clamped, the error function 
74 
is defined as 
F = E(_Y) - E(X) 
(6) 
and the gradients are simply given by 
F 
aw.. - Y' Y' -x.x. (7) 
� 3 � 3 
The second scheme, called backpropagation or generalized delta 
rule 1'3, probably represents the most widely used learning algorithm. 
In its original form, it applies to networks with feedforward connec- 
tions only, and it uses gradient descent to minimize the mean squared 
error of the output signal, 
oi )2 
F =  Y (T i - (8) 
1 
For a weight W.o from an (input or hidden) unit j to an output 
unit i, we simply hv 
F 
- (T - O.)f'(Y Wik Sk)S (9) 
8Wij i  k J ' 
where f' is the derivative of the nonlinear activation function 
introduced in Eq. (1), and for weights which do not connect to an 
output unit, the gradients can successively be determined by apply- 
ing the chain rule of differentiation. 
In the case of discrete units, f is a threshold function, so 
that the backpropagation algorithm described above cannot be applied. 
We remark, however, that the perceptron learning rule 4, 
AWi3 = s(T. - O )Sj (10) 
� x i ' 
is nothing else than Eq. (9) with f' replaced by a constant s. 
Therefore, we propose that a generalized delta rule for linear 
threshold units can be obtained if f' is replaced by a constant g in 
all the backpropagation expressions for 3F/3W... This generalization 
of the perceptron rule is, of course, not nque. In layered net- 
works, e.g., the value of the constant which replaces f' need not be 
the same for the different layers. 
ANALYSIS OF LEARNING ALGORITHMS 
The proposed learning algorithms suffer from all the problems 
of gradient descent on a complicated landscape. If we use small 
weight changes, learning becomes prohibitively slow, while large 
weight changes inevitably lead to oscillations which prevent the 
algorithm from converging to a good solution. The error surface, 
moreover, may contain many local minima, so that gradient descent is 
not guaranteed to find a global minimum. 
There are several ways to improve a stochastic gradient descent 
procedure. The weight changes may, e.g., be accumulated over a 
number of learning examples before the weights are actually changed. 
Another often used method consists in smoothing the weight changes 
by overrelaxation, 
F +  AWoo(k) , (11) 
AWij (k+l) = - Wo. 3 
z0 
where AW.o(k) refers to the weight change after the presentation of 
the k-thearning example (or group of learning examples, respecti- 
vely). The use of a weight decay term, 
� = 
AWi 3 8W.. 3 ' 
x3 
prevents the algorithm from generating very large weights which may 
create such high barriers that a solution cannot be found in reason- 
able time. 
Such smoothing methods suppress the occurrence of oscillations, 
at least to a certain extent, and thus allow us to use higher lear- 
ning rates. They cannot prevent, however, that the algorithm may 
become trapped in bad local minimum. An obvious way to deal with the 
problem of local minima is to restart the algorithm with different 
initial weights or, equivalently, to randomize the weights with a 
certain probability p during the learning procedure. More sophisti- 
cated approaches involve, e.g., the use of hill-climbing methods. 
The properties of the error-surface over the weight space not 
only depend on the choice of the error function F, but also on the 
network architecture, on the type of units used, and on possible 
restrictions concerning the values which the weights are allowed to 
a s s u/e. 
The performance of a learning algorithm thus depends on many 
factors and parameters. These dependences are conveniently analyzed 
in terms of the behavior of an appropriately defined learning curve. 
For our small examples, where the learning set always consists of 
all input/output cases, we have chosen to represent the performance 
of a learning procedure by the fraction of networks that are 
perfect after the presentation of N input patterns. (Perfect net- 
works are networks which for every input pattern produce the correct 
output). Such learning curves give us much more detailed information 
about the behavior of the system than, e.g., averaged quantities 
like the mean learning time. 
RESULTS 
In the following, we shall present and discuss some represen- 
tative results of our empirical study. All learning curves refer to 
a set of 100 networks that have been exposed to the same learning 
procedure, where we have varied the initial weights, or the sequence 
76 
of learning examples, or both. With one exception (Figure 4), the 
sequences of learning examples are always random. 
A prototype pattern association problem is the exclusive-or 
(XOR) problem. Corresponding networks have two input units and one 
output unit. Let us first consider an XOR-network with only one 
hidden unit, but in which the input units also have direct connec- 
tions to the output unit. The weights are symmetric, and we use the 
deterministic version of the Boltzmann learning rule (see Eqs. (5) 
to (7)). Figure 1 shows results for the case of tabula rasa initial 
conditions, i.e. the initial weights are all set equal to zero. If 
the weights are changed after every learning example, about 2/3 of 
the networks learn the problem with less than 25 presentations per 
pattern (which corresp
