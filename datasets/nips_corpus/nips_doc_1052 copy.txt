Statistical Theory of Overtraining- 
Cross-Validation Asymptotically 
Effective? 
Is 
S. Amari, N. Murata, K.-R. Milllet* 
Dept. of Math. Engineering and Inf. Physics, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113, Japan 
M. Finke 
Inst. f. Logik, University of Karlsruhe 
76128 Karlsruhe, Germany 
H. Yang 
Lab. f. Inf. Representation, RIKEN, 
Wakoshi, Saitama, 351-01, Japan 
Abstract 
A statistical theory for overtraining is proposed. The analysis 
treats realizable stochastic neural networks, trained with Kullback- 
Leibler loss in the asymptotic case. It is shown that the asymptotic 
gain in the generalization error is small if we perform early stop- 
ping, even if we have access to the optimal stopping time. Consider- 
ing cross-validation stopping we answer the question: In what ratio 
the examples should be divided into training and testing sets in or- 
der to obtain the optimum performance. In the non-asymptotic 
region cross-validated early stopping always decreases the general- 
ization error. Our large scale simulations done on a CM5 are in 
nice agreement with our analytical findings. 
I Introduction 
Training multilayer neural feed-forward networks, there is a folklore that the gen- 
eralization error decreases in an early period of training, reaches the minimum and 
then increases as training goes on, while the training error monotonically decreases. 
Therefore, it is considered advantageous to stop training at an adequate time or to 
use regularizers (Hecht-Nielsen [1989], Hassoun [1995], Wang et al. [1994], Poggio 
and Girosi [1990], Moody [1992], LeCun et al. [1990] and others). To avoid over- 
training, the following stopping rule has been proposed based on cross-validation: 
*Permanent address: GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany. 
E-mail: Klaus@first.gmd.de 
Statistical Theory of Overtraining--Is Cross-Validation Asymptotically Effective? 177 
Divide all the available examples into two disjoint sets. One set is used for train- 
ing. The other set is used for testing such that the behavior of the trained network 
is evaluated by using the test examples and training is stopped at the point that 
minimizes the testing error. 
The present paper gives a mathematical analysis of the so-called overtraining phe- 
nomena to elucidate the folklore. We analyze the asymptotic case where the number 
t of examples are very large. Our analysis treats 1) a realizable stochastic machine, 
2) Kullback-Leibler loss (negative of the log likelihood loss), 3) asymptotic behavior 
where the number t of examples is sufficiently large (compared with the number m 
of parameters). We firstly show that asymptotically the gain of the generalization 
error is small even if we could find the optimal stopping time. We then answer the 
question: In what ratio, the examples should be divided into training and testing 
sets in order to obtain the optimum performance. We give a definite answer to this 
problem. When the number rn of network parameters is large, the best strategy is 
to use almost all t examples in the training set and to use only 1/2x/r- examples 
in the testing set, e.g. when m = 100, this means that only 7% of the training 
patterns are to be used in the set determining the point for early stopping. 
Our analytic results were confirmed by large-scale computer simulations of three- 
layer continuous feedforward networks where the number m of modifiable param- 
eters are m = 100. When t > 30m, the theory fits well with simulations, showing 
cross-validation is not necessary, because the generalization error becomes worse 
by using test examples to obtain an adaequate stopping time. For an intermediate 
range, where t < 30rn overtraining occurs surely and the cross-validation stopping 
improves the generalization ability strongly. 
2 Stochastic feedforward networks 
Let us consider a stochastic network which receives input vector x and emits 
output vector y. The network includes a modifiable vector parameter w = 
(Wl,..-,w,) and is denoted by N(w). The input-output relation of the net- 
work N(w) is specified by the conditional probability p(ylx;w). We assume (a) 
that there exists a teacher network N(w0) which generates training examples 
for the student N(w). And (b) that the Fisher information matrix Gi/(w) = 
[0 logp(x,y;w)0__ logp(x, y; w)] exists, is non-degenerate and is smooth in 
E  
w, where E denotes the expectation with respect to p(x,y;w) = q(x)p(ylx; w). 
The training set Dt = {(xl,yl), '-.,(xt,yt)} consists of t independent examples 
generated by the distribution p(x, y; w0) of N(w0). The maximum likelihood es- 
timator (m.l.e.)  is the one that maximizes the likelihood of producing Dr, or 
equivalently minimizes the training error or empirical risk function 
Rtrain(W) = -7 E logp(xi, Yi ;w). (2.1) 
i=1 
The generalization error or risk function R(w) of network N(w) is the expectation 
with respect to the true distribution, 
R(w) =-E011ogp(x, y;w)] = H0+D(w0 II w)- H0+E0 log p(x,y;w) ]' 
where E0 denotes the expectation with respect to p(x, y; w0), H0 is the entropy 
of the teacher network and D(w0 ][ w) is the Kullback-Leibler divergence from 
probability distribution p(x,y;w0) to p(x,y;w) or the divergence of N(w) from 
N(w0). Hence, minimizing R(w) is equivalent to minimizing D(w0 I[ w), and the 
1 78 S. AMARI, N. MURATA, K. R. M 'DLLER, M. FINKE, H. YANG 
minimum is attained at w -- w0. The asymptotic theory of statistics proves that the 
m.l.e. vt is asymptotically subject to the normal distribution with mean w0 and 
variance G- /t, where G-  is the inverse of the Fisher information matrix G. We 
can expand for example the risk R(w) - H0 + �(w - w0)T G(w0)(w -- w0) + O () 
to obtain 
(Rgen()) - H0 +  + O , (Rtrain(V)) - H0 -  + O , (2.3) 
as asymptotic result for training and test error (see Murata et al. [1993] and Amari 
and Murata [1990]). An extension of (2.3) including higher order corrections was 
recently obtained by Milllet et al. [1995]. 
Let us consider the gradient descent learning rule (Amari [1967], Rumelhart et al. 
[1986], and many others), where the parameter v(n) at the nth step is modified by 
c9Rtrain(V) (2.4) 
.(n + 1) = .(n) - e 0w ' 
and where e is a small positive constant. This is batch learning where all the 
training examples are used for each iteration of modifying v(n).  The batch process 
is deterministic and v(n) converges to v, provided the initial w(0) is included in 
its basin of attraction. For large n we can argue, that v(n) is approaching v 
isotropically and the learning trajectory follows a linear ray towards  (for details 
see Amari et al. [1995]). 
3 Virtual optimal stopping rule 
During learning as the parameter v(n) approaches v, the generalization behavior 
of network N{(n)) is evalulated by the sequence R(n)- R{(n)), n- 1,2,... 
The folklore says that R(n) decreases in an early period of learning but it increases 
later. Therefore, there exists an optimal stopping time n at which R(n) is mini- 
mized. The stopping time hop t is a random variable depending on v and the initial 
w(0). We now evaluate the ensemble average of (R(nopt)). 
The true w0 and the m.l.e. v are in general different, and they are apart of order 
1/v/. Let us compose a sphere $ of which the center is at (1/2)(w0 + v) and which 
passes through both w0 and v, as shown in Fig.lb. Its diameter is denoted by d, 
where d 2 = [17v- w012 and 
Eo[d 2] = E0[(v- w0) rG-(* - w0)] = 3tr(G-G) = m. (3.1) 
t 
Let A be the ray, that is the trajectory v(n) starting at v(0) which is not in the 
neighborhood of w0. The optimal stopping point w* that minimizes 
1 
R() - H0 + 1() - w0l = (3.2) 
is given by the first intersection of the ray A and the sphere $. 
Since w* is the point on A such that w0 - w* is orthogonal to A, it lies on the 
sphere $ (Fig.lb). When ray A  is approaching v from the opposite side of w0 (the 
right-hand side in the figure), the first intersection point is v itself. In this case, 
the optimal stopping never occurs until it converges to v. 
Let 0 be the angle between the ray A and the diameter w0 -  of the sphere $. 
We now calculate the distribution of 0 when the rays are isotropically distributed. 
1We can alternatively use on-line learning, studied by Amari [1967], Heskes and K appen 
[1991], and recently by Barkai et al. [1994] and Solla and Saard [1995]. 
Statistical Theory of Overtraining--Is Cross-Validation Asymptotically Effective? 1 79 
�emma I. When ray A is approaching v from the side in which w0 is included, the 
probability density of 0, 0 _ 0 _ r/2, is given by 
I sin -2 0, where I, = sin TM OdO. (3.3) 
= 
The detailed proof of this lemma can be found in Amari et al. [1995]. Using the 
density of 0 given by Eq.(3.3) and we arrive at the following theorem. 
Theorem 1. 
given by 
The average generalization error at the optimal stopping point is 
I I (3.4) 
(R(nopt)) = H0 + (rn- ). 
Proof. When ray A is at angle 0, 0 _ 0 < r/2, the optimal stopping point w* is on 
the sphere $. It is easily shown that Iw* - w0[ = dsin 0. This is the case where A 
is from the same side as w0 (from the left-hand side in Fig.lb), which occurs with 
probability 0.5, and the average of (dsin 0) 2 is 
Eo[(dsinO) 2] _ E0[d 2] f/2  1) 
- sin 2 0 sin -20dO = m 
I,.-2 v0 t I--2 = (1-- 
IT/ ' 
When 0 is 7r/2 _< 0 _< 7r, that is A approaches v from the opposite side, it does 
not stop until it reaches , so that [w* - w0] 2 = [W- w0[ = d 2. This occurs with 
probability 0.5. Hence, we proved the theorem. 
The theorem shows that, if we could know the optimal stopping time nop t for 
each trajectory, the generalization error decreases by 1/2t, which has an effect of 
decreasing the effective dimensions by 1/2. This effect is neglegible when m is large. 
The optimal stopping time is of the order log t. However, it is impossible to know 
the optimal stopping time. If we stop learning at an estimated optim
