Unsupervised On-Line Learning of 
Decision Trees for Hierarchical Data 
Analysis 
Marcus Held and Joachim M. Buhmann 
Rheinische Friedrich-Wilhelms-Universit/t 
Institut ffir Informatik III, RSmerstrafie 164 
D-53117 Bonn, Germany 
emaih (held, jb).cs.uni-bonn.de 
WWW: http://www-dbv. cs. uni-bonn. de 
Abstract 
An adaptive on-line algorithm is proposed to estimate hierarchical 
data structures for non-stationary data sources. The approach 
is based on the principle of minimum cross entropy to derive a 
decision tree for data clustering and it employs a metalearning idea 
(learning to learn) to adapt to changes in data characteristics. Its 
efficiency is demonstrated by grouping non-stationary artifical data 
and by hierarchical segmentation of LANDSAT images. 
i Introduction 
Unsupervised learning addresses the problem to detect structure inherent in un- 
labeled and unclassified data. The simplest, but not necessarily the best ap- 
proach for extracting a grouping structure is to represent a set of data samples 
2d- xi lRli- 1,...,N t by aset of prototypes J?- y lRlc- 1,...,K1, 
K  N. The encoding usually is represented by an assignment matrix M - (Mi), 
where Mi, - I if and only if xi belongs to cluster c, and Mi, - 0 otherwise. Accord- 
1 iN__l Micl)(xi, yc) 
ing to this encoding scheme, the cost function 7/(M, J?) -  
measures the quality of a data partition, i.e., optimal assignments and prototypes 
(M, )])opt = arg minM,y 7/(M, J?) minimize the inhomogeneity of clusters w.r.t. a 
given distance measure/). For reasons of simplicity we restrict the presentation 
to the'sum-offsquared-error criterion/)(x, y) = IIx - yll 2 in this paper. To fa- 
cilitate this minimization a deterministic annealing approach was proposed in [5] 
which maps the discrete optimization problem, i.e. how to determine the data as- 
signments, via the Maximum Entropy Principle [2] to a continuous parameter es- 
Unsupervised On-line Learning of Decision Trees for Data Analysis 515 
timation problem. Deterministic annealing introduces a Lagrange multiplier/ to 
control the approximation of 7/(M, Y) in a probabilistic sense. Equivalently to 
maximize the entropy at fixed expected K-means costs we minimize the free energy 
i /N=i In (y.uK__ 1 exp (--//)(xi, ya))) w.r.t. the prototypes ya. The assign- 
r= 
ments Mi, are treated as random variables yielding a fuzzy centroid rule 
(1) 
where the expected assignments {Mi,) are given by Gibbs distributions 
exp(-l)(xi,ya)) (2) 
{Mia) = K ' 
Eu= exp (-fid (xi,ya)) 
For a more detailed discussion of the DA approach to data clustering cf. [1, 3, 5]. 
In addition to assigning data to clusters (1,2), hierarchical clustering provides the 
partitioning of data space with a tree structure. Each data sample x is sequentially 
assigned to a nested structure of partitions which hierarchically cover the data 
space lRa. This sequence of special decisions is encoded by decision rules which are 
attached to nodes along a path in the tree (see also fig. 1). 
Therefore, learning a decision tree requires to determine a tree topology, the accom- 
panying assignments, the inner node labels $ and the prototypes y at the leaves. 
The search of such a hierarchical partition of the data space should be guided by 
an optimization criterion, i.e., minimal distortion costs. 
This problem is solvable by a two-stage approach, which on the one hand minimizes 
the distortion costs at the leaves given the tree structure and on the other hand 
optimizes the tree structure given the leaf induced partition of lRa. This approach, 
due to Miller & Rose [3], is summarized in section 2. The extensions for adaptive on- 
line learning and experimental results are described in sections 3 and 4, respectively. 
x 
s s 
s s 
'x / \ 
b c d e f 
partition 
of data space 
Figure 1: Right: Topology of a decision tree. Left: Induced partitioning of the 
data space (positions of the letters also indicate the positions of the prototypes). 
Decisions are made according to the nearest neighbor rule. 
2 Unsupervised Learning of Decision Trees 
Deterministic annealing of hierarchical clustering treats the assignments of data to 
inner nodes of the tree in a probabilistic way analogous to the expected assignments 
of data to leaf prototypes. Based on the maximum entropy principle, the probability 
q.U. that data point xi reaches inner node sj is recursively defined by (see [3])' 
H U. H exp(--/T)(xi,sj)) (3) 
/,root :-- 1, J -- i'parent(j)7ri'J' 7ri'J =  exp (--77) (xi, sk))' 
kEsiblings(j) 
516 M. Held and J. M. Buhmann 
where the Lagrange multiplier - controls the fuzziness of all the transitions ri,j. 
On the other hand, given the tree topology and the prototypes at the leaves, the 
maximum entropy principle naturally recommends an ideal probability ,a at leaf 
y,, resp. at an inner node s j, 
exp (-fT) (xi, ya) ) and (I)Ii,j - )  'i,k' (4) 
,a = y]. exp(--fD(xi,yt)) 
/EY kEdescendants(j) 
We apply the principle of minimum cross entropy for the calculation of the proto- 
types at the leaves given a priori the probabilities for the parents of the leaves. Min- 
imization of the cross entropy with fixed expected costs (Hx,) =  (Mi,)T)(xi, y,) 
for the data point xi yields the expression 
min Z ({(Mia)} H ) 
II{i,parent()/tf} -- min y.(Mi.)In .H , (5) 
{(Mi)) {(Mi)) ,parent(.) 
where 27 denotes the Kullback-Leibler divergence and K defines the degree of the 
inner nodes. The tilted distribution 
(Mi() -- i,parent(c,) exp (-fiT) (xi, y.)) (6) 
Et  /H, parent(t ) exp (--fT)(xi,Yt))' 
generalizes the probabilistic assignments (2). In the case of Euclidinn distances 
we again obtain the centroid formula (1) as the minimum of the free energy 
' -- -- E//V=i In [Y-aeY /H, parent(-)exp(--fT)(xi,y,))]. Constraints induced by 
the tree structure are incorporated in the assignments (6). For the optimization 
of the hierarchy, Miller and Rose in a second step propose the minimization of the 
distance between the hierarchical probabilities .. and the ideal probabilities !,., 
the distance being measured by the Kullback-Lei)ler divergence 
N I 
(I)i, j 
min  27 ({!,j}ll{.H,j}) -- min E E ,J In .H.' (7) 
ff'$ %$ sj parent(y) i----1 
s parent(y) 
Equation (7) describes the minimization of the sum of cross entropies between the 
probability densities !. and .. over the parents of the leaves. Calculating the 
gradients for the inner hodes sj and the Lagrange multiplier ff we receive 
N N 
0 Z ---- --2y (i- Sj) (I)I --(I)i,parent(j)7ri, j '-- --2E A 1 (i,sj) (8) 
i,j , 
08j i=1 i=1 
N N 
I 
0 27 __ T)(i, Sj) {ii,j _i,parentrj)7ri,j} ;__ EA2(i, Sj) ' (9) 
07 
i=! j$ i=1 jE$ 
The first gradient is a weighted average of the difference vectors (xi - s j), where the 
weights measure the mismatch between the probability ,j and the probability in- 
duced by the transition ri,j. The second gradient (9) measures the scale - T) (xi, s j) 
- on which the transition probabilities are defined, and weights them with the mis- 
match between the ideal probabilities. This procedure yields an algorithm which 
starts at a small value f with a complete tree and identical test vectors attached 
to all nodes. The prototypes at the leaves are optimized according to (6) and the 
centroid rule (1), and the hierarchy is optimized by (8) and (9). After convergence 
one increases f and optimizes the hierarchy and the prototypes at the leaves again. 
The increment of f leads to phase transitions where test vectors separate from each 
other and the formerly completely degenerated tree evolves its structure. For a 
detailed description of this algorithm see [3]. 
Unsupervised On-line Learning of Decision Trees for Data Analysis 517 
3 On-Line Learning of Decision Trees 
Learning of decision trees is refined in this paper to deal with unbalanced trees 
and on-line learning of trees. Updating identical nodes according to the gradients 
(9) with assignments (6) weighs parameters of unbalanced tree structures in an 
unsatisfactory way. A detailed analysis reveals that degenerated test vectors, i.e., 
test vectors with identical components, still contribute to the assignments and to 
the evolution of % This artefact is overcome by using dynamic tree topologies 
instead of a predefined topology with indistinguishable test vectors. On the other 
hand, the development of an on-line algorithm makes it possible to process huge 
data sets and non-stationary data. For this setting there exists the need of on-line 
learning rules for the prototypes at the leaves, the test vectors at the inner nodes 
and the parameters - and/. Unbalanced trees also require rules for splitting and 
merging nodes. 
Following Buhmann and Kfihnel [1] we use an expansion of order O(1/n) of (1) to 
estimate the prototypes for the Nth datapoint 
y  ya N- + rk, p.N_M (N--y]V--1), (10) 
where p  p- q- 1/M ((M ) - p-) denotes the probability of the occurence 
of class ct. The parameters M and 77, are introduced in order to take the possible 
non-stationarity of the data source into account. M denotes the size of the data 
window, and r/, is a node specific learning rate. 
Adaptation of the inner nodes and of the parameter ? is performed by stochastic 
approximation using the gradients (8) and (9) 
S 7 :- S7 --1 q-77jyN-1A 1 (XN, S7-1), (11) 
N 1:N--1 __ 77.7 E A2 (XN 8'N-l) (12) 
' J ' 
For an appropriate choice of the learning rates 77, the learning to learn approach of 
Murata et al. [4] suggests the learning algorithm 
w/V = w/V-1 _ 77/v-if (xv, w/V-). (13) 
The flow f in parameter space determines the change of w/v- given a new datapoint 
x/v. Murata et al. derive the following update scheme for the learning rate: 
/.N = (1 __ )/.N--1 q-f (XN,WN-1) , (14) 
77v = rff-1 + ulrff- (u2[[rVl[_ r/N-i), (15) 
where u, u2 and 5 are control parameters to balance the tradeoff between accuracy 
and convergence rate. r N denotes th
