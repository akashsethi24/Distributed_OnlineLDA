Designing Linear Threshold Based Neural 
Network Pattern Classifiers 
Terrence L. Fine 
School of Electrical Engineering 
Cornell University 
Ithaca, NY 14853 
Abstract 
The three problems that concern us are identifying a natural domain of 
pattern classification applications of feedforward neural networks, select- 
ing an appropriate feedforward network architecture, and assessing the 
tradeoff between network complexity, training set size, and statistical reli- 
ability as measured by the probability of incorrect classification. We close 
with some suggestions, for improving the bounds that come from Vapnik- 
Chervonenkis theory, that can narrow, but not close, the chasm between 
theory and practice. 
1 Speculations on Neural Network Pattern Classifiers 
(1) The goal is to provide rapid, reliable classification of new inputs from a 
pattern source. Neural networks are appropriate as pattern classifiers when the 
pattern sources are ones of which we have little understanding, beyond perhaps a 
nonparametric statistical model, but we have been provided with classified samples 
of features drawn from each of the pattern categories. Neural networks should be 
able to provide rapid and reliable computation of complex decision functions. The 
issue in doubt is their statistical response to new inputs. 
(2) The pursuit of optimality is misguided in the context of Point (1). Indeed, it 
is unclear what might be meant by 'optimality' in the absence of a more detailed 
mathematical framework for the pattern source. 
(3) The well-known, oft-cited 'curse of dimensionality' exposed by Richard Bell- 
man may be a 'blessing' to neural networks. Individual network processing nodes 
(e.g., linear threshold units) become more powerful as the number of their inputs 
increases. For a large enough number n of points in an input space of d dimensions, 
the number of dichotomies that can be generated by such a node grows exponen- 
tially in d. This suggests that, unlike all previous efforts at pattern classification 
that required substantial effort directed at the selection of low-dimensional feature 
vectors so as to make the decision rule calculable, we may now be approaching a 
811 
812 Fine 
position from which we can exploit raw data (e.g., the actual samples in a time 
series or pixel values in an image). Even if we are as yet unable to achieve this, 
it is clear from the reports on actual pattern classifiers that have been presented 
at NIPS90 and the accompanying Keystone Workshop that successful neural net- 
work pattern classifiers have been constructed that accept as inputs feattire vectors 
having hundreds of components (e.g., Guyon, et am. [1990]). 
(4) The blessing of dimensionality is not granted if there is either a large subset 
of critically important components that will force the network to be too complex 
or a sinall subset that contains almost all of the information needed for accurate 
discrimination. The network is liable to be successful in those cases where the input 
or feature vector _x has components that are individually nearly irrelevant, although 
collectively they enable us to discriminate well. Examples of such feature vectors 
might be the responses of individual fibers in the optic nerve, a pixel array for 
an image of an alphanumeric character, or the set of time samples of an acoustic 
transient. No one fiber, pixel value, or time sample provides significant information 
as to the true pattern category, although all of them taken together may enable 
us to do nearly error-free classification. An example in which all components are 
critically important is the calculation of parity. On our account, this is the sort of 
problem for which neural networks are inappropriate, albeit it has been repeatedly 
established that they can calculate parity. 
We interpret 'critically important' very weakly as meaning that the subspace 
spanned by the subset of critically important features/inputs needs to be pa.rti- 
tioned by the classifier so that there is at least one bounded region. If the nodes are 
linear threshold units then to carve out a bounded region, minimally a simplex, in 
a subspace of dimension c, where c is the size of the subset of critically important 
inputs, will require a network having at least c + i nodes in the first layer. 
(5) Neural networks have opened up a new application domain wherein in practice 
we can intelligently construct nonlinear pattern classifiers characterized by thou- 
sands of parameters. In practice, nonlinear statistical models, ones not defined in 
terms of a covariance matrix, seem to be restricted to a few parameters. 
(6) Nonetheless, Occam's Razor advises us to be sparing of parameters. We 
should be particularly cautious about the problem of overfitting when the number 
of parameters in the network is not much less than the number of training samples. 
Theory needs to provide practice with better insight and guidelines for avoiding 
overfitting and for the use of restrictions on training time as a guard against over- 
fitting a system with almost as many adjustable parameters as there are data points. 
(7) Points (1) and (5) combine to suggest that analytical approaches to network 
performance evaluation based upon typical statistical ideas may either be difficult 
to carry out or yield conclusions of little value to practice. There is no mismatch 
between statistical theory and neural networks in principle, but there does seem to 
be a significant mismatch in practice. Whil we are usually dealing with thousands 
of training samples, the complexity of the network means that we are not in a regime 
where asymptotic analyses (large sample behavior) will prove informative. On the 
other hand, the problem is far to complex to be resolved by 'exact' small sample 
analyses. These considerations serve to validate the widespread use of simulation 
studies to assess network design and performance. 
Designing Linear Threshold Based Neural Network Pattern Classifiers 813 
2 The QED Architecture 
2.1 QED Overview 
One may view a classifier as either making the decision as to the correct class 
or as providing 'posterior' probabilities for the various classes. If we adopt the 
latter approach, then the use of sigmoidal units having a continuum of responses is 
appropriate. If, however, we adopt the first approach, then we require hard-limiting 
devices to select one of only finitely many (in our case only two) pattern classes. 
This is the approach that we adopt and it leads us to reliance upon linear threshold 
units (LTUs). 
We have focused our attention upon a flexible architecture consisting of a first 
hidden layer that is viewed as a quantizer of the input feature vector _x and is 
therefore referred to as the Q-layer. The binary outputs from the Q-layer are then 
input to a second hidden layer whose function is to expand the dimension of the set 
of Q-layer outputs. The E-layer enables us to exploit the blessing of dimensionality 
in that by choosing it wide enough we can ensure that all Boolean functions of the 
binary outputs of the Q-layer are now implementable as linearly separable finctions 
of the E-layer outputs. Hence, to implement a binary classifier we need a third layer 
consisting of only a single node to effect the desired decision, and this output layer is 
refcrred to as the D-layer. The layers taken together are called a QED architecture. 
2.2 Constructing the Q-Layer 
The first layer in a feedforward neural network having LTUs can always be viewed 
as a quantizer. Subsequent layers in the network only see the input _x through the 
window provided by the first layer quantization. We do not expect to be able to 
quantize/partition the input space, say R d for large d, into many small compact 
regions; to do so would require that m >> d, as noted in Point (4) of the preceding 
section. Hence, asymptotic results drawn from deterministic approximation theory 
are unlikely to be helpful here. One might have recourse to the large literature on 
vector quantization (e.g., the special issue on quantization of the IEEE Transac- 
tions on Information Theory, March 1982), but we expect to quantize vectors of 
high dimension into a relatively small number of regions. Most of the information- 
theoretic literature on vector quantization does not address this domain of very low 
information rate (bits/coordinate). A more promising direction is that of clustering 
algorithms (e.g., k-means as in Pollard [1982], Darken and Moody [1990]) to guide 
the choice of Q-layer. 
2.3 Constructing the E,D-Layers 
Space limitations prevent us from detailed discussion of the formation of the E,D 
layers. In brief, the E-layer can be composed of 2 'n, often fewer, nodes where the 
weights to the ith node from the m Q-layer nodes are a binary representation of the 
index i with '0' replaced by '-1'. No training is required for the E-layer. The desired 
D-layer responses of 0 or i are formed simply by assigning weight t to connections 
from E-layer nodes corresponding to input patterns from class t, and summing and 
thresholding at 1/2. The training set 7' must be consulted to determine, say, on 
814 Fine 
the basis of majority rule, the category t 6 {0, 1} to assign to a given E-layer node. 
2.4 The Width of the Q-Layer 
The overall complexity of the QED net depends upon the number m of nodes in 
the Q-layer. Hence, our proposal will only be of practical interest if m need not 
be large. As a first argument concerning the size of this parameter, if m _< d 
then m hyperplanes in general position partition R d into 2 m regions/cells. These 
cells are only of interest to us if we know how to assign them to pattern classes. 
From the perspective of Point (1) in the preceding section, we can only determine 
a classification of a cell if we have classified data points lying in the cell. Thus, 
if we wish to make rational use of m nodes in the Q-layer, then we should have 
in ex
