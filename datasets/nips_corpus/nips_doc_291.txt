Speech Recognition using Connectionist Approaches 
Khalid Choukri 
SPRINT Coordinator 
CAP GEMINI INNOVATION 
118 rue de Tocqueville, 75017 Paris. Rance 
e-maih choukriQcapsogeti.fr 
Abstract 
This paper is a summary of SPRINT project aims and results. The project 
focus on the use of neuro-computing techniques to tackle various problems that 
remain unsolved in speech recognition. First results concern the use of feed- 
forward nets for phonetic units classification, isolated word recognition, and 
speaker adaptation. 
1 INTRODUCTION 
Speech is a complex phenomenon but it is useful to divide it into levels of repre- 
sentation. Connectionism paradigms and particularities are exploited to tackle the 
major problems in relationship with intra and inter speaker variabilities in order to 
improve the recognizer performance. For that purpose the project has been split 
into individual tasks which are depicted below: 
Lexicon 
The work described herein concerns: 
� Parameters-to-Phonetic: Classification of speech parameters using a set of 
phonetic  symbols and extraction of speech features from signal. 
� Parameters-to-Lexicah Classification of a sequence of feature vectors by lexi- 
cal access (isolated word recognition) in various environments. 
� Parameters-to-Parameters: Adaptation to new speakers and environments. 
262 
Speech Recognition using Connectionist Approaches 263 
The following sections summarize the work carried out within this project. De- 
tails, including different nets description, are reported in the project deliverables 
(Choukri, 1990), (Bimbot, 1990), (Varga, 1990). 
2 PARAMETERS-TO-PHONETIC 
The objectives of this task were to assess various neural network topologies, and to 
examine the use of prior knowledge in improving results, in the process of acoustic- 
phonetic decoding of natural speech. These results were compared to classical 
pattern classification approaches such as k-nearest neighhour classifiers (K-nn), dy- 
namic programming, and k-means. 
2.1 DATABASES 
The speech was uttered by one male speaker in French. Two databases were used: 
DB_I made of isolated non-sense words (logatomes) which contains 6672 phoneroes 
and DB_2 provided by the recording of 200 sentences which contains 5270 phonemes. 
DB_2 was split equally into training and test sets (2635 data each). 34 different 
labels were used: 1 per phoneme (not per allophone) and one for the silence. For 
each phoneme occurrence, 16 frames of signal (8 on each side of the label) were 
processed to provide a 16 Mel-scaled filter-bank vector. 
2.2 CLASSICAL CLASSIFIERS 
Experiments using k-NN and k-means classifiers were conducted to check the suf- 
ficient consistency of the data and to have some reference scores. A first protocol 
considered each pattern as a 256-dimension vector, and achieved k-nearest neigh- 
bours with the euclidean distance between references and tests. A second protocol 
attempted to decrease the time misalignments influences by carrying out some Dy- 
namic Time Warping between references and tests and taking the sum of distances 
along the best path, as a distance measure between patterns. The same data was 
used in the framework of a k-means classifier, for various values of k (number of 
representatives per class). The best results are: 
Score 61.3 %- 72.2 % 77.5 % 
2.3 NEURAL CLASSIFIERS 
2.3.1 LVQ Classifiers 
Experiments were conducted using Learning Vector Quantization technique (LVQ) 
(Bennani, 1990). A study of the importance of the weights initialization procedure 
proved to be an important parameter for the classification performance. We have 
compared three initialization algorithms: k-means, LBG, Multiedit. With k-means 
and LBG, tests were conducted with different numbers of reference vectors, while 
for Multiedit, the algorithm discovers automatically representative vectors in the 
264 Choukri 
training set, the number of which is therefore not specified in advance. 
Initialization by LBG gave better performance for self-consistency (evaluation on 
the training database: DB_I), whereas test performance on DB_2 (sentences) were 
similar for all procedures and very low. Further experiments were carried out on 
DB_2 both for training and testing. LBG initialization with 16 and 32 classes were 
tried (since they gave the best performances in the previous experiment). Even 
though the self-consistency for sentences is slightly lower than the one for logatomes, 
the improvement of recognition scores are far better as illustrated here: 
nb ref per class 16 32 
K-means 60.3 % 61.3 % 
LBG -. LVQ 62.4 %  66.1% 63.2 %  67.2 % 
This experiment and some others (not presented here) (Bimbot, 1990) confirm that 
the failure of previous experiments is more due to a mismatch between the corpora 
for this recognition method, than an inadequacy of the classification technique itself. 
2.3.2 The Time-Delay Neural Network (TDNN) Classifiers 
A TDNN, as introduced by A. Waibel (Waibel, 1987), can be described by its set 
of typological parameters, i.e.: 
Mox/Vo / Po,$o - Mx/V / P,$ - M2x/V2 - Kxl. 
In the following a  TDNN-derived  network has a similar architecture, except that 
M2 is not constrained to be equal to K, and the connectivity between the last 
2 layers is full. Various TDNN-derived architectures were tested on recognizing 
phonemes from sentences (DB_2) after learning on the 1ogatomes (DB_I). Best re- 
sults are given below: 
TDNN-derived structure self-consist. reco score 
16x16 / 2,1 - 8x15 / 7,2 - 5x5 - 34xl 63.9 % 48.1% 
16x16 / 2,1 - 16x15 / 7,4- 11x3 - 34xl 75.1% 54.8 % 
16x16 / 4,1 - 16x13 / 5,2 - 16x5 - 34xl 81.0 % 60.5 % 
16x16 / 2,1 - 16x15 / 7,4- 16x3 - 34xl 79.8 % 60.8 % 
The first net is clearly not powerful enough for the task, so the number of free pa- 
rameters has be increased. This upgraded the results immediately as can be seen for 
the other nets. The third and fourth nets have equivalent performance, they differ 
in the local windows width and delays. Other tested architectures did not increase 
this performance. The main difference between training and test sets is certainly 
the different speaking rate, and therefore the existence of important time distor- 
sions. Though TDNN-derived architectures seem more able to handle this kind of 
distorsions than LVQ, as the generalization performance is signi�cantly higher for 
similar learning self-consistency, but both fail to remove all temporel misalignment 
Speech Recognition using Connectionist Approaches 265 
effects. 
In order to upgrade classification performance we changed the cost function which 
is minimized by the network: the error term corresponding to the desired output 
is multiplied by a constant H superior to 1, the terms of the error corresponding to 
other outputs being left unchanged to compensate the deficiency of the simple mean 
square error procedure. We obtained our best results with the best TDDN-derived 
net we experimented for H=2: 
Database Net: self-consist. reco score 
DB_I 16x16 / 4,1 - 16x13 / 5,2 - 16x5 - 34xl 87.0 % 63.0 % 
DB_2 16x16 / 4,1 - 16x13 / 5,2 - 16x5 - 34xl 87.0 % 78.0 % 
The too small number of independent weights (too low-dimensioned TDNN-derived 
architecture) makes the problem too constrained. A well chosen TDNN-derived 
architecture can perform as well as the best k-nearest neighbours strategy. Perfor- 
mance gets lower for data that mainly differ by a significant speaking rate mismatch 
which could indicate that TDNN-derived architectures do not manage to handle all 
kinds of time distortions. So it is encouraging to combine different networks and 
classical methods to deal with the temporal and sequential aspects of speech. 
2.3.3 Combination of TDNN and 
A set of experiments using a combined TDNN-derived network and LVQ architec- 
ture were conducted. For these experiments, we have used the best nets found in 
previous experiments. The main parameter of these experiments is the number of 
hidden cells in the last layer of the TDNN-derived network which is the input layer 
of LVQ {Bennani, 1990). 
Evaluation on DB_i with various numbers of references per class gave the following 
recognition scores: 
refs per class 4 8 16 
TDNN +k-means 76.2 % 78.1% 79.8 
TDNN +LBG 77.7 % 79.9 % 81.3 
'TDNN +LVQ (LBG for initialization) 78.4 % 82.1% 81.4 
Best results have been obtained with 8 references per class and the LBG algorithm 
to initialize the LVQ module. The best performance on the test set (82.1% ) rep- 
resents a significant increase (4 % ) compared to the best TDNN-derived network. 
Other experiments were performed on TDNN + LVQ by using a modified LVQ 
architecture, presented in (Bennani, 1990), which is an extension of LVQ built to 
automatically weight the variables according to their importance for the classifi- 
cation. We obtain a recognition score of 83.6 % on DB_2 (training and tests on 
sentences). 
We also used low dimensioned TDNNs for discriminating between phonetic features 
(Bimbot, 1990), assuming that phonetics will provide a description of speech that 
will appropriately constrain a priori a neural network, the TDNN structure war- 
266 Choukri 
ranting the desirable property of shift invariance. 
The feature extraction approach can be considered as an other way to use prior 
knowledge for solving a complex problem with neural networks. The results ob- 
tained in these experiments are an interesting starting point for designing a large 
modular network where each module is in charge of a simple task, directly related 
to a well-defined linguistic phenomenon (Bimbot, 1990). 
2.4 CONCLUSIONS 
Experiments with LVQ alone, a TDNN-derived network alone and combined TDNN- 
LVQ architectures proved the combined architecture to be the most efficient with 
respect to our databases as summarized below (training and tests on DB_2): 
I k-means [ LVQ 
61.3 % 67.2 
72.2 % 77.5 % 78.0 % 83.6 % 
3 PARAMETERS-TO-LEXICAL 
The main objective of this task is to use neural ne
