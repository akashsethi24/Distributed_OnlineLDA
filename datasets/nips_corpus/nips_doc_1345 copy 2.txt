Training Methods for Adaptive Boosting 
of Neural Networks 
Holger Schwenk 
Dept. IRO 
Universitd de Montrdal 
2920 Chemin de la Tour, 
Montreal, Qc, Canada, H3C 3J7 
schwenkiro. umontreal. ca 
Yoshua Bengio 
Dept. IRO 
Universitd de Montrdal 
and AT&T Laboratories, NJ 
bengioyiro. umontreal. ca 
Abstract 
Boosting is a general method for improving the performance of any 
learning algorithm that consistently generates classifiers which need to 
perform only slightly better than random guessing. A recently proposed 
and very promising boosting algorithm is AdaBoost [5]. It has been ap- 
plied with great success to several benchmark machine learning problems 
using rather simple learning algorithms [4], and decision trees [ 1, 2, 6]. 
In this paper we use AdaBoost to improve the performances of neural 
networks. We compare training methods based on sampling the training 
set and weighting the cost function. Our system achieves about 1.4% 
error on a data base of online handwritten digits from more than 200 
writers. Adaptive boosting of a multi-layer network achieved 1.5% error 
on the UCI Letters and 8.1% error on the UCI satellite data set. 
1 Introduction 
AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially 
training classifiers, while putting more and more emphasis on certain patterns. AriaBoost 
has been applied to rather weak learning algorithms (with low capacity) [4] and to deci- 
sion trees [ 1, 2, 6], and not yet, until now, to the best of our knowledge, to artificial neural 
networks. These experiments displayed rather intriguing generalization properties, such as 
continued decrease in generalization error after training error reaches zero. Previous work- 
ers also disagree on the reasons for the impressive generalization performance displayed 
by AriaBoost on a large array of tasks. One issue raised by Breiman [1] and the authors of 
AdaBoost [4] is whether some of this effect is due to a reduction in variance similar to the 
one obtained from the Bagging algorithm. 
In this paper we explore the application of AdaBoost to Diabolo (auto-associative) net- 
works and multi-layer neural networks (MLPs). In doing so, we also compare three dif- 
648 H. Schwenk and Y. Bengio 
ferent versions of AdaBoost: (R) training each classifier with a fixed training set obtained 
by resampling with replacement from the original training set (as in [1]), (E) training by 
resampling after each epoch a new training set from the original training set, and (W) train- 
ing by directly weighting the cost function (here the squared error) of the neural network. 
Note that the second version (E) is a better approximation of the weighted cost function 
than the first one (R), in particular when many epochs are performed. If the variance re- 
duction induced by averaging the hypotheses from very different models explains a good 
part of the generalization performance of AdaBoost, then the weighted training version 
(W) should perform worse then the resampling versions, and the fixed sample version (R) 
should perform better then the continuously resampled version (E). 
2 AdaBoost 
AdaBoost combines the hypotheses generated by a set of classifiers trained one after the 
other. The t th classifier is trained with more emphasis on certain patterns, using a cost func- 
tion weighted by a probability distribution Dt over the training data (Dr(i) is positive and 
Ei Dt (i) -- 1). Some learning algorithms don't permit training with respect to a weighted 
cost function. In this case sampling with replacement (using the probability distribution 
Dr) can be used to approximate a weighted cost function. Examples with high probability 
would then occur more often than those with low probability, while some examples may 
not occur in the sample at all although their probability is not zero. This is particularly true 
in the simple resampling version (labeled R earlier), and unlikely when a new training 
set is resampled after each epoch (E version). Neural networks can be trained directly 
with respect to a distribution over the learning data by weighting the cost function (this is 
the W version): the squared error on the i-th pattern is weighted by the probability Dt (i). 
The result of training the t th classifier is a hypothesis ht: X --+ Y where Y = {1, ..., k} is 
the space of labels, and X is the space of input features. After the t th round the weighted 
error et of the resulting classifier is calculated and the distribution Dt+l is computed from 
Dr, by increasing the probability of incorrectly labeled examples. The global decision f is 
obtained by weighted voting. Figure 1 (left) summarizes the basic AdaBoost algorithm. It 
converges (learns the training set) if each classifier yields a weighted error that is less than 
50%, i.e., better than chance in the 2-class case. There is also a multi-class version, called 
pseudoloss-AdaBoost, that can be used when the classifier computes confidence scores for 
each class. Due to lack of space, we give only the algorithm (see figure 1, right) and we 
refer the reader to the references for more details [4, 5]. 
AdaBoost has very interesting theoretical properties, in particular it can be shown that the 
error of the composite classifier on the training data decreases exponentially fast to zero [5] 
as the number of combined classifiers is increased. More importantly, however, bounds 
on the generalization error of such a system have been formulated [7]. These are based 
on a notion of margin of classification, defined as the difference between the score of the 
correct class and the strongest score of a wrong class. In the case in which there are just 
two possible labels {-1, +1}, this is yf(z), where f is the composite classifier and y the 
correct label. Obviously, the classification is correct if the margin is positive. We now 
state the theorem bounding the generalization error of Adaboost [7] (and any classifier 
obtained by a convex combination of a set of classifiers). Let H be a set of hypotheses 
(from which the ht hare chosen), with VC-dimenstion d. Let f be any convex combination 
of hypotheses from H. Let $ be a sample of N examples chosen independently at random 
according to a distribution D. Then with probability at least 1 - 6 over the random choice 
of the training set $ from D, the following bound is satisfied for all 0 > 0: 
( 1 '/dl�g2(N/d) 
PD[yf(x) _< 0] _< P$[yf(x) _< 0] + O V ' + log(i/6) (1) 
Note that this bound is independent of the number of combined hypotheses and how they 
Training Methods for Adaptive Boosting of Neural Networks 649 
Input: sequence of N examples (x, y),..., (XN, YN) 
with labels Yi E Y - {1,..., k} 
Init: D (i) = 1/N for all i 
Repeat: 
1. Train neural network with respect 
to distribution D, and obtain 
hypothesis h,: X - Y 
2. calculate the weighted error of h,: 
abort loop 
et = E Dr(i) if et >  
i:ht(xi)yl 
3. set/3 = e,/(1 - e,) 
4. update distribution Dt 
Dt+(i) - 
zt 
with 6 i -- (ht(xi) = Yi) 
and Zt a normalization constant 
Output: final hypothesis: 
f(z)=argmax E log 1 
yY 
t:h(x)=y 
Init: letB = {(i,y): i E {1,...,N},y  Yi} 
Dx(i,y) = X/IBI for all (i,y)  B 
Repeat: 
1. Train neural network with respect 
to distribution Dt and obtain 
hypothesis h: X x Y - [0, 1] 
2. calculate the pseudo-loss of ht: 
1 
et--  EDt(i,y)(1-ht(xi, yi)+ht(xi, y)) 
(i,y)eB 
3. set/3t -- et/(1 - 
4. update distribution Dt 
� 1 
Dt+x(i, y) = Dt(2,y) fq((l+ht(xi,Yl)-ht(ei,Y)) 
Zt 
where Zt is a normalization constant 
Output: final hypothesis: 
f(x) = argmax t log ht(x,y) 
yGY 
Figure 1: AdaBoost algorithm (left), multi-class extension using confidence scores (right) 
are chosen from H. The distribution of the margins however plays an important role. It can 
be shown that the AdaBoost algorithm is especially well suited to the task of maximizing 
the number of training examples with large margin [7]. 
3 The Diabolo Classifier 
Normally, neural networks used for classification are trained to map an input vector to an 
output vector that encodes directly the classes, usually by the so called l-out-of-N encod- 
ing. An alternative approach with interesting properties is to use auto-associative neural 
networks, also called autoencoders or Diabolo networks, to learn a model of each class. 
In the simplest case, each autoencoder network is trained only with examples of the cor- 
responding class, i.e., it learns to reconstruct all examples of one class at its output. The 
distance between the input vector and the reconstructed output vector expresses the likeli- 
hood that a particular example is part of the corresponding class. Therefore classification 
is done by choosing the best fitting model. Figure 2 summarizes the basic architecture. 
It shows also typical classification behavior for an online character recognition task. The 
input and output vectors are (z, y)-coordinate sequences of a character. The visual repre- 
sentation in the figure is obtained by connecting these points. In this example the 1 is 
correctly classified since the network for this class has the smallest reconstruction error. 
The Diabolo classifier uses a distributed representation of the models which is much more 
compact than the enumeration of references often used by distance-based classifiers like 
nearest-neighbor or RBF networks. Furthermore, one has to calculate only one distance 
measure for each class to recognize. This allows to incorporate knowledge by a domain 
specific distance measure at a very low computational cost. In previous work [8], we have 
shown that the well-known tangent-distance [ 11 ] can be used in the objective function of the 
autoencoders. This Diabolo classifier has achieved state-of-the-art results in handwritten 
OCR [8, 9]. Recently, we have also extended the idea of a transformation invariant distance 
650 H. Schwenk and Y. Bengio 
score 2 I 
 A I sc37 
 net 1/q 
j  l-
