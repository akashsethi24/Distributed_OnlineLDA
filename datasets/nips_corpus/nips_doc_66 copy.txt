75O 
A DYNAMICAL APPROACH TO TEMPORAL PATTERN 
PROCESSING 
W. Scott Stornetta 
Stanford University, Physics Department, Stanford, Ca., 94305 
Tad Hogg and B. A. Huberman 
Xerox Palo Alto Research Center, Palo Alto, Ca. 94304 
ABSTRACT 
Recognizing patterns with temporal context is important for 
such tasks as speech recognition, motion detection and signature 
verification. We propose an architecture in which time serves as its 
own representation, and temporal context is encoded in the state of the 
nodes. We contrast this with the approach of replicating portions of the 
architecture to represent time. 
As one example of these ideas, we demonstrate an architecture 
with capacitive inputs serving as temporal feature detectors in an 
otherwise standard back propagation model. Experiments involving 
motion detection and word discrimination serve to illustrate novel 
features of the system. Finally, we discuss possible extensions of the 
architecture. 
INTRODUCTION 
Recent interest in connectionist, or neural networks has emphasized their 
ability to store, retrieve and process patterns 1'2. For most applications, the patterns to 
be processed are static in the sense that they lack temporal context. 
Another important class consists of those problems that require the processing 
of temporal patterns. In these the information to be learned or processed is not a 
particular pattern but a sequence of patterns. Such problems include speech 
processing, signature verification, motion detection, and predictive signal 
processing 3'8. 
More precisely, temporal pattern processing means that the desired output 
depends not only on the current input but also on those preceding or following it as 
well. This implies that two identical inputs at different time steps might yield 
different desired outputs depending on what patterns precede or follow them. 
There is another feature characteristic of much temporal pattern processing. 
Here an entire sequence of patterns is recognized as a single distinct category, 
@ American Institute of Physics 1988 
751 
generating a single output. A typical example of this would be the need to recognize 
words from a rapidly sampled acoustic signal. One should respond only once to the 
appearance of each word, even though the word consists of many samples. Thus, each 
input may not produce an output. 
With these features in mind, there are at least three additional issues which 
networks that process temporal patterns must address, above and beyond those that 
work with static patterns. The first is how to represent temporal context in the state of 
the network. The second is how to train at intermediate time steps before a temporal 
pattern is complete. The third issue is how to interpret the outputs during recognition, 
that is, how to tell when the sequence has been completed. Solutions to each of these 
issues require the construction of appropriate input and output representations. This 
paper is an attempt to address these issues, particularly the issue of representing 
temporal context in the state of the machine. We note in passing that the recognition 
of temporal sequences is distinct from the related problem of generating a sequence, 
given its first few members 9A0'11. 
TEMPORAL CLASSIFICATION 
With some exceptions 10'12, in most previous work on temporal problems the 
systems record the temporal pattern by replicating part of the architecture for each 
time step. In some instances input nodes and their associated links are replicated 3'4. In 
other cases only the weights or links are replicated, once for each of several time 
delays ?'8. In either case, this amounts to mapping the temporal pattern into a spatial 
one of much higher dimension before processing. 
These systems have generated significant and encouraging results. However, 
these approaches also have inherent drawbacks. First, by replicating portions of the 
architecture for each time step the amount of redundant computation is significantly 
increased. This problem becomes extreme when the signal is sampled very 
frequently 4. Next, by relying on replications of the architecture for each time step, the 
system is quite inflexible to variations in the rate at which the data is presented or size 
of the temporal window. Any variability in the rate of the input signal can generate an 
input pattern which bears little or no resemblance to the trained pattern. Such 
variability is an important issue, for example, in speech recognition. Moreover, having 
a temporal window of any fixed length makes it manifestly impossible to detect 
contextual effects on time scales longer than the window size. An additional difficulty 
is that a misaligned signal, in its spatial representation, may have very little 
resemblance to the correctly aligned training signal. That is, these systems typically 
suffer from not being translationally invariant in time. 
Networks based on relaxation to equilibrium ll'13'14 also have difficulties for 
use with temporal problems. Such an approach removes any dependence on initial 
752 
conditions and hence is difficult to reconcile directly with temporal problems, which by 
their nature depend on inputs from earlier times. Also, if a temporal problem is to be 
handled in terms of relaxation to equilibrium, the equilibrium points themselves must 
be changing in time. 
A NON-REPLICATED, DYNAMIC ARCHITECTURE 
We believe that many of the difficulties mentioned above are tied to the 
attempt to map an inherently dynamical problem into a static problem of higher 
dimension. As an alternative, we propose to represent the history of the inputs in the 
state of the nodes of a system, rather than by adding additional units. Such an 
approach to capturing temporal context shows some very immediate advantages over 
the systems mentioned above. First, it requires no replication of units for each distinct 
time step. Second, it does not fix in the architecture itself the window for temporal 
context or the presentation rate. These advantages are a direct result of the decision to 
let time serve as its own representation for temporal sequences, rather than creating 
additional spatial dimensions to represent time. 
In addition to providing a solution to the above problems, this system lends 
itself naturally to interpretation as an evolving dynamical system. Our approach 
allows one to think of the process of mapping an evolving input into a discrete 
sequence of outputs (such as mapping continuous speech input into a sequence of 
words) as a dynamical system moving from one attractor to another 15. 
As a preliminary example of the application of these ideas, we introduce a 
system that captures the temporal context of input patterns without replicating units 
for each time step. We modify the conventional back propagation algorithm by making 
the input units capacitive. In contrast to the conventional architecture in which the 
input nodes are used simply to distribute the signal to the next layer, our system 
performs an additional computation. Specifically, let Xi be the value computed by an 
input node at time t i , and I t be the input signal to this node at the same time. Then the 
node computes successive values according to 
Xi+l= aIi+ t + dXi (1) 
where a is an input amplitude and d is a decay rate. Thus, the result computed by an 
input unit is the sum of the current input value multiplied by a, plus a fractional part, 
d, of the previously computed value of the input unit. In the absence of further input, 
this produces an exponential decay in the activation of the input nodes. The value for d 
is chosen so that this decay reaches 1/e of its original value in a time  characteristic of 
the time scale for the particular problem, i.e., d=e 'r, where r is the presentation rate. 
The value for a is chosen to produce a specified maximum value for X, given by 
753 
aImax/(1-d). We note that Eq. (1) is equivalent to having a non-modifiable recurrent 
link with weight d on the input nodes, as illustrated in Fig. 1. 
o o 
Fig. 1: Schematic architecture with capacitive inputs. The input nodes 
compute values according to Eq. (1). Hidden and output units are 
identical to standard back propagation nets. 
The processing which takes place at the input node can also be thought of in 
terms of an infinite impulse response (IIR) digital filter. The infinite impulse response 
of the filter allows input from the arbitrarily distant past to influence the current 
output of the filter, in contrast to methods which employ fixed windows, which can be 
viewed in terms of finite impulse response (FIR) filters. The capacitive node of Fig. 1 is 
equivalent to pre-processing the signal with a filter with'transfer function a/(1-dz'l). 
This system has the unique feature that a simple transformation of the 
parameters a and d allows it to respond in a near-optimal way to a signal which differs 
from the training signal in its rate. Consider a system initially trained at rate r with 
decay rate d and amplitude a. To make use of these weights for a different presentation 
rate, r', one simply adjusts the values a 'and d'according to 
d' = d r/r' (2) 
1 - d' 
a' = a 1-d (3) 
754 
These equations can be derived by the following argument. The general idea is 
that the values computed by the input nodes at the new rate should be as close as 
possible to those computed at the original rate. Specifically, suppose one wishes to 
change the sampling rate from r to nr, where n is an integer. Suppose that at a time t o 
the computed value of the input node is X 0. [f this node receives no additional input, 
then after m time steps, the computed value of the input node will be X0 dm. For the 
more rapid sampling rate, X0 drn should be the value obtained after nm time steps. 
Thus we require 
Xo dm= Xod'mn (4) 
which leads to Eq. (2) because n-rr. Now suppose that an input I is presented m 
times in succession to an input node that is ini
