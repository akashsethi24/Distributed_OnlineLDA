Convergence of Indirect Adaptive 
Asynchronous Value Iteration Algorithms 
Vijaykumar Gullapalll 
Department of Computer Science 
University of Massachusetts 
Amherst, MA 01003 
vijaycs.umass.edu 
Andrew G. Barto 
Department of Computer Science 
University of Massachusetts 
Amherst, MA 01003 
barto@cs.umass.edu 
Abstract 
Reinforcement Learning methods based on approximating dynamic 
programming (DP) are receiving increased attention due to their 
utility in forming reactive control policies for systems embedded 
in dynamic environments. Environments are usually modeled as 
controlled Markov processes, but when the environment model is 
not known a priori, adaptive methods are necessary. Adaptive con- 
trol methods are often classified as being direct or indirect. Direct 
methods directly adapt the control policy from experience, whereas 
indirect methods adapt a model of the controlled process and com- 
pute control policies based on the latest model. Our focus is on 
indirect adaptive DP-based methods in this paper. We present a 
convergence result for indirect adaptive asynchronous value itera- 
tion algorithms for the case in which a look-up table is used to store 
the value function. Our result implies convergence of several ex- 
isting reinforcement learning algorithms such as adaptive real-time 
dynamic programming (ARTDP) (Barto, Bradtke, & Singh, 1993) 
and prioritized sweeping (Moore & Atkeson, 1993). Although the 
emphasis of researchers studying DP-bascd reinforcement learning 
has been on direct adaptive methods such as Q-Learning (Watkins, 
1989) and methods using TD algorithms (Sutton, 1988), it is not 
clear that these direct methods are preferable in practice to indirect 
methods such as those analyzed in this paper. 
695 
696 Gullapalli and Barto 
1 INTRODUCTION 
Reinforcement learning methods based on approximating dynamic programming 
(DP) are receiving increased attention due to their utility in forming reactive con- 
trol policies for systems embedded in dynamic environments. In most of this work, 
learning tasks are formulated as Markovian Decision Problems (MDPs) in which 
the environment is modeled as a controlled Markov process. For each observed 
environmental state, the agent consults a policy to select an action, which when 
executed causes a probabilistic transition to a successor state. State transitions 
generate rewards, and the agent's goal is to form a policy that maximizes the ex- 
pected value of a measure of the long-term reward for operating in the environment. 
(Equivalent formulations minimize a measure of the long-term cost of operating in 
the environment.) Artificial neural networks are often used to store value functions 
produced by these algorithms (e.g., (Tesauro, 1992)). 
Recent advances in reinforcement learning theory have shown that asynchronous 
value iteration provides an important link between reinforcement learning algo- 
rithms and classical DP methods for value iteration (VI) (Barto, Bradtke, & Singh, 
1993). Whereas conventional VI algorithms use repeated exhaustive sweeps of the 
MDP's state set to update the value function, asynchronous VI can achieve the same 
result without proceeding in systematic sweeps (Bertsekas & Tsitsiklis, 1989). If the 
state ordering of an asynchronous VI computation is determined by state sequences 
generated during real or simulated interaction of a controller with the Markov pro- 
cess, the result is an algorithm called Real-Time DP (RTDP) (Barto, Bradtke, & 
Singh, 1993). Its convergence to optimal value functions in several kinds of prob- 
lems follows from the convergence properties of asynchronous VI (Barto, Bradtke, 
& Singh, 1993). 
2 MDPS WITH INCOMPLETE INFORMATION 
Because asynchronous VI employs a basic update operation that involves computing 
the expected value of the next state for all possible actions, it requires a complete 
and accurate model of the MDP in the form of state-transition probabilities and ex- 
pected transition rewards. This is also true for the use of asynchronous VI in RTDP. 
Therefore, when state-transition probabilities and expected transition rewards are 
not completely known, asynchronous VI is not directly applicable. Problems such 
as these, which are called MDPs with incomplete information, x require more com- 
plex adaptive algorithms for their solution. An indirect adaptive method works by 
identifying the underlying MDP via estimates of state transition probabilities and 
expected transition rewards, whereas a direct adaptive method (e.g., Q-Learning 
(Watkins, 1989)) adapts the policy or the value function without forming an ex- 
plicit model of the MDP through system identification. 
In this paper, we prove a convergence theorem for a set of algorithms we call indirect 
adaptive asynchronous VI algorithms. These are indirect adaptive algorithms that 
result from simply substituting current estimates of transition probabilities and ex- 
pected transition rewards (produced by some concurrently executing identification 
These problems should not be confused with MDPs with incomplete ,tate information, 
i.e., partially observable MDPs. 
Convergence of Indirect Adaptive Asynchronous Value Iteration Algorithms 697 
algorithm) for their actual values in the asynchronous value iteration computation. 
We show that under certain conditions, indirect adaptive asynchronous VI algo- 
rithms converge with probability one to the optimal value function. Moreover, we 
use our result to infer convergence of two existing DP-based reinforcement learning 
algorithms, adaptive real-time dynamic programming (ARTDP) (Barto, Bradtke, 
& Singh, 1993), and prioritized sweeping (Moore & Atkeson, 1993). 
3 
CONVERGENCE OF INDIRECT ADAPTIVE 
ASYNCHRONOUS VI 
Indirect adaptive asynchronous VI algorithms are produced from non-adaptive algo- 
rithms by substituting a current approximate model of the MDP for the true model 
in the asynchronous value iteration computations. An indirect adaptive algorithm 
can be expected to converge only if the corresponding non-adaptive algorithm, with 
the true model used in the place of each approximate model, converges. We therefore 
restrict attention to indirect adaptive asynchronous VI algorithms that correspond 
in this way to convergent non-adaptive algorithms. We prove the following theorem: 
Theorem 1 For any finite state, finite action MDP with an infinite-horizon dis- 
counted performance measure, any indirect adaptive asynchronous VI algorithm (for 
which the corresponding non-adaptive algorithm converges) converges to the optimal 
value function with probability one if 
1) the conditions for convergence of the non-adaptive algorithm are met, 
ï¿½) in the limit, every action is executed from every state infinitely often, and 
3) the estimates of the state-transition probabilities and the expected transition re- 
wards remain bounded and converge in the limit to their true values with probability 
one. 
Proof The proof is given in Appendix A.2. 
4 DISCUSSION 
Condition 2 of the theorem, which is also required by direct adaptive methods 
to ensure convergence, is usually unavoidable. It is typically ensured by using a 
stochastic policy. For example, we can use the Gibbs distribution method for se- 
lecting actions used by Watkins (1989) and others. Given condition 2, condition 3 is 
easily satisfied by most identification methods. In particular, the simple maximum- 
likelihood identification method (see Appendix A.1, items 6 and 7) converges to the 
true model with probability one under this condition. 
Our result is valid only for the special case in which the value function is explicitly 
stored in a look-up table. The case in which general function approximators such 
as neural networks are used requires further analysis. 
Finally, an important issue not addressed in this paper is the trade-off between 
system identification and control. To ensure convergence of the model, all actions 
have to be executed infinitely often in every state. On the other hand, on-line control 
objectives are best served by executing the action in each state that is optimal 
according to the current value function (i.e., by using the certainty equivalence 
698 Gullapalli and Barto 
optimal policy). This issue has received considerable attention from control theorists 
(see, for example, (Kumar, 1985), and the references therein). Although we do not 
address this issue in this paper, for a specific estimation method, it may be possible 
to determine an action selection scheme that makes the best trade-off between 
identification and control. 
5 
EXAMPLES OF INDIRECT ADAPTIVE 
ASYNCHRONOUS VI 
One example of an indirect adaptive asynchronous VI algorithm is ARTDP (Barto, 
Bradtke, & Singh, 1993) with maximum-likelihood identification. In this algorithm, 
a randomized policy is used to ensure that every action has a non-zero probability 
of being executed in each state. The following theorem for ARDTP follows directly 
from our result and the corresponding theorem for RTDP in (Barto, Bradtke, & 
Singh, 1993): 
Theorem 2 For any discounted MDP and any initial value function, trial-based 2 
ARTDP converges with probability one. 
As a special case of the above theorem, we can obtain the result that in similar prob- 
lems the prioritized sweeping algorithm of Moore and Atkeson (Moore & Atkeson, 
1993) converges to the optimal value function. This is because prioritized sweeping 
is a special case of ARTDP in which states are selected for value updates based 
on their priority and the processing time available. A state's priority reflects the 
utility of performing an update for that state, and hence prioritized sweeping can 
improve the efficiency of asynchronous VI. A similar algorithm, Queue-Dyna (Peng 
& Williams, 1992), can also be shown to converge to the optimal value function 
using a simple extension of our result. 
CONCLUSIONS 
We have shown convergence of indirect adaptive asyn
