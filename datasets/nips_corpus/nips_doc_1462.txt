Optimizing Classifiers for Imbalanced 
Training Sets 
Grigoris Karakoulas 
Global Analytics Group 
Canadian Imperial Bank of Commerce 
161 Bay St., BCE-11, 
Toronto ON, Canada M5J 2S8 
Emaih karakoulcibc. ca 
John Shawe-Taylor 
Department of Computer Science 
Royal Holloway, University of London 
Egham, TW20 0EX 
England 
Emaih j st)dcs. rhbnc. ac. uk 
Abstract 
Following recent results [9, 8] showing the importance of the fat- 
shattering dimension in explaining the beneficial effect of a large 
margin on generalization performance, the current paper investi- 
gates the implications of these results for the case of imbalanced 
datasets and develops two approaches to setting the threshold. 
The approaches are incorporated into ThetaBoost, a boosting al- 
gorithm for dealing with unequal loss functions. The performance 
of ThetaBoost and the two approaches are tested experimentally. 
Keywords: Computational Learning Theory, Generalization, fat-shattering, large 
margin, pac estimates, unequal loss, imbalanced datasets 
I Introduction 
Shawe-Taylor [8] demonstrated that the output margin can also be used as an 
estimate of the confidence with which a particular classification is made. In other 
words if a new example has an output value well clear of the threshold we can 
be more confident of the associated classification than when the output value is 
closer to the threshold. The current paper applies this result to the case where 
there are different losses associated with a false positive, than with a false negative. 
If a significant number of data points are misclassified we can use the criterion 
of minimising the empirical loss. If, however, the data is correctly classified the 
empirical loss is zero for all correctly separating hyperplanes. It is in this case that 
the approach can provide insight into how to choose the hyperplane and threshold. 
In summary, the paper suggests ways in which a hyperplane should be optimised for 
irabalanced datasets where the loss associated with misclassifying the less prevalent 
class is higher. 
254 G. Karakoulas and J. Shawe-Taylor 
2 Background to the Analysis 
Definition 2.1 [3] Let . be a set of real-valued functions. We say that a set of 
points X is 7-shattered by r if there are real numbers rx indexed by z 6 X such 
that for all binary vectors b indexed by X, there is a function f6  . realising 
dichotomy b with margin 7. The fat-shattering dimension Faty of the set . is a 
function from the positive real numbers to the integers which maps a value 7 to the 
size of the largest 7-shattered set, if this is finite, or infinity otherwise. 
In general we are concerned with classifications obtained by thresholding real-valued 
functions. The classification values will be {-1, 1} instead of the usual {0, 1} in or- 
der to simplify some expressions. Hence, typically we will consider a set r of 
functions mapping from an input space X to the reals. For the sake of simplifying 
the presentation of our results we will assume that the threshold used for classi- 
fication is 0. The results can be extended to other thresholds without difficulty. 
Hence we implicitly use the classification functions H = T( r) = {T(f): f  r}, 
where T(f) is the function f thresholded at 0. We will say that f has 7 margin on 
the training set {(xi,yi): i = 1,...,rn}, if minl<i<m{yif(xi)} = 7- Note that a 
positive margin implies that T(f) is consistent. 
Definition 2.2 Given a real-valued function f: X -4 [-1, 1] used for classification 
by thresholding at O, and probability distribution P on X x {-1, 1}, we use erp(f) 
to denote the following probability erp(f): P{(x, y): yf(x) < 0}. Further suppose 
0 _< ri _< 1, then we use erp(flr/) to denote the probability 
erp(fl) = P{(x,y): yf(x) _< 011f(x)l _> 
The probability erp(flr/) is the probability of misclassification of a randomly chosen 
example given that it has a margin of r/or more. 
We consider the following restriction on the set of real-valued functions. 
Definition 2.3 
stants if 
The real-valued function class . is closed under addition of con- 
Note that the linear functions (with threshold weights) used in perceptrons [9] 
satisfy this property as do neural networks with linear output units. Hence, this 
property applies to the Support Vector Machine, and the neural network examples. 
We now quote a result from [8]. 
Theorem 2.4 [8] Let . be a class of real-valued functions closed under addition 
of constants with fat-shattering dimension bounded by Fat:(7 ) which is continuous 
from the right. With probability at least 1 -5 over the choice of a random rn sample 
(xi, yi) drawn according to P the following holds. Suppose that for some f  ., 
/>0, 
1. yif(xi) _> -ri+ 2'7 for all (xi,Yi) in the sample, 
2. n = I{i: yif(xi) >_ ri + 27}1, 
3. n > 3V/2m(2dln(288rn ) log2(12ern ) + ln(32rn2/5)), 
Let d = Fat:(7/6 ). Then the probability that a new example with margin rl is 
misclassified is bounded by 
_3 2dlog2(288m )1og2(12em ) +log2 5 ' 
Optimizing Classifers for Irabalanced Training Sets 255 
3 Unequal Loss Functions 
We consider the situation where the loss associated with an example is different 
for misclassification of positive and negative examples. Let Lb(x, y) be the loss 
associated with the classification function h on example (x, y). For the analysis 
considered above the loss function is taken to be Lh(x, y): Ih(x)- Yl, that is 1 if 
the point x is misclassified and 0 otherwise. This is also known as the discrete loss. 
In this paper we consider a different loss function for classification functions. 
Definition 3.1 The toss function L B is defined as LB(x, y) -- /3y + (1- y), if 
h(x)  y, and O, otherwise. 
We first consider the classical approach of minimizing the empirical loss, that is the 
loss on the training set. Since, the loss function is no longer binary the standard 
theoretical results that can be applied are much weaker than for the binary case. The 
algorithmic implications will, however, be investigated under the assumption we are 
using a hyperplane parallel to the maximal margin hyperplane. The empirical risk 
is given by ER(h) = im__l mfi(xi, Yi), for the training set {(xi, Yi) : i= 1,...,rn}. 
Assuming that the training set can be correctly classified by the hypothesis class 
this criterion will not be able to distinguish between consistent hypotheses, hence 
giving no reason not to choose the standard maximal margin choice. However, 
there is a natural way to introduce the different losses into the maximal margin 
quadratic programming procedure [1]. Here, the constraints given are specified as 
yi({w. Zi) q- O) _ 1, i -- 1, 2,..., rn. In order to force the hyperplane away from the 
positive points which will incur greater loss, a natural heuristic is to set Yi = -1 for 
negative examples and yi - 1//3 for positive points, hence making them further from 
the decision boundary. In the case where consistent classification is possible, the 
effect of this will be to move the hyperplane parallel to itself so that the margin on 
the positive side is/3 times that on the negative side. Hence, to solve the problem 
we simply use the standard maximal margin algorithm [1] and then replace the 
threshold 0 with 
1 
b - 1 +-----[(w. x +) +/(w. x-)I, (1) 
where x + (x-) is one of the closest positive (negative) points. 
The alternative approach we wish to employ is to consider other movements of the 
hyperplane parallel to itself while retaining consistency. Let 70 be the margin of the 
maximal margin hyperplane. We consider a consistent hyperplane h, with margin 
70 +/] to the positive examples, and 70 -/] to the negative example. The basic 
analytic tool is Theorem 2.4 which will be applied once for the positive examples 
and once for the negative examples (note that classifications are in the set {-1, 1}). 
Theorem 3.2 Let ho be the maximal margin hyperplane with margin 7o, while h, 
is as above with/] < 70. Set 7 + = (70 +/])/2 and 7- = (70 -/])/2. With probability 
at least i - 5 over the choice of a random rn sample (ci, Yi) drawn according to P 
the following holds. Suppose that for ho 
I. no -- 1{i: yiho(xi) _ 2/] + 70)1, 
2. no >_ 3v/2rn(dln(288rn ) log2(12em) + ln(8/5)), 
Let d + = Fat(7+/6) and d- = Fat(7-/6 ). Then we can bound the expected loss 
by 
3 
--(2 max(/3d + , d-) log2 (288m) log2(12em) +/3 log2(32m 2/5)) 
no 
256 G. Karakoulas and J. Shawe-Taylor 
Proof: Using Theorem 2.4 we can bound the probability of error given that the 
correct classification is positive in terms of the expression with the fat shattering 
dimension d + and n = no, while for a negative example we can bound the probability 
of error in terms of the expression with fat shattering dimension d- and n = m. 
Hence, the expected loss can be bounded by taking the maximum of the second 
bound with n + in place of m together with a factor /3 in front of the second log 
term and the first bound multiplied by fl. ï¿½ 
The bound obtained suggests a way of optimising the choice of r/, namely to minimise 
the expression for the fat shattering dimension of linear functions [9]. Solving for r/ 
in terms of 70 and fl gives 
(2) 
This choice of r/ does not in general agree with that suggested by the choice of 
the threshold b in the previous section. In a later section we report on initial 
experiments for investigating the performance of these different choices. 
4 The ThetaBoost Algorithm 
The above idea for adjusting the margin in the case of unequal loss function can 
also be applied to the AdaBoost algorithm [2] which has been shown to maximise 
the margin on the training examples and hence the generalization can be bounded 
in terms of the margin and the fat-shattering dimension of the functions that can 
be produced by the algorithm [6]. We will first develop a boosting algorithm for 
unequal loss functions and then extend it for adjustable margin. More specifically, 
assume: (i) a set of training examples (x
