Tractable Variational Structures for 
Approximating Graphical Models 
David Barber Wim Wiegerinck 
{davidb, wimw} �mbfys. kun. nl 
RWCP* Theoretical Foundation SNN t University of Nijmegen 
6525 EZ Nijmegen, The Netherlands. 
Abstract 
Graphical models provide a broad probabilistic flamework with ap- 
plications in speech recognition (Hidden Markov Models), medical 
diagnosis (Belief networks) and artificial intelligence (Boltzmann 
Machines). However, the computing time is typically exponential 
in the number of nodes in the graph. Within the variational flame- 
work for approximating these models, we present two classes of dis- 
tributions, decimatable Boltzmann Machines and Tractable Belief 
Networks that go beyond the standard factorized approach. We 
give generalised mean-field equations for both these directed and 
undirected approximations. Simulation results on a small bench- 
mark problem suggest using these richer approximations compares 
favorably against others previously reported in the literature. 
I Introduction 
Graphical models provide a powerful framework for probabilistic inference[l] but 
suffer intractability when applied to large scale problems. Recently, variational ap- 
proximations have been popular [2, 3, 4, 5], and have the advantage of providing 
rigorous bounds on quantities of interest, such as the data likelihood, in contrast to 
other approximate procedures such as Monte Carlo methods[1]. One of the original 
models in the neural networks community, the Boltzmann machine (BM), belongs 
to the class of undirected graphical models. The lack of a suitable algorithm has 
hindered its application to larger problems. The deterministic BM algorithm[6], a 
variational procedure using a factorized approximating distribution, speeds up the 
learning of BMs, although the simplicity of this approximation can lead to unde- 
sirable effects[7]. Factorized approximations have also been successfully applied to 
sigmoid belief networks[4]. One approach to producing a more accurate approxi- 
mation is to go beyond the class of factorized approximating models by using, for 
example, mixtures of factorized models. However, it may be that very many mix- 
ture components are needed to obtain a significant improvement beyond using the 
factorized approximation[5]. In this paper, after describing the variational learn- 
*Real World Computing Paxtnership 
tFoundation for Neural Networks 
184 D. Barber and W. Wiegerinck 
ing flamework, we introduce two further classes of non-factorized approximations, 
one undirected (decimatable BMs in section (3)) and the other, directed (Tractable 
Belief Networks in section (4)). To demonstrate the potential benefits of these 
methods, we include results on a toy benchmark problem in section (5) and discuss 
their relation to other methods in section (6). 
2 �ariational Learning 
We assume the existence of a graphical model P with known qualitative structure 
but for which the quantitative parameters of the structure remain to be learned from 
data. Given that the variables can be considered as either visible (V) or hidden 
(H), one approach to learning is to carry out maximum likelihood on the visible 
variables for each example in the dataset. Considering the KL divergence between 
the true distribution P(HIV ) and a distribution Q(H), 
Q(H) 
KL(Q(H),P(HIV)) =  Q(H)ln ):,(HiV) _> 0 
H 
and using P(HIV ) = P(H, V)/P(V) gives the bound 
lnP(V) > -  Q(H)lnQ(H) +  Q(H)lnP(H,V) (1) 
H H 
Betraying the connection to statistical physics, the first term is termed the en- 
tropy and the second the energy. One typically chooses a variational distribu- 
tion Q so that the entropic term is tractable. We assume that the energy E(Q) 
is similarly computable, perhaps with recourse to some extra variational bound (as 
in section (5)). By tractable, we mean that all necessary marginals and desired 
quantities are computationally feasible, regardless of the issue of the scaling of the 
computational effort with the graph size. Learning consists of two iterating steps: 
first optimize the bound (1) with respect to the parameters of Q, and then with 
respect to the parameters of P(H, V). We concentrate here on the first step. For 
clarity, we present our approach for the case of binary variables si  {0, 1}, i = 1..N. 
We now consider two classes of approximating distributions Q. 
3 Undirected Q: Decimatable Boltzmann Machines 
Boltzmann machines describe probability distributions parameterized by a sym- 
metric weight matrix J 
1 
Q(s) =  exp�, � ----  Jij$isj '- s'Js (2) 
ij 
where the normalization constant, or partition function is Z = , exp �. For 
convenience we term the diagonals of J the biases, hi = Jii. Since In Z(J, h) is a 
generating function for the first and second order statistics of the variables s, the 
entropy is tractable provided that Z is tractable. For general connection structures, 
J, computing Z is intractable as it involves a sum over 2 v states; however, not all 
Boltzmann machines are intractable. A class of tractable structures is described by 
a set of so-called decimation rules in which nodes from the graph can be removed 
one by one, fig(I). Provided that appropriate local changes are made to the BM 
parameters, the partition function of the reduced graph remains unaltered (see eg 
[2]). For example, node c in fig(l) can be removed, provided that the weight matrix 
J and bias h are transformed, J  J', h  h', with J}c = Jc = h' = 0 and 
1 (1 + e he) (1 + e hc+2('*+'*)) 1 + enc+2aa' b.c 
Jb = Jab +  In (1 + e h+2Ja) (1 + e h+2&�) ' hta/b = ha/b + In 1 + e hc (3) 
Tractable Variational Structures for Approximating Graphical Models 185 
Figure 1: A decimation rule for BMs. We can remove the upper node on the left so 
that the partition function of the reduced graph is the same. This requires a simple 
change in the parameters J, h coupling the two nodes on the right (see text). 
By repeatedly applying such rules, Z is calculable in time linear in N. 
3.1 Fixed point (Mean Field) Equations 
Using (2) in (1), the bound we wish to optimize with respect to the parameters 
0 = (J, h) of Q has the form ((.../ denotes averages with respect to Q) 
B(O) = - {qb) + In Z + E(O) 
(4) 
where E(O) is the energy. Differentiating (4) with respect to Jij (i  j) gives 
OB OE 
OJi = - E Fij,ktJkt + OJi 
kl 
(5) 
where Fij,kt = (sissst) - (sis) (sst) is the Fisher information matrix. A similar 
expression holds for the bias parameters, h, so that we can form a linear fixed point 
equation in the total parameter set 0 where the derivatives of the bound vanish. 
This suggests the iterative solution, 0 'ew = F-XVof where the right hand side is 
evaluated at the current parameter values, 0 �ta. 
4 Directed Q: Tractable Belief Networks 
Belief networks are products of conditional probability distributions, 
Q(H) = IX Q(Hilri) (6) 
iH 
in which ri denotes the parents of node i (see for example, [1]). The efficiency 
of computation depends on the underlying graphical structure of the model and is 
exponential in the maximal clique size (of the moralized triangulated graph [1]). We 
now assume that our model class consists of belief networks with a fixed, tractable 
graphical structure. The entropy can then be computed efficiently since it decouples 
into a sum of averaged entropies per site i (Q(z'i) -= 1 if ri = b), 
H iH rrl H 
(7) 
Note that the conditional entropy at each site i is trivial to compute since the values 
required can be read off directly from the definition of Q (6). By assumption, the 
marginals Q(z'i) are tractable, and can be found by standard methods, for example 
using the Junction Tree Algorithm[I]. 
To optimize the bound (1), we parameterize Q via its conditional probabilities, 
qi(wi) = Q(Hi = llwi). The remaining probability (2(Hi = 01wi) follows from 
186 D. Barber and . Wiegerinck 
normalization. We therefore have a set {qi(7ri)171'i -- (0... 0),... , (1... 1)) of vari- 
ational parameters for each node in the graph. Setting the gradient of the bound 
with respect to the qi(7ri)'S equal to zero yields the equations 
+ 
qi(7ri) -- or Q(Tri) 
(8) 
with 
Li,, = - y [X7i,,Q(wj)] y Q(HjIwj)In Q(HjIwy ) 
j rj Hj 
(9) 
where a (z) = 1/(1 + e-Z). The gradient Vi,, is with respect to qi(wi). The 
explicit evaluation of the gradients can be performed efficiently, since all that need 
to be differentiated are at most scalar functions of quantities that depend again 
only linearly on the parameters qi(ri). To optimize the bound, we iterate (8) till 
convergence, analogous to using factorized models[4]. However, the more powerful 
class of approximating distributions described by belief networks should enable a 
much tighter bound on the likelihood of the visible units. 
5 Application to Sigmoid Belief Networks 
We now describe an application of these non-factorized approximations to a par- 
ticular class of directed graphical models, sigmoid belief networks[8] for which the 
conditional distributions have the form 
P(si=l[wi)--cr (Wijsjq-ki) 
J 
(10) 
Wij - 0 if j ( ri. The joint distribution then has the form 
P(H,V) = Hexp[zisi - ln(1 + ea)] (11) 
i 
where zi = Y'}j Wijsj + ki. In (11) it is to be understood that the visible units are 
set to their observed values. In the lower bound (1), unfortunately, the average of 
In P(H, V) is not tractable, since (ln [1 + eZ]) does not decouple into a polynomial 
number of single site averages. Following [4] we use therefore the bound 
{ln [1 + eZ]) < {z)+ In (e -ez + e (-e) } 
(12) 
where  is a variational parameter in [0, 1]. We can then define the energy function 
E(Q, ) =  Wij (sisj)+  [zi (si)- i kiwi- i In (e -i + e(-')i)(13 ) 
ij i ' ' 
where /i = ki - Yj jWji. Expect for the final term, the energy is a function of 
first or second order statistics of the variables. For using a BM as the variational 
distribution, the final terms of (13) (e 
