I 
A Comparative Study Of A Modified 
Bumptree Neural Network With Radial Basis 
Function Networks and the Standard Multi- 
Layer Perceptron. 
Richard T.J. Bostock and Alan J. Harget 
Department of Computer Science & Applied Mathematics 
Aston University 
Birmingham 
England 
Abstract 
Bumptrees are geometric data structures introduced by Omohundro 
(1991) to provide efficient access to a collection of functions on a 
Euclidean space of interest. We describe a modified bumptree structure 
that has been employed as a neural network classifier, and compare its 
performance on several classification tasks against that of radial basis 
function networks and the standard mutli-layer perceptron. 
1 INTRODUCTION 
A number of neural network studies have demonstrated the utility of the multi-layer 
perceptron (MLP) and shown it to be a highly effective paradigm. Studies have also 
shown, however, that the MLP is not without its problems, in particular it requires an 
extensive training time, is susceptible to local minima problems and its performance is 
dependent upon its internal network architecture. In an attempt to improve upon the 
generalisation performance and computational efficiency a number of studies have been 
undertaken principally concerned with investigating the parametrisation of the MLP. It is 
well known, for example, that the generalisation performance of the MLP is affected by 
the number of hidden units in the network, which have to be determined empirically since 
theory provides no guidance. A number of investigations have been conducted into the 
possibility of automatically determining the number of hidden units during the training 
phase (Bostock, 1992). The results show that architectures can be attained which give 
satisfactory, although generally sub-optimal, performance. 
Alternative network architectures such as the Radial Basis Function (RBF) network have 
also been studied in an attempt to improve upon the performance of the MLP network. 
The RBF network uses basis functions in which the weights are effective over only a 
small portion of the input space. This is in contrast to the MLP network where the 
weights are used in a more global fashion, thereby encoding the characteristics of the 
training set in a more compact form. RBF networks can be rapidly trained thus making 
240 
Modified Bumptree Neural Network and Standard Multi-Layer Perceptron 241 
them particularly suitable for situations where on-line incremental learning is required. 
The RBF network has been successfully applied in a number of areas such as speech 
recognition (Renals, 1992) and financial forecasting (Lowe, 1991). Studies indicate that 
the RBF network provides a viable alternative to the MLP approach and thus offers 
encouragement that networks employing local solutions are worthy of further 
investigation. 
In the past few years there has been an increasing interest in neural network architectures 
based on tree structures. Important work in this area has been carded out by Omohundro 
(1991) and Gentric and Withagen (1993). These studies seem to suggest that neural 
networks employing a tree based structure should offer the same benefits of reduced 
training time as that offered by the RBF network. The particular tree based architecture 
examined in this study is the bumptree which provides efficient access to collections of 
functions on a Euclidean space of interest. A bumptree can be viewed as a natural 
generalisation of several other geometric data structures including oct-trees, k-d trees, 
balltrees (Omohundro, 1987) and boxtrees (Omohundro, 1989). 
In this paper we present the results of a comparative study of the performance of the three 
types of neural networks described above over a wide range of classification problems. 
The performance of the networks was assessed in terms of the percentage of correct 
classifications on a test, or generalisation data set, and the time taken to train the 
network. Before discussing the results obtained we shall give an outline of the 
implementation of our bumptree neural network since this is more novel than the other 
two networks. 
2 THE BUMPTREE NEURAL NETWORK 
Bumptree neural networks share many of the underlying principles of decision trees but 
differ from them in the manner in which patterns are classified. Decision trees partition 
the problem space into increasingly small areas. Classification is then achieved by 
determining the lowest branch of the tree which contains a reference to the specified point. 
The bumptree neural network described in this paper also employs a tree based structure to 
partition the problem space, with each branch of the tree being based on multiple 
dimensions. Once the problem space has been partitioned then each branch can be viewed 
as an individual neural network modelling its own local area of the problem space, and 
being able to deal with patterns from multiple output classes. 
Bumptrees model the problem space by subdividing the space allowing each division to 
be described by a separate function. Initial partitioning of the problem space is achieved 
by randomly assigning values to the root level functions. A learning algorithm is applied 
to determine the area of influence of each function and an associated error calculated. If 
the error exceeds some threshold of acceptability then the area in question is further 
subdivided by the addition of two functions; this process continues until satisfactory 
performance is achieved. The bumptree employed in this study is essentially a binary tree 
in which each leaf of the tree corresponds to a function of interest although the possibility 
exists that one of the functions could effectively be redundant if it fails to attract any of 
the patterns from its parent function. 
A number of problems had to be resolved in the design and implementation of the 
bumptree. Firstly, an appropriate procedure had to be adopted for partitioning the 
242 Bostock and Harget 
problem space. Secondly, consideration had to be given to the type of learning algorithm 
to be employed. And finally, the mechanism for calculating the output of the network 
had to be determined. A detailed discussion of these issues and the solutions adopted now 
follows. 
2.1 PARTITIONING THE PROBLEM SPACE 
The bumptree used in this study employed gaussian functions to partition the problem 
space, with two functions being added each time the space was partitioned. Patterns were 
assigned to whichever of the functions had the higher activation level with the restfiction 
that the functions below the root level could only be active on patterns that activated their 
parents. To calculate the activation of the gaussian function the following expression 
was used: 
A = exp -( ï¿½'5'((In?C>/a)2* 1 / (a t * /3.14159 * 2) (1) 
where Afp is the activation of function f on pattern p over all the input dimensions, aft is 
the radius of function f in input dimension i, Cfi is the centre of function f in input 
dimension i, and Inpi is the ith dimension of the pth input vector. 
It was found that the locations and radii of the functions had an important impact on the 
performance of the network. In the original bumptree introduced by Omohundro every 
function below the root level was required to be wholly enclosed by its parent function. 
This restriction was found to degrade the performance of the bumptree particularly if a 
function had a very small radius since this would produce very low levels of activation for 
most patterns. In our studies we relaxed this constraint by assigning the radius of each 
function to one, since the data presented to the bumptree was always normalised between 
zero and one. This modification led to an improved performance. 
A number of different techniques were examined in order to effectively position the 
functions in the problem space. The first approach considered, and the simplest, involved 
selecting two initial sets of centres for the root function with the centre in each dimension 
being allocated a value between zero and one. The functions at the lower levels of the tree 
were assigned in a similar manner with the requirement that their centres fell within the 
area of the problem space for which their parent function was active. The use of non- 
hierarchical clustering techniques such as the Forgy method or the K-means clustering 
technique developed by MacQueen provided other alternatives for positioning the 
functions. The approach finally adopted for this study was the multiple-initial function 
(MIF) technique. 
In the MIF procedure ten sets of functions centres were initially defined by random 
assignment and each pattern in the training set assigned to the function with the highest 
activation level. A goodness measure was then determined for each function over all 
patterns for which the function was active. The goodness measure was defined as the 
square of the error between the calculated and observed values divided by the number of 
active patterns. The function with the best value was retained and the remaining 
functions that were active on one or more patterns had their centres averaged in each 
dimension to provide a second function. The functions were then added to the network 
structure and the patrems assigned to the function which gave the greater activation. 
Modified Bumptree Neural Network and Standard Multi-Layer Perceptron 243 
2.2 THE LEARNING ALGORITHM 
A bumptree neural network comprises a number of functions each function having its 
own individual weight and bias parameters and each function being responsive to different 
characteristics in the training set. The bumptree employed a weighted value for every 
input to output connection and a single bias value for each output unit. Several different 
learning algorithms for determining the weight and bias values were considered together 
with a genetic algorithm approach (Williams, 1993). A one-shot lea
