Learning Bayesian belief networks with 
neural network estimators 
Stefano Monti* 
*Intelligent Systems Program 
University of Pittsburgh 
901M CL, Pittsburgh, PA- 15260 
smont iisp.pitt. edu 
Gregory F. Cooper*,** 
**Center for Biomedica Informatics 
University of Pittsburgh 
8084 Forbes Tower, Pittsburgh, PA - 15261 
gf ccbmi. upmc. edu 
Abstract 
In this paper we propose a method for learning Bayesian belief 
networks from data. The method uses artificial neural networks 
as probability estimators, thus avoiding the need for making prior 
assumptions on the nature of the probability distributions govern- 
ing the relationships among the participating variables. This new 
method has the potential for being applied to domains containing 
both discrete and continuous variables arbitrarily distributed. We 
compare the learning performance of this new method with the 
performance of the method proposed by Cooper and Herskovits 
in [7]. The experimental results show that, although the learning 
scheme based on the use of ANN estimators is slower, the learning 
accuracy of the two methods is comparable. 
Category: Algorithms and Architectures. 
1 Introduction 
Bayesian belief networks (BBN) are a powerful formalism for representing and rea- 
soning under uncertainty. This representation has a solid theoretical foundation 
[13], and its practical value is suggested by the rapidly growing number of areas to 
which it is being applied. BBNs concisely represent the joint probability distribution 
over a set of random variables, by explicitly identifying the probabilistic dependen- 
cies and independencies between these variables. Their clear semantics make BBNs 
particularly. suitable for being used in tasks such as diagnosis, planning, and control. 
The task of learning a BBN from data can usually be formulated as a search 
over the space of network structures, and as the subsequent search for an opti- 
mal parametrization of the discovered structure or structures. The task can be 
further complicated by extending the search to account for hidden variables and for 
Learning Bayesian Belief Networks with Neural Network Estimators 579 
the presence of data points with missing values. Different approaches have been 
successfully applied to the task of learning probabilistic networks from data [5]. 
In all these approaches, simplifying assumptions are made to circumvent practi- 
cal problems in the implementation of the theory. One common assumption that 
is made is that all variables are discrete, or that all variables are continuous and 
normally distributed. 
In this paper, we propose a novel method for learning BBNs from data that makes 
use of artificial neural networks (ANN) as probability distribution estimators, thus 
avoiding the need for making prior assumptions on the nature of the probability 
distribution governing the relationships among the participating variables. The use 
of ANNs as probability distribution estimators is not new [3], and its application to 
the task of learning Bayesian belief networks from data has been recently explored 
in [11]. However, in [11] the ANN estimators were used in the parametrization of the 
BBN structure only, and cross validation was the method of choice for comparing 
different network structures. In our approach, the ANN estimators are an essential 
component of the scoring metric used to search over the BBN structure space. We 
ran several simulations to compare the performance of this new method with the 
learning method described in [7]. The results show that, although the learning 
scheme based on the use of ANN estimators is slower, the learning accuracy of the 
two methods is comparable. 
The rest of the paper is organized as follows. In Section 2 we briefly introduce 
the Bayesian belief network formalism and some basics of h})w to learn BBNs from 
data. In Section 3, we describe our learning method, and detail the use of artificial 
neural networks as probability distribution estimators. In Section 4 we present some 
experimental results comparing the performance of this new method with the one 
proposed in [7]. We conclude the paper with some suggestions for further research. 
2 Background 
A Bayesian belief network is defined by a triple (G,f],P), where G = (X,E) is 
a directed acyclic graph with a set of nodes ,r = {Xl,..., x} representing do- 
main variables, and with a set of arcs E representing probabilistic dependencies 
among domain variables; f] is the space of possible instantiations of the domain 
variables1; and P is a probability distribution over the instantiations in fl. Given 
a node x  ,r, we use 7r to denote the set of parents of x in ,r. The essential 
property of BBNs is summarized by the Markov condition, which asserts that each 
variable is independent of its non-descendants given its parents. This property al- 
lows for the representation of the multivariate joint probability distribution over 
A' in terms of the univariate conditional distributions P(xi 17ri, Oi) of each variable 
xi given its parents 7ri, with Oi the set of parameters needed to fully characterize 
the conditional probability. Application of the chain rule, together with the Markov 
condition, yields the following factorization of the joint probability of any particular 
instantiation of all n variables: 
II I ' 0,). (1) 
i----1 
1An instantiation w of all n variables in ,Y is an n-uple of values {x,..., x'} such that 
xi = x', fori=l...n. 
580 S. Monti and G. E Cooper 
2.1 Learning Bayesian belief networks 
The task of learning BBNs involves learning the network structure and learning 
the parameters of the conditional probability distributions. A well established set 
of learning methods is based on the definition of a scoring metric measuring the 
fitness of a network structure to the data, and on the search for high-scoring network 
structures based on the defined scoring metric [7, 10]. We focus on these methods, 
and in particular on the definition of Bayesian scoring metrics. 
In a Bayesian framework, ideally classification and prediction would be performed 
by taking a weighted average over the inferences of every possible belief network 
containing the domain variables. Since this approach is in general computationally 
infeasible, often an attempt has been made to use a high scoring belief network for 
classification. We will assume this approach in the remainder of this paper. 
The basic idea of the Bayesian approach is to maximize the probability P(Bs I1)) = 
P(Bs,I))/P(I)) of a network structure Bs given a database of cases 1). Because 
for all network structures the term P(1)) is the same, for the purpose of model 
selection it suffices to calculate P(Bs, l)) for all Bs. The Bayesian metrics developed 
so far all rely on the following assumptions: 1) given a BBN structure, all cases 
in 1) are drawn independently from the same distribution (multinomial sample); 
2) there are no cases with missing values (complete database); 3) the parameters 
of the conditional probability distribution of each variable are independent (global 
parameter independence); and 4) the parameters associated with each instantiation 
of the parents of a variable are independent (local parameter independence). 
The application of these assumptions allows for the following factorization of the 
probability P(Bs, 1)) 
P(Bs,1)) = P(Bs)P(1) IBs) = P(Bs) 
1-I s(xi, 1)) , 
i----1 
(2) 
where n is the number of nodes in the network, and each $(xi,Tri,1) ) is a term 
measuring the contribution of xi and its parents ri to the overall score of the 
network Bs. The exact form of the terms s(xi ri, 1)) slightly differs in the Bayesian 
scoring metrics defined so far, and for lack of space we refer the interested reader 
to the relevant literature [7, 10]. 
By looking at Equation (2), it is clear that if we assume a uniform prior distribution 
over all network structures, the scoring metric is decomposable, in that it is just 
the product of the s(xi, ?ri,1)) over all xi times a constant P(Bs). Suppose that 
two network structures Bs and Bs, differ only for the presence or absence of 
given arc into xi. To compare their metrics, it suffices to compute 
for both structures, since the other terms are the same. Likewise, if we assume 
a decomposable prior distribution over network structures, of the form P(Bs) = 
1-[iPi, as suggested in [10], the scoring metric is still decomposable, since we can 
include each Pi into the corresponding s(xi, 7ri, 1)). 
Once a scoring metric is defined, a search for a high-scoring network structure can 
be carried out. This search task (in several forms) has been shown to be NP-hard 
[4, 6]. Various heuristics have been proposed to find network structures with a high 
score. One such heuristic is known as K2 [7], and it implements a greedy search over 
the space of network structures. The algorithm assumes a given ordering on the 
variables. For simplicity, it also assumes that no prior information on the network is 
available, so the prior probability distribution over the network structures is uniform 
and can be ignored in comparing network structures. 
Learning Bayesian Belief Networks with Neural Network Estimators 581 
The Bayesian scoring metrics developed so far either assume discrete variables 
[7, 10], or continuous variables normally distributed [9]. In the next section, we 
propose a possible generalization which allows for the inclusion of both discrete and 
continuous variables with arbitrary probability distributions. 
3 An ANN-based scoring metric 
The main idea of this work is to use'artificial neural networks as probability estima- 
tors, to define a decomposable scoring metric for which no informative priors on the 
class, or classes, of the probability distributions of the participating variables are 
needed. The first three of the four assumptions described in the previous section 
are still needed, namely, the assumption of a multino
