Boosting with Multi-Way Branching in 
Decision Trees 
Yishay Mansour 
David McAllester 
AT&T Labs-Research 
180 Park Ave 
Florham Park NJ 07932 
{mansour, dmac}@research.att.com 
Abstract 
It is known that decision tree learning can be viewed as a form 
of boosting. However, existing boosting theorems for decision tree 
learning allow only binary-branching trees and the generalization to 
multi-branching trees is not immediate. Practical decision tree al- 
gorithms, such as CART and C4.5, implement a trade-off between 
the number of branches and the improvement in tree quality as 
measured by an index function. Here we give a boosting justifica- 
tion for a particular quantitative trade-off curve. Our main theorem 
states, in essence, that if we require an improvement proportional 
to the log of the number of branches then top-down greedy con- 
struction of decision trees remains an effective boosting algorithm. 
I Introduction 
Decision trees have been proved to be a very popular tool in experimental machine 
learning. Their popularity stems from two basic features -- they can be constructed 
quickly and they seem to achieve low error rates in practice. In some cases the 
time required for tree growth scales linearly with the sample size. Efficient tree 
construction allows for very large data sets. On the other hand, although there 
are known theoretical handicaps of the decision tree representations, it seem that 
in practice they achieve accuracy which is comparable to other learning paradigms 
such as neural networks. 
While decision tree learning algorithms are popular in practice it seems hard to 
quantify their success in a theoretical model. It is fairly easy to see that even 
if the target function can be described using a small decision tree, tree learning 
algorithms may fail to find a good approximation. Kearns and. Mansour [6] used 
the weak learning hypothesis to show that standard tree learning algorithms perform 
boosting. This provides a theoretical justification for decision tree learning similar 
Boosting with Multi-Way Branching in Decision Trees 301 
to justifications that have been given for various other boosting algorithms, such as 
AdaBoost [4]. 
Most decision tree learning algorithms use a top-down growth process. Given a 
current tree the algorithm selects some leaf node and extends it to an internal node 
by assigning to it some branching function and adding a leaf to each possible 
output value of this branching function. The set of branching functions may differ 
from one algorithm to another, but most algorithms used in practice try to keep the 
set of branching functions fairly simple. For example, in C4.5 [7], each branching 
function depends on a single attribute. For categorical attributes, the branching 
is according to the attribute's value, while for continuous attributes it performs a 
comparison of the attribute with some constant. 
Of course such top-down tree growth can over-fit the data -- it is easy to construct 
a (large) tree whose error rate on the training data is zero. However, if the class of 
splitting functions has finite VC dimension then it is possible to prove that, with 
high confidence of the choice of the training data, for all trees T the true error rate 
of T is bounded by (T) -{- O (I) where (T) is the error rate of T on the 
training sample, ITI is the number of leaves of T, and rn is the size of the training 
sample. Over-fitting can be avoided by requiring that top-down tree growth produce 
a small tree. In practice this is usually done by constructing a large tree and then 
pruning away some of its nodes. Here we take a slightly different approach. We 
assume a given target tree size s and consider the problem of constructing a tree T 
with ITI = s and (T) as small as possible. We can avoid over-fitting by selecting a 
small target value for the tree size. 
A fundamental question in top-down tree growth is how to select the branching 
function when growing a given leaf. We can think of the target size as a budget. 
A four-way branch spends more of the tree size budget than does a two-way branch 
-- a four-way branch increases the tree size by roughly the same amount as two two- 
way branches. A sufficiently large branch would spend the entire tree size budget in 
a single step. Branches that spend more of the tree size budget should be required to 
achieve more progress than branches spending less of the budget. Naively, one would 
expect that the improvement should be required to be roughly linear in the number 
of new leaves introduced -- one should get a return proportional to the expense. 
However, a weak learning assumption and a target tree size define a nontrivial 
game between the learner and an adversary. The learner makes moves by selecting 
branching functions and the adversary makes moves by presenting options consistent 
with the weak learning hypothesis. We prove here that the learner achieve a better 
value in this game by selecting branches that get a return considerably smaller than 
the naive linear return. Our main theorem states, in essence, that the return need 
only be proportional to the log of the number of branches. 
2 Preliminaries 
We assume a set Y of instances and an unknown target function f mapping Y 
to {0, 1}. We assume a given training set S which is a set of pairs of the form 
(x, f(x)). We let 7/be a set of potential branching functions where each h 6 7/is 
a function from A' to a finite set Rh -- we allow different functions in 7/ to have 
different ranges. We require that for any h 6 7/ we have I Rhl >_ 2. An 7/-tree is 
302 Y. Mansour and D. McAllester 
a tree where each internal node is labeled with an branching function h 6 7/ and 
has children corresponding to the elements of the set Rh. We define [T[ to be the 
number of leaf nodes of T. We let L(T) be the set of leaf nodes of T. For a given tree 
T, leaf node � of T and sample $ we write St to denote the subset of the sample $ 
reaching leaf �. For � 6 T we define/dr to be the fraction of the sample reaching leaf 
�, i.e., [St[/[S[. We define t to be the fraction of the pairs (x, fix)) in St for which 
f(x) = 1. The training error of T, denoted e(T), is -teL(,)ldt min(t, 1 - t). 
3 The Weak Learning Hypothesis and Boosting 
Here, as in [6], we view top-down decision tree learning as a form of Boosting [8, 3]. 
Boosting describes a general class of iterative algorithms based on a weak learning 
hypothesis. The classical weak learning hypothesis applies to classes of Boolean 
functions. Let 7/2 be the subset of branching functions h 6 7/ with [Rh[ = 2. For 
d > 0 the classical d-weak learning hypothesis for 7/2 states that for any distribution 
on A' there exists an h 6 7/2 with Prt(h(x)k fix)) _ 1/2-d. Algorithms designed 
to exploit this particular hypothesis for classes of Boolean functions have proved to 
be quite useful in practice [5]. 
Kearns and Mansour show [6] that the key to using the weak learning hypothesis 
for decision tree learning is the use of an index function I: [0, 1] -+ [0, 1] where 
I(q) <_ 1, I(q) _> min(q,(1- q)) and where I(T)is defined to be 
Note that these conditions imply that g(T) <_ I(T). For any sample W let qw be 
the fraction of pairs (x, f(x))  W such that f(x) = 1. For any h 6 7/ let Ta be 
the decision tree consisting of a single internal node with branching function h plus 
a leaf for each member of IRal. Let Iw(Ta) denote the value of I(Ta) as measured 
with respect to the sample W. Let A(W, h) denote I(qw)- Iw(Th). The quantity 
A(W, h) is the reduction in the index for sample W achieved by introducing a single 
branch. Also note that/dtA($t, h) is the reduction in I(T) when the leaf� is replaced 
by the branch h. Kearns and Mansour [6] prove the following lemma. 
Lemma 3.1 (Kearns & Mansour) Assuming the d-weak learning hypothesis for 
7t2, and taking I(q) to be 2v/q(1 - q), we have that for any sample W there exists 
an h  7t2 such that A (W, h) _> Y6 I(qw). 
This lemma motivates the following definition. 
Definition I We say that 7t2 and I satisfies the y-weak tree-growth hypothesis if 
for any sample W from 2( there exists an h  7t2 such that A(W, h) >_ 7I(qw). 
Lemma 3.1 states, in essence, that the classical weak learning hypothesis implies the 
weak tree growth hypothesis for the index function I(q) = 2v/q(1 - q). Empirically, 
however, the weak tree growth hypothesis seems to hold for a variety of index 
functions that were already used for tree growth prior to the work of Kearns and 
Mansour. The Ginni index I(q) = 4q(1- q) is used in CART [1] and the entropy 
I(q) = -qlog q-(1- q)log(1- q)is used in C4.5 [7]. It has long been empirically 
observed that it is possible to make steady progress in reducing I(T) for these 
choices of I while it is difficult to make steady progress in reducing g(T). 
We now define a simple binary branching procedure. For a given training set $ 
and target tree size s this algorithm grows a tree with ITI = s. In the algorithm 
Boosting with Multi-Way Branching in Decision Trees 303 
0 denotes the trivial tree whose root is a leaf node and Tt,h denotes the result of 
replacing the leaf � with the branching function h and a new leaf for each element 
of Rh. 
WHILE (ITI < s) DO 
� - argmax ]() 
h - argmax7t  A($, h) 
T - T,; 
END-WHILE 
n-1 n-1 _ff 
We now define e(n) to be the quantity ]-Ii= (1- ). Note that e(n) <_ I-Ii= e = 
e-'7 Zia__l 1 1/i < e_, 7 Inn ____ n-3' ' 
Theorem 3.2 (Kearns & Mansour) If Yt2 and I satisfy the y-weak tree growth 
hypothesis then the binary branching procedure produces a tree T with g(T) _ I(T) _ 
e(lTI) _< 
Proof: The proof is by induction on the number of iterations of the procedure. 
We have that I(0) _ 1 = e(1) so the initial tree immediately satisfies the condi- 
tion. We now assume that the condition is satisfied by T at the begining of an 
iteration and prove that it remains satisfied by Tt,a at the e
