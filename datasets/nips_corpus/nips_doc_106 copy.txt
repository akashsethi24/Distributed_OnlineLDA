739 
AN ANALOG SELF-ORGANIZING 
NEURAL NETWORK CHIP 
James R. Mann 
Sheldon Gilbert 
MIT Lincoln Laboratory 
244 Wood Street 
Lexington, MA 02173-0073 
4421 West Estes 
Lincolnwood, IL 60646 
ABSTRACT 
A design for a fully analog version of a self-organizing feature map neural 
network has been completed. Several parts of this design are in fabrication. 
The feature map algorithm was modified to accommodate circuit solutions 
to the various computations required. Performance effects were measured 
by simulating the design as part of a frontend for a speech recognition 
system. Circuits are included to implement both activation computations and 
weight adaption or learning. External access to the analog weight values is 
provided to facilitate weight initialization, testing and static storage. This 
fully analog implementation requires an order of magnitude less area than 
a comparable digital/analog hybrid version developed earlier. 
INTRODUCTION 
This paper describes an analog version of a self-organizing feature map circuit. The design 
implements Kohonen's self-organizing feature map algorithm [Kohonen, 1988] with some 
modifications imposed by practical circuit limitations. The feature map algorithm automatically 
adapts connection weights to nodes in the network such that each node comes to represent a 
distinct class of features in the input space. The system also self-organizes such that neighboring 
nodes become responsive to similar input classes. The prototype circuit was fabricated in two 
parts (for testability); a 4 node, 4 input synaptic array, and a weight adaptation and refresh 
circuit. A functional simulator was used to measure the effects of design constraints. This 
simulator evolved with the design to the point that actual device characteristics and process 
statistics were incorporated. The feature map simulator was used as a front-end processor to 
a speech recognition system whose error rates were used to monitor the effects of parameter 
changes on performance. 
This design has evolved over the past two years from earlier experiments with a perceptron 
classifier [Raffel, 1987] and an earlier version of a self-organizing feature map circuit [Mann, 
1988]. The perceptton classifier used a connection matrix built with multiplying D/A converters 
to perform the product operation for the sum-of-products computation common to all neural 
network algorithms. The feature map circuit also used MDAC's to perform a more complicated 
calculation to realize a squared Euclidean distance measure. The weights were also stored 
digitally, but in a unary encoded format to simplify the weight adjustment operation. This circuit 
contained all of the control necessary to perform weight adaptation, except for selecting a 
maximum responder. 
The new feature map circuit described in this paper replaces the digital weight storage with 
dynamic analog charge storage on a capacitor. This paper will describe the circuitry and discuss 
problems associated with this approach to neural network implementations. 
Reprinted with permission of Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, 
Massachusetts 
740 Mann and Gilbert 
ALGORITHM DESCRIPTION 
The original Kohonen algorithm is based on a network topology such as shown in Figure 1. This 
illustrates a linear array of nodes, consistent with the hardware implementation being described. 
Each node in the circuit computes a level of activity [Dj(t)] which indicates the similarity 
between the current input vector [Xi(t)] and its respective weight vector [Wij(t)]. Traditionally 
this would be the squared Euclidean distance given by the activation equation in the figure. If 
the inputs are normalized, a dot product operation can be substituted. The node most 
representative of the current input will be the one with the minimum or maximum output 
activity (classification), depending on which distance measure is used. The node number of the 
min./max. responder [j*] then comes to represent that class of which the input is a member. 
If the network is still in its learning phase, an adaptation process is invoked. This process 
updates the weights of all the nodes lying within a prescribed neighborhood [NEjj*(t)] of the 
selected node. The weights are adjusted such that the distance between the input and weight 
vector is diminished. This is accomplished by decreasing the individual differences between each 
component pair of the two vectors. The rate of learningis controlled by the gain term [a(t)]. 
Both the neighborhood and gain terms decrease during the learning process, stopping when the 
gain term reaches 0. 
The following strategy was selected for the circuit implementation. First, it was assumed that 
inputs are normalized, thereby permitting the simpler dot product operation to be adopted. 
Second, weight adjustments were reducedto a simple increment/decrement operation determined 
by the sign of the difference between the components of the input and weight vector. Both of 
these simplifications were tested in the simulations described earlier and had negligible effects 
on overall performance as a speech vector quantizer. In addition, the prototype circuits of the 
analog weight version of the feature map vector quantizer do not include either the max. picker 
or the neighborhood operator. To date, a version of a max. picker has not yet been chosen, 
though many forms exist. The neighborhood operator was included in the previous version of 
this design, but was not repeated on this first pass. 
HARDWARE DESCRIPTION 
SYNAPTIC ARRAY 
A transistor constitutes the basic synaptic connection used in this design. An analog input is 
represented by a voltage v(Xi) on the drain of the transistor. The weight is stored as charge 
q(Wij) on the gate of the transistor. If the gate voltage exceeds the maximum input voltage by 
an amount greater than the transistor threshold voltage, the device will be operating in the 
ohmic region. In this region the current [i(Dj)] through the transistor is proportional to the 
product of the input and weight voltages. This effectively computes one contribution to the dot 
product. By connecting many synapses to a single wire, current summing is performed, in 
accordance with Kirchoff's current law, producing the desired sum of products activity. 
Figure 2 shows the transistor current as a function of the input and weight voltages. These 
curves merely serve to demonstrate how a transistor operating in the ohmic region will 
approximate a product operation. 
As the input voltage begins to approach the saturation region of the transistor, the curves begin 
to bend over. For use in competitive learning networks, like the feature map algorithm, it is 
only important that the computation be monotonically increasing. These curves were the 
characteristics of the computation used in the simulations. The absolute values given for output 
current do not reflect those produced in the actual circuit. 
An Analog Self-Organizing Neural Network Chip 741 
� ACTIVATION: 
m 
Di{t) =  {x(t) - w,j(t)) 2 
i:1 
CLASSIFICATION: 
j� -- M!N(Dj(t)) 
! 
� ADAPTATION: 
wii(t + 1 ): w,i(t) + a(t)[xi(t) - w,i(t)] NEji'(t) 
Figure 1. Description of Kohonen's original feature map algorithm using a 
linear array of nodes. 
200 
150 
180 
5O 
I I 1 
o 0.5 I 1.5 
INPUT (Vds) V 
WEIGHT (Vgs) 
5.0V 
4.8V 
4.6V 
4.4V 
4.2V 
4.0V 
3.8V 
3.6V 
3.4V 
3.2V 
3.0V 
Figure 2. Typical I-V curves for a transistor operating in the ohmic region. 
742 Mann and Gilbert 
It should also be noted that there is no true zero weight; even the zero weight voltage 
contributes to the output current. But again, in a competitive network, it is only important that 
it contribute less than a higher weight value at that same input voltage. 
In short, neither small non-linearities nor offsets interfere with circuit operation if the synapse 
characteristic is monotonic with weight value and input. 
SYSTEM 
Figure 3 is a block diagram of the small four-node hardware prototype. The nodes are oriented 
horizontally, their outputs identified as I0 through I3 along the right-hand edge, representing the 
accumulated currents. The analog inputs [X3-X0] come in from the bottom and, traveling 
vertically, make connections with each node at the boxes identified as synapses. Each synapse 
performs its product operation between the analog weight stored at that node and the input 
potential. 
Along the top and left sides are the control circuits for accessing weight information. The two 
storage registers associated with each synapse are the control signals used to select the reading 
and writing of weights. Weights are accessed serially by connecting to a global read and write 
wire, W- and W + respectively. Besides the need for modification, the weights also drift with 
time, much like DRAM storage, and therefore must be refreshed periodically. This is also 
performed by the adaptation circuit that will be presented separately. 
Control is provided by having a single 'T' bit circulating through the DRAM storage bits 
associated with each synapse. This process goes on continuously in the background after being 
initialized, in parallel with the activity calculations. If the circuit is not being trained, the 
adaptation circuit continues to refresh the existing weights. 
WEIGHT MODIFICATION & REFRESH 
A complete synapse, along with the current to voltage conversion circuit used to read the weight 
contents, is shown in Figure 4. The current synapse is approximately the size of two 6 transistor 
static RAM bits. This approximation will be used to make synaptic population estimates from 
current SRAM design experience. The six transistors along the top of the synapse circuit are 
two, three-transistor dynamic RAM cells used to control access to weight contents. These are 
represented in Figure 3 as the two storage elements associated with each synapse and are used 
as described earlier. 
READ
