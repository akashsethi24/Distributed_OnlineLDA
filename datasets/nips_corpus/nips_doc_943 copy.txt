Higher Order Statistical Decorrelation without 
Information Loss 
Gustavo Deco 
Siemens AG 
Central Research 
Otto-Hahn-Ring 6 
81739 Munich 
Germany 
Wiifried Brauer 
Technische UniversitJt M'Onchen 
Institut filr Informatik 
Arcisstr. 21 
80290 Munich 
Germany 
Abstract 
A neural network learning paradigm based on information theory is pro- 
posed as a way to perform in an unsupervisd fashion, rodundancy 
_reduction among the elements of the output layer without loss of infor- 
mation from the sensory input. The model developed performs nonlin- 
ear decorrelation up to higher orders of the cumulant tensors and results 
in probabilistically independent components of the output layer. This 
means that we don't need to assme Gaussian distribution neither at the 
input nor at the output. The theory presented is related to the unsuper- 
vised-leaming theory of Barlow, which proposes redundancy reduction 
as the goal of cognition. When nonlinear units are used nonlinear princi- 
pal component analysis is obtained. In this case nonlinear manifolds can 
be reduced to minimum dimension manifolds. If such units are used the 
network performs a generalized principal component analysis in the 
sense that non-Gaussian distributions can be linearly decorrelated and 
higher orders of the correlation tensors are also taken into account. The 
basic structure of the architecture involves a general transformation that 
is volume conserving and therefore the entropy, yielding a map without 
loss of information. Minimization of the mutual information among the 
output neurons eliminates the redundancy between the outputs and 
results in statistical decorrelation of the extracted features. This is 
known as factorial learning. 
248 Gustavo Deco, Wilfried Brauer 
1 INTRODUCTION 
One of the most important theories of feature extracrion is the one proposed by Barlow 
(1989). Barlow describes the process of cognition as a preprocessing of the sensoffal 
information performed by the nervous system in order to extract the statistically relevant 
and independent features of the inputs without loosing information. This means that the 
brain should statistically decorrelate the extracted information. As a learning strategy Bar- 
low (1989) formulated the principle of redundancy reduction. This kind of learning is 
called factoffal learning. Recently Atick and Redlich (1992) and Redlich (1993) concen- 
trate on the original idea of Barlow yielding a very interesting formularion of early visual 
processing and factoffal learning. Redlich (1993) reduces redundancy at the input by using 
a network structure which is a reversible cellular automaton and therefore guarantees the 
conservation of information in the transformation between input and output. Some nonlin- 
ear extensions of PCA for decorrelation of sensoffal input signals were recently intro- 
duced. These follow very closely Bafiow's original ideas of unsupervised learning. 
Redlich (1993) use similar information theoretic concepts and reversible cellular automata 
architectures in order to define how nonlinear decorrelation can be performed. The aim of 
our work is to formulate a neural network architecture and a novel learning paradigm that 
performs Bafiow's unsupervised learning in the most general fashion. The basic idea is to 
define an architecture that assures perfect transmission without loss of informarion. Con- 
sequently the nonlinear transformation defined by the neural architecture is always bijec- 
rive. The architecture performs a volume-conserving transformation (determinant of the 
Jacobian matrix is equal one). As a particular case we can derive the reversible cellular 
automata architecture proposed by Redlich (1993). The learning paradigm is defined so 
that the components of the output signal are statistically decorrelated. Due to the fact that 
the output distriburion is not necessarily Gaussian, even if the input is Gaussian, we per- 
form a cumulant expansion of the output distriburion and find the rules that should be sat- 
isfied by the higher order correlation tensors in order to be decorrelated. 
2 THEORETICAL FORMALISM 
Let us consider an input vector : of dimensionality d with components distributed 
according to the probability distribution P (), which is not factoffal, i.e. the components 
of : are correlated. The goal of Barlow's unsupervised learning rule is to find a transfor- 
marion 
-- F (x) (2.1) 
such that the components of the output vector d-dimensional  are staristically decorre- 
lated. 
This means that the probability distributions of the components Yi are independent and 
therefore, 
d 
P ()) --' I-I P (Yi)' (2.2) 
i 
The obje_,ctive of factoffal learning is to find a neural network, which performs the transfor- 
mation F ( ) such that the joint probability distribution P () of the output signals is fac- 
toffzed as in eq. (2.2). In order to implement factoffal learning, the information contained 
in the input should be transferred to the output neurons without loss but, the probability 
distriburion of the output neurons should be statistically decorrelated. Let us now define 
Higher Order Statistical Decorrelation without hzformation Loss 249 
these facts from the information theory perspective. The first aspect is to assure the 
entropy is conserved, i.e. 
H () -- H () (2.3) 
where the symbol () denotes the entropy of  and H(a/b) the conditional 
entropy of  given b. One way to achieve this goal is to construct an architecture that 
independently of its synaptic parameters satisfies always eq. (2.3). Thus the architecture 
will conserve information or entropy. The transmitted entropy satisfies 
aP 
H(9) (2.4) 
where equality holds only if F is bijective, i.e. reversible. Conservation of information 
and bijectivity is assured if the neural transformation conserves the volme, which mathe- 
matically can be expressed by the fact that the Jacobian of the transformation should have 
determinant unity. In section 3 we formulate an architecture that always conserves the 
entropy. Let us now concentrate on the main aspect of factoffal learning, namely the 
decorrelation of the output components. Here the problem is to find a volume-conserving 
transformation that satisfies eq. (2.2). The major problem is that the distribution of the out- 
put signal will not necessarily be Gaussian. Therefore it is impossible to use the technique 
of minimizing the mutual information between the components of the output as done by 
Redlich (1993). The only way to decorrelate non-Gaussian distributions is to expand the 
distribution in higher orders of the correlation matrix and impose the independence condi- 
tion of eq. (2.2). In order to achieve this we propose to use a cumulant expansion of the 
output distribution. Let us define the Fourier transform of the output distribution, 
{()----' I d ei(') P(;{(Ki)--'ldYi ei(Ki'Yi) P(Yi) (2.5) 
The cumulant expansion of a distribution is (Papoulis, 1991) 
i n i n 
, (7) --- e O) (ri) -- e ' (2.6) 
In the Fourier space the independence condition is given by (Papoulis, 1991) 
b () -- 1-I qb (K) (2.7) 
t 
which is equivalent to 
-- = (2.8) 
t t 
Putting eq. (2.8) and the cumulant expansions of eq. (2.6) together, we obtain that in the 
case of independence the following equality is satisfied 
oo i n n cl oo i n 
E t-.. E il, i2 ..... in giigi2'gi.--' E E .. Jn)g (2.9) 
n= l t,i 2 ..... i,i= ln= l 
In both expansions we will only consider the first four cumulants. After an extra transfor- 
mation 
Y' -- - (9) (2.10) 
250 Gustavo Deco, Wilfried Brauer 
to remove the bias (), we can rewrite eq. (2.9) using the cumulants expression derived 
in the Papoulius (1991): 
_ i - CJ 3) ijk } 
- E gigj { Cij - C/(2) ij}  E KigJKt { Cijt 
J J'* (2.11) 
q' ---i,j,, KiKjKtK l { (Cijtl- 3CijCtl ) - (CJ 4) -3 (C 2)) 2)ijtl} _- 0 
l 
Equation (2.11) should be satisfied for all values of . The multidimensional correlation 
tensors Ci. ..j and the one-dimensional higher order moments C/(n) are given by 
I 
;i..4 -- dy' P (y') y'i...yj , C n) dY'i P (Y'i) (Y'i) (2.12) 
The6 i . denotes Kroenecker's delta. Due to the fact that eq. (2.11) should be satisfied for 
all K, h'lt coefficients in each summation of eq. (2.11) must be zero. This means that 
Cij -- O, if(i -j) (2.13) 
COt - O, if(i -j v i  k) (2.14) 
Cijtl -- O, if( {ijvikvil} ^-d_) (2.15) 
Ciijj-CiiCii-- O, if(i,j). (2.16) 
In eq. (2.15) L is the logical expression 
L -- { (i--j^k--l^jk) v (i--k^j--l^i-j) v(i-l^jk^i-j)}, (2.17) 
which excludes the cases considered in eq. (2.16). The conditions of independence given 
by eqs. (2.13-2.16) can be achieved by minimization of the cost function 
E--a5.C.+[3 5. C't+�, j Citlq'bE(eiijj-eiicjj )2 (2.18) 
i<j i<j:k '< :l i<j 
where or, [3, �, b are the inverse of the number of elements in each summation respec- 
tively. 
In conclusion, minimizing the cost given by eq. (2.18) with a volume-conserving network, 
we achieve nonlinear decorrelation of non-Gaussian distributions. It is very easy to test 
wether a factorized probability distribution (eq. 2.2) satisfies the eqs. (2.13-2.16). As a 
particular case if only second order terms are used in the cumulant expansion, the learning 
rule reduces to eq. (2.13), which expresses nothing more than the diagonalization of the 
second order covariance matrix. In this case, by anti-transforming the cumulant expansion 
of the Fourier transform of the distribution,we obtain a Gaussian distribution. Diagonali- 
zation of the covariance matrix decorrelates statistically the components of the output only 
if we assume a Gaussian distribution of the outputs. In general the distribution of the out- 
put is not Gaussian and therefore higher orders of the cumulant expansion should be taken 
into account, yielding the learning rule conditions eqs. (2.13-2.16) (up to fourth order, 
generalization to higher orders is straightforward). In the cas
