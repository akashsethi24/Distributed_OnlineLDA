A Comparison of Discrete-Time Operator Models 
for Nonlinear System Identification 
Andrew D. Back, Ah Chung Tsoi 
Department of Electrical and Computer Engineering, 
University of Queensland 
St. Lucia, Qld 4072. Australia. 
e-mail: {back, act}@elec. uq. oz. au 
Abstract 
We present a unifying view of discrete-time operator models used in the 
context of finite word length linear signal processing. Comparisons are 
made between the recently presented gamma operator model, and the delta 
and rho operator models for performing nonlinear system identification 
and prediction using neural networks. A new model based on an adaptive 
bilinear transformation which generalizes all of the above models is 
presented. 
1 INTRODUCTION 
The shift operator, defined as qz(t) = z(t + 1), s frequently used to provide tme-dommn 
signals to neural network models. Using the shift operator, a discrete-time model for system 
identification or time series prediction problems may be constructed. A common method of 
developing nonlinear system identification models is to use a neural network architecture as 
an estimator f'(Y (t), X ( t); 0 ) of F (Y (t), X (t)), where 0 represents the parameter vector 
of the network. Shift operators at the input of the network provide the regression vectors 
Y(t- 1) = [y(t- 1), ..., y(t-N)]', and X(t) = [z(/), ..., z(t-M)]' in a manner analogous 
to linear filters, where [.]' represents the vector transpose. 
It is known that linear models based on the shift operator q suffer problems when used to 
model lightly-damped-low-frequency (LDLF) systems, with poles near (1, 0) on the unit 
circle in the complex plane [5]. As the sampling rate increases, coefficient sensitivity and 
round-off noise become a problem as the difference between successive sampled inputs 
becomes smaller and smaller. 
884 Andrew D. Back, Ah Chung Tsoi 
A method of overcoming this problem is to use an alternative discrete-time operator. 
Agarwal and Burros first proposed the use of the delta operator in digital filters to replace 
the shift operator in an attempt to overcome the problems described above [ 1]. The delta 
operator is defined as 
q-1 
6 - (1) 
where A is the discrete-time sampling interval. Williamson showed that the delta operator 
allows better performance in terms of coefficient sensitivity for digital filters derived from 
the direct form structure [19], and a number of authors have considered using it in linear 
filtering, estimation and control [5, 7, 8] 
More recently, de Vries, Principe at. al. proposed the gamma operator [2, 3] as a means 
of studying neural network models for processing time-varying patterns. This operator is 
defined by 
q-(1-c) 
7 = (2) 
c 
It may be observed that it is a generalization of the delta operator with adjustable parameters 
c. An extension to the basic gamma operator introducing complex poles using a second 
order operator, was given in [ 18]. 
This raises the question, is the gamma operator capable of providing better neural network 
modelling capabilities for LDLF systems ? Further, are there any other operators which 
may be better than these for nonlinear modelling and prediction using neural networks ? 
In the context of robust adaptive control, Palaniswami has introduced the rho operator 
which has shown useful improvements over the performance of the delta operator [9, 10]. 
The rho operator is defined as 
q-(1 
p = (3) 
c2 A 
where q, c2 are adjustable parameters. The rho operator generalizes the delta and gamma 
operators. For the case where c A -- c2A --_ 1, the rho operator reduces to the usual shift 
operator. When c -- 0, and c2 - 1, the rho operator reduces to the delta operator [10]. 
For c A -- c2A -- c, the rho operator is equivalent to the gamma operator. 
One advantage of the rho operator over the delta operator is that it is stably invertible, 
allowing the derivation of simpler algorithms [9]. The p operator can be considered as 
a stable low pass filter, and parameter estimation using the p operator is low frequency 
biased. For adaptive control systems, this gives robustness advantages for systems with 
unmodelled high frequency characteristics [9]. 
By defining the bilinear transformation (BLT) as an operator, it is possible to introduce 
an operator which generalizes all of the above operators. We can therefore define the pi 
operator as 
2 (c q - c2) 
a' = (4) 
A (c3q q- C4) 
with the restriction that c c4 :fi c2 c3 (to ensure r is not a constant function [ 14]). The bilinear 
mapping produced has a pole at q -- -c4/c. By appropriate setting of the q, c2, c3, c4 
parameters each operator, the pi operator can be reduced to each of the previous operators. 
In the work reported here, we consider these alternative discrete-time operators in feed- 
forward neural network models for system identification tasks. We compare the popular 
A Comparison of Discrete-Time Operator Models for Nonlinear System Identification 885 
gamma model [4] with other models based on the shift, delta, rho and pi operators. A 
framework of models and Gauss-Newton training algorithms is provided, and the models 
are compared by simulation experiments. 
2 
OPERATOR MODELS FOR NONLINEAR SIGNAL 
PROCESSING 
A model which generalizes the usual discrete-time linear moving average model, ie, a single 
layer network is given by 
= o0,, 0)x(t) (5) 
M 
= 
i=0 
q-i shift operator 
6-i delta operator 
t/-i = ?-i gamma operator (6) 
p-i rho operator 
;r-i pi operator 
This general class of moving average model can be termed MA(v). We define uo(t)  z(t), 
and ui (t) - v -  ui- 1 (t) and hence obtain 
i) 
Aui-(t- 1)+ ui(t- 1) 
ui(t) = cui-(t-1)+(1-c)ui(t-1) 
c2Aui-(t - 1) + (1 - cA)ui(t - 1) 
A 
 (c3ui-(t) + c4ui-(t - 1)) - ui(t - 1) 
shift operator 
delta operator 
gamma operator 
rho operator 
pi operator 
(7) 
A nonlinear model may be defined using a multilayer perceptron (MLP) with the w-operator 
elements at the input stage. The input vector Z(t) to the network is 
(8) 
where zi(t) is the ith input to the system. This model is termed the w-operator multilayer 
perceptron or MLP() model. 
An MLP(v) model having L layers with No, N, ..., N� nodes per layer, is defined in the 
same manner as a usual MLP, with 
= (9) 
4(0 = 
WkiZi (t) (10) 
i=1 
where each neuron i in layer l has an output of z (t); a layer consists of Nl neurons (l = 0 
denotes the input layer, and l - L denotes the output layer, zt = 1.0 may be used for a 
bias); f(.) is a sigmoid function typically evaluated as tanh(.), and a synaptic connection 
between unit i in the previous layer and unit k in the current layer is represented by wi. 
The notation t may be used to represent a discrete time or pattern instance. While the case 
886 Andrew D. Back, Ah Chung Tsoi 
we consider employs the v-operator at the input layer only, it would be feasible to use the 
operators throughout the network as required. 
On-line algorithms to update the operator parameters in the MA(v) model can be found 
readily. In the case of the MLP(v) model, we approach the problem by backpropagating 
the error information to the input layer and using this to update the operator coefficients. 
de Vries and Principe et. al., proposed stochastic gradient descent type algorithms for 
adjusting the c operator coefficient using a least-squares error criterion [2, 12]. For brevity 
we omit the updating procedures for the MLP network weights; a variety of methods may 
be applied (see for example [13, 15]). 
We define an instantaneous output error criterion J(t) = �e2(t), where e(t) = y(t) - O(t). 
Defining  as the estimated operator parameter vector at time t of the parameter vector 0, 
we have 
 gamma operator 
/ = [, 62]  rho operator (11) 
[6, 2, 3, 4]  pi operator 
A first order algorithm to update the coefficients is 
ti(t + 1) - ti(t) + Ati(t) 
where the adjustment in weights is found as 
zxi(t) = 
oJ(t) 
M 
i=1 
(12) 
(13) 
(14) 
where 5 i (t) is the backpropagated error at the jth node of input layer, and i(t) is the first 
order sensitivity vector of the model operator parameters, defined by 
0-40 ] gamma operator 
Ocj 
(t) = [ acj,, acj; rhooperator (15) 
[a(,) a(,) a() a(,)] t piopermor 
[ Ocj ' Oc ' Ocj ' Ocj4 J 
Substituting ui(t) in om (7), e rursive uations for  (t) (noting that  (t)= (t) 
V/)e 
i(t) = i--l(t -- 1) - ui(t - 1) + eii-(t - 1) + (1 - e)i(t - 1) ga operator 
i(t) = Aui-(t - 1) + 52Ai-,2(t - 1) - lA)i,2(t - 1) rho operator 
 (e�,_,l(t) + e,_,(t- 1)) + ,,(t- 1) 
- (e,_(t) + e,_(t - 1)) - ,(t - 1), 
= 1)) + 2)+ 1), pi operator 
 (_(t) + e,_,,(t) + e,_,(t- 1)) + *?,(t - 1), 
 (e,_,(t) + ,_(t - 1) + e�_l,(t- 1)) + ,,(t- l) 
A Comparison of Discrete-Time Operator Models for Nonlinear System Identification 887 
for the gamma, rho, and pi operators respectively, and where bi,j (t) refers to the jth element 
of the ith b vector, with bi,o(t) = O. 
A more powerful updating procedure can be obtained by using the Gauss-Newton method 
[6]. In this case, we replace (14) with (omitting i subscripts for clarity), 
+ 1) = + 
(16) 
where 7(/) is the gain sequence (see [6] for details), A -1 is a weighting matrix which may 
be replaced by the identity matrix [16], or estimated as [6] 
X(t) = X(t- 1)+ ?(t) (2(I 0 -X(i- 1)) 
(17) 
R(t) is an approximate Hessian matrix, defined by 
R(t + 1) = A(t)R(t) + (t)b(t)bt(t) 
(18) 
where (t) = 1 - (t). Efficient computation of R - may be performed using the 
matrix inversion lemma [17], factorization methods such as Cholesky decomposition or 
other fast algorithms. Using the well known matrix inversion lemma [6], we substitute 
P(t) = R-(t), where 
1 P(I) C(I) {' ) (19) 
P(I) = A(t' A(t) k,A(t) + 
The initial values of the coefficients are important in determining convergence. Principe 
et. al. [12] note that setting the coefficients for the gamma operator to unity provided the 
best approach for c
