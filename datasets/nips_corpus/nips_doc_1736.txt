Search for Information Bearing 
Components in Speech 
Howard Hua Yang and Hynek Hermansky 
Department of Electrical and Computer Engineering 
Oregon Graduate Institute of Science and Technology 
20000 NW, Walker Rd., Beaverton, OR97006, USA 
{hyang,hynek)@ece.ogi.edu, FAX:503 7481406 
Abstract 
In this paper, we use mutual information to characterize the dis- 
tributions of phonetic and speaker/channel information in a time- 
frequency space. The mutual information (MI) between the pho- 
netic label and one feature, and the joint mutual information (JMI) 
between the phonetic label and two or three features are estimated. 
The Miller's bias formulas for entropy and mutual information es- 
timates are extended to include higher order terms. The MI and 
the JMI for speaker/channel recognition are also estimated. The 
results are complementary to those for phonetic classification. Our 
results show how the phonetic information is locally spread and 
how the speaker/channel information is globally spread in time 
and frequency. 
I Introduction 
Speech signals typically carry information about number of target sources such 
as linguistic message, speaker identity, and environment in which the speech was 
produced. In most realistic applications of speech technology, only one or a few in- 
formation targets are important. For example, one may be interested in identifying 
the message in the signal regardless of the speaker or the environments in which 
the speech was produced, or the identification of the speaker is needed regardless 
of the words the targeted speaker is saying. Thus, not all components of the signal 
may be equally relevant for a decoding of the targeted information in the signal. 
The speech research community has at its disposal rather large speech databases 
which are mainly used for training and testing automatic speech recognition (ASR) 
systems. There have been relatively few efforts to date to use such databases 
for deriving reusable knowledge about speech and speech communication processes 
which could be used for improvements of ASR technology. In this paper we apply 
information-theoretic approaches to study a large hand-labeled data set of fluent 
speech to learn about the information structure of the speech signal including the 
distribution of speech information in frequency and in time. 
Based on the labeled data set, we analyze the relevancy of the features for phonetic 
804 H. H. Yang and H. Hermansky 
classifications and speaker/channel variability. The features in this data set are 
labeled with respect to underlying phonetic classes and files from which the features 
come from. The phoneme labels relate to the linguistic message in the signal, and 
the file labels carry the information about speakers and communication channels 
(each file contains speech of a single speaker transmitted through one telephone 
channel). Thus, phoneme and file labels are two target variables for statistical 
inference. The phoneme labels take 19 different values corresponding to 19 broad 
phoneme categories in the OGI Stories database [2]. The file labels take different 
values representing different speakers in the OGI Stories database. 
The relevancy of a set of features is measured by the joint mutual information (JMI) 
between the features and a target variable. The phoneme target variable represents 
in our case the linguistic message. The file target variable represents both different 
speakers and different telephone channels. The joint mutual information between 
a target variable and the features quantifies the relevancy of the features for that 
target variable. 
Mutual information measure the statistical dependence between random variables. 
Morris et al (1993) used mutual information to find the critical points of informa- 
tion for classifying French Vowel-Plosive-Vowel utterances. Bilmes(1998) showed 
recently that the information appears to be spread over relatively long tempo- 
ral spans. While Bilmes used mutual information between two variables on non- 
labeled data to reveal the mutual dependencies between the components of the 
spectral energies in time and frequency, we focused on joint mutual information 
between the phoneme labels or file labels and one, two or three feature variables 
in the time-frequency planet7, 6] and used this concept to gain insight into how 
information about phonemes and speaker/channel variability is distributed in the 
time-frequency plane. 
2 Data Set and Preprocessing 
The data set used in this paper is 3-hour phonetically labeled telephone speech, a 
subset of the English portion (Stories) of the OGI multi-lingual database [2] contain- 
ing approximately 50 seconds of extemporaneous speech from each of 210 different 
speakers. The speech data is labeled by a variable Y taking 19 values represent- 
ing 19 most often occurring phoneme categories. The average phoneme duration is 
about 65 ms and the number of phoneme instances is 65421. 
Acoustic features X(fk, t) for the experiments are derived from a short-time anal- 
ysis of the speech signal with a 20 ms analysis window (Hamming) at the frame t 
advanced in 10 ms steps. The logarithmic energy at a frequency fk is computed 
from the squared magnitude FFT using a critical-band spaced (log-like in the fre- 
quency variable) weighting function in a manner similar to that of the computation 
of Perceptual Linear Prediction coefficients [3]. In particular, the 5-th, 8-th and 12- 
th bands are centered around 0.5, i and 2 kHz respectively. Each feature X(f, t) is 
labeled by a phoneme label YP(t) and a file label Yf(t). We use mutual information 
to measure the relevancy of X(f, t - d) across all frequencies f and in a context 
window -D _< d _< +D for the phoneme classification and the speaker/channel 
identification. 
3 Estimation of MI and Bias Correction 
In this paper, we only consider the mutual information (MI) between discrete ran- 
dom variables. The phoneme label and the file label are discrete random variables. 
Search for Information Bearing Components in Speech 805 
However, the feature variables are bounded continuous variables. To obtain the 
quantized features, we divide the maximum range of the observed features into 
cells of equal volume so that we can use histogram to estimate mutual information 
defined by 
p(x,y) 
I(X;Y)- Ep(x,y) log2 p(x)p(y)' 
 In(1 p2 
If X and Y are jointly Gaussian, then I(X;Y) = -3 - ) where p is the 
correlation coefficient between X and Y. However, for speech data the feature vari- 
ables are generally non-Gaussian and target variables are categorical type variables. 
Correlations involving a categorical variable are meaningless. 
The MI can also be written as 
z(x; v) = n(x) + n(v) - n(x, v) 
= n(v)- n(VlX) = n(x)- n(XlV) (1) 
where H(YIX) is a conditional entropy defined by 
H(YIX) = - p(x) p(yl x) log2(ylx ). 
x y 
The two equations in (1) mean that the MI is the uncertainty reduction about Y 
give X or the uncertainty reduction about X give Y. 
Based on the histogram, H(X) is estimated by 
f ( E ni ni 
X) = - -- 1og -- 
i 
where ni is the number of data points in the i-th cell and n is the data size. And 
I(X; Y) is estimated by 
i(x; Y) = (x) +/(Y)- (x, ). 
Miller(1954)[4] has shown that //(X) is an underestimate of H(X) and/(X; Y) is 
an overestimate of I(X; Y). The biases are 
r-1 1 
E[(X)]- n(X) = �j( + 0(7) 
(2) 
E[i(X;Y)]- I(X;Y) = (r- 1)(c- 1) O(nfi__/) (3) 
2 ln(2)n + 
where r and c are the number of cells for X and Y respectively. 
Interestingly, the first order terms in (2) and (3) do not depend on the probability 
distribution. After using these formulas to correct the'estimates, the new estimates 
have the same variances as the old estimates but with reduced biases. However, 
these formulas break down when r and n are of the same order. Extending Miller's 
approach, we find a high order correction for the bias. Let {pi} be the probability 
distribution of X, then 
r- 1 1 (S({pi })- 3r + 2) 
E[//(X)]- H(X) = 21n(2)n + 61n(2)n 2 
i (S({pi}) - 1) + O(4) (4) 
4n a 
where 
806 H. H. Yang and H. Hermansky 
The last two terms in the bias (4) depend on the unknown probabilities (pi}. In 
practice they are approximated by the relative frequency estimates. 
Similarly, we can find the bias formulas of the high order terms O() and O( 
for the MI estimate. 
When X is evenly distributed, pi = l/r, so S({pi}) = r 2 and 
E[/(X)]- H(X) = 
r-1 1 
2 ln(2)n + 6 ln(2)n 2 (r - 3r + 2) - -- 
i 1 
4n a (r 2 - 1) + 0(-). 
Theoretically S({pi}) has no upper bound when one of the probabilities is close to 
zero. However, in practice it is hard to collect a sample to estimate a very small 
probability. For this reason, we assume that pi is either zero or greater than 
where e > 0 is a small constant does not depend on n or r. Under this assumption 
1 [/.2/ 
S({pi}) _< r2/e and the amplitude of the last term in (4)is less than 4x / - 1). 
4 MI in Speech for Phonetic Classification 
The three hour telephone speech in the OGI database gives us a sample size greater 
than i million, n = 1050000. To estimate the mutual information between three 
features and a target variable, we need to estimate the entropy H(X1, X2, X3, Y). 
Take B = 20 as the number of bins for each feature variable and C = 19 is the 
number of phoneme categories. Then the total number of cells is r - B 3 � C. After 
a constant adjustment, assuming e - 1, the bias is 
1 
O() = 6 ln(2)n 2 (r2 - 3r + 2) = 0.005(bits). 
It is shown in Fig. l(a) that X(f4,t) and X(f,t) are most relevant features for 
phonetic classification. From Fig. l(b), at 5 Bark the MI spread around the current 
frame is 200 ms. 
Given one feature X1, the information gain due to the second feature is the difference 
- r(xx; lXx) 
where I(X2; Y]X) is called the information gain of X2 given X. It is a conditional 
mutual information defined by 
z(x;YIX) - p(xl) yp(x2, ylx)log2 
Xl 2,Y 
p(x,ylx) 
p(x2l
