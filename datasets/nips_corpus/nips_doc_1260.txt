Multilayer neural networks: 
one or two hidden layers? 
G. Brightwell 
Dept of Mathematics 
LSE, Houghton Street 
London WC2A 2AE, U.K. 
C. Kenyon, H. Paugam-Moisy 
LIP, URA 1398 CNRS 
ENS Lyon, 46 all4e d'Italie 
F69364 Lyon cedex, FRANCE 
Abstract 
We study the number of hidden layers required by a multilayer neu- 
ral network with threshold units to compute a function f from T�d 
to {0, 1}. In dimension d = 2, Gibson characterized the functions 
computable with just one hidden layer, under the assumption that 
there is no multiple intersection point and that f is only defined 
on a compact set. We consider the restriction of f to the neighbor- 
hood of a multiple intersection point or of infinity, and give neces- 
sary and sufficient conditions for it to be locally computable with 
one hidden layer. We show that adding these conditions to Gib- 
son's assumptions is not sufficient to ensure global computability 
with one hidden layer, by exhibiting a new non-local configuration, 
the critical cycle, which implies that f is not computable with 
one hidden layer. 
I INTRODUCTION 
The number of hidden layers is a crucial parameter for the architecture of multilayer 
neural networks. Early research, in the 60's, addressed the problem of exactly real- 
izing Boolean functions with binary networks or binary multilayer networks. On the 
one hand, more recent work focused on approximately realizing real functions with 
multilayer neural networks with one hidden layer [6, 7, 11] or with two hidden units 
[2]. On the other hand, some authors [1, 12] were interested in finding bounds on 
the architecture of multilayer networks for exact realization of a finite set of points. 
Another approach is to search the minimal architecture of multilayer networks for 
exactly realizing real functions, from T�d to {0, 1}. Our work, of the latter kind, is a 
continuation of the effort of [4, 5, 8, 9] towards characterizing the real dichotomies 
which can be exactly realized with a single hidden layer neural network composed 
of threshold units. 
Multilayer Neural Networks: One or Two Hidden Layers? 149 
1.1 NOTATIONS AND BACKGROUND 
A finite set of hyperplanes {Hi}l_<i<_h defines a partition of the d-dimensional space 
into convex polyhedral open regions, the union of the Hi's being neglected as a 
subset of measure zero. A polyhedral dichotomy is a function f � T�d -- {0, 1}, 
obtained by associating a class, equal to 0 or to 1, to each of those regions. Thus both 
f-(0) and f-x(1) are unions of a finite number of convex polyhedral open regions. 
The h hyperplanes which define the regions are called the essential hyperplanes of 
f. A point P is an essential point if it is the intersection of some set of essential 
hyperplanes. 
In this paper, all multilayer networks are supposed to be feedforward neural net- 
works of threshold units, fully interconnected from one layer to the next, without 
skipping interconnections. A network is said to realize a function f: T�d -- {0, 1} if, 
for an input vector x, the network output is equal to f(x), almost everywhere in T�d. 
The functions realized by our multilayer networks are the polyhedral dichotomies. 
By definition of threshold units, each unit of the first hidden layer computes a binary 
function yj of the real inputs (x,... ,xd). Therefore, subsequent layers compute 
a Boolean function. Since any Boolean function can be written in DNF-form, two 
hidden layers are sufficient for a multilayer network to realize any polyhedral di- 
chotomy. Two hidden layers are sometimes also necessary, e.g. for realizing the 
four-quadrant dichotomy which generalizes the XOR function [4]. 
For all j, the jth unit of the first hidden layer can be seen as separating the space 
by the hyperplane Hj � id= WijXi -- Oj. Hence the first hidden layer necessarily 
contains at least one hidden unit for each essential hyperplane of f. Thus each 
region R can be labelled by a binary number y - (yx,..., yh) (see [5]). The jth 
digit yj will be denoted by Hj(R). 
Usually there are fewer than 2 h regions and not all possible labels actually exist. 
The Boolean family B! of a polyhedral dichotomy f is defined to be the set of all 
Boolean functions on h variables which are equal to f on all the existing labels. 
1.2 PREVIOUS RESULTS 
It is straightforward that all polyhedral dichotomies which have at least one linearly 
separable function in their Boolean family can be realized by a one-hidden-layer 
network. However the converse is far from true. A counter-example was produced 
in [5]: adding extra hyperplanes (i.e. extra units on the first hidden layer) can 
eliminate the need for a second hidden layer. Hence the problem of finding a minimal 
architecture for realizing dichotomies cannot be reduced to the neural computation 
of Boolean functions. Finding a generic description of all the polyhedral dichotomies 
which can be realized exactly by a one-hidden-layer network is still an open problem. 
This paper is a new step towards its resolution. 
One approach consists of finding geometric configurations which imply that a func- 
tion is not realizable with a single hidden layer. There are three known such geomet- 
ric configurations: the XOR-situation, the XOR-bow-tie and the XOR-at-infinity 
(see Figure 1). 
A polyhedral dichotomy is said to be in an XOR-situation iff one of its essential 
hyperplanes H is inconsistent, i.e. if there are four regions B, B t, W, W t such that 
B and B t are in class 1, W and W t are in class 0, B and W t are on one side of H, 
B t and W are on the other side of H, and B and W are adjacent along H, as well 
as B t and W t. 
150 G. Brightwell, C. Kenyon and H. Paugam-Moisy 
Given a point P, two regions containing P in their closure are called opposite with 
respect to P if they are in different halfspaces w.r.t. all essential hyperplanes going 
through P. A polyhedral dichotomy is said to be in an XOR-bow-tie iff there exist 
four distinct regions B, B , W, W , such that B and B , both in class 1 (resp. W 
and W , both in class 0), are opposite with respect to point P. 
The third configuration is the XOR-at-infinity, which is analogous to the XOR-bow- 
tie at a point c added to Td a. There exist four distinct unbounded regions B, B  
(in class 1), W, W' (in class 0) such that, for every essential hyperplane H, either 
all of them are on the same side of H (e.g. the horizontal line), or B and B  are on 
opposite sides of H, and W and W' are on opposite sides of H (see [3]). 
Figure 1: Geometrical representation of XOR-situation, XOR-bow-tie and XOR-at- 
infinity in the plane (black regions are in class 1, grey regions are in class 0). 
Theorem 1 If a polyhedral dichotomy f, from T a to {0, 1}, can be realized by a 
one-hidden-layer network, then it cannot be in an XOR-situation, nor in an XOR- 
bow-tie, nor in an XOR-at-infinity. 
The proof can be found in [5] for the XOR-situation, in [13] for the XOR-bow-tie, 
and in [5] for the XOR-at-infinity. 
Another research direction, implying a function is realizable by a single hidden 
layer network, is based on the universal approximator property of one-hidden-layer 
networks, applied to intermediate functions obtained constructively adding extra 
hyperplanes to the essential hyperplanes of f. This direction was explored by Gibson 
[9], but there are virtually no results known beyond two dimensions. Gibson's result 
can be reformulated as follows: 
Theorem 2 If a polyhedral dichotomy f is defined on a compact subset of T 2, if f 
is not in an XOR-situation, and if no three essential hyperplanes (lines) intersect, 
then f is realizable with a single hidden layer network. 
Unfortunately Gibson's proof is not constructive, and extending it to remove some 
of the assumptions or to go to higher dimensions seems challenging. Both XOR- 
bow-tie and XOR-at-infinity are excluded by his assumptions of compactness and 
no multiple intersections. In the next section, we explore the two cases which are 
excluded by Gibson's assumptions. We prove that, in 7 2, the XOR-bow-tie and 
the XOR-at-infinity are the only restrictions to local realizability. 
2 LOCAL REALIZATION IN 2 
2.1 MULTIPLE INTERSECTION 
Theorem 3 Let f be a polyhedral dichotomy on T  and let P be a point of multiple 
intersection. Let Cp be a neighborhood of P which does not intersect any essential 
hyperplane other than those going through P. The restriction off to Cp is realizable 
by a one-hidden-layer network iff f is not in an XOR-bow-tie at P. 
Multilayer Neural Networks: One or Two Hidden Layers? 151 
The proof is in three steps: first, we reorder the hyperplanes in the neighborhood 
of P, so as to get a nice looking system of inequalities; second, we apply Farkas' 
lemma; third, we show how an XOR-bow-tie can be deduced. 
Proof.' Let P be the intersection of k ) 3 essential hyperplanes of f. All the 
hyperplanes which intersect at P can be renumbered and re-oriented so that the 
intersecting hyperplanes are totally ordered. Thus the label of the regions which 
have the point P in their closure is very regular. If one drops all the digits corre- 
sponding to the essential hyperplanes of f which do not contain P, the remaining 
part of the region labels are exactly like those of Figure 2. 
q 0 ...... 0 
e2 e2 0 ... 0 
: _ 0 : 
k ...... k k --k 
0 k+l ...... k+l -k+ 
0 0 ek+2 : 
 '.. '.. : 
2k-1 --2k-1 
0 ......... 0 
Figure 2: Labels of the regions in the neighborhood of P, and matrix A. 
The problem of finding a one-hidden-layer network which realizes f can be rewritten 
as a system of inequalities. The unknown variables are the weights wi and threshold 
0 of the output unit. Let ($) denote the subsystem of inequalities obtained from 
the 2k regions which have the point P in their closure. The regular numbering of 
these 2k regions allows us to write the system as follows 
l<i<k [ Y].m=Wm < 0 if classO 
y]nWm > 0 if class 1 
Y],=i_+ wm < 0 i
