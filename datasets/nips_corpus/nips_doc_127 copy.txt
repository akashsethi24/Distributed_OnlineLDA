653 
AN ADAPTIVE NETWORK THAT LEARNS 
SEQUENCES OF TRANSITIONS 
C. L. Winter 
Science Applications International Corporation 
5151 East Broadway, Suite 900 
Tucson, Arizona 85711 
ABSTRACT 
We describe an adaptive network, TIN 2, that learns the transition 
function of a sequential system from observations of its behavior. It 
integrates two subnets, TIN-1 (Winter, Ryan and Turner, 1987) and 
TIN-2. TIN-2 constructs state representations from examples of 
system behavior, and its dynamics are the main topics of the paper. 
TIN-1 abstracts transition functions from noisy state representations 
and environmental data during training, while in operation it produces 
sequences of transitions in response to variations in input. Dynamics 
of both nets are based on the Adaptive Resonance Theory of Carpenter 
and Grossberg (1987). We give results from an experiment in which 
TIN 2 learned the behavior of a system that recognizes strings with an 
even number of l's. 
INTRODUCTION 
Sequential systems respond to variations in their input environment with sequences of 
activities. They can be described in two ways. A black box description characterizes a 
system as an input-output function, m = B(11), mapping a string of input symbols, u, 
into a single output symbol, m. A sequential automaton description characterizes a 
system as a sextuple (U, M, S, s 0, f, g) where U and M are alphabets of input and output 
symbols, S is a set of states, s o is an initial state and f and g are transition and output 
functions respectively. The transition function specifies the current state, s t, as a 
function of the last state and the current input, u t, 
st = f( st-1, ut ) � 
(1) 
In this paper we do not discuss output functions because they are relatively simple. To 
further simplify discussion, we restrict ourselves to binary input alphabets, although the 
neural net we describe here can easily be extended to accomodate more complex alphabets. 
654 Winter 
A common engineering problem is to identify and then simulate the functionality of a 
system from observations of its behavior. Simulation is straightforward when we can 
actually observe the internal states of a system, since then the function f can be specified 
by learning simple associations among internal states and external inputs. In robotic 
systems, for instance, internal states can often be characterized by such parameters as 
stepper motor settings, strain gauge values, etc., and so are directly accessible. Artificial 
neural systems have ,been found useful in such simulations because they can associate 
large, possibly noisy state space and input variables with state and output variables (Tolat 
and Widrow, 1988; Winter, Ryan and Turner, 1987). 
Unfortunately, in many interesting cases we must base simulations on a limited set of 
examples of a system's black box behavior because its internal workings are 
unobservable. The black box description is not, by itself, much use as a simulation tool 
since usually it cannot be specified without resorting to infinitely large input-output 
tables. As an alternative we can try to develop a sequential automaton description of the 
system by observing regularities in its black box behavior. Artificial neural systems can 
contribute to the development of physical machines dedicated to system identification 
because i) frequently state representations must be derived from many noisy input 
variables, ii) data must usually be processed in continuous time and iii) the explicit 
dynamics of artificial neural systems can be used as a framework for hardware 
implementations. 
In this paper we give a brief overview of a neural net, TIN 2, which learns and processes 
state transitions from observations of correct black box behavior when the set of 
observations is large enough to characterize the black box as an automaton. The TIN 2 
net is based on two component networks. Each uses a modified adaptive resonance circuit 
(Carpenter and Grossberg, 1987) to associate heterogeneous input patterns. TIN-1 
(Winter, Ryan and Turner, 1987) learns and executes transitions when given state 
representations. It has been used by itself to simulate systems for which explicit state 
representations are available (Winter, 1988a). TIN-2 is a highly parallel, continuous time 
implementation of an approach to state representation first outlined by Nerode (1958). 
Nerode's approach to system simulation relies upon the fact that every string, u, moves a 
machine into a particular state, s(11), once it has been processed. The s(11) state can be 
characterized by putting the system initially into s(11) (by processing ID and then 
presenting a set of experimental strings, {w 1, ..., Wn}, for further processing. 
Experiments consist of observing the output m i = B(ll..wi) where � indicates 
concatenation. A state can then be represented by the entries in a row of a state 
characterization table, C (Table 1). The rows of C are indexed by strings, 11, its columns 
are indexed by experiments, wi, and its entries are mi. In Table 1 annotations in 
parentheses indicate nodes (artificial neurons) and subnetworks of TIN-2 equivalent to the 
corresponding C table entry. During experimentation C expands as states are 
Adaptive Network That Learns Sequences of Transitions 655 
distinguished from one another. The orchestration of experiments, their selection, the 
TABLE 1. C Table Constructed by TIN-2 
(Assembly 1) 1 (Assembly 2) 
) 1 (Node 7) 0 (Node 2) 0 (Node 5) 
1 0 (Node 6) 0 (Node 9) 1 (Node 1) 
0 0 (Node 1) 1 (Node 6) 0 (Node 4) 
10 0 (Node 3) 0 (Node 2) 0 (Node 0) 
role of teachers and of the environment have been investigated by Arbib and Zeiger 
(1969), Arbib and Manes (1974), Gold (1972 and 1978) and Angluin (1987) to name a 
few. TIN-2 provides an architecture in which C can be embedded and expanded as 
necessary. Collections of nodes within TIN-2 learn to associate triples (m i, u, w i) so that 
inputting 1! later results in the output of the representation (m 1, ..., mn) u of the state 
associated with u. 
TIN-2 
TIN-2 is composed of separate assemblies of nodes whose dynamics are such that each 
assembly comes to correspond to a column in the state characterization table C. Thus we 
call them column-assemblies. Competition among column-assemblies guarantees that 
nodes of only one assembly, say the i th, learn to respond to experimental pattern w i. 
Hence column-assemblies can be labelled w 1 , w 2 and so on, but since labelings are not 
assigned ahead of time, arbitrarily large sets of experiments can be learned. 
The theory of adaptive resonance is implemented in TIN-2 column-assemblies through 
partitioned adaptive resonance circuits (cf. Ryan, Winter and Turner, 1987). Adaptive 
resonance circuits (Grossberg and Carpenter, 1987; Ryan and Winter, 1987) are composed 
of four collections of nodes: Input, Comparison (F1), Recognition (F2) and Reset. In 
TIN-2 Input, Comparison and Reset are split into disjoint m, 11 and w partitions. The net 
runs in either training or operational mode, and can move from one to the other as 
required. The training dynamics of the circuit are such that an F 2 node is stimulated by 
the overall triple (m, u, w_.), but can be inhibited by a mismatch with any component. 
During operation input of 11 recalls the state representation s(11) = (m 1, ..., mn)u. 
Node activity for the kth F 1 partition, F1, k, k = m, u, w, is governed by 
x dXi,k/dt =- xi): + Eje F2 Tjif(Yj) + Ii,k. 
Here x < 1 scales time, Ii, k is the value of the i th input node of partition k, xi, k is 
656 Winter 
activity in the corresponding node of F 1 and f is a sigmoid function with range [0, 1]. 
The elements of I are either 1, -1 or 0. The dynamics of the TIN-2 circuit are such that 0 
indicates the absence of a symbol, while 1 and -1 represent elements of a binary alphabet. 
The adaptive feedback filter, T, is a matrix (Tji) whose elements, after training, are also 
1, -1 or0. 
Activity, yj, in the jth F2 node is driven by 
' dyj/dt = -yj + [f(yj) + Y-,ue Fl,uBuj h(xu) + Z we Fl,w Bw h(xw) 
+ , me Fl,m Bmj h(xm)] ' 4[ Y_.,qcj f(yq) + Ru, j + R w] 
(3) 
The feedforward filter B is composed of matrices (Bud), (Bin,j) and (Bw) whose elements 
are normalized to the size of the patterns memorized. Note that (Bw) is the same for 
every node in a given column-assembly, i.e. the rows of (Bw) are all the same. Hence all 
nodes within a column-assembly learn to respond to the same experimental pattern, w, 
and it is in this sense that an assembly evolves to become equivalent to a column in table 
C. During training the sum Y-.,q;ej f(yq) in (3) runs through the recognition nodes of all 
TIN-2 column-assemblies. Thus, during training only one F 2 node, say the jth, can be 
active at a time across all assemblies. In operation, on the other hand, we remove 
inhibition due to nodes in other assemblies so that at any time one node in each 
column-assembly can be active, and an entire state representation can be recalled. 
The Reset terms Ru, j and R w in (3) actively inhibit nodes of F 2 when mismatches 
between memory and input occur. Ru, j is specific to the jth F2 node, 
dRu,j/dt: -Ru, j + f(yj) f(v, !u n-I-Pl,u 
(4) 
R w affects all F 2 nodes in a column-assembly and is driven by 
dRw/dt = 'Rw + [Yje F2 f(Yj)] f(v II I w II-, Pl,w II). 
(5) 
v < 1 is a vigilance parameter (Carpenter and Grossberg, 1987): for either (4) or (5) R > 0 
at equilibrium just when the intersection between memory and input, P1 = T ch I, is 
relatively small, i.e. R > 0 when v II I_ II > II P1 II. When the system is in operation, we 
fix R w = 0 and input the pattern I w = 0. To recall the row in table C indexed by u, we 
input u to all column-assemblies, and at equilibrium xi, m = Yje F2 Tjif(Yj). Thus xi, m 
represents the memory of the element in C corresponding to 11 and the column in C with 
the same label as the column-assembly. Winter (1988b) discusses recall dynamics in 

