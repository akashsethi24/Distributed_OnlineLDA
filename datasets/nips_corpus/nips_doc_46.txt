174 
A Neural Network C1A-sifier Based on Coding Theory 
Tzi-Dar Chiueh and Rodney Goodman 
California Institute of Technology, Pasadena, California 91125 
ABSTRACT
The new neural network classifier we propose transforms the 
classification problem into the coding theory problem of decoding a noisy 
codeword. An input vector in the feature space is transformed into an internal 
representation which is a codeword in the code space, and then error correction 
decoded in this space to classify the input feature vector to its class. Two classes 
of codes which give high performance are the Hadamard matrix code and the 
maximal length sequence code. We show that the number of classes stored in an 
N-neuron system is linear in N and significantly more than that obtainable by 
using the Hopfield type memory as a classifier. 
I. INTRODUCTION 
Associative recall using neural networks has recently received a great deal 
of attention. Hopfield in his papers [1,2] describes a mechanism which iterates 
through a feedback loop and stabilizes at the memory element that is nearest the 
input, provided that not many memory vectors are stored in the machine. He has 
also shown that the number of memories that can be stored in an N-neuron 
system is about 0.15N for N between 30 and 100. McEliece et al. in their work [3] 
showed that for synchronous operation of the Hopfield memory about N/(21ogN) 
data vectors can be stored reliably when N is large. Abu-Mostafa [4] has predicted 
that the upper bound for the number of data vectors in an N-neuron Hopfield 
machine is N. We believe that one should be able to devise a machine with M, the 
number of data vectors, linear in N and larger than the 0.15N achieved by the 
Hopfield method. 
N 
Feature Space = B = {-1 ,1 } 
Ae 
eC 
B. 
N L 
L 
Code Space = B = 
Figure 1 (a) Classification problems versus (b) Error control decoding problems 
In this paper we are specifically concerned with the problem of 
classification as in pattern recognition. We propose a new method of building a 
neural network classifier, based on the well established techniques of error 
control coding. Consider a typical classification problem (Fig. l(a)), in which one 
is given apriori a set of classes, C(a), a =1 ..... M. Associated with each class is a 
feature vector which labels the class ( the exemplar of the class ), i.e. it is the 
@ American Institute of Physics 1988 
175 
most representative point in the class region. The input is classified into the 
class with the nearest exemplar to the input. Hence for each class there is a 
region in the N-dimensional binary feature space BN --- [1,-1)N, in which every 
vector will be classified to the corresponding class. 
A similar problem is that of decoding a codeword in an error correcting 
code as shown in Fig. l(b). In this case codewords are constructed by design and 
are usually at least dmin apart. The received corrupted codeword is the input to 
the decoder, which then finds the nearest codeword to the input. In principle 
then, ff the distance between codewords is greater than 2t + 1, it is possible to 
decode (or classify) a noisy codeword (feature vector) into the correct codeword 
(exemplar) provided that the Hamming distance between the noisy codeword and 
the correct codeword is no more than t. Note that there is no guarantee that the 
exemplars are uniformly distributed in BN, consequently the attraction radius 
(the maximum number of errors that can occur in any given feature vector such 
that the vector can still be correctly classified) will depend on the minimum 
distance between exemplars. 
Many solutions to the minimum Hamming distance classification have 
been proposed, the one commonly used is derived from the idea of matched filters 
in communication theory. Lippmann [5] proposed a two-stage neural network 
that solves this classification problem by first correlating the input with all 
exemplars and then picking the maximum by a winner-take-all circuit or a 
network composed of two-input comparators. In Figure 2, fl,f2 ..... fN are the N 
input bits, and Sl,S2 .... SM are the matching scores(similarity) of f with the M 
exemplars. The second block picks the maximum of Sl,S2 ..... SM and produces the 
index of the exemplar with the largest score. The main disadvantage of such a 
classifier is the complexity of the maximum-picking circuit, for example a 
winner-take-all net needs connection weights of large dynamic range and 
graded-response neurons, whilst the comparator maximum net demands M-1 
comparators organized in log2M stages. 
fl 
S. c(CO+ 
g= e � 
fN 
M 
A 
X 
I 
M 
U 
M 
class(f ) 
f= d(a+ ) e 
Featu re Code 
Space Space 
ODER 
DEC 
Fig. 2 A matched filter type classifier Fig. 3 Structure of the proposed classifier 
Our main idea is thus to transform every vector in the feature space to a 
vector in some code space in such a way that every exemplar corresponds to a 
codeword in that code. The code should preferably (but not necessarily) have the 
property that codewords are uniformly distributed in the code space, that is, the 
Hamming distance between every pair of codewords is the same. With this 
transformation, we turn the problem of classification into the coding problem of 
decoding a noisy codeword. We then do error correction decoding on the vector in 
the code space to obtain the index of the noisy codeword and hence classify the 
original feature vector, as shown in Figure 3. 
This paper develops the construction of such a classification machine as 
follows. First we consider the problem of transforming the input vectors from the 
feature space to the code space. We describe two hetero-associative memories for 
doing this, the first method uses an outer product matrix technique similar to 
176 
that of Hopfield's, and the second method generates its matrix by the 
pseudoinverse technique[6,7]. Given that we have transformed the problem of 
associative recall, or classification, into the problem of decoding a noisy 
codeword, we next consider suitable codes for our machine. We require the 
codewords in this code to have the property of orthogonality or 
pseudo-orthogonality, that is, the ratio of the cross-correlation to the 
auto-correlation of the codewords is small. We show two classes of such good 
codes for this particular decoding problem i.e. the Hadamard matrix codes, and 
the maximal length sequence codes[8]. We next formulate the complete decoding 
algorithm, and describe the overall structure of the classifier in terms of a two 
layer neural network. The first layer performs the mapping operation on the 
input, and the second one decodes its output to produce the index of the class to 
which the input belongs. 
The second part of the paper is concerned with the performance of the 
classifier. We first analyze the performance of this new classifier by finding the 
relation between the maximum number of classes that can be stored and the 
classification error rate. We show (when using a transform based on the outer 
product method) that for negligible misclassification rate and large N, a not very 
tight lower bound on M, the number of stored classes, is 0.22N. We then present 
comprehensive simulation results that confirm and exceed our theoretical 
expectations. The simulation results compare our method with the Hopfield 
model for both the outer product and pseudo-inverse method, and for both the 
analog and hard limited connection matrices. In all cases our classifier exceeds 
the performance of the Hopfield memory in terms of the number of classes that 
can be reliably recovered. 
II. TRANSFORM TECHNIQUF_ 
Our objective is to build a machine that can discriminate among input 
vectors and classify each one of them into the appropriate class. Suppose 
d(a) e BN is the exemplar of the corresponding class C(C0, ct = 1,2 ..... M. Given the 
input f, we want the machine to be able to identify the class whose exemplar is 
closest to f, that is, we want to calculate the following function, 
class(f) = a 
iff If-d(COI <lf-d(l 
where   denotes Hamming distance in BN. 
We approach the problem by seeking a transform , that maps each 
exemplar d(a) in BN to the corresponding codeword w(a) in BL. And an input 
feature vector f = d(�) + e is thus mapped to a noisy codeword g = w(�) + e' where e 
is the error added to the exemplar, and e' is the corresponding error pattern in the 
code space. We then do error correction decoding on g to get the index of the 
corresponding codeword. Note that e' may not have the same Hamming weight as 
e, that is, the transformation , may either generate more errors or eliminate 
errors that are present in the original input feature vector. We require , to 
satisfy the following equation, 
a=o, 1 ..... M-1 
and , will be implemented using a single-layer feedfonvard network. 
177 
Thus we first construct a matrix according to the sets of d(a)'s and w(a)'s, call it T, 
and define  as 
where sgn is the threshold operator that maps a vector in RL to BL and R is the 
field of real numbers. 
Let D be an N x M matrix whose ath column is d(a) and W be an L x M 
matrix whose [th column is w([). The two possible methods of constructing the 
matrix for  are as follows: 
Scheme A [outer product mth0cl) [3,6]: In this scheme the matrix T is 
defined as the sum of outer products of all exemplar-codeword pairs, i.e. 
M-1 
T (A)ij = ,, wi(a)' dj(a) 
a=0 
or equivalently, 
T(A) = WDt 
Scheme B (pseudo-inverse method) [6,7] � We want to find a matrix T(B) 
satisfying the following equation, 
In general D is not a square matrix, moreover D may be singular, so D-1 
may not exist. To circumvent this difficulty, we calculate the pseudo-inverse 
(denoted DS) of the matrix D instead of its real inverse, let DS-- (DtD)-lDt. T(B) can 
be formulated as, 
T(B) = W D = W (Dr D)-IDt 
IlL CODES 
The codes we are looking for should preferably have the property that its 
codewords be distributed uniformly
