Potential Boosters ? 
Nigel Duffy 
Department of Computer Science 
University of California 
Santa Cruz, CA 95064 
nigedufjfcse. ucsc. edu 
David Helmbold 
Department of Computer Science 
University of California 
Santa Cruz, CA 95064 
dphcse. ucsc. edu 
Abstract 
Recent interpretations of the Adaboost algorithm view it as per- 
forming a gradient descent on a potential function. Simply chang- 
ing the potential function allows one to create new algorithms re- 
lated to AdaBoost. However, these new algorithms are generally 
not known to have the formal boosting property. This paper ex- 
mines the question of which potential functions lead to new al- 
gorithms that are boosters. The two main results are general sets 
of conditions on the potential; one set implies that the resulting 
algorithm is a booster, while the other implies that the algorithm 
is not. These conditions are applied to previously studied potential 
functions, such as those used by LogitBoost and Doom II. 
I Introduction 
The first boosting algorithm appeared in Rob Schapire's thesis [1]. This algorithm 
was able to boost the performance of a weak PAC learner [2] so that the resulting 
algorithm satisfies the strong PAC learning [3] criteria. We will call any method 
that builds a strong PAC learning algorithm from a weak PAC learning algorithm 
a PAC boosting algorithm. Freund and Schapire later found an improved PAC 
boosting algorithm called AdaBoost [4], which also tends to improve the hypotheses 
generated by practical learning algorithms [5]. 
The AdaBoost algorithm takes a labeled training set and produces a master hy- 
pothesis by repeatedly calling a given learning method. The given learning method 
is used with different distributions on the training set to produce different base 
hypotheses. The master hypothesis returned by AdaBoost is a weighted vote of 
these base hypotheses. AdaBoost works iteratively, determining which examples 
are poorly classified by the current weighted vote and selecting a distribution on 
the training set to emphasize those examples. 
Recently, several researchers [6, 7, 8, 9, 10] have noticed that Adaboost is performing 
a constrained gradient descent on an exponential potential function of the margins 
of the examples. The margin of an example is yF(x) where y is the :51 valued label 
of the example x and F(x)   is the net weighted vote of master hypothesis F. 
Once Adaboost is seen this way it is clear that further algorithms may be derived 
by changing the potential function [6, 7, 9, 10]. 
Potential Boosters? 259 
The exponential potential used by AdaBoost has the property that the influence 
of a data point increases exponentially if it is repeatedly misclassified by the base 
hypotheses. This concentration on the hard examples allows AriaBoost to rapidly 
obtain a consistent hypothesis (assuming that the base hypotheses have certain 
properties). However, it also means that an incorrectly labeled or noisy example 
can quickly attract much of the distribution. It appears that this lack of noise- 
tolerance is one of AdaBoost's few drawbacks [tt]. Several researchers [7, 8, 9, t0] 
have proposed potential functions which do not concentrate as much on these hard 
examples. However, they generally do not show that the derived algorithms have 
the PAC boosting property. 
In this paper we return to the original motivation behind boosting algorithms and 
ask: for which potential functions does gradient descent lead to PAC boosting 
algorithms (i.e. boosters that create strong PAC learning alg. orithms from arbitrary 
weak PAC learners). We give necessary conditions that are met by some of the 
proposed potential functions (most notably the LogitBoost potential introduced by 
Friedman et al. [7]). Furthermore, we show that simple gradient descent on other 
proposed potential functions (such as the sigmoidal potential used by Mason et 
al. It0]) cannot convert arbitrary weak PAC learning algorithms into strong PAC 
learners. The aim of this work is to identify properties of potential functions required 
for PAC boosting, in order to guide the search for more effective potentials. 
Some potential functions have an additional tunable parameter [10] or change over 
time [12]. Our results do not yet apply to such dynamic potentials. 
2 PAC Boosting 
Here we define the notions of PAC learning  and boosting, and define the notation 
used throughout the paper. 
A concept C is a subset of the learning domain X. A random example of C is a pair 
(x  X,y  {-1, +1)) where x is drawn from some distribution on X and y = I if 
x  C and -1 otherwise. A concept class is a set of concepts. 
Definition 1 A (strong) PAC learner for concept class C has the property that for 
every distribution D on 2d, all concepts C E C, and all 0 < e, 6 < 1/2: with probabil- 
ity at least 1- 6 the algorithm outputs a hypothesis h where PD[h(x)  C(x)] _ e. 
The learning algorithm is given C, e, 6, and the ability to draw random examples of 
C (w.r.t. distribution D), and must run in time bounded by poly(1/e, 1/6). 
Definition 2 A weak PAC learner is similar to a strong PA C learner, except that 
it need only satisfy the conditions for a particular 0 < co, 6o < 1/2 pair, rather than 
for all e,6 pairs. 
Definition 3 A PAC boosting algorithm is a generic algorithm which can leverage 
any weak PA C learner to meet the strong PA C learning criteria. 
In the remainder of the paper we emphasize boosting the accuracy e as it is much 
easier to boost the confidence 6, see Haussler et al. [13] and Freund [14] for details. 
Furthermore, we emphasize boosting by re-sampling, where the strong PAC learner 
draws a large sample, and each iteration the weak learning algorithm is called with 
some distribution over this sample. 
XTo simplify the presentation we omit the instance space dimension and taxget repre- 
sentation length parameters. 
260 N. DuJ and D. Helmbold 
Throughout the paper we use the following notation. 
� m is the cardinality of the fixed sample ((xl,Yl),..., (xm,y,)). 
� ht(x) is the +1 valued weak hypothesis created at iteration t. 
� at is the weight or vote of ht in the master hypothesis, the a's may or may 
not be normalized so that tt,=l oft, = 1. 
t t 
� Ft(x) .t,=l(ott, ht,(x) , is the master hypothesis 2 at iter- 
ation t. 
� Ui,t -- Yi tt,=l ott, ht,(x) is the margin of xi after iteration t; the t sub- 
script is often omitted. Note that the margin is positive when the master 
Zt':l t 
hypothesis is correct, and the normalized margin is Ui,t/ t 
� p(u) is the potential of an instance with margin u, and the total potential 
is Zim=l p(ui). 
� P D[ ],P$[ ], and Es[] are the probability with respect to the unknown 
distribution over the domain, and the probability aud expectations with 
respect to the uniform distribution over the sample, respectively. 
m U 
Our results apply to total potential functions of the form Y-i=l P(i) where p is 
positive and strictly decreasing. 
3 Leveraging Learners by Gradient Descent 
AdaBoost [4] has recently been interpreted as gradient descent independently by 
several groups [6, 7, 8, 9, 10. Under this interpretation AdaBoost is seen as minimiz- 
ing the total potential Y-i=l p(ui) = Y',im_-i exp(-ui) via feasible direction gradient 
descent. On each iteration t + 1, AdaBoost chooses the direction of steepest descent 
as the distribution on the sample, and calls the weak learner to obtain a new base 
hypothesis ht+. The weight ct+l of this new weak hypothesis is calculated to min- 
imize 3 the resulting potential '.im=l P(Ui,t+l ) -- im_-i exp(--(ui,t -{- ott+lYiht+l (xi) ) ). 
This gradient descent idea has been generalized to other potential functions [6, 7, 
10]. Duffy et al. [9] prove bounds for a similar gradient descent technique using a 
non-componentwise, non-monotonic potential function. 
Note that if the weak learner returns a good hypothesis ht (with training error at 
m 
most e < 1/2), then Y]-i=l Dt(xi)yiht(xi) ) I - 2e ) 0. We set r -- I - 2e, and 
assume that each base hypothesis produced satisfies Zim__l Dt(xi)yiht(xi) _ r. 
In this paper we consider this general gradient descent approach applied to various 
potentials -]-i1 p(ui). Note that each potential function p has two corresponding 
gradient descent algorithms (see [6]). The un-normalized algorithms (like AdaBoost) 
continually add in new weak hypotheses while preserving the old a's. The normal- 
ized algorithms re-scale the a's so that they always sum to 1. In general, we call 
such algorithms leveraging algorithms, reserving the term boosting for those 
that actually have the PAC boosting property. 
4 Potentials that Don't Boost 
In this section we describe sufficient conditions on potential functions so that the 
corresponding leveraging algorithm does not have the PAC boosting property. We 
2The prediction of the master hypothesis on instance x is the sign of Ft(x). 
SOur current proofs require that the actual ct's be no greater than a constant (say 1). 
Therefore, this minimizing c may need to be reduced. 
Potential Boosters? 261 
apply these conditions to show that two potentials from the literature do not lead 
to boosting algorithms. 
Theorem 1 Let p(u) be a potential function for which: 
1) the derivative, p'(u), is increasing (-p'(u) decreasing) in +, and 
2) 3/ > 0 such that for all u  O, -/p'(u) _ -p'(-2u). 
Then neither the normalized nor the un-normalized leveraging algorithms 
sponding to potential p have the PA C boosting property. 
co rre - 
This theorem is proven by an adversary argument. Whenever the concept class is 
sufficiently rich 4, the adversary can keep a constant fraction of the sample from 
being correctly labeled by the master hypothesis. Thus as the error tolerance e goes 
to zero, the master hypotheses will not be sufficiently accurate. 
We now apply this theorem to two potential functions from the literature. 
Friedman et al. [7] describe a pote
