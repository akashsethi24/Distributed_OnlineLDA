195 
LEARNING WITH TEMPORAL DERIVATIVES IN 
PULSE-CODED NEURONAL SYSTEMS 
Mark Gluck 
David B. Parker 
Eric S. Reifsnider 
Department of Psychology 
Stanford University 
Stanford, CA 94305 
Abstract 
A number of learning models have recently been proposed which 
involve calculations of temporal differences (or derivatives in 
continuous-time models). These models, like most adaptive network 
models, are formulated in terms of frequency (or activation), a useful 
abstraction of neuronal firing rates. To more precisely evaluate the 
implications of a neuronal model, it may be preferable to develop a 
model which transmits discrete pulse-coded information. We point out 
that many functions and properties of neuronal processing and learning 
may depend, in subtle ways, on the pulse-coded nature of the informa- 
tion coding and transmission properties of neuron systems. When com- 
pared to formulations in terms of activation, computing with temporal 
derivatives (or differences) as proposed by Kosko (1986), Klopf 
(1988), and Sutton (1988), is both more stable and easier when refor- 
mulated for a more neuronally realistic pulse-coded system. In refor- 
mulating these models in terms of pulse-coding, our motivation has 
been to enable us to draw further parallels and connections between 
real-time behavioral models of learning and biological circuit models 
of the substrates underlying learning and memory. 
INTRODUCTION 
Learning algorithms are generally defined in terms of continuously-valued levels of input 
and output activity. This is true of most training methods for adaptive networks, (e.g., 
Parker, 1987; Rumelhart, Hinton, & Williams, 1986; Werbos, 1974; Widrow & Hoff, 
1960), and also for behavioral models of animal and human learning, (e.g, Gluck & 
Bower, 1988a, 1988b; Rescorla & Wagner, 1972), as well as more biologically oriented 
models of neuronal function (e.g., Bear & Cooper, in press; Hebb, 1949; Granger, 
Abros-Ingerson, Staubli, & Lynch, in press; Gluck & Thompson, 1987; Gluck, 
Reifsnider, & Thompson, in press; McNaughton & Nadel, in press; Gluck & Rumelhart, 
in press). In spite of the attractive simplicity and utility of the activation construct 
196 Parker, Gluck and Reifsnider 
neurons use di._qcrete trains of pulses for the transmission of information from cell to cell. 
Frequency (or activation) is a useful abstraction of pulse trains, especially for bridging 
the gap between whole-animal and single neuron behavior. To more precisely evaluate 
the implications of a neuronal model, it may be preferable to develop a model which 
transmits discrete pulse-coded information; it is possible that many functions and proper- 
ties of neuronal processing and learning may depend, in subtle ways, on the pulse-coded 
nature of the information coding and transmission properties of neuron systems. 
In the last few years, a number of learning models have been proposed which involve 
computations of temporal differences (or derivatives in continuous-time models). Klopf 
(1988) presented a formal real-time model of classical conditioning that predicts the 
magnitude of conditioned responses (CRs), given the temporal relationships between 
conditioned stimuli (CSs) and an unconditional stimulus (US). Klopf's model incor- 
porates a differential-Hebbian learning algorithm in which changes in presynaptic lev- 
els of activity are correlated with changes in postsynaptic levels of activity. Motivated 
by the constraints and motives of engineering, rather than animal learning, Kosko (1986) 
proposed the same basic rule and provided extensive analytic insights into its properties. 
Sutton (1988) introduced a class of incremental learning procedures, called temporal 
difference methods, which update associative (predictive) weights according to the 
difference between temporally successive predictions. In addition to the applied potential 
of this class of algorithms, Sutton & Barto (1987) show how their model, like Klopf's 
(1988) model, provides a good fit to a wide range of behavioral data on classical condi- 
tioning. 
These models, all of which depend on computations involving changes over time in 
activation levels, have been successful both for predicting a wide range of behavioral 
animal learning data (Klopf, 1988; Sutton & Barto, 1987) and for solving useful 
engineering problems in adaptive prediction (Kosko, 1986; Sutton, 1988). The possibility 
that these models might represent the computational properties of individual neurons, 
seems, at first glance, highly unlikely. However, we show by reformulating these models 
for pulse-coded communication (as in neuronal systems) rather than in terms of abstract 
activation levels, the computational soundness as well as the biological relevance of the 
models is improved. By avoiding the use of unstable differencing methods in computing 
the time-derivative of activation levels, and by increasing the error-tolerance of the com- 
putations, pulse coding will be shown to improve the accuracy and reliability of these 
models. The pulse coded models will also be shown to lend themselves to a closer com- 
parison to the function of real neurons than do models that operate with activation levels. 
As the ability of researchers to directly measure neuronal behavior grows, the value of 
such close comparisons will increase. As an example, we describe here a pulse-coded 
version of Klopf's differential-Hebbian model of classification learning. Further details 
are contained in Gluck, Parker, & Reifsnider, 1988. 
Learning with Temporal Derivatives 197 
Pulse-Coding in Neuronal Systems 
We begin by outlining the general theory and engineering advantages of pulse-coding 
and then describe a pulse-coded reformulation of differential-Hebbian learning. The key 
idea is quite simple and can be summarized as follows: Frequency can be seen, loosely 
speaking, as an integral of pulses; conversely, therefore, pulses can be thought of as car- 
rying information about the derivatives of frequency. Thus, computing with the deriva- 
tives of frequency is analogous to computing with pulses. As described below, our 
basic conclusion is that differential-Hebbian learning (Klopf, 1988; Kosko, 1986) when 
reformulated for a pulse-coded system is both more stable and easier to compute than is 
apparent when the rule is formulated in terms of frequencies. These results have impor- 
tant implications for any learning model which is based on computing with time- 
derivatives, such as Sutton's Temporal Difference model (Sutton, 1988; Sutton & Barto, 
1987) 
There are many ways to electrically transmit analog information from point to point. 
Perhaps the most obvious way is to transmit the information as a signal level. In elec- 
tronic systems, for example, data that varies between 0 and 1 can be transmitted as a vol- 
tage level that varies between 0 volts and 1 volt. This method can be unreliable, how- 
ever, because the receiver of the information can't tell if a constant DC voltage offset has 
been added to the information, or if crosstalk has occurred with a nearby signal path. To 
the exact degree that the signal is interfered with, the data as read by the receiver will be 
erroneously altered. The consequences of faults appearing in the signal are particularly 
serious for systems that are based on derivatives of the signal. In such systems, even a 
small, but sudden, unintended change in signal level can drastically alter its derivative, 
creating large errors. 
A more reliable way to transmit analog information is to encode it as the frequency of a 
series of pulses. A receiver can reliably determine if it has received a pulse, even in the 
face of DC voltage offsets or moderate crosstalk. Most errors will not be large enough to 
constitute a pulse, and thus will have no effect on the transmitted information. The 
receiver can count the number of pulses received in a given time window to determine 
the frequency of the pulses. Further information on encoding analog information as the 
frequency of a series of pulses can be found in many electrical engineeng textbooks 
(e.g., Horowitz & Hill, 1980). 
As noted by Parker (1987), another advantage of coding an analog signal as the fre- 
quency of a series of pulses is that the time derivative of the signal can be easily and 
stably calculated: If x (t) represents a series of pulses (x equals 1 ff a pulse is occuring at 
time t; otherwise it equals 0) then we can estimate the frequency, f (t), of the series of 
pulses using an exponentially weighted time average: 
f (t)=gtlx(x)e-t'-x dx 
198 Parker, Gluck and Reifsnider 
where !x is the decay constant. The well known formula for the derivative off (t) is 
Thus, the time derivative of pulse-coded information can be calculated without using any 
unstable riflefencing methods, it is simply a function of presence or absence of a pulse 
relative to the current expectation (frequency) of pulses. As described earlier, calculation 
of time derivatives is a critical component of the learning algorithms proposed by Klopf 
(1988), Kosko (1986) and Sutton (Sutton, 1988; Sutton & Barto 1987). They are also an 
important aspect of 2nd order (,pseudo-newtonian) extensions of the backpropogation 
learning rule for multi-layer adaptive connectionist networks (Parker, 1987). 
Summary of Klopf s Model 
Klopf (1988) proposed a model of classical conditioning which incorporates the same 
!earning rule proposed by Kosko (1986) and which extends some of the ideas presented 
in Sutton and Barto' s (1981) real-time generalization of Rescorla and Wagner' s (1972) 
model of classical conditioning. The mathematical specification of Klopf's model con- 
sism of two equations: one which calculates output signals based on a weighted sum of 
input signals (drives) and one which determines changes in synapse efficacy due to 
changes in signal levels. The specification of signal output level is defined as 
y (
