SPEAKER RECOGNITION USING 
NEURAL TREE NETWORKS 
Kevin R. Farrell and Richard J. Mammone 
CAIP Center, Rutgers University 
Core Building, Frelinghuysen Road 
Piscataway, New Jersey 08855 
Abstract 
A new classifier is presented for text-independent speaker recognition. The 
new classifier is called the modified neural tree network (MNTN). The 
NTN is a hierarchical classifier that combines the properties of decision 
trees and feed-forward neural networks. The MNTN differs from the stan- 
dard NTN in that a new learning rule based on discriminant learning 
is used, which minimizes the classification error as opposed to a norm 
of the approximation error. The MNTN also uses leaf probability mea- 
sures in addition to the class labels. The MNTN is evaluated for several 
speaker identification experiments and is compared to multilayer percep- 
ttons (MLPs) decision trees, and vector quantization (VQ) classifiers. The 
VQ classifier and MNTN demonstrate comparable performance and per- 
form significantly better than the other classifiers for this task. Addition- 
ally the MNTN provides a logarithmic saving in retrieval time over that 
of the VQ classifier. The MNTN and VQ classifiers are also compared 
for several speaker verification experiments where the MNTN is found to 
outperform the VQ classifier. 
1 INTRODUCTION 
Automatic speaker recognition consists of having a machine recognize a person based 
on his or her voice. Automatic speaker recognition is comprised of two categories: 
speaker identification and speaker verification. The objective of speaker identifi- 
cation is to identify a person within a fixed population based on a test utterance 
from that person. This is contrasted to speaker verification where the objective is 
to verify a person's claimed identity based on the test utterance. 
1035 
1036 Farrell and Mammone 
Speaker recognition systems can be either text dependent or text independent. 
Text-dependent speaker recognition systems require that the speaker utter a specific 
phrase or a given password. Text-independent speaker identification systems iden- 
tify the speaker regardless of the utterance. This paper focuses on text-independent 
speaker identification and speaker verification tasks. 
A new classifier is introduced and evaluated for speaker recognition. The new clas- 
sifier is the modified neural tree network (MNTN). The MNTN incorporates mod- 
ifications to the learning rule of the original NTN [1] and also uses leaf probability 
measures in addition to the class labels. Also, vector quantization (VQ) classifiers, 
multilayer perceptrons (MLPs), and decision trees are evaluated for comparison� 
This paper is organized as follows. Section 2 reviews the neural tree network and 
discusses the modifications. Section 3 discusses the feature extraction and classifica- 
tion phases used here for text-independent speaker recognition� Section 4 describes 
the database used and provides the experimental results. The summary and con- 
clusions of the paper are given in Section 5. 
2 THE MODIFIED NEURAL TREE NETWORK 
The NTN [1] is a hierarchical classifier that uses a tree architecture to implement 
a sequential linear decision strategy. Each node at every level of the NTN divides 
the input training vectors into a number of exclusive subsets of this data. The 
leaf nodes of the NTN partition the feature space into homogeneous subsets i.e., 
a single class at each leaf node. The NTN is recursively trained as follows. Given 
a set of training data at a particular node, if all data within that node belongs to 
the same class, the node becomes a leaf. Otherwise, the data is split into several 
subsets, which become the children of this node. This procedure is repeated until 
all the data is completely uniform at the leaf nodes. 
For each node the NTN computes the inner product of a weight vector w and an 
input feature vector :, which should be approximately equal to the the output 
label y  {0, 1}. Traditional learning algorithms minimize a norm of the error 
e = (y- < w,: >), such as the �2 or � norm. The splitting algorithm of the 
modified NTN is based on discriminant learning [2]. Discriminant learning uses a 
cost function that minimizes the classification error� 
For an M class NTN, the discriminant learning approach first defines a misclassifi- 
cation measure d(:) as [2]: 
1 
1 
- < > + :u---1 < , 
j#i 
where n is a predetermined smoothing constant. If : belongs to class i, then d(:) 
will be negative, and if : does not belong to class i, d(:) will be positive. The 
misclassification measure d(:) is then applied to a sigmoid to yield: 
1 
g[d(:)] = 1 + �-a(x)' (2) 
The cost finction in equation (2) is approximately zero for correct classifications 
and one for misclassifications. Hence, minimizing this cost function will tend to 
Speaker Recognition Using Neural Tree Networks 1037 
'0 0 
 0 0 
\4 01100..' 
000X 
0, 
_ 00 ' 
\1 
'. 10 
I I : '\ l 1 
01 I :\.. 
LABEL = 0 LABEL = I LABEL = 0 LABEL = 1 
CONFIDENCE = I_.0 CONFIDENCE = 0.6 CONFIDENCE = 0.8 CONFIDENCE = 0.7 
Figure 1: Forward Pruning and Confidence Measures 
minimize the classificat;ion error. It is noted that for binary NTNs, the weight 
updat;es obtained by t;he discriminant learning approach and t;he L norm of t;he 
error are equivalent; [3]. 
The NTN training algorit;hm described above const;ructs a t;ree having 100% per- 
formance on the t;raining set;. However, an NTN t;rained t;o t;his level may not; have 
optimal generalizat;ion due t;o overt;raining. The generalizat;ion can be improved 
by reducing t;he number of nodes in a t;ree, which is known as pruning. A t;ech- 
nique known as backward pruning was recently proposed [1] for t;he NTN. Given a 
fully grown NTN, i.e., 100% performance on t;he t;raining set;, t;he backward pruning 
met;hod uses a Lagrangian cost funct;ion t;o minimize t;he classificat;ion error and 
t;he number of leaves in the tree. The met;hod used here prunes t;he t;ree during it;s 
growl;h, hence it; is called .forward pruning. 
The forward pruning algorit;hm consist;s of simply t;runcat;ing t;he growt;h of t;he t;ree 
beyond a cert;ain level. For t;he leaves at; t;he t;runcat;ed level, a vot;e is t;aken and 
the leaf is assigned the label of t;he majorit;yo In addit;ion t;o a label, t;he leaf is 
also assigned a confidence. The confidence is comput;ed as t;he rat;io of the number 
of element;s for the vot;e winner t;o t;he t;ot;al number of element;s. The confidence 
provides a measure of confusion for t;he different regions of feat;ure space. The 
concept; of forward pruning is illust;rat;ed in Figure 1. 
3 FEATURE EXTRACTION AND CLASSIFICATION 
The process of feature extraction consists of obtaining characteristic parameters of 
a signal t;o be used t;o classify the signal. The extraction of salient features is a 
key step in solving any pattern recognition problem. For speaker recognition, the 
features extracted from a speech signal should be invariant with regard t;o the desired 
1038 Farrell and Mammone 
Feature X i 
Vector 
I Speakerl [yl_i 
(NTN, VQ  
' Codebook) 
Speaker 2 I  
(NTN VQ y2 I 
Codebook) 
Speaker N I ' 
(NTN, VQ yN : 
Codebook) 
Decision 
Speaker 
Identity 
or 
Authenticity 
Figure 2: Classifier Structure for Speaker Recognition 
speaker while exhibiting a large distance to any imposter. Cepstral coefficients are 
commonly used for speaker recognition [4] and shall be considered here to evaluate 
the classifiers. 
The classification stage of text-independent speaker recognition is typically imple- 
mented by modeling each speaker with an individual classifier. The classifier struc- 
ture for speaker recognition is illustrated in Figure 2. Given a specific feature vectors 
each speaker model associates a number corresponding to the degree of match with 
that speaker. The stream of numbers obtained for a set of feature vectors can be 
used to obtain a likelihood score for each speaker model. For speaker identification, 
the feature vectors for the test utterance are applied to all speaker models and the 
corresponding likelihood scores are computed. The speaker is selected as having 
the largest score. For speaker verification, the feature vectors are applied only to 
the speaker model for the speaker to be verified. If the likelihood score exceeds a 
threshold, the speaker is verified or else is rejected. 
The classifiers for the individual speaker models are trained using either supervised 
or unsupervised training methodso For supervised training methods the classifier for 
each speaker model is presented with the data for all speakers. Here, the extracted 
feature vectors for that speaker are labeled as %he and the extracted feature vec- 
tors for everyone else are labeled as zero The supervised classifiers considered 
here are the multilayer perceptron (MLP), decision trees and modified neural tree 
network (MNTN). For unsupervised training methods each speaker model is pre- 
sented with only the extracted feature vectors for that speaker. This data can 
then be clustered to determine a set of centfolds that are representative of that 
speaker. The unsupervised classifiers evaluated here are the full-search and tree- 
structure vector quantization classifiers, henceforth denoted as FSVQ and TSVQo 
Speaker models based on supervised training capture the differences of that speaker 
to other speakers, whereas models based on unsupervised training use a similarity 
measllre. 
Specifically, a trained NTN can be applied to speaker recognition as follows. Given 
a sequence of feature vectors as from the test utterance and a trained NTN for 
Speaker Recognition Using Neural Tree Networks 1039 
speaker $i, the corresponding speaker score is found as the hit ratio: 
M 
PvT;v(x]Si) : N + M � (3) 
Here, M is the number of vectors classified as %ne and N is the number of vec- 
tors classified as 'zero' The modified NTN computes a hit ratio
