Nonlinear Markov Networks for Continuous 
Variables 
Reimar Hofmann and Volker Tresp* 
Siemens AG, Corporate Technology 
Information and Communications 
81730 Mnchen, Germany 
Abstract 
We address the problem of learning structure in nonlinear Markov networks 
with continuous variables. This can be viewed as non-Gaussian multidi- 
mensional density estimation exploiting certain conditional independencies 
in the variables. Markov networks are a graphical way of describing con- 
ditional independencies well suited to model relationships which do not ex- 
hibit a natural causal ordering. We use neural network structures to model 
the quantitative relationships between variables. The main focus in this pa- 
per will be on learning the structure for the purpose of gaining insight into 
the underlying process. Using two data sets we show that interesting struc- 
tures can be found using our approach. Inference will be briefly addressed. 
1 Introduction 
Knowledge about independence or conditional independence between variables is most help- 
ful in understanding a domain. An intuitive representation of independencies is achieved by 
graphical models in which independency statements can be extracted from the structure of the 
graph. The two most popular types of graphical stochastical models are Bayesian networks 
which use a directed graph, and Markov networks which use an undirected graph. Whereas 
Bayesian networks are well suited to represent causal relationships, Markov networks are 
mostly used in cases where the user wants to express statistical correlation between variables. 
This is the case in image processing where the variables typically represent the grey levels 
of pixels and the graph encourages smootheness in the values of neighboring pixels (Markov 
random fields, Geman and Geman, 1984). We believe that Markov networks might be a useful 
representation in many domains where the concept of cause and effect is somewhat artificial. 
The learned structure of a Markov network also seems to be more easily communicated to 
non-experts; in a Bayesian network not all arc directions can be uniquely identified based on 
training data alone which makes a meaningful interpretation for the non-expert rather difficult. 
As in Bayesian networks, direct dependencies between variables in Markov networks are rep- 
resented by an arc between those variables and missing edges represent independencies (in 
Section 2 we will be more precise about the independencies represented in Markov networks). 
Whereas the graphical structure in Markov networks might be known a priori in some cases, 
Reimar. Hofmannmchp.siemens.de Volker. Trespmchp.siemens. de 
522 R. Hofmann and V. Tresp 
the focus of this work is the case that structure is unknown and must be inferred from data. 
For both discrete variables and linear relationships between continuous variables algorithms 
for structure learning exist (Whittaker, 1990). Here we address the problem of learning struc- 
ture for Markov networks of continuous variables where the relationships between variables 
are nonlinear. In particular we use neural networks for approximating the dependency be- 
tween a variable and its Markov boundary. We demonstrate that structural learning can be 
achieved without a direct reference to a likelihood function and show how inference in such 
networks can be performed using Gibbs sampling. From a technical point of view, these 
Markov boundary networks perform multi-dimensional density estimation for a very general 
class of non-Gaussian densities. 
In the next section we give a mathematical description of Markov networks and a formulation 
of the joint probability density as a product of compatibility functions. In Section 3.1 we 
discuss strucural learning in Markov networks based on a maximum likelihood approach and 
show that this approach is in general unfeasible. We then introduce our approach which is 
based on learning the Markov boundary of each variable. We also show how belief update can 
be performed using Gibbs sampling. In Section 4 we demonstrate that useful structures can 
be extraced from two data sets (Boston housing data, financial market) using our approach. 
2 Markov Networks 
The following brief introduction to Markov networks is adapted from Pearl (1988). Consider 
a strictly positive  joint probability density p(z) over a set of variables A' := {z, ..., z/v}. 
For each variable zi, let the Markov boundary of zi, 15i C_ ,Y - {:ei}, be the smallest set of 
variables that renders zi and A' - ({ z i } U Bi) independent under p(z) (the Markov boundary 
is unique for strictly positive distributions). Let the Markov network  be the undirected 
graph with nodes z,..., z/v and edges between zi and zj if and only if:e/G/Sj (which also 
implies zj  15i). In other words, a Markov network is generated by connecting each node to 
the nodes in its Markov boundary. Then for any set Z C_ (A' - {zi, zj}), zi is independent 
of zj given Z if and only if every path from zi to zj goes through at least one node in Z. In 
other words, two variables are independent if any path between those variables is blocked 
by a known variable. In particular a variable is independent of the remaining variables if the 
variables in its Markov boundary are known. 
A clique in G is a maximal fully connected subgraph. Given a Markov Network G for p(z) it 
can be shown that p can be factorized as a product of positive functions on the cliques of G, 
i.e. 
1 
i 
where the product is over all cliques in the graph. zcziqe, is the projection of z to the 
variables of the i-th clique and the gi are the compatibility functions w.r.t. cliquei. K = 
f I-Ii ]i (Zclique,)dz is the normalization constant. Note, that a state whose clique functions 
have large values has high probability. The theorem of Hammersley and Clifford states that 
the normalized product in equation 1 embodies all the conditional independencies portrayed 
by the graph (Pearl, 1988) 2 for any choice of the . 
If the graph is sparse, i.e. if many conditional independencies exist then the cliques might 
 To simplify the discussion we will assume strict positivity for the rest of this paper. For some of the 
statements weaker conditions may also be sufficient. Note that strict positivity implies that functional 
consmints (for example, a = b) are excluded. 
2In terms of graphical models: The graph G is an I-map ofp. 
Nonlinear Markov Networks for Continuous Variables 523 
be small and the product will be over low dimensional functions. Similar to Bayesian net- 
works where the complexity of describing a joint probability density is greatly reduced by 
decomposing the joint density in a product of ideally low-dimensional conditional densities, 
equation 1 describes the decomposition of a joint probability density function into a product 
of ideally low-dimensional compatibility functions. It should be noted that Bayesian networks 
and Markov networks differ in which specific independencies they can represent (Pearl, 1988). 
3 Learning the Markov Network 
3.1 Likelihood Function Based Learning 
Learning graphical stochastical models is usually decomposed into the problems of learning 
structure (that is the edges in the graph) and of learning the parameters of the joint density 
function under the constraint that it obeys the independence statements made by the graph. 
The idea is to generate candidate structures according to some search strategy, learn the param- 
eters for this structure and then judge the structure on the basis of the (penalized) likelihood 
of the model or, in a fully Bayesian approach, using a Bayesian scoring metric. 
Assume that the compatibility functions in equation 1 are approximated using a function ap- 
proximator such as a neural network gi()  g�(x). Let {xp}= be a training set. With 
= pM (xP) (where the M in pM indicates a probability density model in 
likelihood L 1-IpV__  
contrast to the true distribution), the gradient of the log-likelihood with respect to weight wi 
in 9i (') becomes 
0   0  xp  
Owi 1�gpM (xp) :   1oggi ( ctique -- f j g (Xctiqu% )dx 
p:l p=1 
(2) 
where the sins e over N gaining paRems. e gradient decomposes into o tes. Note, 
that only in the flint te the traing paRems appe explicitly d that, conveniently, the st 
te is oy dependent on the clique i wch contains pmeter wi. e second tern emerges 
from the noalimtion constt K in uation 1. e difficul W is that the inteals in the 
second te  not be solved in closed fo for iversal es of compatibili W nctions gi 
d have to be approxated numecally, ically usg a fo of Monte Clo integration. 
is is exactly what is done in the Boltm machine, which is a special ce of a Mkov 
neork with discrete vmables. 3 
Cently, we consider mimm likelihood leg bed on the compatibiliW nctions un- 
suitable, considering the complexi W d slomess of Monte Clo integration (i.e. stochtic 
stapling). Note, that for structural leing the mimm likelihood leming is in the ier 
loop d would have to be exuted repeatedly for a lge nmber of structures. 
3.2 Markov Boundary Learning 
The difficulties in using maximum likelihood learning for finding optimal structures motivated 
the approach pursued in this paper. If the underlying true probability density is known the 
structure in a Markov network can be found using either the edge deletion method or the 
aA fully connected Boltzmann machine does not display any independencies and we only have one 
clique consisting of all variables. The compatibility function is g() = exp (- y wi s i s j). The Boltz- 
mann machine typically contains hidden variables, such that not only the second term (corresponding to 
the unclamped phase) in equation 2 has to be approximated using stochastic sampling but also the first 
term. (In this paper we only consider the case that data are complete). 
524 
R. Hofmann and V. Tresp 
Markov
