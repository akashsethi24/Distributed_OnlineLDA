Computing with Arrays of Bell-Shaped and 
Sigmoid Functions 
Pierre Baldi* 
Jet Propulsion Laboratory 
California Institute of Technology 
Pasadena, CA 91109 
Abstract 
We consider feed-forward neural networks with one non-linear hidden layer 
and linear output units. The transfer function in the hidden layer are ei- 
ther bell-shaped or sigmoid. In the bell-shaped case, we show how Bern- 
stein polynomials on one hand and the theory of the heat equation on the 
other are relevant for understanding the properties of the corresponding 
networks. In particular, these techniques yield simple proofs of universal 
approxhnation properties, i.e. of the fact that any reasonable function can 
be approximated to any degree of precision by a linear combination of bell- 
shaped functions. In addition, in this framework the problem of learning 
is equivalent to the problem of reversing the time course of a diffusion pro- 
cess. The results obtained in the bell-shaped case can then be applied to 
the case of sigmoid transfer functions in the hidden layer, yielding similar 
universality results. A conjecture related to the problem of generalization 
is briefly examined. 
I INTRODUCTION 
Bell-shaped response curves are commonly found in biological neurons whenever a 
natural metric exist on the corresponding relevant stimulus variable (orientation, 
position in space, frequency, time delay, ...). As a result, they are often used in 
neural models in different context ranging from resolution enhancement and inter- 
polation to learning (see, for instance, Baldi et al. (1988), Moody et al. (1989) 
*and Division of Biology, California Institute of Technology. The complete title of 
this paper should read: Computing with arrays of bell-shaped and sigmoid functions. 
Bernstein polynmnials, the heat equation and universal approximation properties. 
735 
736 Baldi 
and Poggio et al. (1990)). Consider then the problem of approximating a function 
y = f(x) by a weighted sum of bell-shaped functions B(k, x), i.e. of finding a 
suitably good set of weights H(k) satisfying 
f(x) ,  H(k)B(k,x). (1) 
In neural network terminology, this corresponds to using a feed-forward network 
with a unique hidden layer of bell-shaped units and a linear ouput layer. In this note, 
we first briefly point out how this question is related to two different mathematical 
concepts: Bernstein Polynomials on one hand and the Heat Equation on the other. 
The former shows how such an approximation is always possible for any reasonable 
function whereas through the latter the problem of learning, that is of finding H(k), 
is equivalent to reversing the time course of a diffusion process. For simplicity, the 
relevant ideas are presented in one dimension. However, the extension to the general 
setting is straightforward and will be sketched in each case. We then indicate how 
these ideas can be applied to similar neural networks with sigmoid transfer functions 
in the hidden layer. A conjecture related to the problem of generalization is briefly 
examined. 
2 BERNSTEIN POLYNOMIALS 
In this section, without any loss of generality, we assume that all the functions to 
be considered are defined over the interval [0,1]. For a fixed integer n, there are n 
Bernstein polynomials of degree n (see, for instance, Feller (1971)) given by 
Bn(k,x) = (2)x}(1- x) n-}. (2) 
B,(k, x) can be interpreted as being the probability of having k successes in a coin 
flipping experiment of duration n, where x represents the probability of a single 
success. It is easy to see that B,(k,x) is bell-shaped and reaches its maximum 
for x = kin. Can we then approxhnate a function f using linear combinations 
of Bernstein polynomials of degree n? Let us first consider, as an example, the 
simple case of the identity function f(x) = x (x e [0, 1]). If we interpret x as the 
probability of success on a single coin toss, then the expected number of successes 
in n trials is given by 
k=0 
or equivalently 
f(x)=-.f() x(1 - x> - . (4> 
k=0 
The remarkable theorem of Bernstein is that (4) remains approximately true for a 
general function f. More precisely: 
Theorem: Assume f is a bounded function defined over the interval [0, 1]. Then 
k=0 
Computing with Arrays of Bell-Shaped and Sigmoid Functions 737 
at any point x where f is continuous. Moreover, if f is continuous everywhere, 
sequence in (5) approaches f uniformly. 
Proof: The proof is beautiful and elementary. It is easy to see that 
for any 0 _< 5 _< 1. To bound the first term in the right hand side of this inequality, 
we use the fact that for fixed e and for n large enough, at a point of continuity x, 
we can find a 5 such that If(x)- f()l < e as soon as Ix- 1 < 5. Thus the first 
term is bounded by e. If f is continuous everywhere, then it is uniformly continuous 
and 5 can be found independently of x. For the second term, since f is bounded 
(If(x)l _< M), we have If(x)- f()l -< 2M. Now we use Tchebycheff inequality 
(P(IX - E(X)I >_ a) <_ (VarX)/a 2) to bound the tail of the binomial series 
(;) nx(1- x) 1 
I x( 1-)-1-< <_ 4n5' 
Collecting terms, we finally get 
,f(x) - E f(k) 
k--0 
M 
2n6e ï¿½ 
which completes the proof. 
Bernsteins's theorem provides a probabilistic constructive proof of Weierstrass the- 
orem which asserts that every continuous function over a compact set can be uni- 
formly approximated by a sequence of polynomials. Its connectionist interpre- 
tation is that every reasonable function can be computed by a two layer network 
consisting of one array of equally spaced bell-shaped detectors feeding into one lin- 
ear output unit. In addition, the weighting function H(k) is the function f itself 
(see also Baldi et al. (1988)). Notice that the shape of the functions Bn(k,x) in the 
array depends on k: in the center (k  n/2) they are very symmetric and similar to 
gaussians, as one moves towards the periphery the shape becomes less symmetric. 
Two additional significant properties of Bernstein polynomials are that, for fixed 
n, they form a partition of unity:  Bn(k,x)= (x + (1- x)) n = 1 and that they 
have constant energy fo 1Bn(k,x) = 1/(n + 1). One important advantage of the 
approximation defined by (5) is its great smoothness. If f is differentiable, then not 
only (5) holds but also 
lie zz(yf() z*(1- z)-*) - (6) 
k=0 
738 Baldi 
uniformly on [0, 1] and the same is true for higher order derivatives (see, for instance, 
Davis (1963)). Thus the Bernstein polynomials provide simultaneous approxima- 
tion of a function and of its derivatives. In particular, they preserve the convexity 
properties of the function f being approximated and mimic extremely well its qual- 
itative behavior. The price to be paid is in precision, for the convergence in (5) 
can sometimes be slow. Good qualitative properties of the approximation may 
be relevant for biological systems, whereas precision there may not be a problem, 
especially in light of the fact that n is often large. 
Finally, this approach can be extended to the general case of an input space with d 
dimensions by defining the generalized Bernstein polynomials 
Bn,...,na (kl, ..., kd, Xl,..., Xd) = kl kd 
(7) 
If f(xl, ...,xa) is a continuous function over the hypercube [0, 1] a, then 
n n kl kd (kl, kd, Xl, Xd) (8) 
... ..., ..., 
21 d 
k=0 ka=O 
approaches uniformly f on [0, 1] a as min ni -' c. 
3 LEARNING AND THE HEAT EQUATION 
Consider again the general problem of approximating a function f by a linear com- 
bination of bell-shaped functions, but where now the bell-shaped functions are gaus- 
sians B(w, x), of the form 
B(w, x) - _1 
' (9) 
The fixed centers w of the gaussians are distributed in space according to a density 
It(w) (this enables one to treat the continuous and discrete case together and also 
to include the case where the centers are not evenly distributed). This idea was 
directly suggested by a presentation of R. Durbin (1990), where the limiting case of 
an infinite number of logistic hidden units in a connectionist network was considered. 
In this setting, we are trying to express f as 
f (x) m f_+oo h(w) ae-('-'? /2'It(w)dw 
(10) 
or 
/+oo i dw (11) 
f ( ) ,, 
where H = hIt. Now, diffusion processes or propagation of heat are usually modeled 
by a partial differential equation of the type 
- (12) 
Ot Ox  
Computing with Arrays of Bell-Shaped and Sigmoid Functions 739 
(the heat equation) where u(x,t) represents the temperature (or the concentration) 
at position x at time t. Given a set of initial conditions of the form u(x, O) - g(x), 
then the distribution of temperatures at time t is given by 
u(x,t) = g(w)--l e-('-w)/4tdw 
(13) 
Technically, (13) can be shown to give the correct distribution of temperatures at 
time t provided g is continuous, Ig(x)l = O(exp(hx2)) and 0 < t < 1/4h. Under 
these conditions, it can be seen that u(x,t) = O(exp(kx2)) for some constant k > 0 
(depending on h) and is the unique solution satisfying this property (see Friedman 
(1964) and John (1975) for more details). 
The connection to our problem now becomes obvious. If the initial set of temper- 
atures is equal to the weights in the network (H(w) = g(w)), then the function 
computed by the network is equal to the temperature at x at time t = rre/2. Given 
a function f(x) we can view it as a description of temperature values at time 
the problem of learning, i.e. of determining the optimal h(w) (or H(w)) consists in 
finding a distribution of initial temperatures at time t = 0 from which f could have 
evolved. In this sense, learning is equivalent to reversing time in a diffusion process. 
If the continuous case is viewed as a limiting case where units with bell-shaped 
tuning curves are very densely packed, then it is reasonable to consider that, as 
the density is increased, the width r of the curves tends to O. As r - O, the final 
distribution of temperatures approaches the initial one and this is another heuris
