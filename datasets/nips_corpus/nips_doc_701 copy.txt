Analyzing Cross Connected Networks 
Thomas R. Shultz 
Department of Psychology & 
McGill Cognitive Science Centre 
McGill University 
Montr6al, Qu6bec, Canada H3A lB 1 
sh ultz@ psych. mcgill .ca 
and 
Jeffrey L. Elman 
Center for Research on Language 
Department of Cognitive Science 
University of California at San Diego 
LaJolla, CA 92093-0126 U.S.A. 
elman@crl.ucsd.edu 
Abstract 
The non-linear complexities of neural networks make network solutions 
difficult to understand. Sanger's contribution analysis is here extended to 
the analysis of networks automatically generated by the cascade- 
correlation learning algorithm. Because such networks have cross 
connections that supersede hidden layers, standard analyses of hidden 
unit activation patterns are insufficient. A contribution is defined as the 
product of an output weight and the associated activation on the sending 
unit, whether that sending unit is an input or a hidden unit, multiplied 
by the sign of the output target for the current input pattern. 
Intercorrelations among contributions, as gleaned from the matrix of 
contributions x input patterns, can be subjected to principal 
components analysis (PCA) to extract the main features of variation in 
the contributions. Such an analysis is applied to three problems, 
continuous XOR, arithmetic comparison, and distinguishing between 
two interlocking spirals. In all three cases, this technique yields useful 
insights into network solutions that are consistent across several 
networks. 
1 INTRODUCTION 
Although neural network researchers are typically impressed with the performance 
achieved by their learning networks, it often remains a challenge to explain or even 
characterize such performance. The latter difficulties stem principally from the complex 
non-linear properties of neural nets and from the fact that information is encoded in a form 
that is distributed across many weights and units. The problem is exacerbated by the fact 
that multiple nets generate unique solutions depending on variation in both starting states 
and training patterns. 
Two techniques for network analysis have been applied with some degree of success, 
focusing respectively on either a network's weights or its hidden unit activations. Hinton 
(e.g., Hinton & Sejnowski, 1986) pioneered a diagrammatic analysis that involves 
plotting a network's learned weights. Occasionally, such diagrams yield interesting 
insights but often, because of the highly distributed nature of network representations, the 
most notable features of such analyses are the complexity of the pattern of weights and its 
variability across multiple networks learning the same problem. 
1117 
1118 Shultz and Elman 
Statistical analysis of the activation patterns on the hidden units of three layered feed- 
forward nets has also proven somewhat effective in understanding network performance. 
The relations among hidden unit activations, computed from a matrix of hidden units x 
input patterns, can be subjected to either cluster analysis (Elman, 1990) or PCA (Elman, 
1989) to determine the way in which the hidden layer represents the various inputs. 
However, it is not clear how this technique should be extended to multi-layer networks or 
to networks with cross connections. 
Cross connections are direct connections that bypass intervening hidden layers. Cross 
connections typically speed up learning when used in static back-propagation networks 
(Lang & Witbrock, 1988) and are an obligatory and ubiquitous feature of some generative 
learning algorithms, such as cascade-correlation (Fahlman & Lebiere, 1990). Generative 
algorithms construct their own network topologies as they learn. In cascade-correlation, 
this is accomplished by recruiting new hidden units into the network, as needed, installing 
each on a separate layer. In addition to layer-to-layer connections, each unit in a cascade- 
correlation network is fully cross connected to all non-adjacent layers downstream. 
Because such cross connections carry so much of the work load, any analysis restricted to 
hidden unit activations provides a partial picture of the network solution at best. 
Generafive networks seem to provide a number of advantages over static networks, 
including more principled network design, leaner networks, faster learning, and more 
realistic simulations of human cognitive development (Fahlman & Lebiere, 1990; Shultz, 
Schmidt, Buckingham, & Mareschal, in press). Thus, it is important to understand how 
these networks function, even if they seem impervious to standard analytical tools. 
2 CONTRIBUTION ANALYSIS 
One analytical technique that might be adapted for multi-layer, cross connected nets is 
contribution analysis (Sanger, 1989). Sanger defined a contribution as the triple product 
of an output weight, the activation of a sending unit, and the sign of the output target for 
that input. He argued that contributions are potentially more informative than either 
weights alone or hidden unit activations alone. A large weight may not contribute much 
if it is connected to a sending unit with a small activation. Likewise, a large sending 
activation may not contribute much if it is connected via a small weight. In contrast, 
considering a full contribution, using both weight and sending activation, would more 
likely yield valid comparisons. 
Sanger (1989) applied contribution analysis to a small version of NETtalk, a net that 
learns to convert written English into spoken English (Sejnowski & Rosenberg, 1987). 
Sanger's analysis began with the construction of an output unit x hidden unit x input 
pattern array of contributions. Various two-dimensional slices were taken from this three- 
dimensional array, each representing a particular output unit or a particular hidden unit. 
Each two-dimensional slice was then subjected to PCA, yielding information about either 
distributed or local hidden unit responsibilities, depending on whether the focus was on an 
individual output unit or individual hidden unit, respectively. 
3 CONTRIBUTION ANALYSIS FOR MULTI-LAYER, 
CROSS CONNECTED NETS 
We adapted contribution analysis for use with multi-layered, cross connected cascade- 
correlation nets. Assume a cascade-correlation network with j units (input units + hidden 
units) and k output units, being trained with i input patterns. There are j x k output 
weights in such a network, where an output weight is defined as any weight connected to 
Analyzing Cross-Connected Networks 1119 
an output unit. A contribution c for a particular ijk combination is defined as 
Cijk = Wjk aij 2tki (1) 
where Wjk is the weight connecting sending unit j with output unit k, aij is the activation 
of sending unit j given input pattern i, and tki is the target for output unit k given input 
pattern i. The term 2tki adjusts the sign of the contribution so that it provides a measure 
of correctness. That is, positive contributions push the output activation towards the 
target, whereas negative contributions push the output activation away from the target. In 
cascade-correlation, sigmoid output units have targets of either -0.5 or +0.5. Hence, 
multiplying a target by 2 yields a positive sign for positive targets and a negative sign for 
negative targets. Our term 2tki is analogous to Sanger's (1989) term 2tik - 1, which is 
appropriate for targets of 0 and 1, commonly used in back-propagation learning. 
In contrast to Sanger's (1989) three-dimensional army of contributions (output unit x 
hidden unit x input pattern), we begin with a two-dimensional output weight (k *j) x 
input pattern (i) array of contributions. This is because we want to include all of the 
contributions coming into the output units, including the cross connections from more 
than one layer away. Since we begin with a two-dimensional array, we do not need to 
employ the somewhat cumbersome slicing technique used by Sanger to isolate particular 
output or hidden units. Nonetheless, as will be seen, our technique does allow the 
identification of the roles of specific contributions. 
4 PRINCIPAL COMPONENTS ANALYSIS 
Correlations among the various contributions across input patterns are subjected to PCA. 
PCA is a statistical technique that identifies significant dimensions of variation in a 
multi-dimensional space (Flury, 1988). A component is a line of closest fit to a set of 
points in multi-dimensional space. The goal of PCA is to summarize a multivariate data 
set using as few components as possible. It does this by taking advantage of possible 
correlations among the variables (contributions, in our case). 
We apply PCA to contributions, as defined in Equation 1, taken from networks learning 
three different problems: continuous XOR, arithmetic comparisons, and distinguishing 
between interlocking spirals. The contribution matrix for each net, as described in section 
3, is subjected to PCA using 1.0 as the minimum eigenvalue for retention. Varimax 
rotation is applied to improve the interpretability of the solution. Then the scree test is 
applied to eliminate components that fail to account for much of the variance (Cattell, 
1966). In cases where components are eliminated, the analysis is repeated with the correct 
number of components, again with a varimax rotation. Component scores for the retained 
components are plotted to provide an indication of the function of the components. 
Finally, component loadings for the various contributions are examined to determine the 
roles of the contributions from hidden units that had been recruited into the networks. 
5 APPLICATION TO THE CONTINUOUS XOR PROBLEM 
The simplicity of binary XOR and the small number of training patterns (four) renders 
application of contribution analysis superfluous. However, it is possible to construct a 
continuous version of the XOR problem that is more suitable for contribution analysis. 
We do this by dividing the input space into four quadrants. Input valu
