Hierarchical Transformation of Space in the 
Visual System 
Alexandre Pouget 
Stephen A. Fisher 
Terrence J. Sejnowski 
Computational Neurobiology Laboratory 
The SMk Institute 
La Jolla, CA 92037 
Abstract 
Neurons encoding simple visual features in area V1 such as orientation, 
direction of motion and color are organized in retinotopic maps. How- 
ever, recent physiological experiments have shown that the responses of 
many neurons in V1 and other cortical areas are modulated by the di- 
rection of gaze. We have developed a neural network model of the visual 
cortex to explore the hypothesis that visual features are encoded in head- 
centered coordinates at early stages of visual processing. New experiments 
are suggested for testing this hypothesis using electrical stimulations and 
psychophysical observations. 
I Introduction 
Early visual processing in cortical areas V1, V2 and MT appear to encode visual 
features in eye-centered coordinates. This is based primarily on anatomical data 
and recordings from neurons in these areas, which are arranged in retinotopic maps. 
In addition, when neurons in the visual cortex are electrically stimulated [9], the 
direction of the evoked eye movement depends only on the retinotopic position of 
the stimulation site, as shown in figure 1. Thus, when a position corresponding 
to the left part of the visual field is stimulated, the eyes move toward the left (left 
figure), and eye movements in the opposite direction are induced if neurons on the 
right side are stimulated (right figure). 
412 
Hierarchical Transformation of Space in the Visual System 413 
D D 
Initial Eye Position End Point of the Sccde 
Figure 1: Eye Movements Evoked by Electrical Stimulations in V1 
A variety of psychophysical experiments provide further evidence that simple vi- 
sual features are organized according to retinal coordinates rather than spatiotopic 
coordinates [10, 5]. 
At later stages of visual processing the receptive fields of neurons become very large 
and in the posterior parietal cortex, containing areas believed to be important for 
sensory-motor coordination (LIP, VIP and 7a), the visual responses of neurons are 
modulated by both eye and head position [1, 2]. A previous model of the parietal 
cortex showed that the gain fields of the neurons observed there are consistent with 
a distributed spatial transformation from retinal to head-centered coordinates [14]. 
Recently, several investigators have found that static eye position also modulates 
the visual response of many neurons at early stages of visual processing, including 
the LGN, V1 and V3a [3, 6, 13, 12]. Furthermore, the modulation appears to be 
qualitatively similar to that previously reported in the parietal cortex and could 
contribute to those responses. These new findings suggest that coordinate transfor- 
mations from retinal to spatial representations could be initiated much earlier than 
previously thought. 
We have used network optimization techniques to study the spatial transformations 
in a feedforward hierarchy of cortical maps. The goals of the model were 1) to 
determine whether the modulation of neural responses with eye position as observed 
in V1 or V3a is sufficient to provide a head-centered coordinate frame, 2) to help 
interpret data based on the electrical stimulation of early visual areas, and 3) to 
provide a framework for designing experiments and testing predictions. 
2 Methods 
2.1 Network Task 
The task of the network was to compute the head-centered coordinates of objects. 
If/ is the eye position vector and . is the vector for the retinal position of the 
414 
Pouget, Fisher, and Sejnowski 
Hed -Centm'ed Position 
H 
o o 
I/ 
Otpu t 
Hidden Layer 3 
Hidden !ayr 2 
Hidden Lny I 
H 
Eye Posion 
Retina 
Figure 2: Network Architecture 
object, then the head-centered position/ is given by: 
/ =/+/ (1) 
A two layer network with linear units can solve this problem. However, the goal of 
our study was not to find the optimal architecture for this task, but to explore the 
types of intermediate representation developed in a multilayer network of non-linear 
units and to compare these results with physiological recordings. 
2.2 Network Architecture 
We trained a partially-connected multilayer network to compute the head-centered 
position of objects from retinal and eye position signals available at the input layer. 
Weights were shared within each hidden layer [7] and adjusted with the backprop- 
agation algorithm [11]. All simulations were performed with the SN2 simulator 
developed by Botou and LeCun. 
In the hierarchical architecture illustrated in figure 2, the sizes of the receptive 
fields were restricted in each layer and several hidden units were dedicated to each 
location, typically 3 to 5 units, depending on the layer. Although weights were 
shared between locations within a layer, each type of hidden unit was allowed to 
develop its own receptive field properties. This architecture preserves two essential 
aspects of the visual cortex: 1) restricted receptive fields organized in retinotopic 
maps and 2) the sizes of the receptive fields increase with distance from the retina. 
Training examples consisted of an eye position vector and a gaussian pattern of 
activity placed at a particular location on the input layer and these were system- 
Hierarchical Transformation of Space in the Visual System 
(i.e. Or- Ac.. B, ctsroud Ac.) 
415 
Figure 3: Spatial Gain Fields: Comparison Between Hidden Units and Cortical 
Neurons (background activity not shown for V3a neurons) 
atically varied throughout the training. For some trials there were no visual inputs 
and the output layer was trained to reproduce the eye position. 
2.3 Electrical Stimulation Experiments 
Determining the head-centered position of an object is equivalent to computing 
the position of the eye required to foveate the object (i.e. for a foveated object 
. = O, which, according to equation 1, implies that / = /). Thus, the output 
of our network can be interpreted as the eye position for an intended saccadic eye 
movement to acquire the object. 
For the electrical stimulation experiments we followed the protocol suggested by 
Goodman and Andersen [4] in an earlier study of the Zipset-Andersen model of 
parietal cortex [14]. The cortical model was stimulated by clamping the activity of 
a set of hidden units at a location in one of the layers to 1, their maximum values, 
and setting all visual inputs to 0. The changes in the activity of the output units 
were computed and interpreted as an intended saccade. 
3 Results 
We trained several networks with various numbers of hidden units per layer and 
found that they all converged to a nearly perfect solution in a few thousand sweeps 
through the training set. 
3.1 Comparison Between Hidden Units and Cortical Neurons 
The influence of eye position on the visual response of a cortical neuron is typically 
assessed by finding the visual stimulus eliciting the best response and measuring the 
gain of this response at nine different eye fixations [1]. The responses are plotted as 
circles with diameters proportional to activity and the set of nine circles is called 
the spatial gain field of a unit. We adopted the same procedure for studying the 
hidden units in the model. 
416 Pouget, Fsher, and Sejnowski 
Figure 4: Eye Movements Evoked by Stimulating the Retina 
The units in a fully-developed network have properties that are similar to those 
observed in cortical neurons (figure 3). Despite having restricted receptive fields, 
the overall activity of most units increased monotonically in one direction of eye 
position, each unit having a different preferred direction in head-centered space. 
Also, the inner and outer circles, corresponding to the visual activity and the overall 
activity (visual plus background) did not always increase along the same direction 
due to the nonlinear sigmoid squashing function of the unit. 
3.2 Significance of the Spatial Gains Fields 
Each hidden layer of the network has a retinotopic map but also contains spatiotopic 
(i.e. head-centered) information through the spatial gain fields. We call these 
retinospatiotopic maps. 
At each location on a map, / is implicitly encoded by the position of a unit on 
the map, and / is provided by the inputs from the eye position units. Thus, 
each location contains all the information needed to recover/3, the head-centered 
coordinate. Therefore, all of the visual features in the map, such as orientation or 
color, are encoded in head-centered coordinates. This suggests that some visual 
representations in V1 and V3a may be retinospatiotopic. 
3.3 Electrical Stimulation Experiments 
Can electrical stimulation experiments distinguish between a purely retinotopic 
map, like the retina, and retinospatiotopic maps, like each of the hidden layers? 
When input units in the retina are stimulated, the direction of the evoked movement 
is determined by the location of the stimulation site on the map (figure 4), as 
expected from a purely retinotopic map. For example, stimulating units in the upper 
Hierarchical Trans�ormation o� Space in the Visual System 417 
Hidden layer 2 
Hidden layer 3 
One Hidden Unit Type Stimulated 
Figure 5: Eye Movements Evoked by Stimulating one Hidden Unit Type 
left corner of the map produces an output in the upper left direction, regardless of 
initial eye position. 
There were several types of hidden units at each spatial position of a hidden layer. 
When the hidden units were stimulated independently, the pattern of induced eye 
movements was no longer a function solely of the location of the stimulation (fig- 
ure 5). Other factors, such as the preferred head-centered direction of the stim- 
ulated cell, were also important. Hence, the intermediate maps were not purely 
retinotopic. 
If all the hidden units present at one location in a hidden layer were activated 
togeth
