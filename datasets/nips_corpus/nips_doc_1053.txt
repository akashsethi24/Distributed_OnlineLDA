Modeling Saccadic Targeting in Visual Search 
Rajesh P. N. Rao 
Computer Science Department 
University of Rochester 
Rochester, NY 14627 
rao@cs. rochester. edu 
Gregory J. Zelinsky 
Center for Visual Science 
University of Rochester 
Rochester, NY 14627 
greg@cvs. rochester. edu 
Mary M. Hayhoe 
Center for Visual Science 
University of Rochester 
Rochester, NY 14627 
mary@cvs. rochester. edu 
Dana H. Ballard 
Computer Science Department 
University of Rochester 
Rochester, NY 14627 
dana@cs. roches ter. edu 
Abstract 
Visual cognition depends critically on the ability to make rapid eye movements 
known as saccades that orient the fovea over targets of interest in a visual 
scene. Saccades are known to be ballistic: the pattern of muscle activation 
for foveafing a prespecified target location is computed prior to the movement 
and visual feedback is precluded. Despite these distinctive properties, there 
has been no general model of the saccadic targeting strategy employed by 
the human visual system during visual search in natural scenes. This paper 
proposes a model for saccadic targeting that uses iconic scene representations 
derived from oriented spatial filters at multiple scales. Visual search proceeds 
in a coarse-to-fine fashion with the largest scale filter responses being compared 
first. The model was empirically tested by comparing its performance with 
actual eye movement data from human subjects in a natural visual search task; 
preliminary results indicate substantial agreement between eye movements 
predicted by the model and those recorded from human subjects. 
1 INTRODUCTION 
Human vision relies extensively on the ability to make saccadic eye movements. These rapid 
eye movements, which are made at the rate of about three per second, orient the high-acuity 
foveal region of the eye over targets of interest in a visual scene. The high velocity of saccades, 
reaching up to 700 � per second for large movements, serves to minimize the time in flight; most 
of the time is spent fixating the chosen targets. 
The objective of saccades is currently best understood for reading text [ 13] where the eyes fixate 
almost every word, sometimes skipping over small function words. In general scenes, however, 
the purpose of saccades is much more difficult to analyze. It was originally suggested that 
Modeling Saccadic Targeting in Visual Search 831 
%+++ 
+qx x 
x 
x x -t- 
x x X 
x 
xx -k' 
x 
Fi'irs t Saccare 
Seco'rid Sa��ade 
Third Saccade 
(a) (b) 
Figure 1: Eye Movements in Visual Search. (a) shows the typical pattern of multiple saccares (shown 
here for two different subjects) elicited during the course of searching for the object composed of the fork 
and knife. The initial fixation point is denoted by '+'. (b) depicts a summary of such movements over many 
experiments as a function of the six possible locations of a target object on the table� 
the movements and their resultant fixations formed a visual-motor memory (or scan-paths) of 
objects [ 11] but subsequent work has suggested that the role of saccades is more tightly coupled 
to the momentary problem solving strategy being employed by the subject. In chess, it has 
been shown that saccades are used to assess the current situation on the board in the course of 
making a decision to move, but the exact information that is being represented is not yet known 
[5]. In a task involving the copying of a model block pattern located on a board, fixations have 
been shown to be used in accessing crucial information for different stages of the copying task 
[2]. In natural language processing, there has been recent evidence that fixations reflect the 
instantaneous parsing of a spoken sentence [ 18]. However, none of the above work addresses the 
important question of what possible computational mechanisms underlie saccadic targeting. 
The complexity of the targeting problem can be illustrated by the saccades employed by subjects 
to solve a natural visual search task. In this task, subjects are given a 1 second preview of a 
single object on a table and then instructed to determine, in the shortest possible amount of time, 
whether the previewed object is among a group of one to five objects on the same table in a 
subsequent view. The typical eye movements elicited are shown in Figure 1 (a). Rather than a 
single movement to the remembered target, several saccades are typical, with each successive 
saccade moving closer to the goal object (Figure 1 (b)). 
The purpose of this paper is to describe a mechanism for programming saccades that can approx- 
imately model the saccadic targeting method used by human subjects. Previous models of human 
visual search have focused on simple search tasks involving elementary features such as hori- 
zontal/vertical bars of possibly different color [ 1, 4, 8] or have relied exclusively on bottom-up 
input-driven saliency criteria for generating scan-paths [10, 19]. The proposed model achieves 
targeting in arbitrary visual scenes by using bottom-up scene representations in conjunction with 
previously memorized top-down object representations; both of these representations are iconic, 
based on oriented spatial filters at multiple scales. 
One of the difficult aspects of modeling saccadic targeting is that saccades are ballistic, i.e., 
their final location is computed prior to making the movement and the movement trajectory is 
uninterrupted by incoming visual signals. Furthermore, owing to the structure of the retina, the 
central 1.5 � of the visual field is represented with a resolution that is almost 100 times greater 
than that of the periphery. We resolve these issues by positing that the targeting computation 
proceeds sequentially with coarse resolution information being used in the computation of target 
coordinates prior to fine resolution information. The method is compared to actual eye movements 
made by human subjects in the visual search task described above; the eye movements predicted 
by the model are shown to be in close agreement with observed human eye movements. 
832 R.P.N. RAO, G. J. ZELINSKY, M. M. HAYHOE, D. H. BALLARD 
G o G 1 G 2 G 3 
Figure 2: Multiscale Natural Basis Functions. The 10 oriented spatial filters used in our model to 
generate iconic scene representations, shown here at three octave-separated scales. These filters resemble 
the receptive field profiles of cells in the primate visual cortex [20] and have been shown to approximate 
the dominant eigenvectors of natural image distributions as obtained from principal component analysis 
[7, ]7]. 
2 ICONIC REPRESENTATIONS 
The current implementation of our model uses a set of non-orthogonal basis functions as given 
by a zeroth order Gaussian Go and nine of its oriented derivatives as follows [6]: 
G�,n = 1,2, 3,0, = 0,... ,mr/(n + 1),m = 1,... ,n (1) 
where n denotes the order of the filter and 0, refers to the preferred orientation of the filter 
(Figure 2). The response of an image patch I centered at (Xo, Yo) to a particular basis filter G? 
can be obtained by convolving the image patch with the filter: 
The iconic representation for the local image patch centered at (Xo, Yo) is formed by combining 
into a high-dimensional vector the responses from the ten basis filters at different scales: 
rs(zo, y/o) = [ri,j,8 (zo, Y/o)] (3) 
where i = 0, 1,2, 3 denotes the order of the filter, j = 1,..., i + 1 denotes the different filters per 
order, and s = s,i,, � �., s,az denotes the different scales as given by the levels of a Gaussian 
image pyramid. 
The use of multiple scales is crucial to the visual search model (see Section 3). In particular, the 
larger the number of scales, the greater the perspicuity of the representation as depicted in Figure 3. 
A multiscale representation also allows inteolation strategies for scale invariance. The high- 
dimensionality of the vectors mattes them remarkably robust to noise due to the orthogonali 
inherent in high-dimensional spaces: given any vector, most of the other vectors in the space 
tend to be relatively uncorrelated with the given vector. The iconic representations can also be 
made invariant to rotations in the image plane (for a fixed scale) without additional convolutions 
by exploiting the property of steerability [6]. Rotations about an image plane axis are handled 
by storing feature vectors from different views. We refer the interested reader to [14] for more 
details regarding the above properties. 
3 THE VISUAL SEARCH MODEL 
Our model for visual search is derived from a model for vision that we previously proposed in 
[14]. This model decomposes visual behaviors into sequences of two visual routines, one for 
identifying the visual image near the fovea (the what routine), and another for locating a stored 
prototype on the retina (the where routine). 
Modeling Saccadic Targeting in Visual Search 833 
70OO 
60OO 
(a) (b) 
Figure 3: The Effect of Scale. The distribution of distances (in terms of correlations) between the response 
vector for a selected model point in the dining table scene and all other points in the scene is shown for 
single scale response vectors (a) and multiple scale vectors (b). Using responses from multiple scales (five 
in this case) results in greater perspicuity and a sharper peak near 0.0; only one point (the model point) 
had a correlation greater than 0.94 in the multiple scale case (b) whereas 936 candidate points fell in this 
category in the single scale case (a). 
The visual search model assumes the existence of three independent processes running concur- 
rently: (a) a targeting process (similar to the where routine of [14]) that computes the next 
location to be fixated; (b) an oculomotor process that accepts target locations and executes a 
saccade to foveate that location (see [ 16] for more details); and (c) a decision process that models 
the cortico-cortical dynamics of the V1  V2  V4  IT pathway related to t
