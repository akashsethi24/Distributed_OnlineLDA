Mean field 
methods for classification with 
Gaussian processes 
Manfred Opper 
Neural Computing Research Group 
Division of Electronic Engineering and Computer Science 
Aston University Birmingham B4 7ET, UK. 
opperm�ast on. ac. uk 
Ole Winther 
Theoretical Physics II, Lund University, SSlvegatan 14 A 
S-223 62 Lund, Sweden 
CONNECT, The Niels Bohr Institute, University of Copenhagen 
Blegdamsvej 17, 2100 Copenhagen 0, Denmark 
winther�thep. lu. se 
Abstract 
We discuss the application of TAP mean field methods known from 
the Statistical Mechanics of disordered systems to Bayesian classifi- 
cation models with Gaussian processes. In contrast to previous ap- 
proaches, no knowledge about the distribution of inputs is needed. 
Simulation results for the Sonar data set are given. 
I Modeling with Gaussian Processes 
Bayesian models which are based on Gaussian prior distributions on function spaces 
are promising non-parametric statistical tools. They have been recently introduced 
into the Neural Computation community (Neal 1996, Williams &; Rasmussen 1996, 
Mackay 1997). To give their basic definition, we assume that the likelihood of the 
output or target variable - for a given input s  R N can be written in the form 
p(rlh(s)) where h: R N -+ R is a priori assumed to be a Gaussian random field. 
If we assume fields with zero prior mean, the statistics of h is entirely defined by 
the second order correlations C'(s, s ) -' E[h(s)h(s)], where E denotes expectations 
310 M. Opper and O. t4qnther 
with respect to the prior. Interesting examples are 
C(s, st) = 2 arcsin ( EiWi$isti 
' V/( 1 + Y'4 wis,si)(1 + Y'-i 
= exp - 
i 
(1) 
(2) 
The choice (1) can be motivated as a limit of a two-layered neural network with 
infinitely many hidden units with factorizable input-hidden weight priors (Williams 
1997). wi are hyperparameters determining the relevant prior lengthscales of h(s). 
The simplest choice C(s, s ) - Y'.i wisisi corresponds to a single layer perceptron 
with independent Gaussian weight priors. 
In this Bayesian framework, one can make predictions on a novel input s after 
having received a set Dm of m training examples (-u, sU), / = 1,..., m by using 
the posterior distribution of the field at the test point s which is given by 
(3) 
p(h(s)l{h'}) is a conditional Gaussian distribution and 
) - 
(4) 
is the posterior distribution of the field variables at the training points. Z is a 
normalizing partition function and 
p({ht,)) 1 -� y'., h- (C-),h  
= 
V/(2r)m der C 
is the prior distribution of the fields at the training points. Here, we have introduced 
the abbreviations h  = h(s ) and C,,--' C(s,s. 
The major technical problem of this approach comes from the difficulty in per- 
forming the high dimensional integrations. Non-Gaussian likelihoods can be only 
treated by approximations, where e.g. Monte Carlo sampling (Neal 1997), Laplace 
integration (Barber 85 Williams 1997) or bounds on the likelihood (Gibbs 85 Mackay 
1997) have been used so far. In this paper, we introduce a further approach, which 
is based on a mean field method known in the Statistical Physics of disordered 
systems (Mzard, Parisi 85 Virasoro 1987). 
We specialize on the case of a binary classification problem, where a binary class 
label r = +1 must be predicted using a training set corrupted by i.i.d label noise. 
The likelihood for this problem is taken as 
- + (1 - 
where n is the probability that the true classification label is corrupted, i.e. flipped 
and the step function, O(x) is defined as O(x) - 1 for x > 0 and 0 otherwise. For 
such a case, we expect that (by the non-smoothness of the model), e.g. Laplace's 
method and the bounds introduced in (Gibbs 85 Mackay 1997) are not directly 
applicable. 
Mean Field Methods for Classification with Gaussian Processes 311 
2 Exact posterior averages 
In order to make a prediction on an input s, ideally the label with maximum poste- 
rior probability should be chosen, i.e. -Bayes = argmaxr p(rIDm), where the predic- 
tive probability is given by p(rlDm) = f dhp(rlh ) p(hlDm). For the binary case the 
Bayes classifier becomes -Bayes __ sign{signh(s)), where we throughout the paper let 
brackets {...} denote posterior averages. Here, we use a somewhat simpler approach 
by using the prediction 
r = sign((h(s))) . 
This would reduce to the ideal prediction, when the posterior distribution of h(s) 
is symmetric around its mean {h(s)}. The goal of our mean field approach will be 
to provide a set of equations for approximately determining {h(s)}. The starting 
point of our analysis is the partition function 
f dx'dh'  , (6) 
where the new auxiliary variables x u (integrated along the imaginary axis) have 
been introduced in order to get rid of C - in (5). 
It is not hard to show from (6) that the posterior averages of the fields at the m 
training inputs and at a new test point s are given by 
(h(s)>: C(s, (7) 
We have thus reduced our problem to the calculation of the microscopic orderpa- 
rameters (x u}. Averages in Statistical Physics can be calculated from derivatives 
of -In Z with respect to small external fields, which are then set to zero. An 
equivalent formulation uses the Legendre transform of -In Z as a function of the 
expectations, which in our case is given by 
1 
G({(xU)' ((xU)2)}) = - In Z(7U' A) + Y' (xu)?u +  Z Au((xU)2) ' (8) 
with 
' 2ri 
The additional averages ((xU) 2) have been introduced, because the dynamical vari- 
ables x u (unlike Ising spins) do not have fixed length. The external fields ?u, A m 
must be eliminated from o6 _ o6 = 0 and the true expectation values of x u and 
OA r -- 0- 
(xU) 2 are those which satisfy OG _ OG 
o(()2) - o(x) = 0. 
(9) 
3 Naive mean field theory 
So far, this description does not give anything new. Usually G cannot be calculated 
exactly for the non-Gaussian likelihood models of interest. Nevertheless, based on 
mean field theory (MFT) it is possible to guess an approximate form for G. 
Although the integrations are over the imaginary axis, these expectations come out 
positive. This is due to the fact that the integration measure is complex as well. 
312 M. Opper and O. Winther 
Mean field methods have found interesting applications in Neural Computing within 
the framework of ensemble learning, where the the exact posterior distribution is 
approximated by a simpler one using product distributions in a variational treat- 
ment. Such a standard mean field method for the posterior of the h u (for the 
case of Gaussian process classification) is in preparation and will be discussed some- 
where else. In this paper, we suggest a different route, which introduces nontrivial 
corrections to a simple or naive MFT for the variables z u. Besides the variational 
method (which would be purely formal because the distribution of the z  is complex 
and does not define a probability), there are other ways to define the simple MFT. 
E.g., by truncating a perturbation expansion with respect to the interactions Cu 
in G after the first order (Plefka 1982). These approaches yield the result 
1 1 
G  Gnaive -- GO --  Z C/tt <(x/')2) 2  
(10) 
Go is the contribution to G for a model without any interactions i.e. when C,,= 0 
in (9), i.e. it is the Legendre transform of 
-In Zo: ' In n + (1 - 2n)(I) 
where (I)(z) = f_. tt e-t2/2 is an error function. For simple models in Statistical 
Physics, where all interactions C,,are positive and equal, it is easy to show that 
C,aive will become exact in the limit of an infinite number of variables x . Hence, 
for systems with a large number of nonzero interactions having the same orders of 
magnitude, one may expect that the approximation is not too bad. 
4 The TAP approach 
Nevertheless, when the interactions Cu can be both positive and negative (as one 
would expect e.g. when inputs have zero mean), even in the thermodynamic limit 
and for nice distributions of inputs, an additional contribution AG must be added 
to the naive mean field theory (10). Such a correction (often called an Onsager 
reaction term) has been introduced for a spin glass model by (Thouless, Anderson 
&; Palmer 1977) (TAP). It was later applied to the statistical mechanics of single 
layer perceptrons by (Mzard 1989) and then generalized to the Bayesian framework 
by (Opper &; Winther 1996, 1997). For an application to multilayer networks, see 
(Wong 1995). In the thermodynamic limit of infinitely large dimension of the input 
space, and for nice input distributions, the results can be shown coincide with the 
results of the replica framework. The drawback of the previous derivations of the 
TAP MFT for neural networks was the fact that special assumptions on the input 
distribution had been made and certain fluctuating terms have been replaced by 
their averages over the distribution of random data, which in practice would not be 
available. In this paper, we will use the approach of (Parisi &; Potters 1995), which 
allows to circumvent this problem. They concluded (applied to the case of a spin 
model with random interactions of a specific type), that the functional form of AG 
should not depend on the type of the single particle contribution Go. Hence, one 
may use any model in Go, for which G can be calculated exactly (e.g. the Gaussian 
regression model) and subtract the naive mean field contribution to obtain the 
Mean FieldMethods for Classification with Gaussian Processes 313 
desired AG. For the sake of simplicity, we have chosen the even simpler model 
p(rUlh ') .. 5(h ') without changing the final result. A lengthy but straightforward 
calculation for this problem leads to the result 
1 1 m 1 
AG: lndet(A +C') +  Z(A, -C'm,)R t, + - +  Zln(-Rt,) 
(11) 
with R, -' ((x) 2) - (x} 2. The A, must be eliminated using oG = 0, which leads 
to the equation 
= (12) 
-[(^ + 
Note, that with this choice, the TAP mean field theory becomes exact for Gaussi
