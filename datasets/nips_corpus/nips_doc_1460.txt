Almost Linear VC Dimension Bounds for 
Piecewise Polynomial Networks 
Peter L. Bartlett 
Department of System Engineering 
Australian National University 
Canberra, ACT 0200 
Australia 
Peter. Bart lett�anu. edu. au 
Vitaly Maiorov 
Department of Mathematics 
Technion, Haifa 32000 
Israel 
Ron Meir 
Department of Electrical Engineering 
Technion, Haifa 32000 
Israel 
rmeir�dumbo. technion. ac. il 
Abstract 
We compute upper and lower bounds on the VC dimension of 
feedforward networks of units with piecewise polynomial activa- 
tion functions. We show that if the number of layers is fixed, then 
the VC dimension grows as W log W, where W is the number of 
parameters in the network. This result stands in opposition to the 
case where the number of layers is unbounded, in which case the 
VC dimension grows as W 2. 
I MOTIVATION 
The VC dimension is an important measure of the complexity of a class of binary- 
valued functions, since it characterizes the amount of data required for learning in 
the PAC setting (see [BEHW89, Vap82]). In this paper, we establish upper and 
lower bounds on the VC dimension of a specific class of multi-layered feedforward 
neural networks. Let jr be the class of binary-valued functions computed by a 
feedforward neural network with W weights and k computational (non-input) units, 
each with a piecewise polynomial activation function. Goldberg and Jerrum [GJ95] 
have shown that VCdim() < Cl(W 2 + Wk) = O(W2), where c is a constant. 
Moreover, Koiran and Sontag [KS97] have demonstrated such a network that has 
VCdim() >_ c2W 2 = (W2), which would lead one to conclude that the bounds 
Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks 191 
are in fact tight up to a constant. However, the proof used in [KS97] to establish 
the lower bound made use of the fact that the number of layers can grow with W. 
In practical applications, this number is often a small constant. Thus, the question 
remains as to whether it is possible to obtain a better bound in the realistic scenario 
where the number of layers is fixed. 
The contribution of this work is the proof of upper and lower bounds on the VC 
dimension of piecewise polynomial nets. The upper bound behaves as O(WL 2 + 
WL log WL), where L is the number of layers. If L is fixed, this is O(W log W), 
which is superior to the previous best result which behaves as O(W2). Moreover, 
using ideas from [KS97] and [GJ95] we are able to derive a lower bound on the VC 
dimension which is f(WL) for L = O(W). Maass [Maa94] shows that three-layer 
networks with threshold activation functions and binary inputs have VC dimension 
f(W log W), and Sakurai [Sak93] shows that this is also true for two-layer networks 
with threshold activation functions and real inputs. It is easy to show that these 
results imply similar lower bounds if the threshold activation function is replaced by 
any piecewise polynomial activation function f that has bounded and distinct limits 
limx__ f(x) and limx_ f(x). We thus conclude that if the number of layers L is 
fixed, the VC dimension of piecewise polynomial networks with L _ 2 layers and real 
inputs, and of piecewise polynomial networks with L _ 3 layers and binary inputs, 
grows as W log W. We note that for the piecewise polynomial networks considered 
in this work, it is easy to show that the VC dimension and pseudo-dimension are 
closely related (see e.g. [Vid96]), so that similar bounds (with different constants) 
hold for the pseudo-dimension. Independently, Sakurai has obtained similar upper 
bounds and improved lower bounds on the VC dimension of piecewise polynomial 
networks (see [Sak99]). 
2 UPPER BOUNDS 
We begin the technical discussion with precise definitions of the VC-dimension and 
the class of networks considered in this work. 
Definition 1 Let X be a set, and .4 a system of subsets of X. A set S = 
{Xl,... , xh} is shattered by .4 if, for every subset B C_ S, there exists a set A E .4 
such that S n A = B. The VC-dimension of.4, denoted by VCdim(.4), is the largest 
integer n such that there exists a set of cardinality n that is shattered by .4. 
Intuitively, the VC dimension measures the size, n, of the largest set of points for 
which all possible 2 n labelings may be achieved by sets A E .4. It is often convenient 
to talk about the VC dimension of classes of indicator functions jc. In this case we 
simply identify the sets of points x  X for which f(x) = I with the subsets of .4, 
and use the notation VCdim(JC). 
A feedforward multi-layer network is a directed acyclic graph that represents a 
parametrized real-valued function of d real inputs. Each node is called either an 
input unit or a computation unit. The computation units are arranged in L layers. 
Edges are allowed from input units to computation units. There can also be an 
edge from a computation unit to another computation unit, but only if the first 
unit is in a lower layer than the second. There is a single unit in the final layer, 
called the output unit. Each input unit has an associated real value, which is one 
of the components of the input vector x  R . Each computation unit has an 
associated real value, called the unit's output value. Each edge has an associated 
real parameter, as does each computation unit. The output of a computation unit 
is given by  (-e WeZe + W0), where the sum ranges over the set of edges leading to 
192 P. L. Bartlett, 14. Maiorov and R. Meir 
the unit, w, is the parameter (weight) associated with edge e, ze is the output value 
of the unit from which edge e emerges, w0 is the parameter (bias) associated with 
the unit, and rr: R  R is called the activation function of the unit. The argument 
of rr is called the net input of the unit. We suppose that in each unit except the 
output unit, the activation function is a fixed piecewise polynomial function of the 
form 
a(u) = qSi(u) for u E [ti-,ti), 
for i = 1,... ,p+ 1 (and set to = -c and tp+l -- c3), where each �i is a polynomial 
of degree no more than 1. We say that rr has p break-points, and degree 1. The 
activation function in the output unit is the identity function. Let ki denote the 
number of computational units in layer i and suppose there is a total of W param- 
eters (weights and biases) and k computational units (k = k + k2 +..-+ k�_ + 1). 
For input x and parameter vector a E A = R TM, let f(x, a) denote the output of 
this network, and let jr = {x  f(x,a): a  R TM} denote the class of functions 
computed by such an architecture, as we vary the W parameters. We first dis- 
cuss the computation of the VC dimension, and thus consider the class of functions 
sgn(J r) = {x  sgn(f(x,a)):a  RW}. 
Before giving the main theorem of this section, we present the following result, 
which is a slight improvement of a result due to Warren (see [ABar], Chapter 8). 
Lemma 2.1 Suppose fl('), f2('),... , f,(') are fixed polynomials of degree at 
most 1 in n <_ m variables. Then the number of distinct sign vectors 
{sgn(f(a)),... ,sgn(fm(a))} that can be generated by varying a  R n is at most 
2(2eml/n) . 
We then have our main result: 
Theorem 2.1 For any positive integers W, k _< W, L <_ W, l, and p, consider a 
network with real inputs, up to W parameters, up to k computational units arranged 
in L layers, a single output unit with the identity activation function, and all other 
computation units with piecewise polynomial activation functions of degree 1 and 
with p break-points. Let .T' be the class of real-valued functions computed by this 
network. Then 
VCdim(sgn(Y')) < 2WLlog(2eWLpk) + 2WL21og(1 + 1) + 2L. 
Since L and k are O(W), for fixed 1 and p this implies that 
VCdim(sgn(Y')) = O(WL log W + WL2). 
Before presenting the proof, we outline the main idea in the construction. For 
any fixed input x, the output of the network f(x, a) corresponds to a piecewise 
polynomial function in the parameters a, of degree no larger than (1 + 1) �- (recall 
that the last layer is linear). Thus, the parameter domain A = R TM can be split 
into regions, in each of which the function f(x, .) is polynomial. From Lemma 2.1, 
it is possible to obtain an upper bound on the number of sign assignments that can 
be attained by varying the parameters of a set of polynomials. The theorem will be 
established by combining this bound with a bound on the number of regions. 
PROOF OF THEOREM 2.1 For an arbitrary choice of m points x,x2,... ,Xm, we 
wish to bound 
K = I{(sgn(f(x,a)),... ,sgn(f(xm,a)))'a G A}I. 
Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks 193 
Fix these m points, and consider a partition {S1, Se,... , SN) of the parameter 
domain A. Clearly 
N 
K _ Z I{(sgn(f(xi'a))'' ,sgn(f(xm,a)))'a E $i}[. 
i=1 
We choose the partition so that within each region $i, f(xi,-),... , f(Zm, ') are all 
fixed polynomials of degree no more than (1 + 1) L-1. Then, by Lemma 2.1, each 
term in the sum above is no more than 
w 
The only remaining point is to construct the partition and determine an upper 
bound on its size. The partition is constructed recursively, using the following 
procedure. Let $ be a partition of A such that, for all $ E $, there are constants 
bn,i,j  {0, 1} for which 
sgn(pn,x (a) - ti) - bn,i,j for all a  $, 
wherej  {1,...,m), h  {1,...,k) andi E {1,...,p). Here ti are the break- 
points of the piecewise polynomial activation functions, and pn,x is the affine func- 
tion describing the net input to the h-th unit in the first layer, in response to xj. 
That is, 
Ph,x ---- ah � Xj d- ah,O, 
where an  R , an,0  R are the weights of the h-th unit in the first layer. Note 
that the partition $ is determined solely by the parameters corresponding to the 
first hidden layer, as the input to this layer is unaffected by the other parameters. 
Clearly, for a  $, the output of any first layer unit in response to an xj is a fixed 
polynomial in a. 
Now, let W1,... , W� be the num
