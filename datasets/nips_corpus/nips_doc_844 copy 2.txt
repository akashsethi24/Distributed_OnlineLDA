Learning from queries for maximum 
information gain in imperfectly learnable 
problems 
Peter SolItch David Saad 
Department of Physics, University of Edinburgh 
Edinburgh EH9 3JZ, U.K. 
P.Solliched.ac.uk, D.Saaded.ac.uk 
Abstract 
In supervised learning, learning from queries rather than from 
random examples can improve generalization performance signif- 
icantly. We study the performance of query learning for problems 
where the student cannot learn the teacher perfectly, which occur 
frequently in practice. As a prototypical scenario of this kind, we 
consider a linear perceptron student learning a binary perceptron 
teacher. Two kinds of queries for maximum information gain, i.e., 
minimum entropy, are investigated: Minimum student space en- 
tropy (MSSE) queries, which are appropriate if the teacher space 
is unknown, and minimum teacher space entropy (MTSE) queries, 
which can be used if the teacher space is assumed to be known, but 
a student of a simpler form has deliberately been chosen. We find 
that for MSSE queries, the structure of the student space deter- 
mines the efficacy of query learning, whereas MTSE queries lead 
to a higher generalization error than random examples, due to a 
lack of feedback about the progress of the student in the way queries 
are selected. 
I INTRODUCTION 
In systems that learn from examples, the traditional approach has been to study 
generalization from random examples, where each example is an input-output pair 
288 Peter Sollich, David Saad 
with the input chosen randomly from some fixed distribution and the corresponding 
output provided by a teacher that one is trying to approximate. However, random 
examples contain less and less new information as learning proceeds. Therefore, 
generalization performance can be improved by learning from queries, i.e., by choos- 
ing the input of each new training example such that it will be, together with its 
expected output, in some sense 'maximally useful'. The most widely used mea- 
sure of 'usefulness' is the information gain, i.e., the decrease in entropy of the 
post-training probability distributions in the parameter space of the student or the 
teacher. We shall call the resulting queries 'minimum (student or teacher space) 
entropy (MSSE/MTSE) queries'; their effect on generalization performance has re- 
cently been investigated for perfectly learnable problems, where student and teacher 
space are identical (Seung el al., 1992, Freund et al., 1993, Sollich, 1994), and was 
found to depend qualitatively on the structure of the teacher. For a linear percep- 
tron, for example, one obtains a relative reduction in generalization error compared 
to learning from random examples which becomes insignificant as the number of 
training examples, p, tends to infinity. For a perceptron with binary output, on the 
other hand, minimum entropy queries result in a generalization error which decays 
exponentially as p increases, a marked improvement over the much slower algebraic 
decay with p in the case of random examples. 
In practical situations, one almost always encounters imperfectly learnable problems, 
where the student can only approximate the teacher, but not learn it perfectly. 
Imperfectly learnable problems can arise for two reasons: Firstly, the teacher space 
(i.e., the space of models generating the data) might be unknown. Because the 
teacher space entropy is then also unknown, MSSE (and not MTSE) queries have 
to be used for query learning. Secondly, the teacher space may be known, but a 
student of a simpler structure might have deliberately been chosen to facilitate or 
speed up training, for example. In this case, MTSE queries could be employed as 
an alternative to MSSE queries. The motivation for doing this would be strongest 
if, as in the learning scenario that we consider below, it is known from analyses 
of perfectly learnable problems that the structure of the teacher space allows more 
significant improvements in generalization performance from query learning than 
the structure of the student space. 
With the above motivation in mind, we investigate in this paper the performance 
of both MSSE and MTSE queries for a prototypical imperfectly learnable prob- 
lem, in which a linear perceptron student is trained on data generated by a binary 
perceptron teacher. Both student and teacher are specified by an N-dimensional 
weight vector with real components, and we will consider the thermodynamic limit 
N  oo, p  cx>, o = pin = const. In Section 2 below we calculate the general- 
ization error for learning from random examples. In Sections 3 and 4 we compare 
the result to MSSE and MTSE queries. Throughout, we only outline the neces- 
sary calculations; for details, we refer the reader to a forthcoming publication. We 
conclude in Section 5 with a summary and brief discussion of our results. 
2 LEARNING FROM RANDOM EXAMPLES 
We denote students and teachers by iV' (for 'Neural network') and l/ (for 'element 
of the Version space', see Section 4), respectively, and their corresponding weight 
Learning from Queries for Maximum Information Gain 289 
vectors by w and Wv. For an input vector x, the outputs of a given student and 
teacher are 
1 xTw, Yv = sgn( N xTwv)- 
Y=  
Assuming that inputs are drawn from a uniform distribution over the hypersphere 
x 2 = N, and taking as our error measure the standard squared output difference 
� (y.- yv) , the generalization error, i.e., the average error between student iV' and 
teacher 12 when tested on random test inputs, is given by 
g(JV',V) = Q+1-2 , (1) 
1 T 1 2 1 2 
where we have set R = WWv, Q = w, Qv = Wv. 
As our training algorithm we take s[och[ic gradient descent on the training 
error Et, which for a training set 0 � = {(x,u  = Uv(X)), = 1...p} is 
1 . 1 
Et =  ( -(x)) 2 A weight decay term Aw is added for regulariza- 
lion, i.e., o prevent overfitting. S[ochtic gradient descent on the resulting en- 
x Aw} yields a Gibbs post4raining distribution of students, 
ergy function E = Et +  
 exp(-E/T), where the training temperature T menures the amount 
of s[ochticity in the training algorithm. For the linear perceptton students con- 
sidered here, this distribution is Gaussian, with covariance matrix TM , where 
(1N denotes the N x N identity matrix) 
M = A1N +  Z[:l XY(XY) T' 
Since the length of the teacher weight vector Wv does not affect the teacher outputs, 
we sume aspherical prior on teacher space, P(wv) m 5(w}-N), for which Qv = 1. 
Restricting attention to the limit of zero training temperature, it is straightforward 
to calculate from eq. (1) the average generalization error obtained by training on 
random examples 
(g-- (g,min : -- optO + (opt - ) , (2) 
with the hnction G = (tr M})m({,}) given by (Krogh and Hertz, 1992) 
i [1-a-A+(1-a-A) 2+4A] (3) 
In eq. (2) we have explicitly subtracted the minimum achievable generalization error, 
1 
(g,min  (1--2/), which is nonzero since a linear perceptron cannot approximate a 
binary perceptron perfectly. At finite a, the generalization error is minimized when 
the weight decay is set to its optimal value A = Aopt  /2 - 1. Note that since 
both G and OG/OA tend to zero  a  , the generalization error for random 
examples approaches the minimum achievable generalization error in this limit. 
3 MINIMUM STUDENT SPACE ENTROPY QUERIES 
We now calculate the generalization performance resulting from MSSE queries. For 
the training algorithm introduced in the last section, the student space entropy 
(normalized by N) is given by 
290 Peter Sollich, David Saad 
3.0 
2.0- 
1.5- 
1.0 I I I I 
0 1 2 3 4 5 
Figure 1: Relative improvement n in generalization error due to MSSE queries, for 
weight decay A = 0.01, 0.1, 1. 
1 
Sv = --- In der Mv, 
2N 
where we have omitted an unimportant constant which depends on the training 
temperature only. This entropy is minimized by choosing each new query along 
the direction corresponding to the minimal eigenvalue of the existing Mv (Sollich, 
1994). The expression for the resulting average generalization error is given by 
eq. (2) with G replaced by its analogue for MSSE queries (Sollich, 1994) 
Ac 1 - Ao 
GQ= A+[c]+l + A+[c]' 
where [] is the greatest integer less than or equal to  and Z: -[]. We define 
the improvement factor  as the ratio of the generalization error (with the minimum 
achievable generalization error subtracted as in eq. (2)) for random examples to that 
for MSSE queries. Figure 1 shows (c) for several values of the weight decay A. 
Comparing with existing results (Sollich, 1994), we find that n is exactly the same 
as if our linear student were trying to approximate a linear teacher with additive 
noise of variance Aopt on the outputs. For large a, one can show (Sollich, 1994) that 
 = 1 + 1/a + O(1/a 2) and hence the relative reduction in generalization error due 
to querying tends to zero as a  . We investigate in the next section whether it 
is possible to improve generalization performance more significantly by using MTSE 
queries. 
4 MINIMUM TEACHER SPACE ENTROPY QUERIES 
We now consider the generalization performance achieved by MTSE queries. We 
remind readers that such queries could be used if the teacher space is known, but 
a student of a simpler functional form has deliberately chosen. The aim in using 
MTSE rather than MSSE queries would be to exploit the structure of the teacher 
space if this is known (for perfectly learnable problems) to make query learning very 
efficient compared to random examples. 
For the case of noise free training data under consideration, the posterior probability 
distribution in teacher space given a certain training set is proportional to the prior 
Learning from Queries for Maximum Information Gain 291 
distribution on the version space (the set of all teachers that could have produced 
the training set without error) and zero everywhere else. 
