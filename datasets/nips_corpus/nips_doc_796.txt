Optimality Criteria for LMS and 
Backpropagation 
Babak Hassibi 
Information Systems Laboratory 
Stanford University 
Stanford, CA 94305 
All H. Sayed 
Dept. of Elec. and Comp. Engr. 
University of California Santa Barbara 
Santa Barbara, CA 93106 
Thomas Kailath 
Information Systems Laboratory 
Stanford University 
Stanford, CA 94305 
Abstract 
We have recently shown that the widely known LMS algorithm is 
an H a optimal estimator. The H a criterion has been introduced, 
initially in the control theory literature, as a means to ensure ro- 
bust performance in the face of model uncertainties and lack of 
statistical information on the exogenous signals. We extend here 
our analysis to the nonlinear setting often encountered in neural 
networks, and show that the backpropagation algorithm is locally 
H a optimal. This fact provides a theoretical justification of the 
widely observed excellent robustness properties of the LMS and 
backpropagation algorithms. We further discuss some implications 
of these results. 
I Introduction 
The LMS algorithm was originally conceived as an approximate recursive procedure 
that solves the following problem (Widrow and Hoff, 1960): given a sequence of n x 1 
input column vectors {hi), and a corresponding sequence of desired scalar responses 
{di), find an estimate of an n x 1 column vector of weights w such that the sum 
of squared errors, -./N=0[di- hiw[ 2, is minimized. The LMS solution recursively 
351 
352 HassiN, Sayed, and Kailath 
updates estimates of the weight vector along the direction of the instantaneous gra- 
dient of the squared error. It has long been known that LMS is an approximate 
minimizing solution to the above least-squares (or H 2) minimization problem. Like- 
wise, the celebrated backpropagation algorithm (Rumelhart and McClelland, 1986) 
is an extension of the gradient-type approach to nonlinear cost functions of the form 
/N=0 ]di- hi(w)l 2, where hi(.) are known nonlinear functions (e.g., sigmoids). It 
also updates the weight vector estimates along the direction of the instantaneous 
gradients. 
We have recently shown (HassiN, Sayed and Kailath, 1993a) that the LMS algo- 
rithm is an H ��-optimal filter, where the H �� norm has recently been introduced 
as a robust criterion for problems in estimation and control (Zames, 1981). In gen- 
eral terms, this means that the LMS algorithm, which has long been regarded as 
an approximate least-mean squares solution, is in fact a minimizer of the H �� error 
norm and not of the H 2 norm. This statement will be made more precise in the 
next few sections. In this paper, we extend our results to a nonlinear setting that 
often arises in the study of neural networks, and show that the backpropagation 
algorithm is a locally H��-optimal filter. These facts readily provide a theoretical 
justification for the widely observed excellent robustness and tracking properties of 
the LMS and backpropagation algorithms, as compared to, for example, exact least 
squares methods such as RLS (Haykin, 1991). 
In this paper we attempt to introduce the main concepts, motivate the results, and 
discuss the various implications. We shall, however, omit the proofs for reasons of 
space. The reader is refered to (Hassibi et al. 1993a), and the expanded version of 
this paper for the necessary details. 
2 Linear H �� Adaptive Filtering 
We shall begin with the definition of the H �� norm of a transfer operator. As 
will presently become apparent, the motivation for introducing the H �� norm is to 
capture the worst case behaviour of a system. 
Let h2 denote the vector space of square-summable complex-valued causal sequences 
{fk, 0 _< k < co}, viz., 
h2 = {set of sequences {fk} such that ff < co} 
k=0 
with inner product < {f),{g} > = y�=ofgk , where � denotes complex 
conjugation. Let T be a transfer operator that maps an input sequence {ui} to an 
output sequence {Yi}. Then the H �� norm of T is equal to 
IIrlloo- sup Ilyl12 
11'112 
the notation I111= anote the n-norm or sequence viz., 
k=Ok k 
The H  norm may thus be regarded as the maximum ener9 gain from the input 
u to the output F. 
OptimaLity Criteria for LMS and Backpropagation 353 
Suppose we observe an output sequence {di} that obeys the following model: 
& = + vi 
where hi T = [ hi hi2 ... hi,, ] is aknown input vector, w is an unknown weight 
vector, and {vi} is an unknown disturbance, which may also include modeling errors. 
We shall not make any assumptions on the noise sequence {vi} (such as whiteness, 
normally distributed, etc.). 
Let wi = .T'(do, di,..., di) denote the estimate of the weight vector w given the 
observations {d j} from time 0 up to and including time i. The objective is to 
determine the functional .T', and consequently the estimate wi, so as to minimize a 
certain norm defined in terms of the prediction error 
=hTw-Tw,- 
which is the difference between the true (uncorrupted) output hTw and the pre- 
dicted output hiZwi_. Let T denote the transfer operator that maps the unknowns 
{w- w_, {vi}} (where w-1 denotes an initial guess of w) to the prediction errors 
{ei}. The H �� estimation problem can now be formulated as follows. 
Problem 1 (Optimal H �� Adaptive Problem) Find an H��-optimal estima- 
tion strategy wi = .T'(do, dl,..., di) that minimizes Ilrlloo, and obtain the resulting 
7o 2 = inf []Tll2 = inf sup Ilel122 
w-l' + Ilvlll (2) 
where Iw- w_112 = (w- w_)'(w - w_), and I u is a positive constant that reflects 
apriori knowledge as to how close w is to the initial guess w_. 
Note that the infimum in (2) is taken over all causal estimators .T'. The above 
problem formulation shows that H �� optimal estimators guarantee the smallest 
prediction error energy over all possible disturbances of fixed energy. H �� estimators 
are thus over conservative, which reflects in a more robust behaviour to disturbance 
variation. 
Before stating our first result we shall define the input vectors {hi} exciting if, and 
only if, 
N 
lim 
N-+oo 
i=0 
Theorem 1 (LMS Algorithm) Consider the model (I), and suppose we wish to 
minimize the H �� norm of the transfer operator from the unknowns w - w_ and 
vi to the prediction errors el. If the input vectors hi are exciting and 
1 
0 <  < inf (3) 
i hiThi 
then the minimum H �� 
given by the LMS algorithm witIs learning rate I u, viz. 
Wi --' Wi--1 '- hi(di -- hiTwi-1) 
norm is '/opt -- 1. In this case an optimal H  estimator is 
, w_ (4) 
354 Hassibi, Sayed, and Kailath 
In other words, the result states that the LMS algorithm is an Ha-optimal filter. 
Moreover, Theorem 1 also gives an upper bound on the learning rate/ that ensures 
the H �� optimality of LMS. This is in accordance with the well-known fact that 
LMS behaves poorly if the learning rate is too large. 
Intuitively it is not hard to convince oneself that 7opt cannot be less than one. To 
this end suppose that the estimator has chosen some initial guess w_l. Then one 
may conceive of a disturbance that yields an observation that coincides with the 
output expected from w_l, i.e. 
hiT w-1 = hiT w q- vi -- di 
In this case one expects that the estimator will not change its estimate of w, so that 
W i -- W_ 1 for all i. Thus the prediction error is 
�i -- hi T w -- hi T wi_ 1 = hi T w - hi T w_ 1 -- -v i 
and the ratio in (2) can be made arbitrarily close to one. 
The surprising fact though is that '/opt is one and that the LMS algorithm achieves 
it. What this means is that LMS guarantees that the energy of the prediction 
error will never exceed the energy of the disturbances. This is not true for other 
estimators. For example, in the case of the recursive least-squares (RLS) algorithm, 
one can come up with a disturbance of arbitrarily small energy that will yield a 
prediction error of large energy. 
To demonstrate this, we consider a special case of model (1) where hi is now a 
scalar that randomly takes on the values +1 or -1. For this model/ must be less 
than 1 and we chose the value tt -- .9. We compute the H *� norm of the transfer 
operator from the disturbances to the prediction errors for both RLS and LMS. We 
also compute the worst case RLS disturbance, and show the resulting prediction 
errors. The results are illustrated in Fig. 1. As can be seen, the H *� norm in 
the RLS case increases with the number of observations, whereas in the LMS case 
it remains constant at one. Using the worst case RLS disturbance, the prediction 
error due to the LMS algorithm goes to zero, whereas the prediction error due to 
the RLS algorithm does not. The form of the worst case RLS disturbance is also 
interesting; it competes with the true output early on, and then goes to zero. 
We should mention that the LMS algorithm is only one of a family of H *� optimal 
estimators. However, LMS corresponds to what is called the central solution, and 
has the additional properties of being the maximum entropy solution and the risk- 
sensitive optimal solution (Whittle 1990, Glover and Mustafa 1989, Hassibi et al. 
199ab). 
If there is no disturbance in (1) we have the following 
Corollary 1 If in addition to the assumptions of Theorem I there is no disturbance 
in (I), then LMS guarantees 11 e 112_< y-llw- w_ll 2, meaning that the prediction 
error converges to zero. 
Note that the above Corollary suggests that the larger y is (provided (3) is satisfied) 
the faster the convergence will be. 
Before closing this section we should mention that if instead of the prediction error 
one were to consider the filtered error eli -- hiw - hiwi, then the H � optimal 
estimator is the so-called normalized LMS algorithm (Hassibi et al. 1993a). 
Optimality Criteria for LMS and Backpropagation 355 
2.5 
2 
1.5 
1 
0.5 
0 
1 
0.98 
0.96 
0.94 
0.92 
0.9 
o 
5O 5O 
0.5 
0 
-0.5 
(c) 
I 
0.5 
(d) 
o 
o.I It , ,., , .r- c-:r, ; 
-1 -1 
0 50 0 50 
Figure 1: H � norm of transfer operator as a function of the number of observations 
for (a) RLS, and 
