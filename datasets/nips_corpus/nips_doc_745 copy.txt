A Learning Analog Neural Network Chip 
with Continuous-Time Recurrent 
Dynamics 
Gert Cauwenberghs* 
California Institute of Technology 
Department of Electrical Engineering 
128-95 Caltech, Pasadena, CA 91125 
E-mail: gertcco. caltech. edu 
Abstract 
We present experimental results on supervised learning of dynam- 
ical features in an analog VLSI neural network chip. The recur- 
rent network, containing six continuous-time analog neurons and 42 
free parameters (connection strengths and thresholds), is trained to 
generate time-varying outputs approximating given periodic signals 
presented to the network. The chip implements a stochastic pertur- 
bative algorithm, which observes the error gradient along random 
directions in the parameter space for error-descent learning. In ad- 
dition to the integrated learning functions and the generation of 
pseudo-random perturbations, the chip provides for teacher forc- 
ing and long-term storage of the volatile parameters. The network 
learns a I kHz circular trajectory in 100 sec. The chip occupies 
2mm x 2mm in a 2pm CMOS process, and dissipates 1.2 mW. 
I Introduction 
Exact gradient-descent algorithms for supervised learning in dynamic recurrent net- 
works [1-3] are fairly complex and do not provide for a scalable implementation in 
a standard 2-D VLSI process. We have implemented a fairly simple and scalable 
*Present address: Johns Hopkins University, ECE Dept., Baltimore MD 21218-2686. 
858 
A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics 859 
learning architecture in an analog VLSI recurrent network, based on a stochastic 
perturbative algorithm which avoids calculation of the gradient based on an explicit 
model of the network, but instead probes the dependence of the network error on 
the parameters directly [4]. As a demonstration of principle, we have trained a 
small network, integrated with the learning circuitry on a CMOS chip, to gener- 
ate outputs following a prescribed periodic trajectory. The chip can be extended, 
with minor modifications to the internal structure of the cells, to accommodate 
applications with larger size recurrent networks. 
2 System Architecture 
The network contains six fully interconnected recurrent neurons with continuous- 
time dynamics, 
d 6 
r = + + , 
j=l 
with xi(t) the neuron states representing the outputs of the network, y(t) the 
external inputs to the network, and a(.) a sigmoidal activation function. The 36 
connection strengths Wi and 6 thresholds Oj constitute the free parameters to be 
learned, and the time constant r is kept fixed and identical for all neurons. Below, 
the parameters W and 0 are denoted as components of a single vector p. 
The network is trained with target output signals x(t) and x(t) for the first two 
neuron outputs. Learning consists of minimizing the time-averaged error 
1�2 
�(p) = li__}rn  T E [x(t) -- xk(t)[Vdt , (2) 
using a distance metric with norm y. The learning algorithm [4] iteratively specifies 
incremental updates in the parameter vector p as 
p(+) = p() - t () *r � (3) 
with the perturbed error 
(k) = _1 (�(p() + r()) _ �(p() _ r()) ) (4) 
2 
obtained from a two-sided parallel activation of fixed-amplitude random perturba- 
tions h(k) onto the parameters pi(t); .i(t) _ -t-a with equal probabilities for both 
polarities. The algorithm basically performs random-direction descent of the error 
as a multi-dimensional extension to the Kiefer-Wolfowitz stochastic approximation 
method [5], and several related variants have recently been proposed for optimiza- 
tion [6,7] and hardware learning [8-10]. 
To facilitate learning, a teacher forcing signal is initially applied to the external 
input y according to 
yi(t) = X ff(xiT(t)-xi(t)) , i= 1,2 (5) 
providing a feedback mechanism that forces the network outputs towards the tar- 
gets [3]. A symmetrical and monotonically increasing squashing function for '7(-) 
serves this purpose. The teacher forcing amplitude A needs to be attenuated along 
the learning process, as to suppress the bias in the network outputs at convergence 
that might result from residual errors. 
860 Cauwenberghs 
3 Analog VLSI Implementation 
The network and learning circuitry are implemented on a single analog CMOS chip, 
which uses a transconductance current-mode approach for continuous-time opera- 
tion. Through dedicated transconductance circuitry, a wide linear dynamic range 
for the voltages is achieved at relatively low levels of power dissipation (experimen- 
tally 1.2 mW while either learning or refreshing). While most learning functions, 
including generation of the pseudo-random perturbations, are integrated on-chip in 
conjunction with the network, some global and higher-level learning functions of 
low dimensionality, such as the evaluation of the error (2) and construction of the 
perturbed error (4), are performed outside the chip for greater flexibility in tailoring 
the learning process. The structure and functionality of the implemented circuitry 
are illustrated in Figures i to 3, and a more detailed description follows below. 
3.1 Network Circuitry 
Figure 1 shows the schematics of the synapse and neuron circuitry. A synapse cell of 
single polarity is shown in Figure I (a). A high output impedance triode multiplier, 
using an adjustable regulated cascode [11], provides a constant current Ii. linear in the 
voltage Wij over a wide range. The synaptic current Iff feeds into a differential pair, 
injecting a differential current Ii.i cr(xj -0.) into the diode-connected Io+ut and I-ut output 
lines. The double-stack transistor configuration of the differential pair offers an expanded 
linear sigmoid range. The summed output currents Io+, and I-, of a row of synapses are 
collected in the output cell, Figure I (b), which also subtracts the reference currents 
and /f obtained from a reference row of dummy synapses defining the zero-point 
synaptic strength Woa for bipolar operation. The thus established current corresponds to 
the summed synaptic contributions in (1). Wherever appropriate (i = 1, 2), a differential 
transconductance element with inputs xi and x is added to supply an external input 
current for forced teacher action in accordance with (5). 
xof 
(a) co) 
Figure 1 Schematics of synapse and neuron circuitry. (a) Synapse of single polarity. 
(b) Output cell with current-to-voltage converter. 
The output current is converted to the neuron output voltage xi, through an active resistive 
element using the same regulated high output impedance triode circuitry as used in the 
synaptic current source. The feedback delay parameter r in (1) corresponds to the RC 
A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics 861 
product of the regulated triode active resistance value and the capacitance Oot. With 
Cout = 5 pF, the delay ranges between 20 and 200/sec, adjustable by the control voltage 
of the regulated cascode. Figure 2 shows the measured static characteristics of the synapse 
and neuron functions for different values of Wij and 0j ( i = j = 1), obtained by disabling 
the neuron feedback and driving the synapse inputs externally. 
II i I I I I 
�'�V 
, -0.4- 
-0.8 - -- - 1.6 V 
I I I I I 
4.o -o.s o.o o.s .o 
Input Voltage xj (v) 
(a) 
0.0 
.0.4 
4).6 
4).8 
! I I I I 
I I I I 
4.o -o.s o.o o.s .o 
Input Voltage x j (v) 
Figure 2 Measured static synapse and neuron characteristics, for various values of 
(a) the connection strength Wil, and (b) the threshold 0. 
3.2 Learning Circuitry 
Figure 3 (a) shows the simplified schematics of the learning and storage circuitry, replicated 
locally for every parameter (connection strength or threshold) in the network. Most of 
the variables relating to the operation of the cells are local, with exception of a few global 
signals communicating to all cells. Global signals include the sign and the amplitude 
of the perturbed error  and predefined control signals. The stored parameter and its 
binary perturbation are strictly local to the cell, in that they do not need to communicate 
explicitly to outside circuitry (except trivially through the neural network it drives), which 
simplifies the structural organization and interconnection of the learning cells. 
The parameter voltage Pi is stored on the capacitor Cstore, which furthermore couples 
to capacitor Cpert for activation of the perturbation. The perturbation bit *ri selects 
either of two complementary signals V+ and V_ with corresponding polarity. With 
the specific shape of the waveforms V+ and V- depicted in Figure 3 (b), the proper 
sequence of perturbation activations is established for observation of the complementary 
error terms in (4). The obtained global value for  is then used, in conjunction with 
the local perturbation bit *ri, to update the parameter value pi according to (3). A fine- 
resolution charge-pump, shown in the dashed-line inset of Figure 3 (a), is used for this 
purpose. The charge pump dumps either of a positive or negative update current, of equal 
amplitude, onto the storage capacitor whenever it is activated by means of an EN_UPD 
high pulse, effecting either of a given increment or decrement on the parameter value pi 
respectively. The update currents are supplied by two complementary transistors, and are 
switched by driving the source voltages of the transistors rather than their gate voltages 
in order to avoid typical clock feed-through effects. The amplitude of the incremental 
update, set proportionally to [[, is controlled by the VUPD n and Vum> p gate voltage 
levels, operated in the sub-threshold region. The polarity of the increment or decrement 
action is determined by the control signal DECR/INCR, obtained from the polarities of 
862 Cauwenberghs 
the perturbed error  and the perturbation bit *ri through an exclusive-or operation. The 
learning cycle is completed by activating the update b
