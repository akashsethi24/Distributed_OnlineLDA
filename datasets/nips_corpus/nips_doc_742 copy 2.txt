Unsupervised Parallel Feature Extraction 
from First Principles 
Mats sterberg 
Image Processing Laboratory 
Dept. EE., LinkSping University 
S-58183 LinkSping Sweden 
Reiner Lenz 
Image Processing Laboratory 
Dept. EE., LinkSping University 
S-58183 LinkSping Sweden 
Abstract 
We describe a number of learning rules that can be used to train 
supervised parallel feature extraction systems. The learning rules 
are derived using gradient ascent of a quality function. We con- 
sider a number of quality functions that are rational functions of 
higher order moments of the extracted feature values. We show 
that one system learns the principle components of the correla- 
tion matrix. Principal component analysis systems are usually not 
optimal feature extractors for classification. Therefore we design 
quality functions which produce feature vectors that support unsu- 
pervised classification. The properties of the different systems are 
compared with the help of different artificially designed datasets 
and a database consisting of all Munsell color spectra. 
I Introduction 
There are a number of unsupervised Hebbian learning algorithms (see Oja, 1992 
and references therein) that perform some version of the Karhunen-Love expan- 
sion. Our approach to unsupervised feature extraction is to identify some desirable 
properties of the extracted feature vectors and to construct a quality functions that 
measures these properties. The filter functions are then learned from the input pat- 
terns by optimizing this selected quality function. In comparison to conventional 
unsupervised Hebbian learning this approach reduces the amount of communication 
between the units needed to learn the weights in parallel since the complexity now 
lies in the learning rule used. 
136 
Unsupervised Parallel Feature Extraction from First Principles 137 
The optimal (orthogonal) solution to two of the proposed quality functions turn out 
to be related to the Karhunen-Lo6ve expansion: the first learns an arbitrary rota- 
tion of the eigenvectors whereas the later learns the pure eigenvectors. A common 
problem with the Karhunen-Lo6ve expansion is the fact that the first eigenvector is 
normally the mean vector of the input patterns. In this case one filter function will 
have a more or less uniform response for a wide range of input patterns which makes 
it rather useless for classification. We will show that one quality function leads to 
a system that tend to learn filter functions which have a large magnitude response 
for just one class of samples (different for each filter function) and low magnitude 
response for samples from all other classes. Thus, it is possible to classify an in- 
coming pattern by simply observing which filter function has the largest magnitude 
response. Similar to Intrator's Projection Pursuit related network (see Intrator g; 
Cooper, 1992 and references therein) some quality functions use higher order (> 2) 
statistics of the input process but in contrast to Intrator's network there is no need 
to specify the amount of lateral inhibition needed to learn different filter functions. 
All systems considered in this paper are linear but at the end we will briefly discuss 
possible non-linear extensions. 
2 Quality functions 
In the following we consider linear filter systems. These can be described by the 
equation: 
O(t) = W(t)P(t) (1) 
where P(t)  R M:I is the input pattern at iteration t, W(t)  R N:M is the filter 
coefficient matrix and O(t) - (o(t),...,ON(t)) t  R N: is the extracted feature 
vector. Usually M > N, i.e. the feature extraction process defines a reduction of 
the dimensionality. Furthermore, we assume that both the input patterns and the 
filter functions are normed; IlP(t)l I - 1 and IIw(t)ll- 1, w vn. This implies that 
Iog(t)l _< 1,Vt �n. 
Our first decision is to measure the scatter of the extracted feature vectors around 
the origin by the determinant of the output correlation matrix: 
QMS(t) = det Et{O(t)O'(l)} (2) 
QMS(t) is the quality function used in the Maximum Scatter Filter System (MS- 
system). The use of the determinant is motivated by the following two observations: 
1. The determinant is equal to the product of the eigenvalues and hence the product 
of the variances in the principal directions and thus a measure of the scattering 
volume in the feature space. 2. The determinant vanishs if some filter functions are 
linearly dependent. 
In (Lenz &: 6sterberg, 1992) we have shown that the optimal filter functions to 
QMS(I) are given by an arbitrary rotation of the N eigenvectors corresponding to 
the N largest eigenvalues of the input correlation matrix: 
Wop = RU.9 (3) 
where U, 0 contains the largest eigenvectors (or principal components) of the in- 
put correlation matrix Et{P(t)Pt(t)}. R is an arbitrary rotation matrix with 
det(R) = 1. To differentiate between these solutions we need a second criterion. 
138 0stepberg and Lenz 
One attempt to define the best rotation is to require that the mean energy E,{o} (t)} 
should be concentrated in as few components on(t) of the extracted feature vector 
as possible. Thus, the mean energy Et{o} (t)} of each filter function should be either 
very high (i.e. near 1) or very low (i.e. near 0). This leads to the following second 
order concentration measure: 
N 
Q2(t) = E E'{�a(t)} (1 - E,{oa,(t)}) (4) 
which has a low non-negative value if the energies are concentrated. 
Another idea is to find a system that produces feature vectors that have unsuper- 
vised discrimination power. In this case each learned filter function should respond 
selectively, i.e. have a large response for some input samples and low response for 
others. One formulation of this goal is that each extracted feature vector should be 
(up to the sign) binary; oi(t) = :kl and o,(t) = O, n y i, �t. This can be measured 
 E,{o}(0)- E,{o(0) (5) 
by the following fourth order expression: 
N N 
Q() = ,{ o() ( - o())) = 
n=l n=l 
which has a low non-negative value if the features are binary. Note that it is not 
sufficient to use on(t) instead of o}(t) since Qa(t) will have a low value also for 
feature vectors with components equal in magnitude but with opposite sign. A 
third criterion can be found as follows: if the filter functions have selective filter 
response then the response to different input patterns differ in magnitude and thus 
the variance of the mean energy E,{oan(t)} is large. The total variance is measured 
by: 
N N 
Qv,() =  v, {4(0) =  E,{(o(0- E,{o(0)) 2) 
n--1 
N 
=  E,{o4(t)) - (E,{o())) 2 
() 
Q,() = Q() 
Q(O (7) 
Qro(t) = 
Q4(t) (8) 
Qa4v(t) = Qvar(t)Qa4s(t) (9) 
Following (Darlington, 1970) it can be shown that the distribution of o} should 
be bimodal (modes below and above Et{o}}) to maximize Qvar(t). The main 
difference between QVar(t) and the quality function used by Intrator is the use 
4 
of a fourth order term Et{o,(t)} instead of a third order term Et{oa(t)}. With 
Et{oan(t)} the quality function is a measure of the skewness of the distribution 
o(t) and it is maximized when one mode is at zero and one (or several) is above 
E,{o(O). 
In this paper we will examine the following non-parametric combinations of the 
quality functions above: 
Unsupervised Parallel Feature Extraction from First Principles 139 
We refer to the corresponding filter systems as: the Karhunen-Lodve Filter Sys- 
tem (KL-system), the Fourth Order Filter System (FO-system) and the Maximum 
Variance Filter System (MV-system). 
Since each quality function is a combination of two different functions it is hard to 
find the global optimal solution. Instead we use the following strategy to determine 
a local optimal solution. 
Definition 1 The optimal orthogonal solution to each quality function is of the 
form: 
Wop = 9 (10) 
where Rort is the rotation of the largest eigenvectors which minimize Qe(t), Q4(t) 
or maximize QVar(t). 
In (Lenz g; 0sterberg, 1992 and 6sterberg, 1993) we have shown that the optimal 
orthogonal solution to the KL-system are the N pure eigenvectors if the N largest 
eigenvalues are all distinct (i.e. Ropt -- I). If some eigenvalues are equal then the 
solution is only determined up to an arbitrary rotation of the eigenvectors with 
equal eigenvalues. The fourth order term Et{o4(t)} in Q4(t) and Qvar(t) makes it 
difficult to derive a closed form solution. The best we can achieve is a numerical 
method (in the case of Q4(t) see 0sterberg, 1993) for the computation of the optimal 
orthogonal filter functions. 
3 Maximization of the quality function 
The partial derivatives of QMs(t), Qa(t), Q4(t) and Qvar(t) with respect to Wnm(t) 
(the rn ta weight in the n ta filter function at iteration t) are only functions of the 
input pattern P(t), the output values O(t): (o(t),. ON(t)) and the previous 
values of the weight coefficients (w} (t - 1),.. , w(t -iii within the alter function 
(see 0sterberg, 1993). Especially, they are not functions of the internal weights 
((w(t- 1),...,w(t 1)),i y n) of the other filter functions in the system. This 
implies that the filter coefficients can be learned in parallel using a system of the 
structure shown in Figure 1. 
In (0sterberg, 1993) we used on-line optimization techniques based on gradient 
ascent. We tried two different methods to select the step length parameter. One 
rather heuristical depending on the output o,(t) of the filter function and one 
inverse proportional to the second partial derivative of the quality function with 
respect to w(t). In each iteration the length of each filter function was explicitly 
normalized to one. Currently, we investigate standard unconstrained optimization 
methods (Dennis g; Schnabel, 1983) based on batch learning. Now the step length 
parameter is selected by line search in the search direction $(t): 
max Q(W(t) + ,XS(t)) (11) 
Typical choices of S(t) include S(t) = I and S(t) = H-. With the identity matrix 
we get
