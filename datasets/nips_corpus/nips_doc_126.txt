CONSTRAINTS ON ADAPTIVE NETWORKS 
FOR MODELING HUMAN GENERALIZATION 
M. Payel 
Mark A. Gluck 
Van Henkle 
Department of Psychology 
Stanford University 
Stanford, CA 94305 
ABSTRACT 
The potential of adaptive networks to learn categorization rules and to 
model human performance is studied by comparing how natural and 
artificial systems respond to new inputs, i.e., how they generalize. Like 
humans, networks can learn a deterministic categorization task by a 
variety of alternative individual solutions. An analysis of the con- 
straints imposed by using networks with the minimal number of hidden 
units shows that this minimal configuration conslxaint is not 
sufficient to explain and predict human performance; only a few solu- 
tions were found to be shared by both humans and minimal adaptive 
networks. A further analysis of human and network generalizations 
indicates that initial conditions may provide important constraints on 
generalization. A new technique, which we call reversed learning, 
is described for finding appropriate initial conditions. 
INTRODUCTION 
We are investigating the potential of adaptive networks to learn categorization tasks and 
to model human performance. In particular we have studied how both natural and 
artificial systems respond to new inputs, that is, how they generalize. In this paper we 
first describe a computational technique to analyze generalizations by adaptive networks. 
For a given network structure and a given classification problem, the technique 
enumerates all possible network solutions to the problem. We then report the results of 
an empirical study of human categorization learning. The generalizations of human sub- 
jects are compared to those of adaptive networks. A cluster analysis of both human and 
network generalizations indicates. significant differences between human performance 
and possible network behaviors. Finally, we examine the role of the initial state of a net- 
work for biasing the solutions found by the network. Using data on the relations between 
human subjects' initial and final performance during training, we develop a new tech- 
nique, called reversed learning, which shows some potential for modeling human 
learning processes using adaptive networks. The scope of our analyses is limited to gen- 
eralizations in deterministic pattern classification (categorization) tasks. 
Modeling Human Generalization 3 
The basic difficulty in generalization is that there exist many different classification rules 
(solutions) that that correctly classify the training set but which categorize novel 
objects differently. The number and diversity of possible solutions depend on the 
language defining the pattern recognizer. However, additional constraints can be used in 
conjunction with many types of pattern categorizers to eliminate some, hopefully 
undesirable, solutions. 
One typical way of introducing additional constraints is to minimize the representation. 
For example minimizing the number of equations and parameters in a mathematical 
expression, or the number of rules in a rule-based system would assure that some 
identification maps would not be computable. In the case of adaptive networks, minimiz- 
ing the size of adaptive networks, which reduces the number of possible encoded func- 
tions, may result in improved generalization performance (Rumelhart, 1988). 
The critical theoretical and applied questions in pattern recognition involve characteriza- 
tion and implementation of desirable constraints. In the first part of this paper we 
describe an analysis of adaptive networks that characterizes the solution space for any 
particular problem. 
ANALYSES OF ADAPTIVE NETWORKS 
Feed-forward adaptive networks considered in this paper will be defined as directed 
graphs with linear threshold units (LTLO as nodes and with edges labeled by real-valued 
weights. The output or activations of a unit is determined by a monotonic nonlinear func- 
tion of a weighted sum of the activation of all units whose edges terminate on that unit. 
There are three types of units within a feed-forward layered architecture: (1) Input units 
whose activity is determined by external input; (2) output units whose activity is taken as 
the response; and (3) the remaining units, called hidden units. For the sake of simplicity 
our discussion will be limited to objects represented by binary valued vectors. 
A fully connected feed-forward network with an unlimited number of hidden units can 
compute any boolean function. Such a general network, therefore, provides no con- 
straints on the solutions. Therefore, additional constraints must be imposed for the net- 
work to prefer one generalization over another. One such constraint is minimizing the 
size of the network. In order to explore the effect of minimizing the number of hidden 
units we first identify the minimal network architecture and then examine its generaliza- 
tions. 
Most of the results in this area have been limited to finding bounds on the expected 
number of possible patterns that could be classified by a given network (e.g. Cover, 1965; 
Volper and Hampson, 1987; Valiant, 1984; Baum & Haussler, 1989). The bounds found 
by these researchers hold for all possible categorizations and are, therefore, too broad to 
be useful for the analysis of particular categorization problems. 
To determine the generalization behavior for a particular network archilecture, a specific 
4 Gluck, Pavel and Henkle 
categorization problem and a training set it is necessary to find find all possible solutions 
and the corresponding generalizations. To do this we used a computational (not a simu- 
lation) procedure developed by Pavel and Moore (1988) for finding minimal networks 
solving specific categorization problems. Pavel and Moore (1988) defined two network 
solutions to be different if at least one hidden unit categorized at least one object in the 
training set differently. Using this definition their algorithm finds all possible different 
solutions. Because finding network solutions is NP-complete (Judd, 1987), for larger 
problems Payel and Moore used a probabilistic version of the algorithm to estimate the 
dislribution of generalization responses. 
One way to characterize the consWaints on generalization is in terms of the number of 
possible solutions. A larger number of possible solutions indicates that generalizations 
will be less predictable. The critical result of the analysis is that, even for minimal net- 
works, the number of different network solutions is often quite large. Moreover, the 
number of solutions increases rapidly with increases in the number of hidden units. The 
apparent lack of consWaints can also be demonstrated by finding the probability that a 
network with a randomly selected hidden layer can solve a given categorization problem. 
That is, suppose that we selgct n different hidden units, each unit representing a linear 
discriminant function. The activations of these random hidden units can be viewed as a 
Uansformation of the input patterns. We can ask what is the probability that an output 
unit can be found to perform the desired dichotomization. A typical example of a result 
of this analysis is shown in Figure 1 for the three-dimensional (3D) parity problem. In 
the minimal configuration involving three hidden units there were 62 different solutions 
to the 3D parity problem. The rapid increase in probability (high slope of the curve in 
Figure 1) indicatea that adding a few more hidden units rapidly increases the probability 
that a random hidden layer will solve the 3D parity problem. 
10o 
HIDDEN UNITS 
Figure I The prolmrfion of solutions to 3D parity problem (solid line) and the 
experimental task (dashed line) as a function of the number of hidden units. 
The results of a more detailed analysis of the generalization performance of the minimal 
networks will be discussed following a description of a categorization experiment with 
Modeling Human Generalization 
human subjects. 
HUMAN CATEGORIZATION EXPERIMENT 
In this experiment human subjects learned to cate objects which were defined by 
four dimensional binary vectors. Of the 2  possible objects, subjects were trained to clas- 
sify a subset of 8 objects into two categories of 4 objects each. The specific assignments 
of objects into categories was patterned after Medin et al. (1982) and is shown in Figure 
2. Eight of the patterns are designated as a training set and the remaining eight comprise 
the test set. The assignment of the patterns in the training set into two categories was 
such that there were many combinations of rules that could be used to correctly perform 
the categorization. For example, the first two dimensions could be used with one other 
dimension. The training patterns could also be categorized on the basis of an exclusive 
or (XOR) of the last two dimensions. The type of solution obtained by a human subject 
could only be determined by examining responses to the test set as well as the training 
seL 
TRAINING SET TEST SET 
X1 110 I 0010 0001 1101 
DIMENSIONS X= 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 
X, 1010 1010 0101 0101 
X 1010 0101 0101 0101 
CATEGORY AAA A B B B B ? ? ? ? ? ? ? ? 
Figure 2. Pattam to b clusified. (Adapted from Medin et al., 1982). 
In the actual experiments, subjects were asked to perform a medical diagnosis for each 
pattern of four symptoms (dimensions). The experimental procure will be described 
here only briefly because the details of this experiment have been described elsewhere in 
detail (Payel, Gluck, Henkle, 1988). Each of the patterns was presented serially in a ran- 
domized order. Subjects responded with one of the categories and then received feed- 
back. The training of each individual continued until he reached a criterion (responding 
correctly to 32 consecutive stimuli) or until each pattern had been presenwat 32 times. 
The data reported here is bas on 78 subje
