Bayesian averaging is well-temperated 
Lars Kai Hansen 
Department of Mathematical Modelling 
Technical University of Denmark B321 
DK-2800 Lyngby, Denmark 
lkhansen @imm. dtu. dk 
Abstract 
Bayesian predictions are stochastic just like predictions of any other 
inference scheme that generalize from a finite sample. While a sim- 
ple variational argument shows that Bayes averaging is generaliza- 
tion optimal given that the prior matches the teacher parameter 
distribution the situation is less clear if the teacher distribution is 
unknown. I define a class of averaging procedures, the temperated 
likelihoods, including both Bayes averaging with a uniform prior 
and maximum likelihood estimation as special cases. I show that 
Bayes is generalization optimal in this family for any teacher dis- 
tribution for two learning problems that are analytically tractable: 
learning the mean of a Gaussian and asymptotics of smooth learn- 
ers. 
1 Introduction 
Learning is the stochastic process of generalizing from a random finite sample of 
data. Often a learning problem has natural quantitative measure of generalization. 
If a loss function is defined the natural measure is the generalization error, i.e., the 
expected loss on a random sample independent of the training set. Generalizability 
is a key topic of learning theory and much progress has been reported. Analytic 
results for a broad class of machines can be found in the litterature [8, 12, 9, 10] 
describing the asymptotic generalization ability of supervised algorithms that are 
continuously parameterized. Asymptotic bounds on generalization for general ma- 
chines have been advocated by Vapnik [11]. Generalization results valid for finite 
training sets can only be obtained for specific learning machines, see e.g. [5]. A 
very rich framework for analysis of generalization for Bayesian averaging and other 
schemes is defined in [6]. 
Averaging has become popular as a tool for improving generalizability of learning 
machines. In the context of (time series) forecasting averaging has been investigated 
intensely for decades [3]. Neural network ensembles were shown to improve general- 
ization by simple voting in [4] and later work has generalized these results to other 
types of averaging. Boosting, Bagging, Stacking, and Arcing are recent examples 
of averaging procedures based on data resampling that have shown useful see [2] 
for a recent review with references. However, Bayesian averaging in particular is 
attaining a kind of cult status. Bayesian averaging is indeed provably optimal in a 
266 L. K. Hansen 
number various ways (admissibility, the likelihood principle etc) [1]. While it fol- 
lows by construction that Bayes is generalization optimal if given the correct prior 
information, i.e., the teacher parameter distribution, the situation is less clear if 
the teacher distribution is unknown. Hence, the pragmatic Bayesians downplay the 
role of the prior. Instead the averaging aspect is emphasized and vague priors are 
invoked. It is important to note that whatever prior is used Bayesian predictions 
are stochastic just like predictions of any other inference scheme that generalize 
from a finite sample. 
In this contribution I analyse two scenarios where averaging can improve gener- 
alizability and I show that the vague Bayes average is in fact optimal among the 
averaging schemes investigated. Averaging is shown to reduce variance at the cost 
of introducing bias, and Bayes happens to implement the optimal bias-variance 
trade-off. 
2 Bayes and generalization 
Consider a model that is smoothly parametrized and whose predictions can be 
described in terms of a density function x. Predictions in the model are based on a 
given training set: a finite sample D N 
-- {xa}a= of the stochastic vector x whose 
density - the teacher - is denoted p(xlOo ). In other words the true density is assumed 
to be defined by a fixed, but unknown, teacher parameter vector 00. The model, 
denoted H, involves the parameter vector 0 and the predictive density is given by 
p(xlD, H) - / p(xl O, H)p(OID, Z)dO 
(1) 
p(OID , H) is the parameter distribution produced in training process. In a maxi- 
mum likelihood scenario this distribution is a delta function centered on the most 
likely parameters under the model for the given data set. In ensemble averaging 
approaches, like boosting bagging or stacking, the distribution is obtained by train- 
ing on resampled traning sets. In a Bayesian scenario, the parameter distribution 
is the posterior distribution, 
p(OID, H ) --- P(DIO, H)P(OIH) 
f p( D[O', H)p( O'lH)dO' 
(2) 
where p(OIH ) is the prior distribution (probability density of parameters if D is 
empty). In the sequel we will only consider one model hence we suppress the model 
conditioning label H. 
The generalization error is the average negative log density (also known as simply 
the log loss - in some applied statistics works known as the deviance) 
r(DlO0) = / -logp(xlD)p(xlOo)dx, (3) 
The expected value of the generalization error for training sets produced by the 
given teacher is given by 
F(00) = / / -logp(x[D)p(x[Oo)dxp(D[Oo)dD. 
(4) 
This does not limit us to conventional density estimation; pattern recognition and 
many functional approximations problems can be formulated as density estimation prob- 
lems as well. 
Bayesian Averaging is Well- Temperated 267 
Playing the game of guessing a probability distribution [6] we not only face a 
random training set, we also face a teacher drawn from the teacher distribution 
p(00). The teacher averaged generalization must then be defined as 
r = f r(Oo)p(eo)deo. (5) 
This is the typical generalization error for a random training set from the randomly 
chosen teacher - produced by the model H. The generalization error is minimized 
by Bayes averaging if the teacher distribution is used as prior. To see this, form the 
Lagrangian functional 
�[q(xlD)]- f f f-logq(xlD)p(x[Oo)dxp(DlOo)dDp(Oo)dOo+A f q(xlD)dx (6) 
defined on positive functions q(xID ). The second term is used to ensure that q(xlD ) 
is a normalized density in x. Now compute the variational derivative to obtain 
(� 1 / 
5q(xlD ) = q(x[D) p(xlO�)p(DlO�)P(O�)dO� + '' (7) 
Equating this derivative to zero we recover the predictive distribution of Bayesian 
averaging, 
p(DlO)p(O) 
q(xID ) - p(xlO ) f P(DlO,)p(O,)do, dO, (8) 
where we used that A - f p(DlO)p(O)dO is the appropriate normalization constant. 
It is easily verified that this is indeed the global minimum of the averaged gener- 
alization error. We also note that if the Bayes average is performed with another 
prior than the teacher distribution p(0o), we can expect a higher generalization er- 
ror. The important question from a Bayesian point of view is then: Are there cases 
where averaging with generic priors (e.g. vague or uniform priors) can be shown to 
be optimal? 
3 Temperated likelihoods 
To come closer to a quantative statement about when and why vague Bayes is the 
better procedure we will analyse two problems for which some analytical progress is 
possible. We will consider a one-parameter family of learning procedures including 
both a Bayes and the maximum likelihood procedure, 
p(OI,D,H) = f p(DlO')dO (9) 
where / is a positive parameter (plying the role of an inverse temperature). The 
family of procedures are all averaging procedures, and/ controls the width of the 
average. Vague Bayes (here used synonymously with Bayes with a uniform prior) 
is recoved for/ - 1, while the maximum posterior procedure is obtained by cooling 
to zero width/ - o. 
In this context the generalization design question can be frased as follows: is there 
an optimal temperature in the family of the temperated likelihoods? 
3.1 Example: 1D normal variates 
Let the teacher distribution be given by 
1 ( 1 (x_00)2) 
p(xlOo ) -  exp 2/r2 
(10) 
268 L. K. Hansen 
The model density is of the same form with 0 unknown and er 2 assumed to be 
known. For N examples the posterior (with a uniform prior) is, 
p(OlD) = 2--- exp - , 
(11) 
with 7 = 1/N Y'o zoo. The temperated likelihood is obtained by raising to the fi'th 
power and normalizing, 
p(O]D, fi) = 2---- 2 exp  . 
(12) 
The predictive distribution is found by integrating w.r.t. 0, 
p(xlD,) = p(x10)p(01D,)d0 = (7- , (la) 
with  =  (1 + 1/N). We note that this distribution is wider for all the averaging 
procedures than it is for maximum likelihood (  ), i.e., less variant. Por very 
small  the predictive distribution is almost independent of the data set, hence 
highly bied. 
It is straightforward to compute the generalization error of the predictive distribu- 
tion for general . Pirst we compute the generalization error for the specific training 
set D, 
r(D, fi, Oo) = -logp(x[D, fi)p(x]Oo)dx = log + 2 ((- O�)2 +2), 
(14) 
The average generalization error is then found by averaging w.r.t the sampling 
distribution using  (Oo,a2/N)., 
f  a2(1 ) 
F(fi) = F(D, fi)dDp(DlOo) = log + 2  + i , (15) 
We first note that the generalization error is independent of the teacher 0 param- 
eter, this happened because  is a location parameter. The fi-dependency of the 
averaged generalization error is depicted in Figure 1. Solving OF(fi)/Ofi = 0 we find 
that the optimal fi solves 
+1 =a2 +1  fi=l (16) 
Note that this result holds for any N and is independent of the teacher parameter. 
The Bayes averaging at unit temperature is optimal for any given value of 0, hence, 
for any teacher distribution. We may say that the vague Bayes scheme is robust 
to the teacher distribution in this ce. Clearly this is a much stronger optimality 
than the more general result proven above. 
3.2 Bias-variance tradeoff 
It is interesting to decompose the generalization error in Eq. 15 in bias and variance 
components. We follow Heskes [7] and define the bias error as the generalization 
error of the geometric average distribution, 
B(fi) --/-log
