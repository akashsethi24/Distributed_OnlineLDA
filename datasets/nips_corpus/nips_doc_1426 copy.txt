From Regularization Operators 
to Support Vector Kernels 
Alexander J. Smola 
GMD FIRST 
Rudower Chaussee 5 
12489 Berlin, Germany 
smola@first.gmd.de 
Bernhard Schiilkopf 
Max-Planck-Institut far biologische Kybernetik 
Spemannstrage 38 
72076 Ttibingen, Germany 
bs@mpik~tueb.mpg.de 
Abstract 
We derive the correspondence between regularization operators used in 
Regularization Networks and Hilbert Schmidt Kernels appearing in Sup- 
port Vector Machines. More specifically, we prove that the Green's Func- 
tions associated with regularization operators are suitable Support Vector 
Kernels with equivalent regularization properties. As a by-product we 
show that a large number of Radial Basis Functions namely condition- 
ally positive definite functions may be used as Support Vector kernels. 
1 INTRODUCTION 
Support Vector (SV) Machines for pattern recognition, regression estimation and operator 
inversion exploit the idea of transforming into a high dimensional feature space where 
they perform a linear algorithm. Instead of evaluating this map explicitly, one uses Hilbert 
Schmidt Kernels k(x, y) which correspond to dot products of the mapped data in high 
dimensional space, i.e. 
k(x,y) = 
with I, � ' --> ' denoting the map into feature space. Mostly, this map and many of 
its properties are unknown. Even worse, so far no general rule was available which kernel 
should be used, or why mapping into a very high dimensional space often provides good 
results, seemingly defying the curse of dimensionality. We will show that each kernel 
k(x, y) corresponds to a regularization operator/5, the link being that k is the Green's 
function of/5,/5 (with/5* denoting the adjoint operator of/5). For the sake of simplicity 
we shall only discuss the case of regression -- our considerations, however, also hold true 
foi the other cases mentioned above. 
We start by briefly reviewing the concept of SV Machines (section 2) and of Regularization 
Networks (section 3). Section 4 contains the main result stating the equivalence of both 
344 A. J. Smola and B. Sch61kopf 
methods. In section 5, we show some applications of this finding to known SV machines. 
Section 6 introduces a new class of possible SV kernels, and, finally, section 7 concludes 
the paper with a discussion. 
2 SUPPORT VECTOR MACHINES 
The SV algorithm for regression estimation, as described in [Vapnik, 1995] and [Vapnik 
et al., i 997], exploits the idea of computing a linear function in high dimensional feature 
space )r (furnished with a dot product) and thereby computing a nonlinear function in the 
space of the input data 1 '. The functions take the form f(x) = (w. I,(x)) + b with 
I,: l ' --> Y'and w  Y'. 
In order to infer f from a training set ( (xi, Yi) I i = 1,..., g, xi C 1 ', yi C 1}, one tries 
to minimize the empirical risk functional -Re,,v[f] together with a complexity term IIll 2, 
thereby enforcing flatness in feature space, i.e. to minimize 
Area[f]- emp[f] + ll11 
:  E �(f(xi),ti) -� llll 
i=1 
(2) 
with c(f(xi),Yi) being the cost function determining how deviations of f(xi) from the 
target values yi should be penalized, and A being a regularization constant. As shown in 
[Vapnik, 1995] for the case of e-insensitive cost functions, 
c(f(x),y)= { f(x)-y,-e for ,f(x)-yl_>e 
otherwise ' (3) 
(2) can be minimized by solving a quadratic programming problem formulated in terms 
of dot products in f. It turns out that the solution can be expressed in terms of Support 
Vectors, w = -i=1 �i(I(xi) , and therefore 
i=1 i----1 
(4) 
where k(xi, x) is a kernel function computing a dot product in feature space (a concept 
introduced by Aizerman et al. [1964]). The coefficients cti can be found by solving a 
quadratic programming problem (with tij '= k(Xi, Xj) and cti =/3i -/3?): 
minimize 
subject to 
(5) 
Note that (3) is not the only possible choice of cost functions resulting in a quadratic pro- 
gramming problem (in fact quadratic parts and infinities are admissible, too). For a detailed 
discussion see [Smola and Sch61kopf, 1998]. Also note that any continuous symmetric 
function k(x, y)  L2 � L2 may be used as an admissible Hilbert-Schmidt kernel if it 
satisfies Mercer's condition 
/ / k(x, y)g(x)g(y)dxdy _> 0 for all g  L2(] n). (6) 
3 REGULARIZATION NETWORKS 
Here again we start with minimizing the empirical risk functional Remp[f] plus a regu- 
larization term 1115fll 2 defined by a regularization operator t 5 in the sense of Arsenin and 
From Regularization Operators to Support Vector Kernels 345 
Tikhonov [1977]. Similar to (2), we minimize 
1 
-re9[f] ---- e,,,p q- 1115fll 2 --   c(f(xi),yi) + 11Pfll 2. (7) 
i=1 
Using an expansion of f in terms of some symmetric function k(xi, xj) (note here, that k 
need not fulfil Mercer's condition), 
f(x) = y ik(x,, x) + b, (8) 
i 
and the cost function defined in (3), this leads to a quadratic programming problem similar 
to the one for SVs: by computing Wolfe's dual (for details of the calculations see [Smola 
and Sch61kopf, 1998]), and using 
Dij :- ((Pk)(xi, .). (Pk)(xj,.)) (9) 
((f � g) denotes the dot product of the functions f and g in Hilbert Space, i.e. 
f f(x)g(x)dx), we get G = D - K(/ -/*), with/3i,/37 being the solution of 
minimize 
subject to 
i , , 
 E (/i -- /3i)(/3; -- flj)(KD-1I()ij - E(/3i -/3i)Yi- (/3 +/3i)e 
i,j=l i-1 
i----1 
(o) 
Unfortunately this setting of the problem does not preserve sparsity in terms of the coef- 
ficients, as a potentially sparse decomposition in terms of/3i and/3 is spoiled by D-Xlf, 
which in general is not diagonal (the expansion (4) on the other hand does typically have 
many vanishing coefficients). 
4 THE EQUIVALENCE OF BOTH METHODS 
Comparing (5) with (10) leads to the question if and under which condition the two methods 
might be equivalent and therefore also under which conditions regularization networks 
might lead to sparse decompositions (i.e. only a few of the expansion coefficients in f 
would differ from zero). A sufficient condition is D =/( (thus KD-XK = K), i.e. 
k(xi, xj): ((P)(x, .). (Pk)(xj,.)) (1 ) 
Our goal now is twofold: 
� Given a regularization operator/5, find a kernel k such that a SV machine using k 
will not only enforce flatness in feature space, but also correspond to minimizing 
a regularized risk functional with/5 as regularization operator. 
� Given a Hilbert Schmidt kernel k, find a regularization operator 15 such that a SV 
machine using this kernel can be viewed as a Regularization Network using/5. 
These two problems can be solved by employing the concept of Green's functions as de- 
scribed in [Girosi et al., 1993]. These functions had been introduced in the context of 
solving differential equations. For our purpose, it is sufficient to know that the Green's 
functions Gx, (x) of/5*/5 satisfy 
(/5*PG,,)(x): 6x,(X). (12) 
Here, 6x (x) is the 6-distribution (not to be confused with the Kronecker symbol 6i) which 
has the property that (f � 6x) -- f(xi). Moreover we require for all xi the projection of 
Gx (x) onto the null space of/5,/5 to be zero. The relationship between kernels and 
regularization operators is formalized in the following proposition. 
346 A. J. Smola and B. SchOlkopf 
'Proposition 1 
Let 16 be a regularization operator, and G be the Green's function of 16'16. Then G is a 
Hilbert Schmidt-Kernel such that D = K. SV machines using G minimize risk functional 
(7) with 13 as regularization operator. 
Proof: Substituting (12)into Gx (xi) = (Gx (.)' 8x (.)) yields 
axi(X,): ((16ax,)(.). Ox,(Xj), (13) 
hence G(xi, xj) := Gx,(XS) is symmetric and satisfies (11). Thus the SV optimization 
problem (5) is equivalent to the regularization network counterpart (10). Furthermore G is 
an admissible positive kernel, as it can be written as a dot product in Hilbert Space, namely 
G(xi,xj) = ((I)(xi). (I)(xj)) with (I) 'xi, > (16Gxi)(.). (14) 
In the following we will exploit this relationship in both ways: to compute Green's func- 
tions for a given regularization operator/3 and to infer the regularization operator from a 
given kernel k. 
5 TRANSLATION INVARIANT KERNELS 
Let us now more specifically consider regularization operators 16 that may be written as 
multiplications in Fourier space [Girosi et al., 1993] 
with ./(co) denoting the Fourier transform of f(x), and P(co) =/>(-co) real valued, non- 
negative and converging uniformly to 0 for Icol  o and f -- supp[P(co)]. Small values 
of/>(co) correspond to a strong attenuation of the corresponding frequencies. 
For regularization operators defined in Fourier Space by (15) it can be shown by exploiting 
P(co) = P(-co) = P(co) that 
G(xi,x) = I jf ei(x'-x)P(co)&v (16) 
(27r)n/2 
is a corresponding Green's function satisfying translational invariance, i.e. G(xi, xj) -- 
G(xi - x5), and (2(co) = P(co). For the proof, one only has to show that G satisfies (11). 
This provides us with an efficient tool for analyzing SV kernels and the types of capacity 
control they exhibit. 
Example 1 (Bq-splines) 
Vapnik et al. [1997]propose to use Bq-splines as building blocks for kernels, i.e. 
k(X) : H Bq(Xi) (17) 
i=1 
with x C I 7. For the sake of simplicity, we consider the case n -- 1. Recalling the 
definition 
Bq = �q+l 1[-o.,o.] 
(18) 
(� denotes the convolution and l x the indicator function on X), we can utilize the above 
result and the Fourier-Plancherel identity to construct the Fourier representation of the 
corresponding regularization operator. Up to a multiplicative constant, it equals 
P(co) = c(co) = sinc(q+)(). (19) 
From Regularization Operators to Support Vector Kernels 347 
This shows that only B-splines of odd order are admissible, as the even ones have negative' 
parts in the Fourier spectrum (which would result in an amplification of the corresponding 
frequency components). The zeros in [c stem from the fact that Bt has only compact support 
[-(k + 1)/2, (k + 1)/2]. By using this kerne
