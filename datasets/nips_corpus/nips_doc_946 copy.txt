Grammar Learning by a Self-Organizing 
Network 
Michiro Negishi 
Dept. of Cognitive and Neural Systems, Boston University 
111 Cummington Street 
Boston, MA 02215 email: negishi@cns.bu.edu 
Abstract 
This paper presents the design and simulation results of a self- 
organizing neural network which induces a grammar from exam- 
ple sentences. Input sentences are generated from a simple phrase 
structure grammar including number agreement, verb transitiv- 
ity, and recursive noun phrase construction rules. The network 
induces a grammar explicitly in the form of symbol categorization 
rules and phrase structure rules. 
1 Purpose and related works 
The purpose of this research is to show that a self-organizing network with a certain 
structure can acquire syntactic knowledge from only positive (i.e. grammatical) 
data, without rocluiring any initial knowledge or external teachers that correct 
errors. 
There has been research on supervised neural network models of language acquisi- 
tion tasks [Elman, 1991, Miikkulainen and Dyer, 1988, John and McClelland, 1988]. 
Unlike these supervised models, the current model self-organizes word and phrasal 
categories and phrase construction rules through mere exposure to input sentences, 
without any artificially defined task goals. There also have been self-organizing 
models of language acquisition tasks [Ritter and Kohonen, 1990, Scholtes, 1991]. 
Compared to these models, the current model acquires phrase structure rules in 
more explicit forms, and it learns wider and more structured contexts, as will be 
explained below. 
2 Network Structure and Algorithm 
The design of the current network is motivated by the observation that humans 
have the ability to handle a frequently occurring sequence of symbols (chunk) 
as an unit of information [Grossberg, 1978, Mannes, 1993]. The network consists 
of two parts: classification networks and production networks (Figure 1). The 
classification networks categorize words and phrases, and the production networks 
28 Michiro Negishi 
evaluate how it is likely for a pair of categories to form a phrase. A pair of combined 
categories is given its own symbol, and fed back to the classifiers. 
After weights are formed, the network parses a sentence as follows. Input words 
are incrementally added to the neural sequence memory called the Gradient Field 
[Grossberg, 1978] (GF hereafter). The top (i.e. most recent) two symbols and the 
lookahead token are classified by three classification networks. Here a symbol 
is either a word or a phrase, and the lookahead token is the word which will be 
read in next. Then the lookahead token and the top symbol in the GF are sent to 
the right production network, and the top and the second ones are sent to the left 
production network. If the latter pair is judged to be more likely to form a phrase, 
the symbol pair reduces to a phrase, and the phrase is fed back to the GF after 
removing the top two symbols. Otherwise, the lookahead token is added to the 
sequence memor causing a shift in the sequence memory. If the input sentence is 
grammatical, the repetition of this process reduces the whole sentence to a single 
S (sentence) symbol. The sequence of shifts and reductions (annoted with the 
resultant symbols) amounts to a parse of the sentence. 
During learning, the operations stated above are carried out as weights are grad- 
ually formed. In classification networks, the weights record a distribution pattern 
with respect to each symbol. That is, the weights record the co-occurrence of 
up to three adjacent symbols in the corpus. An symbol is classified in terms of 
this distribution in the classification networks. The production networks keep 
track of the categories of adjacent symbols. If the occurrence of one category reli- 
ably predicts the next or the previous one, the pair of categories forms a phrase, 
and is given the status of an symbol which is treated just like a word in the 
sentence. Because the symbols include phrases, the learned context is wider 
and more structured than the mere bigram, as well as the contexts utilized in 
[Ritter and Kohonen, 1990, Scholtes, 1991]. 
3 Simulation 
3.1 The Simulation Task 
The grammar used to generate input sentences (Table 3) is identical to that used 
in [Elman, 1991], except that it does not include optionally transitive verbs and 
proper nouns. Lengths of the input sentences are limited to 16 words. To deter- 
mine the completion of learning, after accepting 200 consecutive sentences with 
learning, learning is suppressed and other 200 sentences are processed to see if all 
are accepted. In addition, the network was tested for 44 ungrammatical sentences 
to see that they are correctly rejected. Ungrammatical sentences are derived by 
hand from randomly generated grammatical sentences. Parameters used in the 
simulation are: number of symbol nodes = 30 (words) + 250 (phrases), number 
of category nodes = 150, e = 10 -9, 7 '- 0.25, p -- 0.65, Ctl = 0.00005, fll -- 0.005, 
 = 0.2, Ct 3 -- 0.0001, f13 -- 0.001, and T = 4.0. 
Grammar Learning by a Self-Organizing Nem,ork 2 9 
3.2 Acquired Syntax Rules 
Learning was completed after learning 19800 grammatical sentences. Tables 1 and 
2 show the acquired syntax rules extracted from the connection weights. Note that 
category names such as Ns, VPp, are not given a priori, but assigned by the author 
for the exposition. Only rules that eventually may reach the Ssentence) node 
are shown. There were a small number of uninterpretable rules, which are marked 
. These rules might disturb normal parsing for some sentences, but they were 
not activated while testing for 200 sentences after learning. 
3.3 Discussion 
Recursive noun phrase structures should be learned by finding equivalences of 
distribution between noun phrases and nouns. However, nouns and noun phrases 
have the same contextual features only when they are in certain contexts. An 
examination of the acquired grammar reveals that the network finds equivalence 
of features not of N and N RC(where RC is a relative clause) but of N V and 
N RC V (when N RC is subjective), or V N and V N RC(when N RC is objective). As an example, let us examine the parsing of the sentence [19912] 
below. The rule used to reduce FEEDS CATS WHO LIVE (V N RC) is P0, which 
is classified as category C4, which includes P121 (V N) where V are the singular 
forms of transitive verbs, and also includes the Vwhere V are singular forms of 
intransitive verbs. Thus, GIRL WHO FEEDS CATS WHO LIVE is reduced to GIRL 
WHO VPsingle. 
+---141---+ 
I +---88 ...... + 
I I +---206 ...... + 
I I I + .... 0 .... + 
I I I I +-21-+ 
I t +-41-+ I +-36-+ I 
BOYS CHASE GIRL WHO FEEDS CATS WHO LIVE 
<<Accepted>> Top symbol was 77 
4 Conclusion and Future Direction 
In this paper, a self-organizing neural network model of grammar learning was 
presented. A basic principle of the network is that all words and phrases are 
categorized by the contexts in which they appear, and that familiar sequence of 
categories are chunked. 
As it stands, the scope of the grammar used in the simulation is extremely limited. 
Also, considering the poverty of the actual learning environment, the learning 
of syntax should also be guided by the cognitive competence to comprehend the 
utterance situations and conversational contexts. However, being a self-organizing 
network, the current model offers a plausible model of natural language acquisition 
through mere exposures to only grammatical sentences, not requiring any external 
teacher or an explicit goal. 
30 Michiro Negishi 
Table 1. Acquired categorization rules 
$ := C29/* Nrps VPs '/ 
C30/* ? '/ I 
CT//' NPp VPp '/ 
C4 := LIVES I WALKS I 
P0/ VTs Np RC '/ 
P74/' VTs Ns RC*/ 
P121/ VTs Ns '/ 
P157/' v'rs Np '/ 
C3 := GIRLI DOG I 
CAT I BOY 
C16 := CHASE t FEED 
C18 := WHO 
C20 := CHASES t FEEDS 
C26 := BOYSICATSI 
DOGS I GIRLS 
C29 := P93/'NsRCVPs'/ 
P138/' Ns VPs '/ 
C30 := P2/'VTpNPpVPp 
P94/'VTp N VT'/ I 
P137/' ? '/ 
C32 := WALKILIVE I 
PI/' VTp Np RC '/ 
P61/ VTp Np '/ 
P88/' VTp Ns RC 
P122/' VTp Ns'/ 
ï¿½ -/' VPs '/ 
=/' Ns '/ 
=/' VTp '/ 
=/' R '/ 
=/' VTs '/ 
=/' Np '/ 
=/' NPs VPs '/ 
=/. ? / 
=/' VPp '/ 
C52 := P41/*NsR*/ 
C56 := P36/' Np R'/ 
C58 := P28/' Ns VTs'/ 
P34/'NpVTp'/  
P68/' Ns RC V'rs 
P147/* Np RC V'rp 
C69 := P206/' Ns R VPs 
P238/' Ns R N VT'/ 
C74 := P219/* Np R VPp 
P249/'Np R N VT'/ 
C77 := P141/'NpVPp 
P217/' Np RC VPp 
Cl19 := P148 
C122 := P243 
C139 := P10/' VTs hips 
P32/' VTs NPp 
where 
RCs = RVPs I RNVT 
RCp = R VPp I R N VT 
NPp = NpI Np RCp 
Nrps = Ns I Ns RCs 
=/*NVT*/ 
=/* Ns RCs */ 
=/* Np RCp */ 
=/* NPp VPp '/ 
=/* VTs NVT*/ 
=/* Ns R VTs N VT '/ 
=/* VPs' VPp/s ?*/ 
Table 2. Acquired production rules 
P0 := C20/* VTs */ C74/* Np RCp / 
P1 := C16/* VTp */ C74/* Np RCp */ 
P2 := C16/* VTp */ C77/* NPp VPp */ 
P10 := C20/* VTs */ C29/* NPs VPs */ 
P28 :=C13 /*Ns*/ C20/*VTs*/ 
P32 := C20/* VTs */ C77/* NPp VPp */ 
P34 := C26/* Np */ C16/* VTp */ 
P36 := C26/* Np */ C18/* R */ 
P41 := C13/* Ns */ C18/* R */ 
P61 := C16/* VTp */ C26/* Np */ 
P68 := C69/* Ns RCs */ C20/*VTs*/ 
P74 := C20/* VTs */ C69/* Ns RCs */ 
1:'88 := C16/*VTp*/ C69/*NsRCs*/ 
P93 := C69/* Ns RCs */ C4/* VPs */ 
P94 := C16/* VTp '/ C58/* N VT '/ 
P121 := C20/* VTs */ C13/* Ns */ 
P122 := C16/*.V.,p */ C13/* Ns */ 
P137 :=C122/ NsRVTsNVT*/ C32/*VPp*/ 
P138 := C13/* Ns */ C4/* VPs*/ 
P141 := C26/' Np '/ C32/' VPp '/ 
P147 := C74/' vsR.? '/ C161'VTp'/ 
P148 := C20/* C58/* N VT*/ 
P157 :-- C20/* VTs */ C26/* Np */ 
P206 := C52/* Ns R */ C4/* VPs*/ 
P217 := C/* Np RCs */ C32/* VPp*/ 
P219 := /* Np R */ C32/* VPp */ 
P238 := C52/* Ns R */ C58/* N VT */ 
P243 :=C52/*NsR*/ Cl19/*VTsNVT*/ 
P249 := C56/* Np R */ C58/* N VT */ 
=/* VTs Np RCp */ 
=/* VTp Np RCp */ 
/* VTp NPp VPp */ 
=/* VTs NPs VPs */ 
=/ Ns VTs '/ 
=/* VTs NPp VPp '/ 
=/* Np VTp */ 
=/* Np R */ 
=/* Ns R */ 
=/- VTp Np '/ 
=/* Ns RCs VTs */ 
--/*
