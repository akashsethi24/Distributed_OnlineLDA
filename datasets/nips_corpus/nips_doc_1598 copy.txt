Information Factorization in 
Connectionist Models of Perception 
Javier It. Movellan 
Department of Cognitive Science 
Institute for Neural Computation 
University of California San Diego 
James L. McClelland 
Center for the Neural Bases of Cognition 
Department of Psychology 
Carnegie Mellon University 
Abstract 
We examine a psychophysical law that describes the influence of 
stimulus and context on perception. According to this law choice 
probability ratios factorize into components independently con- 
trolled by stimulus and context. It has been argued that this pat- 
tern of results is incompatible with feedback models of perception. 
In this paper we examine this claim using neural network models 
defined via stochastic differential equations. We show that the law 
is related to a condition named channel separability and has little 
to do with the existence of feedback connections. In essence, chan- 
nels are separable if they converge into the response units without 
direct lateral connections to other channels and if their sensors are 
not directly contaminated by external inputs to the other chan- 
nels. Implications of the analysis for cognitive and computational 
neurosicence are discussed. 
1 Introduction 
We examine a psychophysical law, named the Morton-Massaro law, and its implica- 
tions to connectionist models of perception and neural information processing. For 
an example of the type of experiments covered by the Morton-Massaro law consider 
an experiment by Massaro and Cohen (1983) in which subjects had to identify syn- 
thetic consonant sounds presented in the context of other phonemes. There were 
two response alternatives, seven stimulus conditions, and four context conditions. 
The response alternatives were/1/and/r/, the stimuli were synthetic sounds gen- 
erated by varying the onset frequency of the third formant, followed by the vowel 
/i/. Each of the 7 stimuli was placed after each of four different context consonants, 
/v/, /s/, /p/, and/t/. Morton (1969) and Massaro independently showed that in a 
remarkable range of experiments of this type, the influence of stimulus and context 
on response probabilities can be accounted for with a factorized version of Luce's 
strength model (Luce, 1959) 
!s(i, k) !c(j, k) 
P(R = k lS = i,C = j) = Evs(i, 1)vc(j, 1)' for (i,j,k) e $ x � x 7. (1) 
Here $, C and R are random variables representing the stimulus, context and the 
subject's response, $, � and 7 are the set of stimulus, context and response al- 
46 J.. R. Movellan and J.. L. McClelland 
ternatives, $(i, k) > 0 represents the support of stimulus i for response k, and 
r/c(j, k) > 0 the support of context j for response k. Assuming no strength param- 
eter is exactly zero, (1) is equivalent to 
P(R=kIS=i,C=j) 
P(R-11$ -i,C -j) 
= \ n(i, t) ] j 
, for all (i, j, k)  $ x � x 7. 
(2) 
This says that response probability ratios factorize into two components, one which 
is affected by the stimulus but unaffected by the context and one affected by the 
context but unaffected by the stimulus. 
2 Diffusion Models of Perception 
Massaro (1989) conjectured that the Morton-Massaro law may be incompatible 
with feedback models of perception. This conjecture was based on the idea that in 
networks with feedback connections the stimulus can have an effect on the context 
units and the context can have an effect on the stimulus units making it impossible 
to factorize the influence of information sources. In this paper we analyze such 
a conjecture and show that, surprisin. gly, the Morton-Massaro law has little to do 
with the existence of feedback and lateral connections. We ground our analysis 
on continuous stochastic versions of recurrent neural networks . We call these 
models diffusion (neural) networks for they are stochastic diffusion processes defined 
by adding Brownian motion to the standard recurrent neural network dynamics. 
Diffusion networks are defined by the following stochastic differential equation 
dYi(t) - !i(Y(t),X) dt + rr dBi(t) for i  {1,..-,n}, (3) 
where Y/(t) is a random variable representing the internal potential at time t of the 
i th unit, Y(t) = (Y (t),-.. , Yn(t))', X represents the external input, which consists 
of stimulus and context, and Hi is Brownian motion, which acts as a stochastic 
driving term. The constant rr > 0, known as the dispersion, controls the amount 
of noise injected onto each unit. The function/i, known as the dr/f, determines 
the average instantaneous change of activation and is borrowed from the standard 
recurrent neural network literature: this change is modulated by a matrix w of 
connections between units, and a matrix v that controls the influence of the external 
inputs onto each unit. 
1 
ti(ri(t),X) -- i(ri(t) ) (i(t) - ri(t)), for all/ {1,.--,n}, (4) 
where 1/;i is a positive function, named the capacitance, controlling the speed of 
processing and 
?i(t)=Ewi,jZj(t)+Evi,kXk, for all i e {1,-.- ,n}, (5) 
Zj(t) = 9i(Yj(t)) = 9(ci Yj(t)) = 1/(1 + e -a' (t)). (6) 
Here wi,j, an element of the connection matrix w, is the weight from unit j to unit i, 
vi,& is an element of the matrix v, 9 is the logistic activation function and the ci > 0 
terms are gain parameters, that control the sharpness of the activation functions. 
For large values of ci the activation function of unit i converges to a step function. 
The variable Zj (t) represents a short-time mean firing rate (the activation) of unit 
1For an analysis grounded on discrete time networks with binary states see McClelland 
Information Factorization 47 
j scaled in the (0, 1) range. Intuition for equation (4) can be achieved by thinking 
of it as a the limit of a discrete time difference equation, in such case 
Y(t + At) = Y/(t) + !i(Yi(t),X)At + av/Ni(t), 
(7) 
where the Ni(t) are independent standard Gaussian random variables. For a fixed 
state at time t there are two forces controlling the change in activation: the drift, 
which is deterministic, and the dispersion which is stochastic. This results in a 
distribution of states at time t + At. As At goes to zero, the solution to the 
difference equation (7) converges to the diffusion process defined in (4). In this 
paper we focus on the behavior of diffusion networks at stochastic equilibrium, i.e., 
we assume the network is given enough time to approximate stochastic equilibrium 
before its response is sampled. 
3 Channel Separability 
In this section we show that the Morton-Massaro is related to an architectural con- 
straint named channel separability, which has nothing to do with the existence of 
feedback connections. In order to define channel separability it is useful to char- 
acterize the function of different units using the following categories: 1) Response 
specification units: A unit is a response specification unit, if, when the state of all 
the other units in the network is fixed, changing the state of this unit affects the 
probability distribution of overt responses. 2) Stimulus units: A unit belongs to 
the stimulus channel if: a) it is not a response unit, and b) when the state of the 
response units is fixed, the probability distribution of the activations of this unit is 
affected by the stimulus. 3) Context units: A unit belongs to the context channel if: 
a) it is not a response unit, and b) when the states of the response units are fixed, 
the probability distribution of the activations of this unit can be affected by the 
context. Given the above definitions, we say that a network has separable stimulus 
and context channels if the stimulus and context units are disjoint: no unit simul- 
taneously belongs to the stimulus and context channels. In essence, channels are 
structurally separable if they converge into the response units without direct lateral 
connections to other channels and if their sensors are not directly contaminated by 
external inputs to the other channels (see Figure 1). 
In the rest of the paper we show that if a diffusion network is structurally separable 
the Morton-Massaro law can be approximated with arbitrary precision regardless of 
the existence of feedback connections. For simplicity we examine the case in which 
the weight matrix is symmetric. In such case, each state has an associated goodness 
function that greatly simplifies the analysis. In a later section we discuss how the 
results generalize to the non-symmetric case. 
Let y  11 ' represent the internal potential of a diffusion network. Let zi = 99(otiYi) 
for i = 1,..-,n represent the firing rates corresponding to y. Let z s, z c and 
z r represent the components of z for the units in the stimulus channel, context 
channel and response specification module. Let x be a vector representing an input 
and let x s, x c be the components of x for the external stimulus and context. Let 
ot = (Oil,''' ,Otn) be a fixed gain vector and Za(t) a random vector representing 
the firing rates at time t of a network with gain vector c. Let Z a = limt-oo Z a (t), 
represent the firing rates at stochastic equilibrium. In Movellan (1998) it is shown 
that if the weights are symmetric i.e., w = w' and 1/ni(x) = d99i(x)/dx then the 
equilibrium probability density of Z a is as follows 
s z r 1 exp((2/cr 2) Ga(zS,zrlxs xc)) (S) 
PzIx(z ,z c, I x*'xC) = K(x,,xc) ' ' 
48 J. R. Movellan and J. L. McClelland 
Rqonse 
Spicatlon 
Units 
Stimulus Context 
!iays Rdays 
Stimulus Context 
Smsors Sensors 
$timuim / 
Input 
Context 
Figure 1: A network with separable context and stimulus processing channels. The 
stimulus sensor and stimulus relay units make up the stimulus channel units, and 
the context sensor and context channel units make up the context channel units. 
Note that any of the modules can be empty except the response module. 
where 
Ka(xs,xc) = f exp((2/a 2) Ga(z I xs,xc)) dz, (9) 
n 
o( Ix) = H(z Ix) -  S, (z), (10) 
i=1 
H(z Ix) = z' w z/2 + z' v x, (11) 
Sc,,(zi)= ai(log(zi)+log(1 
