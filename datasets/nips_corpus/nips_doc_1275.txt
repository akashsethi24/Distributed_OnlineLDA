Smoothing Regularizers for 
Projective Basis Function Networks 
John E. Moody and Thorsteinn S. RSgnvaldsson * 
Department of Computer Science, Oregon Graduate Institute 
PO Box 91000, Portland, OR 97291 
moody@cse.ogi.edu denni@cca.hh.se 
Abstract 
Smoothing regularizers for radial basis functions have been studied extensively, 
but no general smoothing regularizers for projective basis functions (PBFs), such 
as the widely-used sigmoidal PBFs, have heretofore been proposed. We de- 
rive new classes of algebraically-simple rn h-order smoothing regularizers for 
networks of the form f(W, x) = Y'j%l uj7 [X Tvj '}' Vj0] + It0, with general 
projectire basis functions 9[']. These reguladzers are: 
N 
R(W,,) -  ,11vjll 2'- Global Form 
j=l 
N 
Rt(W,m) -  2 112,,, 
- u I lvj al Form 
./=1 
These regularizers bound the corresponding rn �'-order smoothing integral 
where W denotes all the network weights { u, uo, v, vo}, and (x) is a weight- 
ing function on the D-dimensional input space. The global and local cases are 
distinguished by different choices of 
The simple algebraic forms R(W, m) enable the direct enforcement of smooth- 
ness without the need for cosily Monte-Carlo integrations of S(W, m). The new 
regularizers are shown to yield better generalization errors than weight decay 
when the implicit assumptions in the latter are wrong. Unlike weight decay, the 
new regularizers distinguish between the roles of the input and output weights 
.and capture the interactions between them. 
*Address as of September 1, 1996: Centre for Computer Architecture, University of Halmstad, 
P.O.Box 823, S-301 18 Halmstad, Sweden 
586 J. E. Moody and T. S. R6gnvaldsson 
1 Introduction: What are the right biases? 
Regularizafion is a technique for reducing prediction risk by balancing model bias and 
model variance. A regularizer It(W) imposes prior constraints on the network parameters 
W. Using squared error as the most common example, the objective functional that is 
minimized during training is 
M 
1 [y(i) _ f(W, a:(i))] 2 
E= 2M i= 
+ ,XR(W) , (1) 
where y(i) are target values corresponding to the inputs a: (i), M is the number of training 
patterns, and the regularization parameter A controls the importance of the prior constraints 
relative to the fit to the data. Several approaches can be applied to estimate ) (e.g. Eubank 
(1988) or Wahba (1990)). 
Regularization reduces model variance at the cost of some model bias. An important 
question arises: What are the right biases? (Geman, Bienenstock & Doursat 1992). A good 
choice of It(W) will result in lower expected prediction error than will a poor choice. 
Weight decay is often used effectively, but it is an ad hoc technique that controls weight 
values without regard to the function f(.). It is thus not necessarily optimal and not appro- 
priate for arbitrary function parameterizafions. It will give very different results, depending 
upon whether a function is parameterized, for example, as f(w, c) or as f(w - , c). 
Since many real world problems are intrinsically smooth, we propose that in many cases, 
an appropriate bias to impose is to favor solutions with low ruth-order curvature. Direct pe- 
nalizafion of curvature is a parametrization-independent approach. The desired regularizer 
is the standard D dimensional curvature functional of order m: 
Oa: m 
(2) 
Here II II denotes the ordinary euclidean tensor norm and 0'/0a:' denotes the mth order 
differential operator. The weighting function f/(a:) ensures that the integral converges and 
determines the region over which we require the function to be smooth. f/(a:) is not required 
to be equal to the input density p(a:), and will most often be different. 
The use of smoothing funcfionals like (2) has been extensively studied for smoothing splines 
0Eubank 1988, Hasfie & Tibshirani 1990, Wahba 1990) and for radial basis function (RBF) 
networks (Powell 1987, Poggio & Girosi 1990, Girosi, Jones & Poggio 1995). However, 
no general class of smoothing regularizers that directly enforce smoothness $(W, m) for 
projectire basis functions (PBFs), such as the widely used sigmoidal PBFs, has been 
previously proposed. 
Since explicit enforcement of smoothness using (2) requires cosfly, impractical Monte- 
Carlo integrations,  we derive algebraically-simple regularizers R(W, m) that tightly bound 
2 Derivation of Simple Regularizers from Smoothing Functionals 
We consider single hidden layer networks with D input variables, Nh nonlinear hidden 
units, and No linear output units. For clarity, we set No = 1, and drop the subscript on Nh 
I Note that (2) is not just one integral, but actually (9 (Dm ) integrals, since the norm of the operator 
om/ox m has O(D') terms. This is extremely expensive to compute for large D or large m. 
Smoothing Regularizers for Projective Basis Function Networks 587 
(the derivation is trivially extended to the case No > 1). Thus, our network function is 
N 
= + -o 
j=l 
(3) 
where #[.] are the nonlinear transfer functions of the internal hidden units, a:  R D is the 
input vector 2 , 0 5 are the parameters associated with internal unit j, and W denotes all 
parameters in the network. 
For regularizers R(W), we will derive strict upper bounds for S(W, m). We desire the 
regularizers to be as general as possible so that they can easily be applied to different 
network models. Without making any assumptions about fl(a:) or #(.), we have the upper 
bound 
2 dDxQ(a) (4) 
S(W) NZtlJ Oxm ' 
j=l 
( )2 2 We consider two possible 
which follows from the inequality 'j;=, ai <_ NE=i a i . 
options for the weighting function fl(a:). One is to require global smoothness, in which 
case fl(a:) is a very wide function that covers all relevant parts of the input space (e.g. a 
very wide gaussian distribution or a constant distribution). The other option is to require 
local smoothness, in which case fl(a:) approaches zero outside small regions around some 
reference points (e.g. the training data). 
2.1 Projective Basis Representations 
Projective basis functions (PBFs) are of the form g[Oj, a:] = g [a:'vj + vo], where Oj = 
{vd, rio }, vj = (v5 , v52,... , VdD ) is the vector of weights connecting hidden unit j to the 
inputs, and v5o is the bias, offset, or threshold. For PBFs, expression (4) simplifies to 
N 
211vllmta(W, (5) 
with 
Is(W,m)-- /d�xQ(a:) f dm#[ZS(a:)]  ,6) 
where zs( ) m rj + rS0. 
Although the most commonly used #[-]s are sigmoids, our analysis applies to many other 
forms, for example flexible fourier units, polynomials, andrational functions. 3 The classes 
of PBF transfer functions #[.] that are applicable (as determined by fl(x)) are those for 
which the integral (8) is finite and well-defined. 
2.2 Global weighting 
For the global case, we select a gaussian form for the weighting function 
= exp L ] 
2Throughout, we use small letter boldface to denote vector quantifies. 
3See for example Moody & Yarvin (1992). 
(7) 
588 J. E. Moody and T. S. Rtgnvaldsson 
and require rr to be large. Integrating out all dimensions, except the one associated with the 
projection vector v j, we are left with 
(8) 
If (d'g[z]/dz ')2 is integrable and approaches zero outside a region that is small compared 
to a, we can bound (8) by setting the exponential equal to unity. This implies 
j(w,-0 < II,,jl---- with I(m)  rrx7- dz dz---  (9) 
oo 
The bound of equation (5) then becomes 
N 
211 .112m-l 
s(w, m) < , 
j=l 
= NI(m)Ra(W, m) , 
(10) 
where the subscript (7 stands for global. Since A absorbs all constant multiplicative factors, 
we need only weigh Ra(W, m) into the training objective function. 
2.2.1 Local weighting 
For the local case, we consider weighting functions of the general form 
i=1 
(11) 
where a: (i) are a set of points, and (a: (I), a) is a function that decays rapidly for large 
- We require that lim,-,0 (a: (i), a) = J(a: - a:(i)). Thus, when the a: (i) are the 
training data points, the limiting distribution of (11) is the empirical distribution. 
In the limit a --} 0, equation (5) becomes 
211Vl12m l dmg 
j=l �-- 
(12) 
For the empirical distribution, we could compute the expression within parenthesis in (12) 
for each input pattern a: (i) during training and use it as our regularization cost. This is done 
by Bishop (1993) for the special case m = 2. However, this requires explicit design for 
each transfer function and becomes increasingly complicated as we go to higher m. To 
th 
construct a simpler and more general form, we instead assume that the m derivative of 
{dmg[z] 2 
g[.] is bounded from aboveby C;(rn) -- maxz k, az, } 
This gives the bound 
N 
211,Jll 2m 
s(w, . _< VC(m) 
j----1 
= NC;(m)R;( m) 
(13) 
for the maximum local curvature of the function (the subscript L denotes local limit). 
Smoothing Regularizers for Projective Basis Function Networks 589 
3 Empirical Example 
We have done extensive simulation studies that demonstrate the efficacy of our new reg- 
ularizers for PBF networks on a variety of problems. An account is given in Moody & 
RSgnvaldsson (1996). Here, we demonstrate the value of using smoothing regularizers on a 
simple problem which illustrates a key difference between smoothing and quadratic weight 
decay, the two dimensional bilinear function 
(14) 
This example was used by Friedman & Stuetzle (1981) to demonstrate projection pursuit 
regression. It is the simplest function with interactions between input variables. 
We fit this function with one hidden layer networks using the m = { 1,2, 3} smoothing 
regularizers, comparing the results with using weight decay. In a large set of experiments, 
we find that both the global and local smoothing regularizers with m = 2 and m = 3 
outperform weight decay. An example is shown in figure 1. The local m = 1 case performs 
poorly, which is unsurprising, given that the target function is quadratic. Weight decay 
performs poorly because it lacks any fo
