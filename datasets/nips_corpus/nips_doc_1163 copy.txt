The Generalisation Cost of RAMnets 
Richard Rohwer and Michat Morciniec 
rohwerrj �s. aston. a�. uk mor�inim�s. aston. a�. uk 
Neural Computing Research Group 
Aston University 
Aston Triangle, Birmingham B4 7ET, UK. 
Abstract 
Given unlimited computational resources, it is best to use a crite- 
rion of minimal expected generalisation error to select a model and 
determine its parameters. However, it may be worthwhile to sac- 
rifice some generalisation performance for higher learning speed. 
A method for quantifying sub-optimality is set out here, so that 
this choice can be made intelligently. Furthermore, the method 
is applicable to a broad class of models, including the ultra-fast 
memory-based methods such as RAMnets. This brings the added 
benefit of providing, for the first time, the means to analyse the 
generalisation properties of such models in a Bayesian framework. 
I Introduction 
In order to quantitatively predict the performance of methods such as the ultra-fast 
RAMnet, which are not trained by minimising a cost function, we develop a Bayesian 
formalism for estimating the generalisation cost of a wide class of algorithms. 
We consider the noisy interpolation problem, in which each output data point yi 
results from adding noise to the result y = f(x) of applying unknown function f 
to input data point x, which is generated from a distribution P (x). We follow a 
similar approach to (Zhu k Rohwer, to appear 1996) in using a Gaussian process to 
define a prior over the space of functions, so that the expected generalisation cost 
under the posterior can be determined. The optimal model is defined in terms of 
the restriction of this posterior to the subspace defined by the model. The optimum 
is easily determined for linear models over a set of basis functions. We go on to 
compute the generalisation cost (with an error bar) for all models of this class, 
which we demonstrate to include the RAMnets. 
254 R. Rohwer and M. Morciniec 
Section 2 gives a brief overview of RAMnets. Sections 3 and 4 supply the formalism 
for computing expected generalisation costs under Gaussian process priors. Numer- 
ical experiments with this formalism are presented in Section 5. Finally, we discuss 
the current limitations of this technique and future research directions in Section 6. 
2 RAMnets 
The RAMnet, or n-tuple network is a very fast 1-pass learning system that of- 
ten gives excellent results competitive with slower methods such as Radial Basis 
Function networks or Multi-layer Perceptrons (Rohwer & Morciniec, 1996). Al- 
though a semi-quantitative theory explains how these systems generalise, no formal 
framework has previously been given to precisely predict the accuracy of n-tuple 
networks. 
Essentially, a RAMnet defines a set of features which can be regarded as Boolean 
functions of the input variables. Let the a th feature of x be given by a {0, 1}-valued 
function qba(x). We will focus on the n-tuple regression network (Allinson & Kotcz, 
1995), which outputs 
E+a(x)Eyi+(xi) Eyir(x, 
a i _ i 
y= f(x)= Eq(x)Eq(xi ) - Eu(x, xi) (1) 
a i i 
'Y )}i=1' 
in response to input x, if trained on the set of N samples {X(N)Y(N)} -- {(x i i N 
Here U(x, x') -' y'}. qba(x)qba(x') can be seen to play the role of a smoothing kernel, 
provided that it turns out to have a suitable shape. It is well-know that it does, for 
appropriate choices of feature sets. The strength of this method is that the sums 
over training data can be done in one pass, producing a table containing two totals 
for each feature. Only this table is required for recognition. 
It is interesting to note that there is a familiar way to expand a kernel into the form 
U(x,x') =  q,(z)q,(z'), at least when U(z,z') = U(x - z'), if the range of  is 
not restricted to {0, 1}: an eigenfunction expansion 1. Indeed, principal component 
analysis a applied to a Gaussian with variance V shows that the smallest feature 
set for a given generalisation cost consists of the (real-valued) projections onto 
the leading eigenfunctions of V. Be that as it may, the treatment here applies to 
arbitrary feature sets. 
3 Bayesian inference with Gaussian priors 
Gaussian processes provide a diverse set of priors over function spaces. To 
avoid mathematical details of peripheral interest, let us approximate the infinite- 
dimensional space of functions by a finite-dimensional space of discretised functions, 
so that function f is replaced by high-dimensional vector f, and f(x) is replaced 
by fz, with f(x)  fz within a volume Ax around x. We develop the case of scalar 
functions f, but the generalisation to vector-valued functions is straightforward. 
In physics, this is essentially the mode function expansion of U -, the differential 
operator with Green's function U. 
2V- needs to be a compact operator for this to work in the infinite-dimensional limit. 
The Generalisation Cost of RAMnets 255 
We assume a Gaussian prior on f, with zero mean and covariance V/a: 
p(f)=(1/Za)e- fTv-f 
(2) 
where Za = det(-V)�. The overall scale of variation of f is controlled by a. 
Illustrative samples of the functions generated from various choices of covariance 
are given in (Zhu & Rohwer, to appear 1996). With qz// denoting the (possibly 
position-dependent) variance of the Gaussian output noise, the likelihood of outputs 
Y(N) given function f and inputs X(N) is 
/ E(fx ' _ yi)qf.l(fx, _ yi) 
P (y(N)IX(N), f) - (1/Z/)exp- 
i 
where Z = H -qz, = det [-Q] with Qff = qz.6ij. 
i 
Because f and X(N) are independent the joint distribution is 
P (Y(N), fiX(N)) = P (y(N)lf, X(N)) P(f) = (e�b**b+*)/(ZZ)e 
(4) 
where 6x,x, is understood to be i whenever x i is in the same cell of the discreti- 
sation as x, and Azz, = aVzz, +/Y'.q,Sz,z,6,,z,, bz = /3Y.yiq,Sz,z., and 
i i 
 _�OZyi - i 
= q, y . One can readily verify that 
$ 
(5) 
where K is the N x N matrix defined by 
(6) 
The posterior is readily determined to be 
P (fIX(N),Y(N)) = 
P (Y(N), fiX(N)) 
P (Y(N) IX(N)) 
= det [2rA] -� e - �(f-[)xA-(f-;) (7) 
where f = Ab is the posterior mean estimate of the true function f. 
4 Calculation of the expected cost and its variance 
m 
Let us define the cost of associating an output f of the model with an input x that 
actually produced an output y as 
m m 
C(f:, y) = �(f: - y)2r 
where r is a position dependent cost weight. 
256 R. Rohwer and M. Morciniec 
The average of this cost defines a cost functional, given input data X(N): 
C(f,flx(v)) = C(f, y)P (zlx(v)) P(ylx,f)dxdy. 
(8) 
This form is obtained by noting that the function f carries no information about 
the input point x, and the input data X(N) supplies no information about y beyond 
that supplied by f. The distributions in (8) are unchanged by further conditioning 
m m 
on yuv), so we could write C(f,flxuv)) = C(f,flxuv),yuv)). This cost functional 
therefore has the posterior expectation value 
(Clx(v), y(v)) = / C(, ylx(N) )P (x, Y, flX(N), Y(N)) dxdydf 
and variance 
(9) 
- / C(', �1x(v))2P (flxuv), y(v))dr- 
(10) 
Plugging in the distributions (2) (applied to a single sample), (3) and (7) leads to: 
* m � m 
(Clx(v),y(v)) =�tr [AR] + �(f- f)TR(f-- f)+ 
tr [QR] 
(11) 
where the diagonal matrices R and q have the elements R, = P(xlX)rAxS., 
and Q, = q5,. 
Similar calculations lead to the expression for the variance 
var [C(,flx(N),Y(N))] = �tr [ARAR] + tr [ARRFF] . 
(12) 
$71. * 
where the elements of F are F, = (f, - f)5,,. 
m 
Note that the RAMnet (1) has the form f = y]jyi linear in the output data 
i 
yuv), with J, = U(x,xi)/y]j U(x, xj). Let us take V to have the form V(x, x') = 
p(x)G(x - x')p(x'), combining translation-invariant and non-invariant factors in a 
plausible way. Then with the sums over x replaced by integrals, (11) becomes 
explicitly 
2 (Clx(), y(v)) 
+z 
yU / dxP (XlX(N)) rzJz,,zJzzvy v . 
(13) 
The Generalisation Cost of RAMnets 257 
1.5 
0.5 
0 
-1 
-1 
a) True, optimal and suboptimal functions 
-0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.6 
-0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 Or. 8 
,T 
350 
b) Distribution of the cost C 
0.01 0.012 0.014 0.016 0.018 
Figure 1: a) The lower figure shows the input distribution. The upper figure shows 
the true function f generated from a Gaussian prior with covariance matrix V (dot- 
ted line), the optimal function f - Ab (solid line) and the suboptimal solution 
7 
f (dashed line). b)The distribution of the cost function obtained by generating 
functions from the posterior Gaussian with covariance matrix A and calculating 
the cost according to equation 14. The mean and one standard deviation calcu- 
lated analytically and numerically are shown by the lower and upper error bars 
respectively. 
Taking P (xlx(N)) to be Gaussian (the maximum likelihood estimate would be 
reasonable) and p, r, and q uniform, the first four integrals are straightforward. 
The latter two involve the model J, and were evaluated numerically in the work 
reported below. 
5 Numerical results 
We present one numerical example to illustrate the formalism, and another to illus- 
trate its application. 
For the first illustration, let the input and output variables be one dimensional real 
numbers. Let the input distribution P(x) be a Gaussian with mean/tx = 0 and 
standard deviation a = 0.2. Nearly all inputs then fall within the range [-1, 1], 
which we uniformly quantise into 41 bins. The true function f is generated from a 
Gaussian distribution with tf = 0 and 41 x 41 covariance matrix V with elements 
V, - e-I-'l. 50 training inputs x were generated from the input distribution 
and assigned corresponding outputs y = �x + e, where e is Gaussian noise with zero 
mean and standard deviation x/r// _= 0.01. The cost weight rx = 1. 
The inputs were thermometer coded 3 over 256 bits, from which 100 subsets of 30 
bits were randomly selected. Each of the 100 x 230 patterns formed over these 
bits defines a RAMnet feature which evaluat
