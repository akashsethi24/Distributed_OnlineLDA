Removing Noise in On-Line Search using 
Adaptive Batch Sizes 
Genevieve B. Orr 
Department of Computer Science 
Willamette University 
900 State Street 
Salem, Oregon 97301 
gorr)willamette. edu 
Abstract 
Stochastic (on-line) learning can be faster than batch learning. 
Howevex', at late times, the learning rate must be annealed to re- 
move the noise present in the stochastic weight updates. In this 
annealing phase, tile convergence rate (in mean square) is at best 
proportional to 1/- where - is the number of input presentations. 
An alternative is to increase the batch size to remove the noise. In 
this paper we explore convergence for LMS using 1) small but fixed 
batch sizes and 2) an adaptive batch size. We show that the best 
adaptive batch schedule is exponential and has a rate of conver- 
gence which is the same as for annealing, i.e., at best proportional 
to 
I Introduction 
Stochastic (on-line) learning can speed learning over its batch training particularly 
when data sets are large and contain redundant information [M193]. However, at 
late times in learning, noise present in the weight updates prevents complete conver- 
gence fi'om taking place. To reduce the noise, the learning rate is slowly decreased 
(annealed} at late times. The optimal annealing schedule is asymptotically propor- 
tional to  xvhere t is tile iteration [Go187, LO93, Orr95]. This results in a rate of 
convergence (in mean square) that is also proportional to - 
t' 
An alternative method of reducing tile noise is to simply switch to (noiseless) batch 
mode when the noise regime is reached. Howevex', since batch mode can be slow, 
a better idea is to slowly increase the batch size starting with 1 (pure stochastic) 
and slowly increasing it only as needed until it reaches the training set size (pure 
batd). In this paper we 1) investigate the convergence behavior of LMS when 
Removing Noise in On-Line Search using Adaptive Batch Sizes 233 
using sxnall fixed batch sizes, 2) determine the best schedule when using an adaptive 
batch size at each iteration, 3) analyze the convergence behavior of the adaptive 
batch algorithm, and 4) compare this convergence rate to the alternative method 
of annealing the learning rate. 
Other authors have approached the problem of redundant data by also proposing 
tedmiques fox' training on subsets of the data. For example, Pluto [PW93] uses 
active data selection to choose a concise subset for training. This subset is slowly 
added to over time as needed. Moller [M�193] proposes combining scaled conjugate 
gradient descent (SCG) with what he refers to as blocksize updating. His algorithm 
uses an iterative approach and assumes that tile block size does not vary rapidly 
during training. In this paper, we take tile simpler approach of .just choosing ex- 
emplars at random at each iteration. Given this, we then analyze in detail the 
convergence behavior. Our results are more of theoretical than practical interest 
since the equations we derive are complex functions of quantities such as the Hessian 
that are impractical to compute. 
2 Fixed Batch Size 
In this section we examine the convergence behavior fox' LMS using a fixed batch 
size. We assrune that we are given a large but finite sized training set T -_- {zi --- 
(xi, d)}i= wilere x  T  is the i th input and di   is the corresponding target. 
We further assume that the targets are generated using a signal plus noise model 
so that we can write 
di =WXi +6i (1) 
where w. 6  is the optimal weight vector and the ei is zero me noise. Since 
the training set is sumed to be large we take the average of ei and xi6i over the 
training set to be approximately zero. Note that we consider only the problem of 
optimization of the w over the training set and do not address the issue of obtaining 
good generalization over the distribution from whi( the training set w drawn. 
At each iteration, we sume that exactly n samples are randomly drawn without 
replacement froIn T where 1  n  N. We denote this batch of size n drawn at 
time t by Bn(t)  {z}= 1. When n = 1 we have pure on-line training and when 
n. = N we have pm'e batch. XVe choose to sample without replacement so that  
the batch size is increded, we have a smooth transition from on-line to batch. 
For LMS, the squared error at iteration t for a batch of size n is 
i  wt xi) (2) 
z.B,(t) 
and where ct  T4  is tile current weight in the network. The update weight 
a?XP- wilere p is the fixed learning rate. Rewriting 
equation is then t+l = t --  Owt 
this in terms of the weight error v   - w. and defining g,t  OS()/Ov we 
obtain 
= ,., + - - = - (3) 
z,6B 
Convergence (in mean square) to c'. can be characterized by the rate of change of 
the average squared norm of the weight error E[v ] where v 2 _= vTv. lrom (3) we 
obtain an expression fox' vt2+ in terms of vt, 
Ut2q_l -- Vt 2 2p ,T p2 2 
- - v g,e.,, + g,e.,' (4) 
234 G. B. Orr 
To compute the expected value of vt2+ conditioned on vt we can average the right 
side of (4) over all possible ways that the batch B,(t) can be chosen from the N 
training examples. In appendix A, we show that 
(sB,,). = (s,t)N (5) 
(})B = :v-  2 (- 1):v 
where (.). denotes average over all examples in the training set, (.) denotes average 
over the possible batches drawn at time t, and gi,t  OS(zi)/Ovt. The averages over 
the entire training set are 
1  O(zi) 
�,,) =  .  - - ) ; i, - ii = n, (7) 
=1 i=1 
N 
1 
i=1 
2 
where R  (xx), S  (xxxx) , a  (e 2) d ( R) is the trace of 
These equations together with (5) and (6) in (4) gives the expected value of 
conditioned on v, 
(vt2+lvt)=vtT{i_2pR+p2 (N(n-1)R2 N-n )} p2a2(TrR)(N-n) 
Y-' i2 + (: i. s ' + .(N - 1) 
Note that this reduces to the standard stochastic and batch update equations when 
r, = 1 and r, = N, respectively. 
2.0.1 Special Cases: 1-D solution and Spherically Symmetric 
In 1-dimension we can average over vt in (9) to give 
where 
a = l - 2ftR + ft 2 f N(r- l) Ra N-n _X 
(10) 
7(:v- .) 
: n(N- 1) (11) 
and where R and $ sinplify to R = (x2)., $ = (x4)N. This is a difference equation 
which can be solved exactly to give 
1 - a t-to 
<vta>=a'-t�<v>+ 1--a  (12) 
where (v') is the expected squared weight error at the initial time to. 
Figure la conpares equation (12) with simulations of 1-D LMS with gaussi inputs 
for N = 1000 md batch sizes n =10, 100, and 500. As  be seen, the agreement is 
good. Note that {v 2) decre,es exponentially until flattening out. The equihbrium 
value can be computed from (12) by setting t =  (suming ]a] < 1) to give 
, (-.) 
<'v2> -- 1--rt- 2Rn(N-1)-p(N(n-1)R2+(N-n)S) ' (13) 
Note that (v2) decre,es  n incre, es and is zero only if n = N. 
For real zero mean gaussian inputs, we can apply the gaussi moment factoring 
theorem [Hv91] which states that (xlxxxt) = RORm + RiRit + R, Ri where the 
subscripts on x denote components of x. From this, we find that S = (Trace R)R + 2R 2. 
Removing Noise in On-Line Search using Adaptive Batch Sizes 235 
E[ v^2 ] 
0.1 
0.01 
0.001; 
10 20 
(a) o.00o 
n=l 0 
0.01 
30 _ --60--.. t 0.001 
n=500 (b) 
E[ v^2 ] 
1' n=10 
0,1. 
0 '5;300 1 oof)o 1 .(3('  POOO0 
Figure 1: Simulations(solid) vs Theoretical (dashed) predictions of the squared weight 
error of 1-D LMS a) as function of the number of t, the batch updates (iterations), and 
b) as function of the number of input presentations, r. Training set size is N = 1000 and 
batch sizes are n --10, 100, and 500. Inputs were gaussian with R -- 1, er = 1 and/ - .1. 
Simulations used 10000 networks 
Equation (9) can also be solved exactly in multiple dimensions in the rather restric- 
tive case where we assume that the inputs are spherically symmetric gaussians with 
R = aI where a is a constant, I is the identity matrix, and m is the dimension. 
The update equation and solution are the same as (10) and (12), respectively, but 
where c and fi are now 
c, = l - 2ta + tfi a 2 IN(n-l) 
U2a2ma(N - n) 
N-n (m+2) fi-- (14) 
+ (N- 1)n ' n(N- 1) 
3 Adaptive Batch Size 
The tine it takes to compute the weight update in one iteration is roughly propor- 
tional to the number of input presentations, i.e the batch size. To make the compar- 
ison of convergence rates for different batch sizes meaningful, we must compute the 
change in squared weight error as a function of the number of input presentations, 
r, rather than iteration number t. 
For fixed batch size, r = nt. Figure lb displays our 1-D LMS simulations plotted as 
a function of r. As can be seen, training with a large batch size is slow but results 
in a lower equilibrium value than obtained with a small batch size.. This suggests 
that we could obtain the fastest decrease of (v 2) overall by varying the batch size at 
each iteration. The batch size to choose for the current (v 2) would be the smallest 
n that has yet to reach equilibrium, i.e. for which (v 2) > (v2)o. 
To determine the best batch size, we take the greedy approach by demanding that 
at each iteration the batch size is chosen so as to reduce the weight error at the 
next iteration by the greatest amount per input presentation. This is equivalent to 
asking what value of n naximizes h _= (('v)- (vt2+l))/n? Once we determine n ve 
then express it as a function of r. 
We treat the 1-D case, although the analysis would be similar for the spherically 
symmetric case. From (10)we have h = -} ((rt - 1)(vt 2) + fi). Differentiating h with 
respect to n and solving yields the batch size that decreases the weight error the 
most to be 
nt = min N, (2R(N- 1) + la(S- NR2))(v) + laaR ' (15) 
We have n, exactly equal to N when the current value of (vt 2) satisfies 
�7) < 7 = 2(- ) - ,((X-- 2)- S) (' = )' 
(16) 
236 G. B. Orr 
E[ v^2l 
1 -, Adaptive Batch 
-'-...'.,. -- theory 
O.Ol 
o.ool 
(a) o lO o o .o o o ,o (b) ' ' 
n 
1 oo. . Batc ' 
50.
