REMARKS ON INTERPOLATION AND 
RECOGNITION USING NEURAL NETS 
Eduardo D. Sontag* 
SYCON - Center for Systems and Control 
Rutgers University 
New Brunswick, NJ 08903 
Abstract 
We consider different types of single-hidden-layer feedforward nets: with 
or without direct input to output connections, and using either thresh- 
old or sigmoidal activation functions. The main results show that direct 
connections in threshold nets double the recognition but not the interpo- 
lation power, while using sigmoids rather than thresholds allows (at least) 
doubling both. Various results are also given on VC dimension and other 
measures of recognition capabilities. 
1 INTRODUCTION 
In this work we continue to develop the theme of comparing threshold and sigmoidal 
feedforward nets. In (Sontag and Sussmann, 1989) we showed that the general- 
ized delta rule (backpropagation) can give rise to pathological behavior -namely, 
the existence of spurious local minima even when no hidden neurons are used,- 
in contrast to the situation that holds for threshold nets. On the other hand, in 
(Sontag and Sussmann, 1989) we remarked that provided that the right variant be 
used, separable sets do give rise to globally convergent backpropagation, in com- 
plete analogy to the classical perceptton learning theorem. These results and those 
obtained by other authors probably settle most general questions about the case of 
no hidden units, so the next step is to look at the case of single hidden layers. In 
(Sontag, 1989) we announced the fact that sigmoidal activations (at least) double 
recognition power. Here we provide details, and we make several further remarks 
on this as well as on the topic of interpolation. 
Nets with one hidden layer are known to be in principle sufficient for arbitrary 
recognition tasks. This follows from the approximation theorems proved by various 
*E-mail: sontag@hilbert.rutgers.edu 
940 Sontag 
authors: (Funahashi, 1988), (Cybenko,1989), and (Hornik et. al., 1989). However, 
what is far less clear is how many neurons are needed for achieving a given recog- 
nition, interpolation, or approximation objective. This is of importance both in its 
practical aspects (having rough estimates of how many neurons will be needed is es- 
sential when applying backpropagation) and in evaluating generalization properties 
(larger nets tend to lead to poorer generalization). It is known and easy to prove 
(see for instance (Arai, 1989), (Chester, 1990)) that one can basically interpolate 
values at any n + 1 points using an n-neuron net, and in particular that any n + 1- 
point set can be dichotomized by such nets. Among other facts, we point out here 
that allowing direct input to output connections permits doubling the recognition 
power to 2n, and the same result is achieved if sigmoidal neurons are used but such 
direct connections are not allowed. Further, we remark that approximate interpo- 
lation of 2n - 1 points is also possible, provided that sigmoidal units be employed 
(but direct connections in threshold nets do not suffice). 
The dimension of the input space (that is, the number of input units) can influ- 
ence the number of neurons needed, are least for dichotomy problems for suitably 
chosen sets. In particular, Baum had shown some time back (Baum, 1988) that 
the VC dimension of threshold nets with a fixed number of hidden units is at least 
proportional to this dimension. We give lower bounds, in dimension two, at least 
doubling the VC dimension if sigmoids or direct connections are allowed. 
Lack of space precludes the inclusion of proofs; references to technical reports are 
given as appropriate. A full-length version of this paper is also available from the 
author. 
2 DICHOTOMIES 
The first few definitions are standard. Let N be a positive integer. A dichotomy 
or two-coloring (S_, S+) on a set S C R N is a partition S = S_ [.J S+ of S into two 
disjoint subsets. A function f: R S - R will be said to implement this dichotomy 
if it holds that 
f(u) > O for u & $+ and f(u) < O for u & $_ 
Let jr be a class of functions from R v to R, assumed to be nontrivial, in the sense 
that for each point u  IR v there is some fz  jr so that fz(u) > 0 and some f2  jr 
so that f2 (u) < 0. This class shatters the set $ C_ R v if each dichotomy on $ can 
be implemented by some f  jr. 
Here we consider, for any class of functions jr as above, the following measures of 
classification power. First we introduce  and/z, dealing with best and worst 
cases respectively: (jr) denotes the largest integer I _> 1 (possibly oc>) so that there 
is at least some set $ of cardinality I in R S which can be shattered by jr, while 
/(jr) is the largest integer l _> 1 (possibly c) so that every set of cardinality I can 
be shattered by jr. Note that by definition,/(jr) _< (jr) for every class jr. 
In particular, the definitions imply that no set of cardinality (jr) + i can be 
shattered, and that there is at least some set of cardinality/(jr) + 1 which cannot be 
shattered. The integer  is usually called the Vapnik-Chervonenkis (VC) dimension 
of the class jr (see for instance (Saum, 1988)), and appears in formalizations of 
learning in the distribution-free sense. 
Remarks on Interpolation and Recognition Using Neural Nets 941 
A set may fail to be shattered by . because it is very special (see the example 
below with colinear points). In that sense, a more robust measure is useful: p() 
is the largest integer I >_ 1 (possibly cx) for which the class of sets S that can be 
shattered by  is dense, in the sense that given every/-element set S = {sx,..., st} 
there are points gi arbitrarily close to the respective si's such that  = {x,...,} 
can be shattered by 5. Note that 
<_ <_ 
for all/F. 
To obtain an upper bound rn for p(jr) one needs to exhibit an open class of sets of 
cardinality rn + 1 none of which can be shattered. 
Take as an example the class/F consisting of all arline functions f(a) = aa + by + c 
on 1 2. Since any three points can be shattered by an arline map provided that they 
are not colinear (just choose a line aa + by + c = 0 that separates any point which 
is colored different from the rest), it follows that 3 < p. On the other hand, no set 
of four points can ever be dichotomized, which implies that  < 3 and therefore the 
conclusion p =  = 3 for this class. (The negative statement can be verified by a 
case by case analysis: if the four points form the vertices of a 4-gon color them in 
XOtL fashion, alternate vertices of the same color; if 3 form a triangle and the 
remaining one is inside, color the extreme points differently from the remaining one; 
if all colinear then use an alternating coloring). Finally, since there is some set of 3 
points which cannot be dichotomized (any set of three colinear points is like this), 
but every set of two can, _p = 2 . 
We shall say that . is robust if whenever S can be shattered by . also every small 
enough perturbation of S can be shattered. For a robust class and I = p(), every 
set in an open dense subset in the above topology, i.e. almost every set of I elements, 
can be shattered. 
3 NETS 
We define a neural net as a function of a certain type, corresponding to the idea 
of feedforward interconnections, via additive links, of neurons each of which has a 
scalar response or activation function O. 
Definition 3.1 Let 0 : 1 - 1 be any function. A function f : 1 N - 1 is 
a single-hidden-layer neural net with k hidden neurons of type 0 and N inputs, 
or just a (k, O)-net, if there are real numbers w0, wx,..., wk, r,..., rk and vectors 
v0, v,..., v 6 1 v such that, for all u 6 1 v, 
f(u) = w0 + v0., + w, (2) 
i=1 
where the dot indicates inner product. A net with no direct i/o connections is one 
for which v0 = 0. 
For fixed 0, and under mild assumptions on 0, such neural nets can be used to 
approximate uniformly arbitrary continuous functions on compacts. In particular, 
they can be used to implement arbitrary dichotomies. 
942 Sontag 
In neural net practice, one often takes 0 to be the standard sigmoid a(x) = x 
1 +e_------- 7 or 
equivalently, up to translations and change of coordinates, the hyperbolic tangent 
tanh(x). Another usual choice is the hardlimiter, threshold, or Heaviside function 
{ ifz<O 
7(z) = if z > 0 
which can be approximated well by a(-rx) when the gain -), is large. Yet another 
possibility is the use of the piecewise linear function 
{ -1 if x_<-i 
r(x) = if x _> 1 
x otherwise. 
Most analysis has been done for 7/and no direct connections, but numerical tech- 
niques typically use the standard sigmoid (or equivalently tanh). The activation 
r will be useful as an example for which sharper bounds can be obtained. The 
examples a and r, but not 7'{, are particular cases of the following more general 
type of activation function: 
Definition 3.2 A function 0: IR - IR will be called a sigmoid if these two prop- 
erties hold: 
(Sl) t+ := limx-+oo 0(z) and t_ := limx._oo 0(z) exist, and t+ 
(S2) There is some point c such that 0 is differenttable at c and O'(c) = I  O. 
All the examples above lead to robust classes, in the sense defined earlier. More 
precisely, assume that 0 is continuous except for at most tintrely many points z, 
and it is left continuous at such z, and let $c be the class of (k, 0)-nets, for any 
fixed k. Then jr is robust, and the same statement holds for nets with no direct 
connections. 
4 CLASSIFICATION RESULTS 
We let I(k,O,N) denote/(), where  is the class of (k,O)-nets in IR N with no 
direct connections, and similarly for _ and , and a superscript d is used for the 
class of arbitrary such nets (with possible direct connections from input to output). 
The lower measure E is independent of dimension: 
Lemma 4.1 For each k,O,N, (k,O,N)=/(k, 0, 1) and _a(k,O,N) = _a(k,O, 1). 
This justifies denoting these quantities just as p(k,0) and _d(k
