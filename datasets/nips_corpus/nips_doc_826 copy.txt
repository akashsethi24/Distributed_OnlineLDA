Cross-Validation Estimates IMSE 
Mark Plutowski t, 
Shinichi Sakata  
Halbert White 
Department of Computer Science and Engineering 
 Department of Economics 
, Institute for Neural Computation 
University of Clifornia, San Diego 
Abstract 
Integrated Mean Squared Error (IMSE) is a version of the usual 
mean squared error criterion, averaged over all possible training 
sets of a given size. If it could be observed, it could be used 
to determine optimal network complexity or optimal data sub- 
sets for efficient training. We show that two common methods of 
cross-validating average squared error deliver unbiased estimates 
of IMSE, converging to IMSE with probability one. These esti- 
mates thus make possible approximate IMSE-based choice of net- 
work complexity. We also show that two variants of cross validation 
measure provide unbiased IMSE-based estimates potentially useful 
for selecting optimal data subsets. 
I Summary 
To begin, assume we are given a fixed network architecture. (We dispense with 
this assumption later.) Let z N denote a given set of N training examples. Let 
QN(z N) denote the expected squared error (the expectation taken over all possible 
examples) of the network after being trained on z N. This measures the quality of 
fit afforded by training on a given set of N examples. 
Let IMSEN denote the Integrated Mean Squared Error for training sets of size 
N. Given reasonable assumptions, it is straightforward to show that IMSEN ---- 
E[QN(ZN)] -- o '2, where the expectation is now over all training sets of size N, Z N 
is a random training set of size N, and a2 is the noise variance. 
Let Cs '- CN(z N) denote the delete-one cross-validation squared error measure 
for a network trained on z N. Cs is obtained by training networks on each of the N 
training sets of size N- 1 obtained by deleting a single example; the measure follows 
391 
392 Plutowski, Sakata, and White 
by computing squared error for the corresponding deleted example and averaging the 
results. Let GN, M = GN, M(Z N, M) denote the generalization measure obtained 
by separating the available data of size N + M into a training set z N of size N, and 
a validation (test) set M of size M; the measure follows by training on z N and 
computing averaged squared error over M. 
We show that CN is an unbiased estimator of E[QN_x(zN)], and hence, estimates 
IMSEN_x up to noise variance. Similarly, GN, M is an unbiased estimator of 
E[QN(ZN,M]. Given reasonable conditions on the estimator and on the data 
generating process we demonstrate convergence with probability I of GN, M and 
aN to E[QN(ZN)] as N and M grow large. 
A direct consequence of these results is that when choice is restricted to a set of 
network architectures whose complexity is bounded above a priori, then choosing 
the architecture for which either Cs (or CS, M) is minimized leads to choice of 
the network for which IMSEN is nearly minimized for all N (respectively, N, M) 
sufficiently large. 
We also provide results for training sets sampled at particular inputs. Conditional 
IMSE is an appealing criterion for evaluating a particular choice of training set in 
the presence of noise. These results demonstrate that delete-one cross-validation es- 
timates average MSE (the average taken over the given set of inputs,) and that hold- 
out set cross-validation gives an unbiased estimate of E[QN(ZN)I Zv = (z N, yN)], 
given a set of N input values z N for which corresponding (random) output values 
yN are obtained. Either cross-validation measure can therefore be used to select 
a representative subset of the entire dataset that can be used for data compaction, 
or for more efficient training (as training can be faster on smaller datasets) [4]. 
2 Definitions 
2.1 Learning Task 
We consider the learning task of determining the relationship between a random 
vector X and a random scalar Y, where X takes values in a subset X of r, and Y 
takes values in a subset Y of g. (e.g. X = r and Y = g). We refer to X as the 
input space. The learning task is thus one of training a neural network with r inputs 
and one output. It is straightforward to extend the following analysis to networks 
with multiple targets. We make the following assumption on the observations to be 
used in the training of the networks. 
Assumption I X is a Borel subset of r and Y is a Borel subset of . Let Z = 
X x Y and f2 _-- Z �O =- xxZ. Let (f2,',7 ) be a probability space with .T = 
The observations on Z -- (X ,Y) to be used in the training of the network are a 
realization of an i.i.d. stochastic process {Zi -= (X, Y/): f2 - X x Y}. 
When w 6 f2 is fixed, we write zi = Zi(w) for each i = 1, 2, .... Also write Z v = 
(Zx,..., Zv) and z  -- (zx,..., z). 
Assumption 1 allows uncertainty caused by measurement errors of observations 
as well as a probabilistic relationship between X and Y . It, however, does not 
prevent a deterministic relation ship between X and Y such that Y = g(X) for 
some measurable mapping g � Rr __ R. 
Cross-Validation Estimates IMSE 393 
We suppose interest attaches to the conditional expectation of Y given X, written 
g(z) = E(YIX ). The next assumption guarantees the existence of E(YilXi) and 
E(eilXi), ei -= Yi - E(YilXi). Next, for convenience, we assume homoscedasticity 
of the conditional variance of Y/ given Xi. 
Assumption 2 E(Y 2) < cx. 
Assumption 3 E(qlXx) - where  is a strictly positive constant. 
2.2 Network Model 
Let fP(., .) � X x Wp -- y be a network function with the weight space Wp, 
where p denotes the dimension of the weight space (the number of weights.) We 
impose some mild conditions on the network architecture. 
Assumption 4 For each p  {1,2,...,fi}, i6  N, Wp is a compact subset ofp, 
and fP � X x Wp --  satisfies the following conditions: 
1. if(., w) � X - Y is measurable for each w  Wp ; 
�. if(z, .)' W r -- Y is continuous for all z  X. 
We further make a joint assumption on the underlying data generating process 
and the network architecture to assure that the training dataset and the networks 
behaves appropriately. 
Assumption 5 There ezists a function D � X - R+ = [0, c) such that for each 
z  X and w  We, Ire(z, w)l _< D(z), and E [(D(X)) ] < c. 
Hence, fP is square integrable for each w p  W p. We will measure network per- 
formance using mean squared error, which for weights w p is given by .(wP;p)  
E [(Y - fP(X, wp)2]. The optimal weights are the weights that minimize .(wP;p). 
The set of all optimal weights are given by W p* =- {w*  Wp � A(w*;p) _ 
A(w;p) for any w 6 Wp}. The index of the best network is p*, given by the smallest 
p minimizing minopEwp .(wP;p), p  {1,2,...,}. 
2.3 Least-Squares Estimator 
When assumptions 1 and 4 hold, the nonlinear least-squares estimator exists. 
mally, we have 
For- 
Lemma I Suppose that Assumptions I and d hold. Then 1. For each N 6 N, 
there ezists a measurable function 1N(.;p) ' Z N -- W e such that 1N(ZN;p) solves 
� N-XEiN=x(yi_f(Xi,w))  
the following problem with probability one: mmoEW p � 
�. A(.;p) � Wp -- R is continuous on Wp, and W p* is not empty. 
For convenience, we also define bv � 9 -- We by bv(co ) = 1N(ZN (CO); p) for 
each co  9. Next let i,ie,...,iv be distinct natural numbers and let ,v = 
� ., ir). Then lv(, v) given above solves N Ej=x( ij - f(Xij, wp))2 with 
Z ' y. 
probability one. In particular, we will consider the estimate using the dataset z_i 
made by deleting the ith observation from z v. Let Z i be a random matrix made 
394 Plutowski, Sakata, and White 
by deleting the ith row from Z N. Thus, IN-1 (z_Ni;p) is a measurable least squares 
estimator and we can consider its probabilistic behavior. 
3 Integrated Mean Squared Error 
Integrated Mean Squared Error (IMSE) has been used to regulate network complex- 
ity [9]. Another (conditional) version of IMSE is used as a criterion for evaluating 
training examples [5, 6, 7, 8]. The first version depends only on the sample size, 
not the particular sample. The second (conditional) version depends additionally 
upon the observed location of the examples in the input space. 
3.1 Unconditional IMSE 
The (unconditional) mean squared error (MSE) of the network output at a partic- 
ular input value  is 
MN(i:;p) = E [{g(i) - f(i:,lN(ZN;p))}2] . (1) 
Integrating MSE over all possible inputs gives the unconditional IMSE: 
= f 
(2) 
= E (3) 
where p is the input distribution. 
3.2 Conditional IMSE 
To evaluate exemplars obtained at inputs :r N, we modify Equation (1) by condi- 
tioning on z v, giving 
Mv(lzV;p) - E [{g(5:)- f(i:,lv(Zv))}2lXV - 
The conditional IMSE (given inputs z N) is then 
IMSEN(xN;p) -- f mN(zlzN;p)p(dz) (4) 
= E (5) 
4 Cross-Validation 
Cross-validatory measures have been used successfully to assess the performance of 
a wide range of estimators [10, 11, 12, 13, 14, 15]. Cross-validatory measures have 
been derived for various performance criteria, including the Kullback-Liebler Infor- 
mation Criterion (KLIC) and the Integrated Squared Error (ISE, asymptotically 
equivalent to IMSE) [16]. Although provably inappropriate in certain applications 
[17, 18], optimality and consistency results for the cross-validatory measures have 
been obtained for several estimators, including linear regression, orthogonal series, 
splines, histograms, and kernel density estimators [16, 19, 20, 21, 22, 23, 24]. The 
authors are not aware of similar results applicable to neural networks, although two 
more general, but weaker results do apply [26]. A general result applicable to neural 
networks shows asymptotic equivalence between cross-validation and Akaike's Cri- 
terion for network selection [25, 29], as well as between cross-validation and Moody's 
Criterion [30, 29]. 
Cross-Validation Estimates IMSE 395 
4.1 Expected Network Error 
Given our assumptions, we can relate cross-validation to IMSE. For clarity and 
notational convenience, we 
