Learning from Dyadic Data 
Thomas Hofmann*, Jan Puzicha +, Michael I. Jordan* 
* Center for Biological and Computational Learning, M.I.T 
Cambridge, MA, {holmann, jordan}@ai.mit.edu 
+ Institut ffir Informatik III, Universitat Bonn, Germany, jan@cs.uni-bonn.de 
Abstract 
Dyadzc data refers to a domain with two finite sets of objects in 
which observations are made for dyads, i.e., pairs with one element 
from either set. This type of data arises naturally in many ap- 
plication ranging from computational linguistics and information 
retrieval to preference analysis and computer vision. In this paper, 
we present a systematic, domain-independent framework of learn- 
ing from dyadic data by statistical mixture models. Our approach 
covers different models with fiat and hierarchical latent class struc- 
tures. We propose an annealed version of the standard EM algo- 
rithm for model fitting which is empirically evaluated on a variety 
of data sets from different domains. 
I Introduction 
Over the past decade learning from data has become a highly active field of re- 
search distributed over many disciplines like pattern recognition, neural compu- 
tation, statistics, machine learning, and data mining. Most domain-independent 
learning architectures as well as the underlying theories of learning have been fo- 
cusing on a feature-based data representation by vectors in an Euclidean space. For 
this restricted case substantial progress has been achieved. However, a variety of 
important problems does not fit into this setting and far less advances have been 
made for data types based on different representations. 
In this paper, we will present a general framework for unsupervised learning from 
dladic data. The notion dyadic refers to a domain with two (abstract) sets of ob- 
jects, X = {x,..., xN} and 3/= {y,..., YM} in which observations $ are made for 
dlads (xi, y). In the simplest case - on which we focus - an elementary observation 
consists just of (x, y) itself, i.e., a co-occurrence of x and y, while other cases 
may also provide a scalar value wi (strength of preference or association). Some ex- 
emplary application areas are: (i) Computational linguistics with the corpus-based 
statistical analysis of word co-occurrences with applications in language modeling, 
word clustering, word sense disambiguation, and thesaurus construction. (it) Text- 
based znformatzon retrieval, where ,� may correspond to a document collection, 3/ 
Learning from Dyadic Data 467 
to keywords, and (xi, y) would represent the occurrence of a term y. in a document 
:i. (iii) Modeling of preference and consumption behavior by identifying X with in- 
dividuals and 3/' with objects or stimuli as in collaborative filtering. (iv) Computer 
vston, in particular in the context of image segmentation, where X corresponds to 
image locations, 32 to discretized or categorical feature values, and a dyad (xi, y) 
represents a fea[ure yk observed at a par[icular location 
2 Mixture Models for Dyadic Data 
Across different domains there are at least two tasks which play a fundamental role 
in unsupervised learning from dyadic data: (i) probabilistic modeling, i.e., learning 
a joint or conditional probability model over � x 32, and (it) structure discovery, e.g., 
identifying clusters and data hierarchies. The key problem in probabilistic modeling 
is the data sparseress: How can probabilities for rarely observed or even unobserved 
co-occurrences be reliably estimated? As an answer we propose a model-based ap- 
proach and formulate latent class or mixture models. The latter have the further 
advantage to offer a unifying method for probabilistic modeling and structure dis- 
covery. There are at least three (four, if both variants in (it) are counted) different 
ways of defining latent class models: 
i. The most direct way is to introduce an (unobserved) mapping c: � x 32 - 
{c,..., CK} that partitions � x 32 into I: classes. This type of model is 
called aspect-based and the pre-image c-l(c) is referred to as an aspect. 
it. Alternatively, a class can be defined as a subset of one of the spaces � (or 32 
by symmetry, yielding a different model), i.e., c :X - {el,..., CK} which 
induces a unique partitioning on � x 32 by c(x, y) -- c(:c). This model is 
referred to as one-szded clustering and C-i(Cc) __ ,J is called a cluster. 
iii. If latent classes are defined for both sets, c � X -. {c,...,c}} and c � 
32 - {cY,...,el}, respectively, this induces a mapping c which is a Ix:. L- 
partitioning of � x 32. This model is called two-sided clustering. 
2.1 Aspect Model for Dyadic Data 
In order to specify an aspect model we make the assumption that all co-occurrences 
in the sample set $ are i.i.d. and that xi and y are conditionally independent gqven 
the class. With parameters P(x Ic), P(y Ic) for the class-conditional distributions 
and prior probabilities P(c) the complete data probability can be written as 
P($, c)-  [P(ci)P(milci)(ylci)] (') , (1) 
i,k 
where n(i, y) are the empirical counts for dyads in  and ci  c(i, y). By 
summing over the latent variables c the usual mixture formulation is obtained 
P(S) :  P(xi,y) ('), where P(xi,y) :  P(c)P(xilc)P(ylc) (2) 
i,k  
Following the standard Expectation Maximization approach for maximum likelihood 
,stimation [Dempster ctal.. 1977], the E-step equations for the class posterior prob- 
abilities are given by 1 
: . (3) 
1in the case of multiple observations of dyads it has been assumed that each observation 
may have a different latent class. If only one latent class variable is introduced for each 
dyad, slightly different equations are obtMned. 
468 T. Hofmann, J. Puzicha and M. I. Jordan 
! two 0.18 went 0.10 Ihave 0.381 shalt 0.18 the 0.95 I he 0.51 I I<.> 0.521 Ithee 0.041 
: llh9:ia!'l seven 0.10 go 0.08 I hath 0.221 hast 0.08 his 0.006 I god 0.081 I <:> 0.161 I me 0.03 I 
! t-_, I,-,  three 0.10 come 0.04 I had 0.11 I wilt 0.08 myO.005 llordO.051 I<,> 0.141 lhimO.031 
: x.. I '-'Ct t four 0.06 came 0.04 I hast 0.091 art 0.07 our 0.003 land 0.041 I<;> 0.071 I it 0.02 I 
'! five 0.06 brought 0.03 I be 0.021 if 0.05 thyO.003 0.0 I I< O I l YouO.021 
! . years 0.11 up 0.40 ! not 0.04 I thou 0.85 lord 0.09 [hath 0.14 ! land 0.331 I<?> 0.2_7 I 
: i/ :housand 0.1 down 0.17 Idone o.o41 not 0.01 :hildren 0.0 shall 0.071 I for 0.081 I<, > o.zl 
! -- �-  - x hundred 0.1 forth 0.15 Iiven 0.03l also 0.004 son 0.02 Isaid 0.051 I but 0.071 I <.> 0.121 
:z' klC:Ct, days 0.07 out O.09 ade 0.03] indeed 0.003 land 0.02 I is 0.04 I [then 0.051 I<:> 0.061 
:: .................... .c.u.?...0:.0.. ....... !:..0:.0} ...... !?..:.0.:07.!... :.o):.t.9:0..0  .. 2.?1...o:.o.2....i.w...o:.l..j..s.o..o:.o.z..i...[<.?...o:.! 
Figure 1' Some aspects of the Bible (bigrams). 
It is straightforward to derive the M-step re-estimation formulae 
P(c)cr yn(xi,y)P{ci:c), P(xilc)crEn(xi,y)P{ci:c}, (4) 
i,k k 
and an analogous equation for P(ylca). By re-parameterization the aspect model 
can also be characterized by a cross-entropy criterion. Moreover, formal equiva- 
lence to the aggregate Markov model, independently proposed for language model- 
ing in [Saul, Peteira, 1997], has been established (cf. [Hofmann, Puzicha, 1998] for 
details). 
2.2 One-Sided Clustering Model 
The complete data model proposed for the one-sided clustering model is 
P(,-q,c)= P(c)P(,-qlc)= (HP(c(xi))) (H[P(xi)P(Y'C(Xi))](:')) , (5) 
i i,k 
where we have made the assumption that observations (xi, y) for a particular xi 
are conditionally independent given c(xi). This effectively defines the mixture 
?($) - II ?(&), P(&): Y P(c) II [?()P(ylc)]�'v) , (6) 
i a k 
where & are all observations involving zi. Notice that co-occurrences in & are not 
independent (as they are in the aspect model), but get coupled by the (shared) 
latent variable c(zi). As before, it is straightforward to derive an EM algorithm 
with update equations 
P{c(x,)-c}crP(c) 1-IP(ylc) '(), P(yl%)cr n(x,,y)P{c(x,)--c} (?) 
k i 
and P(c) cr -i P{c(xi)=ca}, P(xi) cr yj n(xi,yj). The one-sided clustering 
model is similar to the distributional clustering model [Pereira et al., 1993], how- 
ever, there are two important differences: (i) the number of likelihood contributions 
in (7) scales with the number of observations - a fact which follows from Bayes' rule 
- and (ii) mixing proportions are .missing in the original distributional clustering 
model. The one-sided clustering model corresponds to an unsupervised version of 
the naive Bayes' classifier, if we interpret 32 as a feature space for objects 
There are also ways to weaken the conditional independence assumption, e.g., by 
utilizing a mixture of tree dependency models [Meila, Jordan, 1998]. 
2.3 Two-Sided Clustering Model 
The latent variable structure of the two-sided clustering model significantly reduces 
the degrees of freedom in the specification of the class conditional distribution. We 
Learning from Dyadic Data 469 
Figure 2: Exemplary segmentation results on Aerial by one-sided clustering. 
propose the following complete data model 
P($,c) = H P(c(xi))P(c(y})) [P(xi)P(y})r,(,),4yk)] (yk) 
i,k 
(8) 
where %g,, are cluster association parameters. In this model the latent variables 
in the 2' and 32 space are coupled by the x-parameters. Therefore, there exists 
no simple mixture model representation for P($). Skipping some of the technical 
details (cf. [Hofmann, Puzicha, 1998]) we obtain P(xi) c  n(xi, y}), P(y) c 
i n(xi, Yk) and the M-step equations 
<'*' = [E, P{c(,) = cg} E, -(,, y,)] [E, P{c(,) = 4} E,-(x,, ,)] 
(9) 
as well as P(c,) = -]i P{c(xi) = c} and P(cY,) = E} P{c(x}) = %Y}. To preserve 
tractability for the remaining problem of computing the posterior probabilities in 
the E-step, we apply a factorial approximation (mean field approximation), i.e., 
P{c(zi) 
