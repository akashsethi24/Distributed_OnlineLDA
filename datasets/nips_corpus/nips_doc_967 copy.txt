Limits on Learning Machine Accuracy 
Imposed by Data Quality 
Corlon_a Cortes, L. D. Jackel, Rnd WRn-Ping Chi_ng 
AT&T Bell Laboratories 
Holmdel, NJ 07733 
Abstract 
Random errors and insufficiencies in databases limit the perfor- 
mance of any classifier trained from and applied to the database. 
In this paper we propose a method to estimate the limiting perfor- 
mance of classifiers imposed by the database. We demonstrate this 
technique on the task of predicting failure in telecommunication 
paths. 
1 Introduction 
Data collection for a classification or regression task is prone to random errors, 
e.g. inaccuracies in the measurements of the input or mis-labeling of the output. 
Missing or insufficient data are other sources that may complicate a learning task 
and hinder accurate performance of the trained machine. These insufficiencies of 
the data limit the performance of any learning machine or other statistical tool 
constructed from and applied to the data collection -- no matter how complex the 
machine or how much data is used to train it. 
In this paper we propose a method for estimating the limiting performance of learn- 
ing machines imposed by the quality of the database used for the task. The method 
involves a series of learning experiments. The extracted result is, however, indepen- 
dent of the choice of learning machine used for these experiments since the estimated 
limiting performance expresses a characteristic of the data. The only requirements 
on the learning machines are that their capacity (VC-dimension) can be varied and 
can be made large, and that the learning machines with increasing capacity become 
capable of implementing any function. 
240 Corinna Cortes, L. D. Jackel, Wan-Ping Chiang 
We have applied the technique to data collected for the purpose of predicting failures 
in telecommunication channels of the AT&T network. We extracted information 
from one of AT&T's large databases that continuously logs performance parame- 
ters of the network. The character and amount of data comes to more material 
than humans can survey. The processing of the extracted information is therefore 
automated by learning machines. 
We conjecture that the quality of the data imposes a limiting error rate on any 
learning machine of ~ 25%, so that even with an unlimited amount of data, and an 
arbitrarily complex learning machine, the performance for this task will not exceed 
~ 75% correct. This conjecture is supported by experiments. 
The relatively high noise-level of the data, which carries over to a poor performance 
of the trained classifier, is typical for many applications: the data collection was 
not designed for the task at hand and proved inadequate for constructing high 
performance classifiers. 
2 Basic Concepts of Machine Learning 
We can picture a learning machine as a device that takes an unknown input vector 
and produces an output value. More formally, it performs some mapping from an 
input space to an output space. The particular mapping it implements depends of 
the setting of the internal parameters of the learning machine. These parameters 
are adjusted during a learning phase so that the labels produced on the training 
set match, as well as possible, the labels provided. The number of patterns that 
the machine can match is loosely called the capacity of the machine. Generally, 
the capacity of a machine increases with the number of free parameters. After 
training is complete, the generalization ability of of the machine is estimated by its 
performance on a test set which the machine has never seen before. 
The test and training error depend on both the the number of training examples 
i, the capacity h of the machine, and, of course, how well suited the machine is to 
implement the task at hand. Let us first discuss the typical behavior of the test 
and training error for a noise corrupted task as we vary h but keep the amount i of 
training data fixed. This scenario can, e.g., be obtained by increasing the number 
of hidden units in a neural network or increasing the number of codebook vectors 
in a Learning Vector Quantization algorithm [6]. Figure la) shows typical training 
and test error as a function of the capacity of the learning machine. For h << i we 
have many fewer free parameters than training examples and the machine is over 
constrained. It does not have enough complexity to model the regularities of the 
training data, so both the training and test error are large (underfitting). As we 
increase h the machine can begin to fit the general trends in the data which carries 
over to the test set, so both error measures decline. Because the performance of the 
machine is optimized on only part of the full pattern space the test error will always 
be larger than the training error. As we continue to increase the capacity of the 
learning machine the error on the training set continues to decline, and eventually 
it reaches zero as we get enough free parameters to completely model the training 
set. The behavior of the error on the test set is different. Initially it decreases, 
but at some capacity, h*, it starts to rise. The rise occurs because the now ample 
resources of the training machine are applied to learning vagaries of the training 
Limits on Learning Machine Accuracy bnposed by Data Qualio' 241 
/oerror 8) t % error b) t.% error ï¿½) 
capacity trainino set size (t) capacity 
Figure 1: Errors as function of capacity and training set size. Figure la) shows 
characteristic plots of training and test error as a function of the learning machine 
capacity for fixed training set size. The test error reaches a minimum at h = h* 
while the training error decreases as h increases. Figure lb) shows the training and 
test errors at fixed h for varying i. The dotted line marks the asymptotic error Eoo 
for infinite i. Figure lc) shows the asymptotic error as a function of h. This error 
is limited from below by the intrinsic noise in the data. 
set, which are not reproduced in the test set (overfitting). Notice how in Figure la) 
the optimal test error is achieved at a capacity h* that is smaller than the capacity 
for which zero error is achieved on the training set. The learning machine with 
capacity h* will typically commit errors on misclassified or outlying patterns of the 
training set. 
We can alternatively discuss the error on the test and training set as a function 
of the training set size i for fixed capacity h of the learning machine. Typical 
behavior is sketched in Figure lb). For small i we have enough free parameters to 
completely model the training set, so the training error is zero. Excess capacity 
is used by the learning machine to model details in the training set, leading to a 
large test error. As we increase the training set size i we train on more and more 
patterns so the test error declines. For some critical size of the training set, ic, the 
machine can no longer model all the training patterns and the training error starts 
to rise. As we further increase i the irregularities of the individual training patterns 
smooth out and the parameters of the learning machine is more and more used to 
model the true underlying function. The test error declines, and asymptotically 
the training and test error reach the same error value Eoo. This error value is the 
limiting performance of the given learning machine to the task. In practice we never 
have the infinite amount of training data needed to achieve Eoo. However, recent 
theoretical calculations [8, 1, 2, 7, 5] and experimental results [3] have shown that 
we can estimate Em by averaging the training and test errors for i > ic. This means 
we can predict the optimal performance of a given machine. 
For a given type of learning machine the value of the asymptotic error Eoo of 
the machine depends on the quality of the data and the set of functions it can 
implement. The set of available functions increases with the capacity of the machine: 
242 Corinna Cortes, L. D. Jackel, Wan-Ping Chiang 
low capacity machines will typically exhibit a high asymptotic error due to a big 
difference between the true noise-free function of the patterns and the function 
implemented by the learning machine, but as we increase h this difference decreases. 
If the learning machine with increasing h becomes a universal machine capable of 
modeling any function the difference eventually reaches zero, so the asymptotic 
error Eo only measures the intrinsic noise level of the data. Once a capacity of 
the machine has been reached that matches the complexity of the true function 
no further improvement in Eo can be achieved. This is illustrated in Figure lc). 
The intrinsic noise level of the data or the limiting performance of any learning 
machine may hence be estimated as the asymptotic value of Eoo as obtained for 
asymptotically universal learning machines with increasing capacity applied to the 
task. This technique will be illustrated in the following section. 
3 Experimental Results 
In this section we estimate the limiting performance imposed by the data of any 
learning machine applied to the particular prediction task. 
3.1 Task Description 
To ensure the highest possible quality of service, the performance parameters of 
the AT&T network are constantly monitored. Due to the high complexity of the 
network this performance surveillance is mainly corrective: when certain measures 
exceed preset thresholds action is taken to maintain reliable, high quality service. 
These reorganizations can lead to short, minor impairments of the quality of the 
communication path. In contrast, the work reported here is preventive: our ob- 
jective is to make use of the performance parameters to form predictions that are 
sufficiently accurate that preemptive repairs of the channels can be made during 
periods of low traffic. 
In our study we have examined the characteristics of long-distance, 45 Mbits/s 
co
