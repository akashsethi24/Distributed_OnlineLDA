Examples of learning curves from a modified 
VC-formalism. 
A. Kowalczyk & J. Szymafski 
Telstra Research Laboratories 
770 Blackburn Road, 
Clayton, Vic. 3168, Australia 
{ a. kowalczyk,j.szymanski }@trl. oz. au) 
P.L. Bartlett & R.C. Williamson 
Department of Systems Engineering 
Australian National University 
Canberra, ACT 0200, Australia 
{bartlett,williams }@syseng.anu.edu.au 
Abstract 
We examine the issue of evaluation of model specific parameters in a 
modified VC-formalism. Two examples are analyzed: the 2-dimensional 
homogeneous perceptron and the 1-dimensional higher order neuron. 
Both models are solved theoretically, and their leaming curves are com- 
pared against true learning curves. It is shown that the formalism has 
the potential to generate a variety of leaming curves, including ones 
displaying phase transitions. 
1 Introduction 
One of the main criticisms of the Vapnik-Chervonenkis theory of leaming [15] is that the 
results of the theory appear very loose when compared with empirical data. In contrast, 
theory based on statistical physics ideas [1] provides tighter numerical results as well as 
qualitatively distinct predictions (such as phase transitions to perfect generalization). 
(See [5, 14] for a fuller discussion.) A question arises as to whether the VC-theory can 
be modified to give these improvements. The general direction of such a modification is 
obvious: one needs to sacrifice the universality of the VC-bounds and introduce model (e.g. 
distribution) dependent parameters. This obviously can be done in a variety of ways. Some 
specific examples are VC-entropy [15], empirical VC-dimensions [ 161, efficient complexity 
[17] or (, C)-uniformity [8, 9] in a VC-formalism with error shells. An extension of the 
last formalism is of central interest to this paper. It is based on a refinement of the 
fundamental theorem of computational learning [2] and its main innovation is to split the 
set of partitions of a training sample into separate error shells, each composed of error 
vectors corresponding to the different error values. 
Such a split introduces a whole range of new parameters (the average number of pattems 
in each of a series of error shells) in addition to the VC dimension. The difficulty of 
determining these parameters then arises. There are some crude, obvious upper bounds 
Examples of Learning Curves from a Modified VC-formalism 345 
on them which lead to both the VC-based estimates [2, 3, 15] and the statistical physics 
based formalism (with phase transitions) [5] as specific cases of this novel theory. Thus 
there is an obvious potential for improvement of the theory with tighter bounds. In particular 
we find that the introduction of a single parameter (order of uniformity), which in a sense 
determines shifts in relative sizes of error shells, leads to a full family of shapes of leaming 
curves continuously ranging in behavior from decay proportional to the inverse of the 
training sample size to phase transitions (sudden drops) to perfect generalization in small 
training sample sizes. We present initial comparison of the leaming curves from this new 
formalism with true leaming curves for two simple neral networks. 
2 Overview of the formalism 
The presentation is set in the typical PAC-style; the notation follows [2]. We consider 
a space X of samples with a probability measure , a subspace// of binary functions 
X --,, {0, 1} (dichotomies) (called the hypothesis space) and a target hypothesis t � tt. 
For each h �//andeach m-sample4 = (at, ..., a:,.,.,) � X r' (m � {1, 2, ...}),we denoteby 
df 1 m 
eh,a =  Y4=i It-hl(a:i) the empiricalerror of h on , d by ea = fx [t-hJ()(dz) 
the exct error of h  H. 
For each m  {1, 2, ...} let us consider the rmdom variable 
hrH 
defined as the maximal expected error of an hypothesis h �//consistent with t on 4. The 
learning curve oftt, defined as the expected value of e, , 
Ex,[e = fx (4 � X (2) 
is of central interest to us. Upper bounds on it can be derived from basic PAC-estimates as 
follows. For e > 0 we denote by H, '/'=/{h � H; ea > e} the subset of e-bad hypotheses 
and by 
Q� ad {4 � x '; 3e.. e, = o} = {4 � x '; 3he- e, = o & e >_ e} (3) 
the subset of m-samples for which there exists an e-bad hypothesis consistent with the 
target t. 
Lemma I I�'(Q?) < b(e, rn.), then e(rn.) <_ fo  rnin(1, b(e, ra))!(de), and equality 
in the assumption implies equality in the conclusion. [] 
Proof outline. If the assumption holds, then 9(e, m) a,_.=/1 - rain(l, b(e, m)) is a lower 
bound on the cumulative distribution of the random variable (1). Thus Ex, [e ] _< 
fore 9(e, re)de and integration by parts yields the conclusion. 
Given 4: (z, ..., z,.,.,) � X r', let us introduce the transformation (projection) -,.://--,, 
{0, 1} r' allocating to each h �//the vector 
- ..., - 
called the error pattern ofh on 4. For a subset G C H, let *rt,(G) = {*rt,.(h): h � G}. 
The space {0,1}  is the disjoint union of error shells � a {(,...,,) � 
{0,1} ;  +--. +, = i} for/ = 0, 1,...,m, and I,.(H) �g[ is the number 
346 A. KOWALCZYK, J. SZYMANSKI, P. L. BARTLETT, R. C. WILLIAMSON 
of different error pattems with i errors which can be obtained for h //. We shall employ 
the following notation for its average: 
(4) 
The central result of this paper, which gives a bound on the probability of the set Q as 
in Lemma 1 in terms of [//[, will be given now. It is obtained by modification of the 
proof of [8, Theorem 1] which is a refinement of the proof of the fundamental theorem of 
computational leaming in [2]. It is a simplified version (to the consistent learning case) of 
the basic estimate discussed in [9, 7]. 
Theorem 2 For any integer k _> 0 and 0 _< e, 7 <_ 1 
where A,,. r 
, (5) 
( (:) )-' 
= x-'L'rJ ei(1 - e) -j fork > 0 andA,o,.r 
d.f 1 - z..,j=o , 
Since error shells are disjoint we have the following relation: 
a.f 2_ /X 
I'z(n)l'(d.) = 2 -' Y] Iml? _< n.(,)/2 ' 
i=0 
(6) 
where r(h) a'd ro,(h), Inl? 'd Io17 and n.(,) 'd maxx. I'()1 is the 
growth function [2] of H. (Note that assuming that the target t _= 0 does not affect the 
cardinality of -t,(H).) If the VC-dimension of m, d = dvc(It), is finite, we have the 
well-known estimate [2] 
d 
j-O 
(7) 
Corollm'y 3 (i) If the VC-dimension d of H is finite and m > S/e, then p'(Qp) _< 
( ii) If H has finite cardinality, then p' ( q ) _< 5'h H, (1 -- eh )'. 
Proof. (i) Use the estimate A,,/2 _< 2 for k _> 8/e resulting from the Chemoffbound 
and set 'r = el2 and k = m in (5). (ii) Substitute the following crude estimate: 
I,17 _<  I.,lT' _<  I17 _< Pu < (emla) , 
i=0 i=0 
into the previous estimate. (iii) Set k = 0 into (i) and use the estimate 
_ = -- -- h) h'[-I 
The inequality in Corollary 3.i (ignoring the factor of 2) is the basic estimate of the VC- 
formalism (c.f. [2]); the inequality in Corollary 3.ii is the union bound which is the starting 
point for the statistical physics based formalism developed in [5]. In this sense both of 
these theories are unified in estimate (5) and all their conclusions (including the prediction 
Examples of Learning Curves from a Modified VC-formalism 347 
10 � 
10 � 
10 -t 
10 -z 10-z 
4 0 
(b) 
10 20 30 40 50 
m/d 
Figure 1' (a) Examples of upper bounds on the learning curves for the case of finite VC- 
dimension d = dvc(H) implied by Corollary 4.ii for C,o,,.,., = const. They split into five 
distinct bands of four curves each, according to the values of the order of uniformity w = 
2, 3, 4, 5, 10 (in the top-down order). Each band contains a solidline (Co,,.,.,  1, d = 100), 
a dotted line (Co,,.,., -- 100, d = 100), a chain line (C,o,, = 1, d = 1000) and a broken line 
(C,, ---- 100, d = 1000). 
(b) Various learning curves for the 2-dimensional homogeneous perceptron. Solid lines 
(top to bottom): (i) - for the VC-theory bound (Corollary 3.ii) with VC-dimension d = 2; 
(ii) - for the bound (for Eqn. 5 and Lemma 1) with 7 = ,/ = m and the upper bounds 
<_ I17 = 2 for i --- 1, ..., m- 1 and _< I17 - I for i = 0, m; (iii)- as in 
(ii) but with the exact values for as in (11); (iv) - true learning curve (Eqn. 13). The 
w-uniformity bound for w = 2 (with the minimal C,o,,r, satisfying (9), which turn out to be 
= const = 1) is shown by dotted line; for w = 3 the chain line gives the result for minimal 
C,o,, and the broken line for C,o,, set to 1. 
of phase transitions to perfect generalization for the Ising perceptron for a = raid < 1.448 
in the thermodynamic limit [5]) can be derived from this estimate, and possibly improved 
with the use of tighter estimates on I//,l. 
We now formally introduce a family of estimates on I//[? in order to discuss a potential 
of our formalism. For any m, e and w >_ 1.0 there exists C,o,, > 0 such that 
(for 0 < i < m). 
(8) 
We shall call such an estimate an w-uniformity bound. 
Corollary 4 (i) ff an w-uniformity bound (8) holds, en 
(9) 
(ii) if additionally d = dvc(H) < oo, then 
Z (2-m(2era/d)a) 
� I-1 (lO) 
3 Examples of learning curves 
In this section we evaluate the above formalism on two examples of simple neural networks. 
348 A. KOWALCZYK, J. SZYMANSKI, P. L. BARTLETT, R. C. WILLIAMSON 
10 0 
10 -! 
20 
...... ' ............... .7.....  ':: 
-,.co = 2: doLLed line 
co = 3 : chain line 
.... I .... I .... I .... ll,ll 0 
0 10 20 30 40 50 0 
m/(d + !) 
15 
I0 
/ . .............. = a 
, ,,/' , I .... J .... J .... J .... 
100 200 300 400 500 
m 
Figure 2: (a) Different learning curves for the higher order neuron (analogous to Fig. 1.b). 
Solid lines (top to bottom) (i) - for the VC-theory bound (Corollary 3.ii) with VC-dimension 
d + 1 = 21; (ii) - for the bound (5) with = e and the upper bounds IJl? _< IJl? with 
Il? given by (15); (iii) - true leaming curve (the upper bound given by (18)). The w- 
uniformity bound/approximation are plotted as chain and 
