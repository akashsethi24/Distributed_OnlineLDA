Grammatical Inference by 
Attentional Control of Synchronization 
in an Oscillating Elman Network 
Bill Baird 
Dept Mathematics, 
U.C.Berkeley, 
Berkeley, Ca. 94720, 
baird@math.berkeley. edu 
Todd Troyer 
Dept of Phys., 
U.C.San Francisco, 
513 Parnassus Ave. 
San Francisco, Ca. 94143, 
todd@phy. ucsf. edu 
Frank Eeckman 
Lawrence Livermore 
National Laboratory, 
P.O. Box 808 (L-270), 
Livermore, Ca. 94550, 
eeckman@.llnl.gov 
Abstract 
We show how an Ehnan network architecture, constructed from 
recurrently connected oscillatory associative memory network mod- 
ules, can employ selective attentional control of synchronization 
to direct the flow of communication and computation within the 
architecture to solve a grammatical inference problem. 
Previously we have shown how the discrete time Ehnan network 
algorithm can be implemented in a network completely described 
by continuous ordinary differential equations. The time steps (ma- 
chine cycles) of the system are implemented by rhythmic variation 
(clocking) of a bifurcation parameter. In this architecture, oscilla- 
tion amplitude codes the information content or activity of a mod- 
ule (unit), whereas phase and frequency are used to softwire the 
network. Only synchronized modules communicate by exchang- 
ing amplitude information; the activity of non-resonating modules 
contributes incoherent crosstalk noise. 
Attentional control is modeled as a special subset of the hidden 
modules with ouputs which affect the resonant frequencies of other 
hidden modules. They control synchrony among the other mod- 
ules and direct the flow of computation (attention) to effect transi- 
tions between two subgraphs of a thirteen state automaton which 
the system emulates to generate a Reber grammar. The internal 
crosstalk noise is used to drive the required random transitions of 
the automaton. 
67 
68 Baird, Troyer, and Eeckman 
1 Introduction 
Recordings of local field potentials have revealed 40 to 80 Hz oscillation in vertebrate 
cortex [Freeman and Baird, 1987, Gray and Singer, 1987]. The amplitude patterns 
of such oscillations have been shown to predict the olfactory and visual pattern 
recognition responses of a trained animal. There is further evidence that although 
the oscillatory activity appears to be roughly periodic, it is actually chaotic when 
exainined in detail. This preliminary evidence suggests that oscillatory or chaotic 
network modules may form the cortical substrate for many of the sensory, motor, 
and cognitive functions now studied in static networks. 
It remains be shown how networks with more complex dynamics can performs these 
operations and what possible advantages are to be gained by such complexity. We 
have therefore constructed a parallel distributed processing architecture that is in- 
spired by the structure and dynalnics of cerebral cortex, and applied it to the prob- 
lem of grammatical inference. The construction views cortex as a set of coupled 
oscillatory associative memories, and is guided by the principle that attractors must 
be used by macroscopic systems for reliable computation in the presence of noise. 
This system must function reliably in the midst of noise generated by crosstalk from 
it's own activity. Present day digital computers are built of flip-flops which, at the 
level of their transistors, are continuous dissipative dynamical systems with differ- 
ent attractors underlying the symbols we call 0 and 1. In a similar manner, the 
network we have constructed is a symbol processing system, but with analog input 
and oscillatory snbsymbolic representations. 
The architecture operates as a thirteen state finite automaton that generates the 
symbol strings of a Reber grammar. It is designed to demonstrate and study the 
following issues and principles of neural computation: (1) Sequential computation 
with coupled associative memories. (2) Computation with attractors for reliable 
operation in the presence of noise. (3) Discrete time and state symbol processing 
arising from continuum dynamics by bifurcations of attractors. (4) Attention as 
selective synchronization controling conmmnication and temporal program flow. (5) 
chaotic dynamics in some network modules driving randoran choice of attractors in 
other network modules. The first three issues have been fully addressed in a previous 
paper [Baird et al., 1993], and are only briefly reviewed. We focus here on the last 
two. 
1.1 Attentional Processing 
An important element of intra-cortical communication in the brain, and between 
modules in this architecture, is the ability of a module to detect and respond to 
the proper input signal from a particular module, when inputs from other modules 
irrelevant to the present computation are contributing crosstalk noise. This is smilar 
to the problem of coding messages in a computer architecture like the Connection 
Machine so that they can be picked up from the common comnmnication buss line 
by the proper receiving module. 
Periodic or nearly periodic (chaotic) variation of a signal introduces additional de- 
grees of freedom that can be exploited in a computational architecture. We investi- 
gate the principle that selective control of synchronization, which we hypopthesize 
to be a model of attention, can be used to solve this coding problem and control 
communication and program flmv in an architecture with dynamic attractors. 
The architecture illustrates the notion that synchronization not only binds sen- 
Grammatical Inference by Attentional Control of Synchronization 69 
sory inputs into objects [Gray and Singer, 1987], but binds the activity of selected 
cortical areas into a functional whole that directs behavior. It is a model of at- 
tended activity as that subset which has been included in the processing of the 
moment by synchronization. This is both a spatial and temporal binding. Only the 
inputs which are synchronized to the internal oscillatory activity of a module can 
effect previously learned transitions of attractors within it. For example, consider 
two objects in the visual field separately bound in primary visual cortex by synchro- 
nization of their components at different phases or frequencies. One object may be 
selectively attended to by its entralmnent to oscillatory processing at higher levels 
such as V4 or IT. These in turn are in synchrony with oscillatory activity in motor 
areas to select the attractors there which are directing motor output. 
In the architecture presented here, we have constrained the network dynamics so 
that there exist well defined notions of amplitude, phase, and frequency. The net- 
work has been designed so that amplitude codes the information content or activity 
of a module, whereas phase and frequency are used to softwire the network. An 
oscillatory network module has a passband outside of which it will not synchro- 
nize with an oscillatory input. Modules can therefore easily be desynchronized 
by perturbing their resonant frequencies. Furthermore, only synchronized modules 
communicate by exchanging amplitude information; the activity of non-resonating 
modules contributes incoherant crosstalk or noise. The flow of communication be- 
tween modules can thus be controled by controlling synchrony. By changing the 
intrinsic frequency of modules in a patterned way, the effective connectivity of the 
network is changed. The same hardware and connection matrix can thus subserve 
many different computations and patterns of interaction between modules without 
crosstalk problems. 
The crosstalk noise is actually essential to the function of the system. It serves as 
the noise source for making random choices of output symbols and automaton state 
transitions in this architecture, as we discuss later. In cortex there is an issue as to 
what may constitute a source of randomness of sufficient magnitude to perturb the 
large ensemble behavior of neural activity at the cortical network level. It does not 
seem likely that the well known molecular fluctuations which are easily averaged 
within one or a few neurons can do the job. The architecture here models the 
hypothesis that deterministic chaos in the macroscopic dynamics of a network of 
neurons, which is the same order of magnitude as the coherant activity, can serve 
this purpose. 
In a set of modules which is alesynchronized by perturbing the resonant frequencies 
of the group, coherance is lost and random phase relations result. The character 
of the model time traces is irregular as seen in real neural ensemble activity. The be- 
havior of the time traces in different modules of the architecture is similar to the tem- 
porary appearance and sxvitching of synchronization between cortical areas seen in 
observations of cortical processing during sensory/motor tasks in monkeys and hu- 
mans [Bressler and Nakanmra, 1993]. The structure of this apparently chaotic sig- 
nal and its use in network learning and operation are currently under investigation. 
2 Normal Form Associative Memory Modules 
The mathenmtical foundation for the construction of network inodules is contained 
in the normal form projection algorithm [Baird and Eeckman, 1993]. This is a 
learning algorithm for recurrent analog neural networks which allows associative 
memory storage of analog patterns, continuous periodic sequences, and chaotic 
70 Baird, Troyer, and Eeckman 
attractors in the same network. An N node module can be shown to function 
as an associative memory for up to N/2 oscillatory, or N/3 chaotic memory at- 
tractors [Baird and Eeckman, 1993]. A key feature of a net constructed by this 
algorithm is that the underlying dynamics is explicitly isomorphic to any of a 
class of standard, well understood nonlinear dynamical systems - a normal .form 
[Guckenheimer and Holmes, 1983]. 
The network modules of this architecture were developed previously as models of 
olfactory cortex with distributed patterns 
