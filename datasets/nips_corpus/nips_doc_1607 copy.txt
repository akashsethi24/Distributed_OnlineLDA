Invariant Feature Extraction and 
Classification in Kernel Spaces 
Sebastian Mika , Gunnar Ritsch  , Jason Weston 2, 
Bernhard Sch51kopf 3, Alex Smola 4, and Klaus-Robert Miiller  
1 GMD FIRST, Kekulstr. 7, 12489 Berlin, Germany 
2 Barnhill BioInformatics, 6709 Waters Av., Savannah, GR 31406, USA 
3 Microsoft Research Ltd., 1 Guildhall Street, Cambridge CB2 3NH, UK 
4 Australian National University, Canberra, 0200 ACT, Australia 
{mika, raetsch, klaus}@first.gmd.de, jasonw@dcs.rhbnc.ac.uk 
bsc@microsoft.com, Alex. Smola.anu.edu.au 
Abstract 
We incorporate prior knowledge to construct nonlinear algorithms 
for invariant feature extraction and discrimination. Employing a 
unified framework in terms of a nonlinear variant of the Rayleigh 
coefficient, we propose non-linear generalizations of Fisher's dis- 
criminant and oriented PCA using Support Vector kernel functions. 
Extensive simulations show the utility of our approach. 
I Introduction 
It is common practice to preprocess data by extracting linear or nonlinear features. 
The most well-known feature extraction technique is principal component analysis 
PCA (e.g. [3]). It aims to find an orthonormal, ordered basis such that the i-th 
direction describes as much variance as possible while maintaining orthogonality to 
all other directions. However, since PCA is a linear technique, it is too limited to 
capture interesting nonlinear structure in a data set and nonlinear generalizations 
have been proposed, among them Kernel PCA [14], which computes the principal 
components of the data set mapped nonlinearly into some high dimensional feature 
space r. 
Often one has prior information, for instance, we might know that the sample is 
corrupted by noise or that there are invariances under which a classification should 
not change. For feature extraction, the concepts of known noise or transformation 
invariance are to a certain degree equivalent, i.e. they can both be interpreted as 
causing a change in the feature which ought to be minimized. Clearly, invariance 
alone is not a sufficient condition for a good feature, as we could simply take the 
constant function. What one would like to obtain is a feature which is as invariant 
as possible while still covering as much of the information necessary for describing 
the particular data. Considering only one (linear) feature vector w and restricting 
to first and second order statistics of the data one arrives at a maximization of the 
so called Rayleigh coefficient 
wTsw 
J(w) = wVSvw , (1) 
Invariant Feature Extraction and Classification in Kernel Spaces 52 7 
where w is the feature vector and $I, $v are matrices describing the desired and 
undesired properties of the feature, respectively (e.g. information and noise). If $I 
is the data covariance and Sv the noise covariance, we obtain oriented PCA [3]. 
If we leave the field of data description to perform supervised classification, it is 
common to choose $ as the separability of class centers (between class variance) 
and $v to be the within class variance. In that case, we recover the well known 
Fisher Discriminant [7]. The ratio in (1) is maximized when we cover much of 
the information coded by $ while avoiding the one coded by $v. The problem is 
known to be solved, in analogy to PCA, by a generalized symmetric eigenproblem 
Sw = Svw [3], where  C ll is the corresponding (biggest) eigenvalue. 
In this paper we generalize this setting to a nonlinear one. In analogy to [8, 14] 
we first map the data via some nonlinear mapping  to some high-dimensional fea- 
ture space r and then optimize (1) in r. To avoid working with the mapped data 
explicitly (which might be impossible if r is infinite dimensional) we introduce sup- 
port vector kernel functions [11], the well-known kernel trick. These kernel functions 
k(x, y) compute a dot product in some feature space r, i.e. k(x, y) - ((x). (y)). 
Formulating the algorithms in r using  only in dot products, we can replace any 
occurrence of a dot product by the kernel function k. Possible choices for k which 
have proven useful e.g. in Support Vector Machines [2] or Kernel PCA [14] are Gaus- 
sian RBF, k(x,y): exp(-llx- yll2/c), or polynomial kernels, k(x,y) = (x. y)a, 
for some positive constants c C ll and d  N, respectively. 
The remainder of this paper is organized as follows: The next section shows how to 
formulate the optimization problem induced by (1) in feature space. Section 3 con- 
siders various ways to find Fisher's Discriminant in r; we conclude with extensive 
experiments in section 4 and a discussion of our findings. 
2 Kernelizing the Rayleigh Coefficient 
To optimize (1) in some kernel feature space r we need to find a formulation which 
uses only dot products of -images. As numerator and denominator are both scalars 
this can be done independently. Furthermore, the matrices $ and $v are basically 
covariances and thus the sum over outer products of -images. Therefore, and due 
to the linear nature of (1) every solution w  r can be written as an expansion in 
terms of mapped training data , i.e. 
i=1 
(2) 
To define some common choices in r let X - {x,... , x} be our training sample 
and, where appropriate, X U X2 = X, X Cl X2 = , two subclasses (with Ixil = i). 
We get the full covariance of X by 
I 1 
C =  Z (I,(x) - m)(I,(x) - m) n- with m = j Z (x), 
(3) 
SB and Sw are operators on a (finite-dimensional) subspace spanned by the I,(xi) (in 
a possibly infinite space). Let w -- v + v2, where v E Span((xi): i = 1,... ,f) and 
v2 _L Span((xi): i = 1,... , f). Then for $ -- $w or $ = $B (which are both symmetric) 
�o,so) = 
= 
= 
As Vl lies in the span of the (I)(:Ci) and S only operates on this subspace there exist an 
expansion of w which maximizes J(w). 
528 S. Mika, G. Riitsch, J. Weston, B. Sch6lkopf A. J. Smola and K.-R. Miiller 
which could be used as St in oriented Kernel PCA. For SN we could use an estimate 
of the noise covariance, analogous to the definition of C but over mapped patterns 
sampled from the assumed noise distribution. The standard formulation of the 
Fisher discriminant in O r, yielding the Kernel Fisher Discriminant (KFD) [8] is 
given by 
SW : Z Z ((x) - mi)((x) -- mi) y and SB -- (m2 -- m)(m2 -- m) -r, 
i----1,2 x Afl 
the within-class scatter Sw (as SN), and the between class scatter Sis (as St). Here 
mi is the sample mean for patterns from class i. 
To incorporate a known invariance e.g. in oriented Kernel PCA, one could use the 
tangent covariance matrix [12], 
1 
T = �t---  Z (I,(x) - (�,x))((x) - (�,x)) q- for some small t > 0. (4) 
Here �t is a local 1-parameter transformation. T is a finite difference approximation 
t of the covariance of the tangent of �t at point (x) (details e.g. in [12]). Using 
St - C and $N -- T in oriented Kernel PCA, we impose invariance under the local 
transformation �t. Crucially, this matrix is not only constructed from the training 
patterns X. Therefore, the argument used to find the expansion (2) is slightly 
incorrect. Neverthless, we can assume that (2) is a reasonable approximation for 
describing the variance induced by T. 
Multiplying either of these matrices from the left and right with the expansion (2), 
we can find a formulation which uses only dot products. For the sake of brevity, we 
only give the explicit formulation of (1) in Or for KFD (cf. [8] for details). Defining 
1 
(i)j: �-. Ea:exi k(xj,x) we can write (1) for KFD as 
= (ru)2 
a--ffNa = dNa , (5) 
where N = KK n-- y.i=,2eilil, / = /2 -/1, M - //'1-, and Kij = k(xi, xj). 
The results for other choices of $I and SN in Or as for the cases of oriented kernel 
PCA or transformation invariance can be obtained along the same lines. Note that 
we still have to maximize a Rayleigh coefficient. However, now it is a quotient in 
terms of expansion coefficients a, and not in terms of w C Or which is a potentially 
infinite-dimensionai space. Furthermore, it is well known that the solution for this 
special eigenproblem is in the direction of N - (/2 -/) [7], which can be solved 
using e.g. a Cholesky factorization of N. The projection of a new pattern x onto 
w in Or can then be computed by 
(w . I,(x)) = Z ai k(xi, x). (6) 
i=1 
3 Algorithms 
Estimating a covariance matrix with rank up to/? from � samples is ill-posed. Fur- 
thermore, by performing an explicit centering in Or each covariance matrix loses one 
more dimension, i.e. it has only rank g- I (even worse, for KFD the matrix N has 
rank �- 2). Thus the ratio in (1) is not well defined anymore, as the denomina- 
tor might become zero. In the following we will propose several ways to deal with 
this problem in KFD. Furthermore we will tackle the question how to solve the 
optimization problem of KFD more efficiently. So far, we have an eigenproblem of 
size � x g. If g becomes large this is numerically demanding. Reformulations of the 
original problem allow to overcome some of these limitations. Finally, we describe 
the connection between KFD and RBF networks. 
Invariant Feature Extraction and Classification in Kernel Spaces 529 
3.1 Regularization and Solution on a Subspace 
As noted before, the matrix N has only rank/ - 2. Besides numerical problems 
which can cause the matrix N to be not even positive, we could think of imposing 
some regularization to control capacity in r. To this end, we simply add a multiple 
of the identity matrix to N, i.e. replace N by Nu where 
N := N + . (7) 
This can be viewed in different ways: (i) for t > 0 it makes the problem feasible 
and numerically more stable as N u becomes positive; (ii) it can be seen as decreas- 
ing the bias in sample based estimation of eigenvalues (cf. [6]); (iii) it imposes a 
regularization on lieill 2, favoring solutions with small expansion coefficients. Fur- 
thermore, one could use other regularization type additives to N, e.g. penalizing 
IIw]] 2 in analogy
