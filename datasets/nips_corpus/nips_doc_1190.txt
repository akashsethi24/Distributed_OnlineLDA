Computing with infinite networks 
Christopher K. I. Williams 
Neural Computing Research Group 
Department of Computer Science and Applied Mathematics 
Aston University, Birmingham B4 7ET, UK 
c. k. i. williamsason. ac. uk 
Abstract 
For neural networks with a wide class of weight-priors, it can be 
shown that in the limit of an infinite number of hidden units the 
prior over functions tends to a Gaussian process. In this paper an- 
alytic forms are derived for the covariance function of the Gaussian 
processes corresponding to networks with sigmoidal and Gaussian 
hidden units. This allows predictions to be made efficiently using 
networks with an infinite number of hidden units, and shows that, 
somewhat paradoxically, it may be easier to compute with infinite 
networks than finite ones. 
I Introduction 
To someone training a neural network by maximizing the likelihood of a finite 
amount of data it makes no sense to use a network with an infinite number of hidden 
units; the network will overfit the data and so will be expected to generalize 
poorly. However, the idea of selecting the network size depending on the amount 
of training data makes little sense to a Bayesian; a model should be chosen that 
reflects the understanding of the problem, and then application of Bayes' theorem 
allows inference to be carried out (at least in theory) after the data is observed. 
In the Bayesian treatment of neural networks, a question immediately arises as to 
how many hidden units are believed to be appropriate for a task. Neal (1996) has 
argued compellingly that for real-world problems, there is no reason to believe that 
neural network models should be limited to nets containing only a small number 
of hidden units. He has shown that it is sensible to consider a limit where the 
number of hidden units in a net tends to infinity, and that good predictions can be 
obtained from such models using the Bayesian machinery. He has also shown that 
for fixed hyperparameters, a large class of neural network models will converge to 
a Gaussian process prior over functions in the limit of an infinite number of hidden 
units. 
296 C. K. I. Williams 
Neal's argument is an existence proof--it states that an infinite neural net will 
converge to a Gaussian process, but does not give the covariance function needed 
to actually specify the particular Gaussian process. In this paper I show that 
for certain weight priors and transfer functions in the neural network model, the 
covariance function which describes the behaviour of the corresponding Gaussian 
process can be calculated analytically. This allows predictions to be made using 
neural networks with an infinite number of hidden units in time O(n3), where n 
is the number of training examples 1. The only alternative currently available is to 
use Markov Chain Monte Carlo (MCMC) methods (e.g. Neal, 1996) for networks 
with a large (but finite) number of hidden units. However, this is likely to be 
computationally expensive, and we note possible concerns over the time needed for 
the Markov chain to reach equilibrium. The availability of an analytic form for 
the covariance function also facilitates the comparison of the properties of neural 
networks with an infinite number of hidden units as compared to other Gaussian 
process priors that may be considered. 
The Gaussian process analysis applies for fixed hyperparameters 0. If it were de- 
sired to make predictions based on a hyperprior P(0) then the necessary O-space 
integration could be achieved by MCMC methods. The great advantage of integrat- 
ing out the weights analytically is that it dramatically reduces the dimensionality 
of the MCMC integrals, and thus improves their speed of convergence. 
1.1 From priors on weights to priors on functions 
Bayesian neural networks are usually specified in a hierarchical manner, so that the 
weights w are regarded as being drawn from a distribution P(wlO ). For example, 
the weights might be drawn from a zero-mean Gaussian distribution, where 0 spec- 
ifies the variance of groups of weights. A full description of the prior is given by 
specifying P(O) as well as P(volO ). The hyperprior can be integrated out to give 
P(w) = f P(wlO)P(O ) dO, but in our case it will be advantageous not to do this as 
it introduces weight correlations which prevent convergence to a Gaussian process. 
In the Bayesian view of neural networks, predictions for the output value y. cor- 
responding to a new input value :r. are made by integrating over the posterior in 
weight space. Let D - ((l,il),(2, i2),...,(n,in)) denote the n training data 
pairs, t = (tl,...,tn) T and f.(w) denote the mapping carried out by the network 
on input a. given weights w. P(wlt, O) is the weight posterior given the training 
data 2. Then the predictive distribution for y. given the training data and hyper- 
parameters 0 is 
P(y, lt, O) = /5(y, - f,(w))P(wlt, O)aw (1) 
We will now show how this can also be viewed as making the prediction using priors 
over functions rather than weights. Let f(w) denote the vector of outputs corre- 
sponding to inputs (a,...,a,) given weights w. Then, using Bayes' theorem we 
have P(wlt, O) = P(tlw)P(wlo)/P(tlo ), and P(tlw) = f P(tly) 6(y- f(w)) dy. 
Hence equation i can be rewritten as 
i // 
P(y, lt, O) = P(tlO) P(tly) 5(y, - f,(w))5(y- f(w)) P(wlo) dw dy (2) 
However, the prior over (y,, yl, ..., yn) is given by P(y,, ylO) = P(Y*IY, O)P(YIO) = 
f 6(y, - f,(w) 6(y- f(w))P(wlO ) dw and thus the predictive distribution can be 
 For large n, various approximations to the exact solution which avoid the inversion of 
an n x n matrix are available. 
2For notationaJ convenience we suppress the x-dependence of the posterior. 
Computing with Infinite Networks 297 
written as 
p(tlO) P(tly)P(y, ly, O)P(ylO ) dy- P(y, ly, O)P(ylt, O) dy 
(3) 
Hence in a Bayesian view it is the prior over function values P(y,, ylO) which is 
important; specifying this prior by using weight distributions is one valid way to 
achieve this goal. In general we can use the weight space or function space view, 
which ever is more convenient, and for infinite neural networks the function space 
view is more useful. 
2 Gaussian processes 
A stochastic process is a collection of random variables x} indexed by 
a set X. In our case X will be Tï¿½ d, where d is the number of inputs. The stochastic 
process is specified by giving the probability distribution for every finite subset 
of variables Y(:r),...,Y(:rk) in a consistent manner. A Gaussian process (GP) 
is a stochastic process which can be fully specified by its mean function tt() = 
E[Y(a:)] and its covariance function C(a:, ') = E[(Y(a:)- p())(Y(')- p('))]; 
any finite set of Y-variables will have a joint multivariate Gaussian distribution. For 
a multidimensional input space a Gaussian process may also be called a Gaussian 
random field. 
Below we consider Gaussian processes which have tt() -- 0, as is the case for the 
neural network priors discussed in section 3. A non-zero tt() can be incorporated 
into the framework at the expense of a little extra complexity. 
A widely used class of covariance functions is the stationary covariance functions, 
whereby C(:r, :r ) -- C(:r- :r). These are related to the spectral density (or power 
spectrum) of the process by the Wiener-Khinchine theorem, and are particularly 
amenable to Fourier analysis as the eigenfunctions of a stationary covariance kernel 
are exp ik.ce. Many commonly used covariance functions are also isotropic, so that 
C(h) = C(h) where h = :r - :r  and h = Ih[. For example C(h) = exp(-(h/a) v) 
is a valid covariance function for all d and for 0 <  < 2. Note that in this case 
a sets the correlation length-scale of the random field, although other covariance 
functions (e.g. those corresponding to power-law spectral densities) may have no 
preferred length scale. 
2.1 Prediction with Gaussian processes 
The model for the observed data is that it was generated from the prior stochastic 
process, and that independent Gaussian noise (of variance au 2) was then added. 
, , 0 '2 .. 
Given a prior covariance function C'o(i a:j), a noise process CN(i j) = 5 3 
2 at each data point) and the training data, 
(i.e. independent noise of variance 0. 
the prediction for the distribution of !/, corresponding to a test point a:, is obtained 
simply by applying equation 3. As the prior and noise model are both Gaussian the 
integral can be done analytically and P(y, It, O) is Gaussian with mean and variance 
0(,) = kp(,)(KP + KN)-t (4) 
0. (ze, ) -- Cp(ze,, ze, ) - k Tp(ze, )( Kp q - KN )- l kp(Ze, ) (5) 
where [Kli J - C(:ri,:ri) for a = P,N and kr(:r,) = (Cr(:r,,:r),..., 
Cr(:r,,:r)) '. 0.(:r,)gives the error bars of the prediction. 
Equations 4 and 5 are the analogue for spatial processes of Wiener-Kolmogorov 
prediction theory. They have appeared in a wide variety of contexts including 
298 C. K. L Williams 
geostatistics where the method is known as kriging (Journel and Huijbregts, 1978; 
Cressie 1993), multidimensional spline smoothing (Wahba, 1990), in the derivation 
of radial basis function neural networks (Poggio and Girosi, 1990) and in the work 
of Whittle (1963). 
3 Covariance functions for Neural Networks 
Consider a network which takes an input , has one hidden layer with H units and 
then linearly combines the outputs of the hidden units with a bias to obtain f(). 
The mapping can be written 
H 
(6) 
j=l 
where h(;u) is the hidden unit transfer function (which we shall assume is 
bounded) which depends on the input-to-hidden weights u. This architecture is 
important because it has been shown by Hornik (1993) that networks with one 
hidden layer are universal approximators as the number of hidden units tends to 
infinity, for a wide class of transfer functions (but excluding polynomials). Let b 
and the v's have independent zero-mean distributions of variance a
