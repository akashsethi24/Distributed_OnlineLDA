Modeling Time Varying Systems 
Using Hidden Control Neural Architecture 
Esther Levin 
AT&T Bell Laboratories 
Speech Research Department 
Murray Hill, NJ 07974 USA 
ABSTRACT 
Multi-layered neural networks have recently been proposed for non- 
linear prediction and system modeling. Although proven successful 
for modeling time invariant nonlinear systems, the inability of neural 
networks to characterize temporal variability has so far been an 
obstacle in applying them to complicated nonstationary signals, such 
as speech. In this paper we present a network architecture, called 
Hidden Control Neural Network (HCNN), for modeling signals 
generated by nonlinear dynamical systems with restricted time 
variability. The approach taken here is to allow the mapping that is 
implemented by a multi layered neural network to change with time 
as a function of an additional control input signal. This network is 
trained using an algorithm that is based on back-propagation and 
segmentation algorithms for estimating the unknown control together 
with the network's parameters. The HCNN approach was applied to 
several tasks including modeling of time-varying nonlinear systems 
and speaker-independent recognition of connected digits, yielding a 
word accuracy of 99.1%. 
I. INTRODUCTION 
Layered networks have attracted considerable interest in recent years due to their 
ability to model adaptively nonlinear multivariate functions. It has been recently proved 
in [1], that a network with one intermediate layer of sigmoidal units can approximate 
arbitrarily well any continuous mapping. However, being a static model, a layered 
network is not capable of modeling signals with an inherent time variability, such as 
speech. 
In this paper we present a hidden control neural network that can implements non- 
linear and time-varying mapping. The hidden control input signal which allows the 
network's mapping to change over time, provides the ability to capture the non- 
stationary properties, and learn the underlying temporal structure of the modeled signal. 
147 
148 Levin 
H. THE MODEL 
H.1 MULTI LAYERED NETWORK 
Multi layered neural network is a c9nnectionist models that iplements a nonlinear 
mapping from and input x  X  R' to an output y  Y  R-o: 
y= Fto(x), (1) 
where to  Ca c R D, the parameter set of the network, consists of the connection 
wegihts and the biases, and x and y are the activation vectors of the input and output 
layers, of dimensionality N and No, respectively. 
Recently layered networks have proven useful for non-linear prediction of signals and 
system modeling [2]. In these applications one uses the values of a real signal x(t), at a 
set of discrete times in the past, to predict x (t) at a point in the future. For example, 
for order-one-predictor, the output of the network y is used as a predicr of the next 
signal sample, when the network is given past sample as input, e.g. y=xt=Fto(xt_ ), 
where xt denotes the predicted value of the signal at time t, which, in general, differs 
from the true value, xt. The parameter set of the network to is estimated from a 
training set of discrete time samples from a segment of known signal { xt, t =0, ..., T }, 
by minimizing a prediction error which measures the distortion between the signal and 
the prediction made by the network, 
T 
II xt-F,o(x,_)II 2, 
t=l 
and the estimated parameter set & is given by argmain E (to). 
In [2] such a neural network predictor is used for modeling chaotic series. One of the 
examples considered in [2] is prediction of time series generated by the classic logistic, 
or Feigenbaum, map, 
xt+ =4'b'xt ( 1 -xt ) (3) 
This iterated map produces an ergodic chaotic time series when b is chosen to equal 1. 
Although this time series passes virtually every test for randomness, it is generated by 
the deterministic Eq.(3), and can be predicted perfectly, once the generating system (3) 
is leamed. Using the back-propagation algorithm [3] to minimize the prediction error 
(2) defined on a set of samples of this time series, the network parameters to were 
adjusted, enabling accurate prediction of the next point xt+ in this random series 
given the present point xt as an input. The mapping Fro implemented by the trained 
network approximated very closely the logistic map (3) that generated the modeled 
series. 
H.2 HIDDEN CONTROL NETWORK 
For a given fixed value of the parameters to, a layered network implements a fixed 
input-output mapping, and therefore can be used for time-invariant system modeling or 
prediction of signals generated by a fixed, time-invariant system. Hidden control 
network that is based on such layered network, has an additional mechanism that 
allows the mapping (1) to change with time, keeping the parameters to fixed. We 
consider the case where the units in the input layer are divided into two distinct 
groups. The first input unit group represents the observable input to the network, 
x  X c R ', and the second represents a control signal c  C c R q, p + q =N, that 
controls the mapping between the observable input x, and the network output y. 
The output of the network y is given, according to (1), by Fro(x, c), where (x, c ) 
denotes the concatenation of the two inputs. We focus on the mapping between the 
observable input x and the output. This mapping is modulated by the control input c � 
Modeling Time Varying Systems Using Hidden Control Neural Architecture 149 
for a fixed value of x and for different values of c, the network produces different 
outputs. For a fixed control input, the network implements a fixed observable input- 
output mapping, but when the control input changes, the network's mapping changes 
as well, modifying the characteristics of the observed signal: 
y=Fto(x,c) __a Fc(x). (4) 
If the control signal is known for all time t, there is no point in distinguishing between 
the observable input, x, and the control input c. The more interesting situation is when 
the control signal is unknown or hidden, i.e., the hidden control case, which we will 
treat in this paper. 
This model can be used for prediction and modeling of nonstationary signals generated 
by time-varying sources. In the case of first order prediction the present value of the 
signal xt is predicted based on xt-i, with respect to the control input ct. If we restrict 
the control signal to take its values from a finite set, c  {Ci, � � � , CN } --C, then 
the network is a finite state network, where in each state it implements a fixed input- 
output mapping Fc,. Such a network with two or more intermidiate layers can 
approximate arbitrarily closely any set {F1, � � � ,FN} of continuous functions of the 
observable input x [4]. 
In the applications we considered for this model, two types of time-structures were 
used, namely 
Fully connected model: In this type of HCNN, every state, corresponding to a specific 
value of the control input, can be reached from any other state in a single time step. It 
means that there are no temporal restrictions on the control signal, and in each time 
step, it can take any of its N possible values { C1, ...,C}. For example, a 2 state fully 
connected model is shown in Fig. la. In a generative mode of operation, when the 
observable input of the network is wired to be the the previous network's output, the 
observable signal x (t) is generated in each one of the states by a different dynamics: 
xt+i=Fc,(xt), ct  {0, 1}, and therefore this network emulates two different dynamical 
systems, with the control signal acting as a switch between them. 
Left-to-right model: For spoken word modeling, we will consider a finite-state, left- 
to-right HCNN (see Fig.lb), where the control signal is further restricted to take value 
Ci only if in the previous time step it had a value of Ci or Ci-1. Each state of this 
network represents an unspecified acoustic unit, and due to the left-to-right structure, 
the whole word is modeled as concatenation of such acoustic units. The time spent in 
each of the states is not fixed, since it varies according to the value of the control 
signal, and therefore the model can take into account the duration variability between 
different utterances of the same word. 
Figure 1: a-Fully connected 2 state HCNN; 
b-Left to fight 8 state HCNN for word modeling. 
150 Levin 
HL USING HCNN 
Given the predictive form of HCNN described in the previous section, there are three 
basic problems of interest that must be solved for the model to be useful in real-world 
applications. This problems are the following: 
Segmentation problem: Here we attempt to uncover the hidden part of the model, 
i.e., given a network to and a sequence of observations { xt, t--O, ..., T }, to find the 
correct control sequence, which best explains the observations. This problem is solved 
using an optimality criterion, namely the prediction error, similar to Eq.(2), 
T 
ECto,cr)=E I1 11 2 , (5) 
t=l 
where c r denotes the control sequence c, � - � , cr, ci e C. For a given network, to, 
the prediction error (5) is a function of the hidden control input sequence, and thus 
segmentation is associated with the minimization: 
^T 
C, =argrirnE ( to, Cl r), (6) 
In the case of a finite-state, fully connected model, this minimization can be performed 
exhaustively, by minimizing for each observation separately, and for a fully connected 
HCNN with a real-valued control signal (i.e. not the finite state case), local 
minimization of (5) can be performed using the back-propagation algorithm. For a 
left-to-right model , global minimum of (5) is attained efficiently using the Viterbi 
algorithm [5]. 
Evaluation problem, namely how well a given network to matches a given sequence 
of observations { xt, t--O, ..., T }. The evaluation is a key point for many applications. 
For example, if we consider the case in which we are trying to choose among several 
competing networks, that repr
