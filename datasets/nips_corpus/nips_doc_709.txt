I I 
Analysis of Short Term Memories for Neural 
Networks 
Jose C. Principe, Hui-H. Hsu and Jyh-Ming Kuo 
Computational NeuroEngineering Laboratory 
Department of Electrical Engineering 
University of Florida, CSE 447 
Gainesville, FL 32611 
principe @ synapse.ee.ufl .edu 
Abstract 
Short term memory is indispensable for the processing of time 
varying information with artificial neural networks. In this paper a 
model for linear memories is presented, and ways to include 
memories in connectionist topologies are discussed. A comparison 
is drawn among different memory types, with indication of what is 
the salient characteristic of each memory model. 
INTRODUCTION 
An adaptive system that has to interact with the external world is faced with the 
problem of coping with the time varying nature of real world signals. Time varying 
signals, natural or man made, carry information in their time structure. The problem 
is then one of devising methods and topologies (in the case of interest here, neural 
topologies) that explore information along time.This problem can be appropriately 
called temporal pattern recognition, as opposed to the more traditional case of static 
pattern recognition. In static pattern recognition an input is represented by a point in 
a space with dimensionality given by the number of signal features, while in temporal 
pattern recognition the inputs are sequence of features. These sequence of features 
can also be thought as a point but in a vector space of increasing dimensionality. 
Fortunately the recent history of the input signal is the one that bears more 
information to the decision making, so the effective dimensionality is finite but very 
large and unspecified a priori. How to find the appropriate window of input data 
1011 
1012 Principe, Hsu, and Kuo 
(memory depth) for a given application is a difficult problem. Likewise, how to 
combine the information in this time window to better meet the processing goal is 
also nontrivial. Since we are interested in adaptive systems, the goal is to let the 
system find these quantities adaptively using the output error information. 
These abstract ideas can be framed more quantitatively in a geometric setting (vector 
space). Assume that the input is a vector [u(1),... u(n), .... ] of growing size. The 
adaptive processor (a neural network in our case) has a fixed size to represent this 
information, which we assign to its state vector [xl(n), .... xN(n)] of size N. The 
usefulness of xl(n) depends on how well it spans the growing input space (defined by 
the vector u(n)), and how well it spans the decision space which is normally 
associated with the minimization of the mean square error (Figure 1). Therefore, in 
principle, the procedure can be divided into a representational and a mapping 
problem. 
The most general solution to this problem is to consider a nonlinear projection 
manifold which can be modified to meet both requirements. In terms of neural 
topologies, this translates to a full recurrent system, where the weights are adapted 
such that the error criterion is minimized. Experience has shown that this is a rather 
difficult proposition. Instead, neural network researchers have worked with a wealth 
of methods that in some way constrain the neural topology. 
' Projection space 
Nonlinear mapping 
x(n) error 
Decision space 
Figure 1. Projection of u(n) and the error for the task. (for simplicity we 
are representing only linear manifolds) 
The solution that we have been studying is also constrained. We consider a linear 
manifold as the projection space, which we call the memory space. The projection of 
u(n) in this space is subsequently mapped by means of a feedforward neural network 
(multilayer perceptton) to a vector in decision space that minimizes the error 
criterion. This model gives rise to the focused topologies. The advantage of this 
constrained model is that it allows an analytical study of the memory structures, since 
they become linear filters. It is important to stress that the choice of the projection 
space is crucial for the ultimate performance of the system, because if the projected 
version of u(n) in the memory space discards valuable information about u(n), then 
Analysis of Short Term Memories for Neural Networks 1013 
the nonlinear mapping will always produce sub-optimal results. 
2 Projection in the memory space 
If the projection space is linear, then the representational problem can be studied with 
linear system concepts. The projected vector u(n) becomes Yn 
N 
Yn = E widen- I 
k=l 
(1) 
where x n are the memory traces. Notice that in this equation the coefficients w k are 
independent of time, and their number fixed to N. What is the most general linear 
structure that implements this projection operation? It is the generalizedfeedforward 
structure [Principe et al, 1992] (Figure 2), which in connectionist circles has been 
called the time lagged recursire network [Back and Tsoi, 1992]. One can show that 
the defining relation for generalized feedforward structures is 
g(n) = g(n) �g_l(n) k>l 
where � represents the convolution operation, and go(n) = 5(n). This relation 
means that the next state vector is constructed from the previous state vector by 
convolution with the same function g(n), yet unspecified. Different choices of g(n) 
will provide different choices for the projection space axes. When we apply the input 
u(n) to this structure, the axes of the projection space become xk(n), the convolution 
of u(n) with the tap signals. The projection is obtained by linearly weighting the tap 
signals according to equation (1). 
[ g(n) f/ g(n) 
gl ((n22(n)  gk(n) 
Figure 2. The generalized feedforward structure 
We define a memory structure as a linear system whose generating kernel g(n) is 
causal g (n) - 0 for n < 0 and normalized, i.e. 
 Ig(n)l - 1 
n=0 
We define memory depth D as the modified center of mass (first moment in time) of 
the last memory tap. 
D = Englc(n) 
n=0 
And we define the memory resolution R as the number of taps by unit time, which 
1014 Principe, Hsu, and Kuo 
becomes 1/D. The purpose of the memory structure is to transform the search for an 
unconstrained number of coefficients (as necessary if we worked directly with u(n)) 
into one of seeking a fixed number of coefficients in a space with time varying axis. 
3 Review of connectionist memory structures 
The gamma memory [deVries and Principe, 1992] contains as special cases the 
context unit [Jordan, 1986] and the tap delay line as used in TDNN [Waibel et al, 
1989]. However, the gamma memory is also a special case of the generalized 
feedforward filters where g (n) =  (1 - [) n which leads to the gamma functions as 
the tap signals. Figure 3, adapted from [deVries and Principe, 1993], shows the most 
common connectionist memory structures and its characteristics. 
As can be seen when k=l, the gamma memory defaults to the context unit, and when 
=1 the gamma memory becomes the tap delay line. In vector spaces the context unit 
represents a line, and by changing [ we are finding the best projection of u(n) on this 
line. This representation is appropriate when one wants long memories but low 
resolution. 
Likewise, in the tap delay line, we are projecting u(n) in a memory space that is 
uniquely determined by the input signal, i.e. once the input signal u(n) is set, the axes 
become u(n-k) and the only degree of freedom is the memory order K. This memory 
structure has the highest resolution but lacks versatility, since one can only improve 
the input signal representation by increasing the order of the memory. In this respect, 
the simple context unit is better (or any memory with a recursive parameter), since 
the neural system can adapt the parameter  to project the input signal for better 
performance. 
We recently proved that the gamma memory structure in continuous time represents 
a memory space that is rigid [Principe et al, 1994]. When minimizing the output mean 
square error, the distance between the input signal and the projection space 
decreases. The recursive parameter in the feedforward structures changes the span of 
the memory space with respect to the input signal u(n) (which can be visualized as 
some type of complex rotation). In terms of time domain analysis, the recursive 
parameter is finding the length of the time window (the memory depth) containing 
the relevant information to decrease the output mean square error. The recursive 
parameter  can be adapted by gradient descent learning [deVries and Principe, 
1992], but the adaptation becomes nonlinear and multiple minima exists.Notice that 
the memory structure is stable for 0<<2. 
The gamma memory when utilized as a linear adaptive filter extends Widrow's 
ADALINE [deVries et al, 1992], and results in a more parsimonious filter for echo 
cancellation [Palkar and Principe, 1994]. Preliminary results with the gamma 
memory in speech also showed that the performance of word spotters improve when 
 is different from one (i.e. when it is not the tap delay line). In a signal such as 
speech where time warping is a problem, there is no need to use the full resolution 
provided by the tap delay line. It is more important to trade depth by resolution. 
Analysis of Short Term Memories for Neural Networks 1015 
4 
Other Memory Structures 
There are other memory structures that fit our definition. Back and Tsoi proposed a 
lattice structure that fits our definition of generalized feedforward structure. 
Essentially this system orthogonalizes the input, uncorrelating the axis of the vector 
space (or the signals at the taps of the memory). This method is known to provide the 
best speed of adaptation because gradient descent becomes Newton's method (after 
the lattice parameters converge). The problem is that it becomes more computational 
demanding (more parameters to adapt, and more calculations to perform). 
u(t) 
Tape delay
