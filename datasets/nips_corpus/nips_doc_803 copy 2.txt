Locally Adaptive Nearest Neighbor 
Algorithms 
Dietrich Wettschereck Thomas G. Dietterich 
Department of Computer Science 
Oregon State University 
Corvallis, OR 97331-3202 
escd�s o ors. edu 
Abstract 
Four versions of a k-nearest neighbor algorithm with locally adap- 
tive k are introduced and compared to the basic k-nearest neigh- 
bor algorithm (kNN). Locally adaptive kNN algorithms choose the 
value of k that should be used to classify a query by consulting the 
results of cross-validation computations in the local neighborhood 
of the query. Local kNN methods are shown to perform similar to 
kNN in experiments with twelve commonly used data sets. Encour- 
aging results in three constructed tasks show that local methods 
can significantly outperform kNN in specific applications. Local 
methods can be recommended for on-line learning and for appli- 
cations where different regions of the input space are covered by 
patterns solving different sub-tasks. 
1 Introduction 
The k-nearest neighbor algorithm (kNN, Dasarathy, 1991) is one of the most ven- 
erable algorithms in machine learning. The entire training set is stored in memory. 
A new example is classified with the class of the majority of the k nearest neighbors 
among all stored training examples. The (global) value of k is generally determined 
via cross-validation. 
For certain applications, it might be desirable to vary the value of k locally within 
184 
Locally Adaptive Nearest Neighbor Algorithms 185 
different parts of the input space to account for varying characteristics of the data 
such as noise or irrelevant features� However, for lack of an algorithm, researchers 
have assumed a global value for k in all work concerning nearest neighbor algorithms 
to date (see, for example, Bottou, 1992, p. 895, last two paragraphs of Section 4.1). 
In this paper, we propose and evaluate four new algorithms that determine different 
values for k in different parts of the input space and apply these varying values to 
classify novel examples. These four algorithms use different methods to compute 
the k-values that are used for classification. 
We determined two basic approaches to compute locally varying values for k. One 
could compute a single k or a set of k values for each training pattern, or training 
patterns could be combined into groups and k value(s) computed for these groups. A 
procedure to determine the k to be used at classification time must be given in both 
approaches. Representatives of these two approaches are evaluated in this paper 
and compared to the global kNN algorithm. While it was possible to construct 
data sets where local algorithms outperformed kNN, experiments with commonly 
used data sets showed, in most cases, no significant differences in performance. A 
possible explanation for this behavior is that data sets which are commonly used 
to evaluate machine learning algorithms may all be similar in that attributes such 
as distribution of noise or irrelevant features are uniformly distributed across all 
patterns. In other words, patterns from data sets describing a certain task generally 
exhibit similar properties. 
Local nearest neighbor methods are comparable in computational complexity and 
accuracy to the (global) k-nearest neighbor algorithm and are easy to implement. In 
specific applications they can significantly outperform kNN. These applications may 
be combinations of significantly different subsets of data or may be obtained from 
physical measurements where the accuracy of measurements depends on the value 
measured. Furthermore, local kNN classifiers can be constructed at classification 
time (on-line learning) thereby eliminating the need for a global cross-validation 
run to determine the proper value of k o 
1.1 Methods compared 
The following nearest neighbor methods were chosen as representatives of the pos- 
sible nearest neighbor methods discussed above and compared in the subsequent 
experiments: 
k-nearest neighbor (kNN) 
This algorithm stores all of the training examples. A single value for k is 
determined from the training data. Queries are classified according to the 
class of the majority of their k nearest neighbors in the training data. 
localKNNk unrestricted 
This is the basic local kNN algorithm. The three subsequent algorithms 
are modifications of this method. This algorithm also stores all of the 
training examples. Along with each training example, it stores a list of 
those values of k that correctly classify that example under leave-one-out 
cross-validation. To classify a query q, the M nearest neighbors of the 
query are computed, and that k which classifies correctly most of these M 
186 Wettschereck and Dietterich 
neighbors is determined. Call this value kM,q. The query q is then classified 
with the class of the majority of its kM,q nearest neighbors. Note that kM,q 
can be larger or smaller than M. The parameter M is the only parameter 
of the algorithm, and it can be determined by cross-validation. 
1ocalKNNks pruned 
The list of k values for each training example generally contains many val- 
ues. A global histogram of k values is computed, and k values that appear 
fewer than L times are pruned from all lists (at least one k value must, 
however, remain in each list). The parameter L can be estimated via cross- 
validation. Classification of queries is identical to localKNNks unrestricted. 
1ocalKNNone  per class 
For each output class, the value of k that would result in the correct (leave- 
one-out) classification of the maximum number of training patterns from 
that class is determined. A query q is classified as follows: Assume there 
are two output classes, C' and C2. Let k and k2 be the k value computed 
for classes C' and C'2, respectively. The query is assigned to class C' if the 
percentage of the k nearest neighbors of q that belong to class C' is larger 
than the percentage of the k nearest neighbors of q that belong to class 
C. Otherwise, q is assigned to class C. Generalization of that procedure 
to any number of output classes is straightforward. 
localKNNone  per cluter 
An unsupervised cluster algorithm (RPCL,  Xu et al., 1993) is used to 
determine clusters of input data. A single k value is determined for each 
cluster. Each query is classified according to the k value of the cluster it is 
assigned to. 
2 Experimental Methods and Data sets used 
To measure the performance of the different nearest neighbor algorithms, we em- 
ployed the training set/test set methodology. Each data set was randomly par- 
titioned into a training set containing approximately 70% of the patterns and a 
test set containing the remaining patterns. After training on the training set, the 
percentage of correct classifications on the test set was measured. The procedure 
was repeated a total of 25 times to reduce statistical variation. In each experi- 
ment, the algorithms being compared were trained (and tested) on identical data 
sets to ensure that differences in performance were due entirely to the algorithms. 
Leave-one-out cross-validation (Weiss & Kulikowski, 1991) was employed in all ex- 
periments to estimate optimal settings for free parameters such as k in kNN and 
M in localKNN. 
 Rival Penalized Competitive Learning is a straightforward modification of the well 
known k-means clustering algorithm. RPCL's main advantage over k-means clustering is 
that one can simply initialize it with a sufficiently large number of clusters. Cluster centers 
are initialized outside of the input range covered by the training examples. The algorithm 
then moves only those cluster centers which are needed into the range of input values and 
therefore effectively ehminates the need for cross-validation on the number of clusters in 
k-means. This paper employed a simple version with the number of initial clusters always 
set to 25, ac set to 0.05 and ar to 0.002. 
Locally Adaptive Nearest Neighbor Algorithms 187 
We report the average percentage of correct classifications and its standard error. 
Two-tailed paired t-tests were conducted to determine at what level of significance 
one algorithm outperforms the other. We state that one algorithm significantly 
outperforms another when the p-value is smaller than 0.05. 
3 Results 
3.1 Experiments with Constructed Data Sets 
Three experiments with constructed data sets were conducted to determine the 
ability of local nearest neighbor methods to determine proper values of k. The data 
sets were constructed such that it was known before experimentation that varying 
k values should lead to superior performance. Two data sets which were presumed 
to require significantly different values of k were combined into a single data set 
for each of the first two experiments. For the third experiment, a data set was 
constructed to display some characteristics of data sets for which we assume local 
kNN methods would work best. The data set was constructed such that patterns 
from two classes were stretched out along two parallel lines in one part of the 
input space. The parallel lines were spaced such that the nearest neighbor for most 
patterns belongs to the same class as the pattern itself, while two out of the three 
nearest neighbors belong to the other class. In other parts of the input space, classes 
were well separated, but class labels were flipped such that the nearest neighbor of a 
query may indicate the wrong pattern while the majority of the k nearest neighbors 
(k > 3) would indicate the correct class (see also Figure 4). 
Figure 1 shows that in selected applications, local nearest neighbor methods can 
lead to significant improvements over kNN in predictive accuracy. 
Experiment 1 
Letter Led- 16 
Recogn. Display Combined 
- 66.4::1:0.4% ]69.14-0.3%i59.9-I-0.4.% 
5001500/16 i 50015001161  0o0/ 0o0/ e 
Experiment 2 
Sine-21 Wave-21 Combined 
54.1-1-0.9% 179.94-0.6%i77.24-0.2% 
280/120/21 [ 280/120/2 ' 560/240
