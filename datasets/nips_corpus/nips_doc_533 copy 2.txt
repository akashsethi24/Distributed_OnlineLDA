Tangent Prop- A formalism for specifying 
selected invariances in an adaptive network 
Patrice Simard 
AT&T Bell Laboratories 
101 Crawford Corner Rd 
Holmdel, NJ 07733 
Bernard Victorri 
Universit de Caen 
Caen 14032 Cedex 
France 
Yann Le Cun 
AT&T Bell Laboratories 
101 Crawford Corner Rd 
Holmdel, NJ 07733 
John Denker 
AT&T Bell Laboratories 
101 Crawford Corner Rd 
Holmdel, NJ 07733 
Abstract 
In many machine learning applications, one has access, not only to training 
data, but also to some high-level a priori knowledge about the desired be- 
havior of the system. For example, it is known in advance that the output 
of a character recognizer should be invariant with respect to small spa- 
tial distortions of the input images (translations, rotations, scale changes, 
etcetera). 
We have implemented a scheme that allows a network to learn the deriva- 
tive of its outputs with respect to distortion operators of our choosing. 
This not only reduces the learning time and the amount of training data, 
but also provides a powerful language for specifying what generalizations 
we wish the network to perform. 
I INTRODUCTION 
In machine learning, one very often knows more about the function to be learned 
than just the training data. An interesting case is when certain directional deriva- 
tives of the desired function are known at certain points. For example, an image 
895 
896 Simard, Victorri, Le Cun, and Denker 
?' 
I / 
J / 
Figure 1: Top: Small rotations of an original digital image of the digit 3 (center). 
Middle: Representation of the effect of the rotation in the input vector space space 
(assuming there are only 3 pixels). Bottom: Images obtained by moving along the 
tangent to the transformation curve for the same original digital image (middle). 
recognition system might need to be invariant with respect to small distortions of 
the input image such as translations, rotations, scalings, etc.; a speech recognition 
system n.ight need to be invariant to time distortions or pitch shifts. In other 
words, the derivative of the system's output should be equal to zero when the input 
is transformed in certain ways. 
Given a large amount of training data and unlimited training time, the system 
could learn these invariances from the data alone, but this is often infeasible. The 
limitation on data can be overcome by training the system with additional data 
obtained by distorting (translating, rotating, etc.) the original patterns (Baird, 
1990). The top of Fig. 1 shows artificial data generated by rotating a digital image of 
the digit 3 (with the original in the center). This procedure, called the distortion 
model, has two drawbacks. First, the user must choose the magnitude of distortion 
and how many instances should be generated. Second, and more importantly, the 
distorted data is highly correlated with the original data. This makes traditional 
learning algorithms such as backpropagation very inefficient. The distorted data 
carries only a very small incremental amount of information, since the distorted 
patterns are not very different from the original ones. It may not be possible to 
adjust the learning system so that learning the invariances proceeds at a reasonable 
rate while learning the original points is non-divergent. 
The key idea in this paper is that it is possible to directly learn the effect on 
the output of distorting the input, independently from learning the undistorted 
Tangent Prop--A formalism for specifying selected invariances in an adaptive network 897 
F(x) 
I I I I  I I I : = 
xl x2 x3 x4 x xl x2 x3 x4 x 
Figure 2: Learning a given function (solid line) from a limited set of example 
to x4). The fitted curves are shown in dotted line. Top: The only constraint is that 
the fitted curve goes through the examples. Bottom: The fitted curves not only 
goes through each examples but also its derivatives evaluated at the examples agree 
with the derivatives of the given function. 
patterns. When a pattern P is transformed (e.g. rotated) with a transformation 
s that depends on one parameter a (e.g. the angle of the rotation), the set of all 
the transformed patterns S(P) = {s(a,P) ï¿½a} is a one dimensional curve in the 
vector space of the inputs (see Fig. 1). In certain cases, such as rotations of digital 
images, this curve must be made continuous using smoothing techniques, as will be 
shown below. When the set of transformations is parameterized by n parameters 
ai (rotation, translation, scaling, etc.), S(P) is a manifold of at most n dimensions. 
The patterns in S(P) that are obtained through small transformations of P, i.e. 
the part of S(P) that is close to P, can be approximated by a plane tangent to 
the manifold S(P) at point P. Small transformations of P can be obtained by 
adding to P a linear combination of vectors that span the tangent plane (tangent 
vectors). The images at the bottom of Fig. I were obtained by that procedure. 
More importantly, the tangent vectors can be used to specify high order constraints 
on the function to be learned, as explained below. 
To illustrate the method, consider the problem of learning a single-valued function 
F from a limited set of examples. Fig. 2 (left) represents a simple case where the 
desired function F (solid line) is to be approximated by a function G (dotted line) 
from four examples {(xi,F(xi))}i=,2,a,4. As exemplified in the picture, the fitted 
function G largely disagrees with the desired function F between the examples. If 
the functions F and G are assumed to be differentiable (which is generally the case), 
the approximation G can be greatly improved by requiring that G's derivatives 
evaluated at the points {xi} are equal to the derivatives of F at the same points 
(Fig. 2 right). This result can be extended to multidimensional inputs. In this case, 
we can impose the equality of the derivatives of F and G in certain direclions, not 
necessarily in all directions of the input space. 
Such constraints find immediate use in traditional learning problems. It is often the 
case that a priori knowledge is available on how the desired function varies with 
898 
Simard, Victorri, Le Cun, and Denker 
pattern P 
pattern P 
rotated by a 
tangent 
vector 
Figure 3: How to compute a tangent vector for a given transformation (in this case 
a rotation). 
respect to some transformations of the input. It is straightforward to derive the 
corresponding constraint on the directional derivatives of the fitted function G in 
the directions of the transformations (previously named tangent vectors). Typical 
examples can be found in pattern recognition where the desired classification func- 
tion is known to be invariant with respect to some transformation of the input such 
as translation, rotation, scaling, etc., in other words, the directional derivatives of 
the classification function in the directions of these transformations is zero. 
2 IMPLEMENTATION 
The implementation can be divided into two parts. The first part consists in com- 
puting the tangent vectors. This part is independent from the learning algorithm 
used subsequently. The second part consists in modifying the learning algorithm 
(for instance backprop) to incorporate the information about the tangent vectors. 
Part I: Let x be an input pattern and s be a transformation operator acting 
on the input space and depending on a parameter a. If s is a rotation operator 
for instance, then s(a, x) denotes the input x rotated by the angle a. We will 
require that the transformation operator s be differentiable with respect to a and 
x, and that s(0, x)= x. The tangent vector is by definition Os(a, x)/Oa. It can be 
approximated by a finite difference, as shown in Fig. 3. In the figure, the input space 
is a 16 by 16 pixel image and the patterns are images of handwritten digits. The 
transformations considered are rotations of the digit images. The tangent vector 
is obtained in two steps. First the image is rotated by an infinitesimal amount a. 
This is done by computing the rotated coordinates of each pixel and interpolating 
the gray level values at the new coordinates. This operation can be advantageously 
combined with some smoothing using a convolution. A convolution with a Gaussian 
provides an efficient interpolation scheme in O(nrn) multiply-adds, where n and m 
are the (gaussian) kernel and image sizes respectively. The next step is to subtract 
(pixel by pixel) the rotated image from the original image and to divide the result 
Tangent Prop--A formalism for specifying selected invariances in an adaptive network 899 
by the scalar a (see Fig. 3). If k types of transformations are considered, there 
will be k different tangent vectors per pattern. For most algorithms, these do not 
require any storage space since they can be generated as needed from the original 
pattern at negligible cost. 
Part II: Tangent prop is an extension of the backpropagation algorithm, allowing 
it to learn directional derivatives. Other algorithms such as radial basis functions 
can be extended in a similar fashion. 
To implement our idea, we will modify the usual weight-update rule: 
OE 0 
Aw = -/ww is replaced with Aw = -/ww(E + ItEr) (1) 
where r/is the learning rate, E the usual objective function, Er an additional objec- 
tive function (a regularizer) that measures the discrepancy between the actual and 
desired directional derivatives in the directions of some selected transformations, 
and It is a weighting coefficient. 
Let x be an input pattern, y = G(x) be the input-output function of the network. 
The regularizer E is of the form 
= 
x ( t rainingset 
where E (z) is 
E,(x) = .[[Ki(x)-, ' [[' (2) 
Here, Ki(x) is the desired directional derivative of G in the direction induced by 
transformation si applied to pattern x. The second term in the norm symbol is the 
actual directional derivative, which can be rewritten as 
I = G'(x). I 
0 a=0 0 a=0 
where G
