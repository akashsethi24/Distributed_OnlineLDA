Generalization in decision trees and DNF: 
Does size matter? 
Mostefa Golea l, Peter L. Bartlettl% Wee Sun Lee 2 nd Llew Mason 1 
Department of Systems Engineering 
Research School of Information 
Sciences and Engineering 
Australian National University 
Canberra, ACT, 0200, Australia 
2 School of Electrical Engineering 
University College UNSW 
Australian Defence Force Academy 
Canberra, ACT, 2600, Australia 
Abstract 
Recent theoretical results for pattern classification with thresh- 
olded real-valued functions (such as support vector machines, sig- 
moid networks, and boosting) give bounds on misclassification 
probability that do not depend on the size of the classifier, and 
hence can be considerably smaller than the bounds that follow from 
the VC theory. In this paper, we show that these techniques can 
be more widely applied, by representing other boolean functions 
as two-layer neural networks (thresholded convex combinations of 
boolean functions). For example, we show that with high probabil- 
ity any decision tree of depth no more than d that is consistent with 
m training examples has misclassification probability no more than 
( 1 (Neff VCdim(L/) log 2 mlogd)) 1/2) where/g is the class of 
node decision functions, and Neff _ N can be thought of as the 
effective number of leaves (it becomes small as the distribution on 
the leaves induced by the training data gets far from uniform). 
This bound is qualitatively different from the VC bound and can 
be considerably smaller. 
We use the same technique to give similar results for DNF formulae. 
* Author to whom correspondence should be addressed 
260 M. Gotea, P Bartlett, W. S. Lee and L Mason 
1 INTRODUCTION 
Decision trees are widely used for pattern classification [2, 7]. For these problems, 
results from the VC theory suggest that the amount of training data should grow 
at least linearly with the size of the tree[4, 3]. However, empirical results suggest 
that this is not necessary (see [6, 10]). For example, it has been observed that the 
error rate is not always a monotonically increasing function of the tree size[6]. 
To see why the size of a tree is not always a good measure of its complexity, consider 
two trees, A with NA leaves and B with Ns leaves, where Ns << NA. Although A 
is larger than B, if most of the classification in A is carried out by very few leaves 
and the classification in B is equally distributed over the leaves, intuition suggests 
that A is actually much simpler than B, since tree A can be approximated well by 
a small tree with few leaves. In this paper, we forrealize this intuition. 
We give misclassification probability bounds for decision trees in terms of a new 
complexity measure that depends on the distribution on the leaves that is induced 
by the training data, and can be considerably smaller than the size of the tree. 
These results build on recent theoretical results that give misclassification probabil- 
ity bounds for thresholded real-valued functions, including support vector machines, 
sigmoid networks, and boosting (see [1, 8, 9]), that do not depend on the size of the 
classifier. We extend these results to decision trees by considering a decision tree as 
a thresholded convex combination of the leaf functions (the boolean functions that 
specify, for a given leaf, which patterns reach that leaf). We can then apply the 
misclassification probability bounds for such classifiers. In fact, we derive and use 
a refinement of the previous bounds for convex combinations of base hypotheses, 
in which the base hypotheses can come from several classes of different complexity, 
and the VC-dimension of the base hypothesis class is replaced by the average (un- 
der the convex coefiqcients) of the VC-dimension of these classes. For decision trees, 
the bounds we obtain depend on the effective number of leaves, a data dependent 
quantity that reflects how uniformly the training data covers the tree's leaves. This 
bound is qualitatively different from the VC bound, which depends on the total 
number of leaves in the tree. 
In the next section, we give some definitions and describe the techniques used. We 
present bounds on the misclassification probability of a thresholded convex combina- 
tion of boolean functions from base hypothesis classes, in terms of a misclassification 
margin and the average VC-dimension of the base hypotheses. In Sections 3 and 4, 
we use this result to give error bounds for decision trees and disjunctive normal 
form (DNF) formulae. 
2 
GENERALIZATION ERROR IN TERMS OF MARGIN 
AND AVERAGE COMPLEXITY 
We begin with some definitions. For a class 7/of {-1, 1 }-valued functions defined on 
the input space X, the convex hull co(7/) of7/is the set of [-1, 1f-valued functions of 
the form '-i aihi, where ai _> O, '-i ai = 1, and hi  7/. A function in co(7/) is used 
for classification by composing it with the threshold function, sgn: IR  {-1, 1}, 
which satisfies sgn(a) = i iff a _> 0. So f  co(7/) makes a mistake on the pair 
(x,y)  X x {-1,1} iff sgn(f(x))  y. We assume that labelled examples (x,y) 
are generated according to some probability distribution Z) on X x {-1, 1}, and we 
let Pv [E] denote the probability under Z) of an event E. If S is a finite subset 
of Z, we let Ps [E] denote the empirical probability of E (that is, the proportion 
of points in S that lie in E). We use Ev [.] and Es [.] to denote expectation in a 
similar way. For a function class H of {-1, 1}-valued functions defined on the input 
Generalization in Decision Trees and DNF: Does Size Matter? 261 
space X, the growth function and VC dimension of H will be denoted by IIy(m) 
and VCdim(H) respectively. 
In [8], Schapire et al give the following bound on the misclassification probability 
of a thresholded convex combination of functions, in terms of the proportion of 
training data that is labelled to the correct side of the threshold by some margin. 
(Notice that Pv [sgn(f(x))  y] <_ Pv [yf(x) <_ 0].) 
Theorem I ([8]) Let D be a distribution on X x (-1,1), 7/ a hypothesis class 
with VCdim(H) = d < cx, and 5 > O. With probability at least 1 -  over a training 
set $ of m examples chosen according to D, every function f 6 co(7/) and every 
0  0 satisfy 
rv[yf(x)_<o]<_rs[yf(x)<_o]q-o   o: 
q- log(1/$)) 
In Theorem 1, all of the base hypotheses in the convex combination f are elements 
of a single class 7/with bounded VC-dimension. The following theorem generalizes 
this result to the case in which these base hypotheses may be chosen from any of k 
classes, 7/x,... , 7/k, which can have different VC-dimensions. It also gives a related 
result that shows the error decreases to twice the error estimate at a faster rate. 
Theorem 2 Let D be a distribution on X x {-1, 1), 7/x,... ,7/k hypothesis classes 
with VCdim(Hi) = di, and 5 > O. With probability at least i - 5 over a training 
) 
set $ of m examples chosen according to D, every function f 6 co Ui=x 7/i and 
every 0 > 0 satisfy both 
rv [yf (x) _< O] _< r s [yf (x) _< 8] + 
0  (dlogm 
+ logk)log(mO2/d)+ log(i/J)) x/2) 
rv [yf(x) _< 0] _< 2rs [yf(x) _< 19] + 
O (- (2 (dlogm+ logk)log(mO2/d) + log(1/5))) , 
where d = Y.i aidj and the ai and ji are defined by f = Y-i aihi and hi  7/ji for 
ji e {1,... ,k). 
Proof sketch: We shall sketch only the proof of the first inequality of the the- 
orem. The proof closely follows the proof of Theorem i (see [8]). We consider 
a number of approximating sets of the form CN,t = { (l/N)= hi: hi  7/t 1' 
where I = (h,...,lN)  {1,...,k}  and N  N. Define C = UtC,t. 
) 
For a given f = Y-i aihi from co Ui= 7/i , we shall choose an approximation 
g  CN by choosing h,... ,hN independently from {h,h2,..., }, according to 
the distribution defined by the coefficients a. Let Q denote this distribution 
on CN. As in [8], we can take the expectation under this random choice of 
g e C to show that, for any 0 > 0, rv [yf(x) < 0] <_ Ea~ [Pt)[yg(x) < 8/2]] + 
exp(-NO/8). Now, for a given I E {1,... ,k} , the probability that there is 
a g in CN,t and a 0 > 0 for which Pv[yg(x) <_ 8/2] > Ps[yg(x) _< 8/2] + eiv,t 
is at most 8(N + 1)rI= (2*a*' exp(-rnev,t/32). Applying the union bound 
k d,, ! 
262 M. Golea, P Bartlett, W. S. Lee and L Mason 
(over the values of /), taking expectation over g . Q, and setting eN,t -' 
- In 8(N + 1) rl/N=l 2era  dr, k N/SN shows that, with probability at least 
i - 5N, every f and 0 > 0 satisfy Pv[yf(x) _ 0] _ Eg[Ps[yg(x) _ S/2]] + 
Eg [eN,t]. As above, we can bound the probability inside the first expectation 
in terms of Ps [yf(x) _ 9]. Also, Jensen's inequality implies that Eg [eN,t] _ 
(- (ln(8(N + 1)/SN)+ Nlnk + N Eiaidj, ln(2em))) / . Setting 5N = 5/(N(N + 
1)) and N= [ln(-)] gives the result. I 
Theorem 2 gives misclassification probability bounds only for thresholded convex 
combinations of boolean functions. The key technique we use in the remainder of the 
paper is to find representations in this form (that is, as two-layer neural networks) 
of more arbitrary boolean functions. We have some freedom in choosing the convex 
coefficients, and this choice affects both the error estimate Ps [yf(x) _< 0] and the 
average VC-dimension d. We attempt to choose the coefficients and the margin 0 
so as to optimize the resulting bound on misclassification probability. In the next 
two sections, we use this approach to find misclassification probability bounds for 
decision trees and DNF formulae. 
3 DECISION TREES 
A two-class decision tree T is a tree whose internal decision nodes are labeled with 
boolean functions from some class 
from {-1, q.1}. For a tree with N leaves, define the leaf functions, hi: X --> {--1, 1} 
by hi(x) - I iff x reaches leaf i, for i - 1,... , N. Note that hi is the conjunction 
of all tests on the path from the root to leaf i. 
For a sample $ and a tree T, let Pi = Ps [hi(x) -' 1]. Clearly, P = (P,..., PN) is 
a probability vector. 
