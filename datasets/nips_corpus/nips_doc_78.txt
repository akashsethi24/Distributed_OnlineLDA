485 
TOWARDS AN ORGANIZING PRINCIPLE FOR 
A LAYERED PERCEPTUAL NETWORK 
Ralph Linsker 
IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598 
Abstract 
An information-theoretic optimization principle is proposed for the development 
of each processing stage of a multilayered perceptual network. This principle of 
maximum information preservation states that the signal transformation that is to be 
realized at each stage is one that maximizes the information that the output signal values 
(from that stage) convey about the input signals values (to that stage), subject to certain 
constraints and in the presence of processing noise. The quantity being maximized is a 
Shannon information rate. I provide motivation for this principle and -- for some simple 
model cases -- derive some of its consequences, discuss an algorithmic implementation, 
and show how the principle may lead to biologically relevant neural architectural 
features such as topographic maps, map distortions, orientation selectivity, and 
extraction of spatial and temporal signal correlations. A possible connection between 
this information-theoretic principle and a principle of minimum entropy production in 
nonequilibrium thermodynamics is suggested. 
Introduction 
This paper describes some properties of a proposed information-theoretic 
organizing principle for the development of a layered perceptual network. The purpose 
of this paper is to provide an intuitive and qualitative understanding of how the principle 
leads to specific feature-analyzing properties and signal transformations in some simple 
model cases. More detailed analysis is required in order to apply the principle to cases 
involving more realistic patterns of signaling activity as well as specific constraints on 
network connectivity. 
This section gives a brief summary of the results that motivated the formulation 
of the organizing principle, which I call the principle of maximum information 
preservation. In later sections the principle is stated and its consequences studied. 
In previous work  I analyzed the development of a layered network of model cells 
with feedforward connections whose strengths change in accordance with a Hebb-type 
synaptic modification rule. I found that this development process can produce cells that 
are selectively responsive to certain input features, and that these feature-analyzing 
properties become progressively more sophisticated as one proceeds to deeper cell 
layers. These properties include the analysis of contrast and of edge orientation, and 
are qualitatively similar to properties observed in the first several layers of the 
mammalian visual pathway. 2 
Why does this happen? Does a Hebb-type algorithm (which adjusts synaptic 
strengths depending upon correlations among signaling activities 3) cause a developing 
perceptual network to optimize some property that is deeply connected with the mature 
network's functioning as an information processing system? 
American Institute of Physics 1988 
486 
Further analysis 4.s has shown that a suitable Hebb-type rule causes a 
linear-response cell in a layered feedforward network (without lateral connections) to 
develop so that the statistical variance of its output activity (in response to an ensemble 
of inputs from the previous layer) is maximized, subject to certain constraints. The 
mature cell thus performs an operation similar to principal component analysis (PCA), 
an approach used in statistics to expose regularities (e.g., clustering) present in 
high-dimensional input data. (Oja 6 had earlier demonstrated a particular form of 
Hebb-type rule that produces a model cell that implements PCA exactly.) 
Furthermore, given a linear device that transforms inputs into an output, and given 
any particular output value, one can use optimal estimation theory to make a best 
estimate of the input values that gave rise to that output. Of all such devices, I have 
found that an appropriate Hebb-type rule generates that device for which this best 
estimate comes closest to matching the input values. 4.5 Under certain conditions, such 
a cell has the property that its output preserves the maximum amount of information 
about its input values? 
Maximum Information Preservation 
The above results have suggested a possible organizing principle for the 
development of each layer of a multilayered perceptual network? The principle can be 
applied even if the cells of the network respond to their inputs in a nonlinear fashion, 
and even if lateral as well as feedforward connections are present. (Feedback from later 
to earlier layers, however, is absent from this formulation.) This principle of maximum 
information preservation states that for a layer of cells L that is connected to and 
provides input to another layer M, the connections should develop so that the 
transformation of signals from L to M (in the presence of processing noise) has the 
property that the set of output values M conveys the maximum amount of information 
about the input values L, subject to various constraints on, e.g., the range of lateral 
connections and the processing power of each cell. The statistical properties of the 
ensemble of inputs L are assumed stationary, and the particular L-to-M transformation 
that achieves this maximization depends on those statistical properties. The quantity 
being maximized is a Shannon information rate. 7 
An equivalent statement of this principle is: The L-to-M transformation is chosen 
so as to minimize the amount of information that would be conveyed by the input values 
L to someone who already knows the output values M. 
We shall regard the set of input signal values L (at a given time) as an input 
message; the message is processed to give an output message M. Each message is in 
general a set of real-valued signal activities. Because noise is introduced during the 
processing, a given input message may generate any of a range of different output 
messages when processed by the same set of connections. 
The Shannon information rate (i.e., the average information transmitted from L 
to M per message) is 7 
R = E� E m P(L,M) log [P(L,M)/P(L)P(M)]. 
(1) 
For a discrete message space, P(L) [resp. P(M)] is the probability of the input (resp. 
output) message being L (resp. M), and P(L,M) is the joint probability of the input 
being L and the output being M. [For a continuous message space, probabilities are 
487 
replaced by probability densities, and sums (over states) by integrals.] This rate can be 
written as 
(2) 
where 
I� -- - Y� P(L) log P(L) 
(3) 
is the average information conveyed by message L and 
ItiM-- ZMP(M) YtP(LlM) logP(LlM) 
(4) 
is the average information conveyed by message L to someone who already knows M. 
Since It. is fixed by the properties of the input ensemble, maximizing R means 
minimizing ILi M, as stated above. 
The information rate R can also be written as 
L (5) 
where IM and I1L are defined by interchanging L and M in Eqns. 3 and 4. This form is 
heuristically useful, since it suggests that one can attempt to make R large by (if 
possible) simultaneously making I large and/ul  small. The term I is largest when 
each message M occurs with equal probability. The term !tl . is smallest when each L 
is transformed into a unique M, and more generally is made small by sharpening the 
P(M[ L) distribution, so that for each L, P(M [ L) is near zero except for a small set of 
messages M. 
How can one gain insight into biologically relevant properties of the L  M 
transformation that may follow from the principle of maximum information preservation 
(which we also call the infomax principle)? In a network, this L  M transformation 
may be a function of the values of one or a few variables (such as a connection strength) 
for each of the allowed connections between and within layers, and for each cell. The 
search space is quite large, particularly from the standpoint of gaining an intuitive or 
qualitative understanding of network behavior. We shall therefore consider a simple 
model in which the dimensionalities of the L and M signal spaces are greatly reduced, 
yet one for which the infomax analysis exhibits features that may also be important 
under more general conditions relevant to biological and synthetic network 
development. 
The next four sections are organized as follows. (i) A model is introduced in 
which the L and M messages, and the L-to-M transformation, have simple forms. The 
infomax principle is found to be satisfied when some simple geometric conditions (on 
the transformation) are met. (ii) I relate this model to the analysis of signal processing 
and noise in an interconnection network. The formation of topographic maps is 
discussed. (iii) The model is applied to simplified versions of biologically relevant 
problems, such as the emergence of orientation selectivity. (iv) I show that the main 
properties of the infomax principle for this model can be realized by certain local 
algorithms that have been proposed to generate topographic maps using lateral 
interactions. 
488 
A Simple Geometric Model 
In this model, each input message L is described by a point in a low-dimensional 
vector space, and the output message M is one of a number of discrete states. For 
definiteness, we will take the L space to be two-dimensional (the extension to higher 
dimensionality is straightforward). The L  M transformation consists of two steps. 
(i) A noise process alters L to a message L  lying within a neighborhood of radius v 
centered on L. (ii) The altered message L  is mapped deterministically onto one of the 
output messages M. 
A given L   M mapping corresponds to a partitioning of the L space into regions 
labeled by the output states M. (We do not exclude a priori the possibility that multiple 
disjoint regions may be labeled by the same M.) Let A denote the total area of the L 
state space. For each M,
