Clustering Sequences with Hidden 
Markov Models 
Padhraic Smyth 
Information and Computer Science 
University of California, Irvine 
CA 92697-3425 
smyhics. uci. edu 
Abstract 
This paper discusses a probabilistic model-based approach to clus- 
tering sequences, using hidden Markov models (HMMs). The prob- 
lem can be framed as a generalization of the standard mixture 
model approach to clustering in feature space. Two primary issues 
are addressed. First, a novel parameter initialization procedure is 
proposed, and second, the more difficult problem of determining 
the number of clusters K, from the data, is investigated. Experi- 
mental results indicate that the proposed techniques are useful for 
revealing hidden cluster structure in data sets of sequences. 
I Introduction 
Consider a data set D consisting of N sequences, D = {Si,...,$N}. $i = 
._ï¿½) is a sequence of length Li composed of potentially multivariate fea- 
ture vectors x_. The problem addressed in this paper is the discovery from data of a 
natural grouping of the sequences into K clusters. This is analagous to clustering in 
multivariate feature space which is normally handled by methods such as k-means 
and Gaussian mixtures. Here, however, one is trying to cluster the sequences $ 
rather than the feature vectors x. As an example Figure i shows four sequences 
which were generated by two different models (hidden Markov models in this case). 
The first and third came from a model with slower dynamics than the second and 
fourth (details will be provided later). The sequence clustering problem consists 
of being given sample sequences such as those in Figure I and inferring from the 
data what the underlying clusters are. This is non-trivial since the sequences can 
be of different lengths and it is not clear what a meaningful distance metric is for 
sequence comparison. 
The use of hidden Markov models for clustering sequences appears to have first 
Clustering Sequences with Hidden Markov Models 649 
0 10 20 30 40 50 0 70 0 90 100 
0 10 20 30 40 50 0 70 80 90 100 
0 10 20 30 40 0 0 70 0 90 100 
0 10 2o 3o 4o ,5o eO 70 0 9o lOO 
Po.ition in Sequence 
Figure 1: Which sequences came from which hidden Markov model ? 
been mentioned in Juang and Rabiner (1985) and subsequently used in the context 
of discovering subfamilies of protein sequences in Krogh et al. (1994). This present 
paper contains two new contributions in this context: a cluster-based method for 
initializing the model parameters and a novel method based on cross-validated like- 
lihood for determining automatically how many clusters to fit to the data. 
A natural probabilistic model for this problem is that of a finite mixture model: 
K 
f:($)- y:.fy($10)pj (1) 
j--1 
where S denotes a sequence, pj is the weight of the jth model, and fi(SlO) is 
the density function for the sequence data $ given the component model f1 with 
parameters 0. Here we will assume that the f1 's are HIMMs: thus, the 0i's are the 
transition matrices, observation density parameters, and initial state probabilities, 
all for the jth component. f(SlOi) can be computed via the forward part of the 
forward backward procedure. More generally, the component models could be any 
probabilistic model for S such as linear autoregressive models, graphical models, 
non-linear networks with probabilistic semantics, and so forth. 
It is important to note that the motivation for this problem comes from the goal 
of building a descriptive model for the data, rather than prediction per se. For the 
prediction problem there is a clearly defined metric for performance, namely average 
prediction error on out-of-sample data (cf. Rabiner et al. (1989) in a speech context 
with clusters of HMMs and Zeevi, Meir, and Adler (1997) in a general time-series 
context). In contrast, for descriptive modeling it is not always clear what the 
appropriate metric for evaluation is, particularly when K, the number of clusters, 
is unknown. In this paper a density estimation viewpoint is taken and the likelihood 
of out-of-sample data is used as the measure of the quality of a particular model. 
2 An Algorithm for Clustering Sequences into K Clusters 
Assume first that K, the number of clusters, is known. Our model is that of a 
mixture of HMMs as in Equation 1. We can immediately observe that this mixture 
can itself be viewed as a single composite HMM where the transition matrix A of 
the model is block-diagonal, e.g., if the mixture model consists of two components 
with transition matrices A and A2 we can represent the overall mixture model as 
650 P. Stnyth 
a single HMM (in effect, a hierarchical mixture) with transition matrix 
( 0 
As ) (2) 
where the initial state probabilities are chosen appropriately to reflect the relative 
weights of the mixture components (the p in Equation 1). Intuitively, a sequence is 
generated from this model by initially randomly choosing either the upper matrix 
A (with probability p) or the lower matrix with probability As (with probability 
1 -p) and then generating data according to the appropriate Ai. There is no 
crossover in this mixture model: data are assumed to come from one component 
or the other. Given this composite HMM a natural approach is to try to learn 
the parameters of the model using standard HMM estimation techniques, i.e., some 
form of initialization followed by Baum-Welch to maximize the likelihood. Note 
that unlike predictive modelling (where likelihood is not necessarily an appropriate 
metric to evaluate model quality), likelihood maximization is exactly what we want 
to do here since we seek a generative (descriptive) model for the data. We will 
assume throughout that the number of states per component is known a priori, i.e., 
that we are looking for K HMM components each of which has m states and m 
is known. An obvious extension is to address the problem of learning K and m 
simultaneously but this is not dealt with here. 
2.1 Initialization using Clustering in Log-Likelihood Space 
Since the EM algorithm is effectively hill-climbing the likelihood surface, the quality 
of the final solution can depend critically on the initial conditions. Thus, using as 
much prior information as possible about the problem to seed the initialization is 
potentially worthwhile. This motivates the following scheme for initializing the A 
matrix of the composite HMM: 
1. Fit N m-state HMMs, one to each individual sequence Si, 1 _ i _ N. 
These HMMs can be initialized in a default manner: set the transition 
matrices uniformly and set the means and covariances using the k-means 
algorithm, where here k = m, not to be confused with K, the number of 
HMM components. For discrete observation alphabets modify accordingly. 
2. For each fitted model Mi, evaluate the log-likelihood of each of the N 
sequences given model Mi, i.e., calculate Lid = log L(,S'jlMi), 1 _ i,j _ N. 
3. Use the log-likelihood distance matrix to cluster the sequences into K 
groups (details of the clustering are discussed below). 
4. Having pooled the sequences into K groups, fit K HMMs, one to each group, 
using the default initialization described above. From the K HMMs we get 
K sets of parameters: initialize the composite HMM in the obvious way, 
i.e., the m x m block-diagonal component A 1 of A (where A is mK x inK) 
is set to the estimated transition matrix from the jth group and the means 
and covariances of the jth set of states are set accordingly. Initialize the pj 
in Equation 1 to Nj/N where N 1 is the number of sequences which belong 
to cluster j. 
After this initialization step is complete, learning proceeds directly on the composite 
HMM (with matrix A) in the usual Baum-Welch fashion using all of the sequences. 
The intuition behind this initialization procedure is as follows. The hypothesis is 
that the data are being generated by K models. Thus, if we fit models to each 
individual sequence, we will get noisier estimates of the model parameters (than 
if we used all of the sequences from that cluster) but the parameters should be 
Clustering Sequences with Hidden Markov Models 651 
clustered in some manner into K groups about their true values (assuming the 
model is correct). Clustering directly in parameter space would be inappropriate 
(how does one define distance?): however, the log-likelihoods are a natural way to 
define pairwise distances. 
Note that step 1 above requires the training of N sequences individually and step 2 
requires the evaluation of N s distances. For large N this may be impractical. Suit- 
able modifications which train only on a small random sample of the N sequences 
and randomly sample the distance matrix could help reduce the computational bur- 
den, but this is not pursued here. A variety of possible clustering methods can be 
used in step 3 above. The symmetrized distance Lid = 1/2(L($ilMj)+ L($j IMi)) 
can be shown to be an appropriate measure of dissimilarity between models Mi and 
Mj (Juang and Rabiner 1985). For the results described in this paper, hierarchical 
clustering was used to generate K clusters from the symmetrized distance matrix. 
The furthest-neighbor merging heuristic was used to encourage compact clusters 
and worked well empirically, although there is no particular reason to use only this 
method. 
We will refer to the above clustering-based initialization followed by Baum-Welch 
training on the composite model as the HMM-Clustering algorithm in the rest of 
the paper. 
2.2 Experimental Results 
Consider a deceptively simple toy problem. I-dimensional feature data are gen- 
erated from a 2-component HMM mixture (K = 2), each with 2 states. We have 
(0.60.4) (0.40.6) 
A1 = 0.4 0.6 As = 0.6 0.4 
and the observable feature data obey a Gaussian density in each state with 
cr = crs = I for each state in each component, and / = 0,/s = 3 for the re- 
spective mean of each state of each component. 4 sample seq
