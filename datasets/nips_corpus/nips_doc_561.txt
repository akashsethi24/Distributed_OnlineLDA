A Comparison of Projection Pursuit and Neural 
Network Regression Modeling 
Jenq-Neng Hwang, Hang Li, 
Information Processing Laboratory 
Dept. of Elect. Engr., FT-10 
University of Washington 
Seattle WA 98195 
Martin Maechler, R. Douglas Martin, Jim Schimert 
Department of Statistics 
Mail Stop: GN-22 
University of Washington 
Seattle, WA 98195 
Abstract 
Two projection based feedforward network learning methods for model- 
free regression problems are studied and compared in this paper: one is 
the popular back-propagation learning (BPL); the other is the projection 
pursuit learning (PPL). Unlike the totally parametric BPL method, the 
PPL non-parametrically estimates unknown nonlinear functions sequen- 
tially (neuron-by-neuron and layer-by-layer) at each iteration while jointly 
estimating the interconnection weights. In terms of learning efficiency, 
both methods have comparable training speed when based on a Gauss- 
Newton optimization algorithm while the PPL is more parsimonious. In 
terms of learning robustness toward noise outliers, the BPL is more sensi- 
tive to the outliers. 
1 INTRODUCTION 
The back-propagation learning (BPL) networks have been used extensively for es- 
sentially two distinct problem types, namely model-free regression and classification, 
1159 
1160 Hwang, Li, Maechler, Martin, and Schimert 
which have no a priori assumption about the unknown functions to be identified 
other than imposes a certain degree of smoothness. The projection pursuit learning 
(PPL) networks have also been proposed for both types of problems (Friedman85 
[3]), but to date there appears to have been much less actual use of PPLs for both 
regression and classification than of BPLs. In this paper, we shall concentrate on re- 
gression modeling applications of BPLs and PPLs since the regression setting is one 
in which some fairly deep theory is available for PPLs in the case of low-dimensional 
regression (Donoho89 [2], Jones87 [6]). 
A multivariate model-free regression problem can be stated as follows: given n 
pairs of vector observations, (Yl , xl) = (Yn,', Ylq; xtl,--., zip), which have been 
generated from unknown models 
Yu = gi(xl) + eli, 1 = 1,2,...,n; i = 1,2,...,q (1) 
where {yl} are called the multivariable response vector and {xt} are called the 
independent variables or the carriers. The {gi} are unknown smooth non- 
parametric (model-free) functions from p-dimensional Euclidean space to the real 
line, i.e., gi : R   R, Vi. The {eli} are random variables with zero mean, 
E[eu] = 0, and independent of {xl}. Often the {eli} are assumed to be independent 
and identically distributed (lid) as well. 
The goal of regression is to generate the estimators, , 2, '-', q, to best approxi- 
mate the unknown functions, g, g2, ..., gq, so that they can be used for prediction 
of a new y given a new x: i = 0i(x), �i. 
2 
A TWO-LAYER PERCEPTRON AND 
BACK-PROPAGATION LEARNING 
Several recent results have shown that a two-layer (one hidden layer) perceptron 
with sigmoidal nodes can in principle represent any Borel-measurable function to 
any desired accuracy, assuming enough hidden neurons are used. This, along with 
the fact that theoretical results are known for the PPL in the analogous two-layer 
case, justifies focusing on the two-layer perceptron for our studies here. 
2.1 MATHEMATICAL FORMULATION 
A two-layer perceptron can be mathematically formulated as follows' 
ui = wjxj-O =wix-O, k= 1, 2, ..., m 
j=l 
Yi -- yfii$(u)= -fiirr(u), i- 1, 2, ..., q 
k=l k=l 
(2) 
where u denotes the weighted sum input of the k th neuron in the hidden layer; 
0 denotes the bias of the k *h neuron in the hidden layer; wj denotes the input- 
layer weight linked between the k t hidden neuron and the jt neuron of the input 
A Comparison of Projection Pursuit and Neural Network Regression Modeling 1161 
layer (or jtn element of the input vector x); /3ik denotes the output-layer weight 
linked between the i th output neuron and the k th hidden neuron; fk is the nonlinear 
activation function, which is usually assumed to be a fixed monotonically increasing 
(logistic) sigmoidal function, a(u) = 1/(1 + e-'). 
The above formulation defines quite explicitly the parametric representation of 
functions which are being used to approximate {gi(x), i = 1,2,...,q}. A sim- 
ple reparametrization allows us to write .0i(x) in the form: 
i(x) = - 
8k 
k=l 
where r, is a unit length version of weight vector w. This formulation reveals how 
{0} are built up as a linear combination of sigmoids evaluated at translates (by 
ttk) and scaled (by s) projection of x onto the unit length vector r,. 
2.2 BACK-PROPAGATION LEARNING AND ITS VARIATIONS 
Historically, the training of a multilayer perceptron uses back-propagation learning 
(BPL). There are two common types of BPL: the batch one and the sequential one. 
The batch BPL updates the weights after the presentation of the complete set of 
training data. Hence, a training iteration incorporates one sweep through all the 
training patterns. On the other hand, the sequential BPL adjusts the network 
parameters as training patterns are presented, rather than after a complete pass 
through the training set. The sequential approach is a form of Robbins-Monro 
Stochastic Approximation. 
While the two-layer perceptron provides a very powerful nonparametric modeling 
capability, the BPL training can be slow and inefficient since only the first derivative 
(or gradient) information about the training error is utilized. To speed up the train- 
ing process, several second-order optimization algorithms, which take advantage of 
second derivative (or Hessian matrix) information, have been proposed for training 
perceptrons (Hwang90 [4]). For example, the Gauss-Newton method is also used in 
the PPL (Friedman85 [3]). 
The fixed nonlinear nodal (sigmoidal) function is a monotone nondecreasing differ- 
entiable function with very simple first derivative form, and possesses nice properties 
for numerical computation. However, it does not interpolate/extrapolate efficiently 
in a wide variety of regression applications. Several attempts have been proposed to 
improve the choice of nonlinear nodal functions; e.g., the Gaussian or bell-shaped 
function, the locally tuned radial basis functions, and semi-parametric (non-fixed 
nodal function) nonlinear functions used in PPLs and hidden Markov models. 
2.3 
RELATIONSHIP TO KERNEL APPROXIMATION AND DATA 
SMOOTHING 
It is instructive to compare the two-layer perceptron approximation in Eq. (3) 
with the well-known kernel method for regression. A kernel K(.) is a non-negative 
symmetric function which integrates to unity. Most kernels are also unimodal, with 
1162 
Hwang, Li, Maechler, Martin, and Schimert 
mode at the origin, I�(t) > I�(t2), 0 _< t < t2. A kernel estimate of gi(x) has the 
form 
n 1 I�( IIx - xtl[), 
.oK,i(x) = y, Yti  hq (4) 
1=1 
where h is a bandwidth parameter and q is the dimension of yt vector. Typically a 
good value of h will be chosen by a data-based cross-validation method. Consider for 
a moment the special case of the kernel approximator and the two-layer perceptton 
in Eq. (3) respectively, with scalaryt and xt, i.e., with p = q = 1 (hence unit length 
interconnection weight a = 1 by definition): 
X )-- Yt Xxt), 
(5) 
I=1 I=1 
- .k ) (6) 
k=l 
This reveals some important connections between the two approaches. 
Suppose that for .o(x), we set a = K, i.e., a is a kernel and in fact identical to the 
kernel K, and that/,tt, s = s have been chosen (trained), say by BPL. That is, 
all {sk} are constrained to a single unknown parameter value s. In general, m _< n, 
or even m is a modest fraction of n when the unknown function g(x) is reasonably 
smooth. Furthermore, suppose that h has been chosen by cross validation. Then one 
can expect .OK(X)  .Oa(X), particularly in the event that the {tt} are close to the 
observed values {xt} and x is close to a specific tt value (relative to h). However, 
in this case where we force s = s, one might expect .OK(X) to be a somewhat better 
estimate overall than .o,(x), since the former is more local in character. 
On the other hand, when one removes the restriction st = s, then BPL leads 
to a local bandwidth selection, and in this case one may expect .o,(x) to provide 
better approximation than .OK(X) when the function g(x) has considerably varying 
curvature, g(x), and/or considerably varying error variance for the noise eli in Eq. 
(1). The reason is that a fixed bandwidth kernel estimate can not cope as well with 
changing curvature and/or noise variance as can a good smoothing method which 
uses a good local bandwidth selection method. A small caveat is in order: if m is 
fairly large, the estimation of a separate bandwidth for each kernel location, ttk, may 
cause some increased variability in .Oo (x) by virtue of using many more parameters 
than are needed to adequately represent a nearly optimal local bandwidth selection 
method. Typically a nearly optimal local bandwidth function will have some degree 
of smoothness, which reflects smoothly varying curvature and/or noise variance, and 
a good local bandwidth selection method should reflect the smoothness constraints. 
This is the case in the high-quality supersmoother, designed for applications like 
the PPL (to be discussed), which uses cross-validation to select bandwidth locally 
(Friedman85 [3]), and combines this feature with considerable speed. 
The above arguments are probably equally valid without the restriction a = K, be- 
cause two sigmoids of opposite signs (via choice of two {ilk }) that are appropriately 
A Comparison of Projection Pursuit and Neural Network Regression Modeling 1163 
shifted, will approximate a kernel up to a scaling to enforce unity area. However, 
there is a novel aspect: one can have a separate local bandwidth for each half of 
the kernel, thereby using an asym
