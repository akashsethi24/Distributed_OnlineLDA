Predictive Approaches For Choosing 
Hyperparameters in Gaussian Processes 
S. Sundararajan 
Computer Science and Automation 
Indian Institute of Science 
Bangalore 560 012, India 
sundarcsa. iisc. ernet. in 
S. Sathiya Keerthi 
Mechanical and Production Engg. 
National University of Singapore 
10 Kentridge Crescent, Singapore 119260 
mpesskguppy. rope. nus. edu. sg 
Abstract 
Gaussian Processes are powerful regression models specified by 
parametrized mean and covariance functions. Standard approaches 
to estimate these parameters (known by the name Hyperparam- 
eters) are Maximum Likelihood (ML) and Maximum APosterior 
(MAP) approaches. In this paper, we propose and investigate pre- 
dictive approaches, namely, maximization of Geisser's Surrogate 
Predictive Probability (GPP) and minimization of mean square er- 
ror with respect to GPP (referred to as Geisser's Predictive mean 
square Error (GPE)) to estimate the hyperparameters. We also 
derive results for the standard Cross-Validation (CV) error and 
make a comparison. These approaches are tested on a number of 
problems and experimental results show that these approaches are 
strongly competitive to existing approaches. 
I Introduction 
Gaussian Processes (GPs) are powerful regression models that have gained popular- 
ity recently, though they have appeared in different forms in the literature for years. 
They can be used for classification also; see MacKay (1997), Rasmussen (1996) and 
Williams and Rasmussen (1996). Here, we restrict ourselves to regression problems. 
Neal (1996) showed that a large class of neural network models converge to a Gaus- 
sian Process prior over functions in the limit of an infinite number of hidden units. 
Although GPs can be created using infinite networks, often GPs are specified di- 
rectly using parametric forms for the mean and covariance functions (Williams and 
Rasmussen (1996)). We assume that the process is zero mean. Let Zv = {Xv, yv} 
whereXv = {x(i)' i = 1,...,N}andyv = {y(i)' i = 1,...,N}. Here, y(i) 
represents the output corresponding to the input vector x(i). Then, the Gaussian 
prior over the functions is given by 
p(yNlXN,) -- exp(-Yv(lYv) (1) 
where v is the covariance matrix with (i,j) th element 
and (.; ) denotes the parametrized covariance function. Now, assuming that the 
632 $. $undararajan and $. S. Keerthi 
observed output tv is modeled as tv = yv + eN and eN is zero mean multivariate 
Gaussian with covariance matrix a2Iv and is independent of yv, we get 
p(tNIXN,0) : �xP(--tTNC ltN) 
(2)lcNl (2) 
where Cv = v +a2Iv. Therefore, [Cv]i,j = [v]i,j +a6i,j, where 6i,j = 1 when 
i = j and zero otherwise. Note that 0 = (0,a ) is the new set of hyperparameters. 
Then, the predictive distribution of the output y(N + 1) for a test case x(N + 1) is 
also Gaussian with mean and variance 
9(N + 1) r -1 
= kN+lC N tN (3) 
and 
ay(N+) = bN+ - k/+ClkN+ (4) 
C(x(N + 1), x(N + 1); I) and kv+l is an N x 1 vector with i th 
where bN+ = 
element given by C(x(N + 1),x(i);0). Now, we need to specify the covariance 
function C(.; 0). Williams and Rasmussen (1996) found the following covariance 
function to work well in practice. 
(5) 
M I M 
(x(i),x(j); O) = ao + al 2 xp(i)xp(j) .- voexp(- y. wp (xp(i) - xp(j)) ) 
p----1 p----1 
where xp(i) is the pth component of i tn input vector x(i). The wp are the Auto- 
matic Relevance Determination (ARD) parameters. Note that C(x(i),x(j); O) = 
(x(i),x(j);O) + aSi,j. Also, all the parameters are positive and it is conve- 
nient to use logarithmic scale. Hence, 0 is given by log(ao, al, v0, Wl,..., wM, a2). 
Then, the question is: how do we handle 0 ? More sophisticated techniques 
like Hybrid Monte Carlo (HMC) methods (Rasmussen (1996) and Neal (1997)) 
are available which can numerically integrate over the hyperparameters to make 
predictions. Alternately, we can estimate 0 from the training data. We restrict 
to the latter approach here. In the classical approach, 0 is assumed to be de- 
terministic but unknown and the estimate is found by maximizing the likelihood 
argmax 
(2). That is, OML = 0 p(tvIXv, 0). In the Bayesian approach, 0 is 
assumed to be random and a prior p(O) is specified. Then, the MAP estimate 
argmax 
OMp is obtained as OMp = 0 p(tlX, O)p(O) with the motivation that 
the the predictive distribution p(y(N + 1)lx(N + 1), Zv) can be approximated as 
p(y(N + 1)lx(N + 1), ZN, 04). With this background, in this paper we propose 
and investigate different predictive approaches to estimate the hyperparameters 
from the training data. 
2 Predictive approaches for choosing hyperparameters 
Geisser (1975) proposed Predictive Sample Reuse (PSR) methodology that can be 
applied for both model selection and parameter estimation problems. The basic 
idea is to define a partition scheme P(N,n,r) such that P()_. = tZ . o 
 v-, Z ) is 
ita partition belonging to a set F of partitions with Z_n, Z/ � representing the 
N - n retained and n omitted data sets respectively. Then, the unknown  is esti- 
mated (or a model Mj is chosen among a set of models indexed by j = 1,..., J) 
by means of optimizing a predictive measure that measures the predictive perfor- 
mance on the omitted observations X/ � by using the retained observations Z_n 
averaged over the partitions (i  F). In the special case of n = 1, we have the 
leave one out strategy. Note that this approach was independently presented in the 
Predictive Approaches for Choosing Hyperparameters in Gaussian Processes 633 
name of cross-validation (CV) by Stone (1974). The well known examples are the 
standard CV error and negative of average predictive likelihood. Geisser and Eddy 
N � Z?, 
(1979) proposed to maximize rIi=lp(t()[x(i), Mj) (known as Geisser's surro- 
gate Predictive Probability (GPP)) by synthesizing Bayesian and PSR methodology 
in the context of (parametrized) model selection. Here, we propose to maximize 
rI=l p(t(i)lx(i), z?, 0) to estimate 0, where Z? is obtained from ZN by removing 
the i th sample. Note that p(t(i)]x(i), Z?, 0) is nothing but the predictive distribu- 
tion p(y(i)[x(i), Z?, 0) evaluated at y(i) - t(i). Also, we introduce the notion of 
i E/N=1 E((y(i) - t(i)) 2) 
Geisser's Predictive mean square Error (GPE) defined as  
(where the expectation operation is defined with respect to p(y(i)lx(i), Z?, 0)) and 
propose to estimate 0 by minimizing GPE. 
2.1 Expressions for GPP and its gradient 
The objective function corresponding to GPP is given by 
N 
1 
G(o) - log(p(t(i)lx(i), (6) 
i----1 
From (S) and (4) we get 
i k (t(i) - + (7) 
G(O) --  i----1 Y ) 
2cr2,i, 
where (i) [c?]r[C?]-lt? and  
-- fly(i) 
is an N - 1 x N - 1 matrix obtained from Cv by removing the i th column and 
i t row. Similarly, [() and c i) are obtained from tv and ci (i.e., i  column of 
Cv) respectively by removing the i  element. Then, G() and is gradien can be 
computed ecienly using the following result. 
N 
1  1 
2N  log fly(i) -]-  log 
i=1 
['(i)]T[g'(i)]--lr'(i) Here, C( ) 
= eli -- ['i J [NJ 'i ' 
Theorem 1 
by 
The objective function G(O) under the Gaussian Process model is given 
G(O) - 1 i.l q(i) I N 1 2 
2N c-it 27 ylogeii + log r (8) 
'---- i1 
where ii denotes the ita diagonal entry of C x and qv (i) denotes the i th element 
of qv = cltN � ItS gradient is given by 
OG(O) = I /1 (1 + q-(i))(sj'--!) + 
OOj 2N .__ eli Cii 
- __ OCN cl tN 
where sj,i - c/T�c----' rj - -C 1 o0 
-- O0 i , 
denotes the i  column of the matrix C 1 . 
1 v (r(i)] 
  qv(i), ii / (9) 
i----1 
and qv = cltN . Here, i 
Thus, using (8) and (9) we can compute the GPP and its gradient. We will give 
meaningful interpretation to the different terms shortly. 
2.2 Expressions for CV function and its gradient 
We define the CV function as 
N 
1 
H(O) =   (t(i) - (i))  
i=1 
(10) 
634 $. $undararajan and $. $. Keerthi 
where ?(i) is the mean of the conditional predictive distribution as given above. 
Now, using the following result we can compute H(O) efficiently. 
Theorem 2 The CV function H(O) under the Gaussian model is given by 
N 
1 (qN(i)) 2 (11) 
i=1 
and its gradient is given by 
OH(O) 1 
ooj N 
il (qv(i)rj(i) qv(i)) (sj,i) 
where 8j,i, rj, qlv(i) and ii are as defined in theorem 1. 
2.3 Expressions for GPE and its gradient 
The GPE function is defined as 
1 
= 
N 
N 
/(t(i) - y(i)) 2 p(y(i)lx(i),z(),o) dy(i) 
i=1 
(13) 
which can be readily simplified to 
N N 
fly(i) 
i=1 i=1 
(14) 
On comparing (14) with (10), we see that while CV error minimizes the deviation 
from the predictive mean, GPE takes predictive variance also into account. Now, 
the gradient can be written as 
0(7(0) OH(O) 1 N [lh2rg.TOC N 
aOj = OOj +  E \ii] ' O0j ei (15) 
i----1 
where we have used the results a 2 = i Oil OCv  OCq  __ 
y(i) ii' 00 -- eiT-i e i and o0 -- 
oC . 
-C 1 o0 C 1 Here ei denotes the i th column vector of the identity matrix IN. 
2.4 Interpretations 
More insight can be obtained from reparametrizing the covariance function as fol- 
lows. 
M I M 
C(x(i),x(j); O) = 0.2 (0_ 1 E xp(i)xp(j)+oexp(- 
p=l p=l 
(16) 
where ao - a 2 ao, al - a 2 al, vo = a 2 Vo. Let us define P(x(i),x(j);O) : 
- - ', where Ci,, i,j 
l-C(x(i),x(j); 0). Then rr 1 -- 0 '2 Cr 1. Therefore, ci,j -  
denote the (i, j)th element of the matrices C31 and pl respectively. From theorem 
2 (see (10) and (11)) we have t(i)- (i) 
(8) as 
I q(i) 
(7(0) = 2N0. 2 Z P-i 
i1 
Then, we can rewrite 
N 
1 logii + log2r0. 
2N 
(17) 
Predictive Approaches for Choosing Hyperparameters in Gaussian Processes 635 
Here, v = pltv and, i, ii denote, respectively, the i th column and i t' diagonal 
entry of the matrix pl. Now, by setting the derivative of (17) with respect to a 2 
to zero, we can infer the noise level as 
I N 
6. 2 _  y (i) (18) 
i=1 Pii 
Similarly, the CV error (10) can be rewritten as 
1 v qv(i) (19) 
i=1 Pii 

