A Short-Term Memory Architecture for the 
Learning of Morphophonemic Rules 
Michael Gasser and Chan-Do Lee 
Computer Science Department 
Indiana University 
Bloomington, IN 47405 
Abstract 
Despite its successes, Rumelhart and McClelland's (1986) well-known ap- 
proach to the learning of morphophonemic rules suffers from two deficien- 
cies: (1) It performs the artificial task of associating forms with forms 
rather than perception or production. (2) It is not constrained in ways 
that humans learners are. This paper describes a model which addresses 
both objections. Using a simple recurrent architecture which takes both 
forms and meanings as inputs, the model learns to generate verbs in 
one or another tense, given arbitrary meanings, and to recognize the 
tenses of verbs. Furthermore, it fails to learn reversal processes unknown 
in human language. 
BACKGROUND 
In the debate over the power of connectionist models to handle linguistic phenom- 
ena, considerable attention has been focused on the learning of simple morphological 
rules. It is a straightforward matter in a symbolic system to specify how the mean- 
ings of a stem and a bound morpheme combine to yield the meaning of a whole 
word and how the form of the bound morpheme depends on the shape of the stem. 
In a distributed connectionist system, however, where there may be no explicit 
morphemes, words, or rules, things are not so simple. 
The most important work in this area has been that of Rumelhart and McClelland 
(1986), together with later extensions by Marchman and Plunkerr (1989). The net- 
works involved were trained to associate English verb stems with the corresponding 
past-tense forms, successfully generating both regular and irregular forms and gen- 
eralizing to novel inputs. This work established that rule-like linguistic behavior 
605 
606 Gasser and Lee 
could be achieved in a system with no explicit rules. However, it did have important 
limitations, among them the following: 
The representation of linguistic form was inadequate. This is clear, for exam- 
ple, from the fact that distinct lexical items may be associated with identical 
representations (Pinker & Prince, 1988). 
The model was trained on an artificial task, quite unlike the perception and 
production that real hearers and speakers engage in. Of course, because it has 
no semantics, the model also says nothing about the issue of compositionality. 
One consequence of both of these shortcomings is that there are few constraints on 
the kinds of processes that can be learned. 
In this paper we describe a model which addresses these objections to the earlier 
work on morphophonemic rule acquisition. The model learns to generate forms 
in one or another tense given arbitrary patterns representing meanings, and 
to yield the appropriate tense, given forms. The network sees linguistic forms 
one segment at a time, saving the context in a short-term memory. This style of 
representation, together with the more realistic tasks that the network is faced with, 
results in constraints on what can be learned. In particular, the system experiences 
difficulty learning reversal processes which do not occur in human language and 
which were easily accommodated by the earlier models. 
2 SHORT-TERM MEMORY AND PREDICTION 
Language takes place in time, and at some point, systems that learn and process 
language have to come to grips with this fact by accepting input in sequential form. 
Sequential models require some form of short-term memory (STM) because the 
decisions that are made depend on context. There are basically two options, window 
approaches, which make available stretches of input events all at once, and dynamic 
memory approaches (Port, 1990), which offer the possibility of a recoded version 
of past events. Networks with recurrent connections have the capacity for dynamic 
memory. We make use of a variant of a simple recurrent network (Elman, 1990), 
which is a pattern associator with recurrent connections on its hidden layer. Because 
the hidden layer receives input from itself as well as from the units representing the 
current event, it can function as a kind of STM for sequences of events. 
Elman has shown how networks of this type can learn a great deal about the struc- 
ture of the inputs when trained on the simple, unsupervised task of predicting the 
next input event. We are interested in what can be expected from such a network 
that is given a single phonological segment (hereafter referred to as a phone) at a 
time and trained to predict the next phone. If a system could learn to do this suc- 
cessfully, it would have a left-to-right version of what phonologists call phonotactics; 
that is, it would have knowledge of what phones tend to follow other phones in given 
contexts. Since word recognition and production apparently build on phonotactic 
knowledge of the language (Church, 1987), training on the prediction task might 
provide a way of integrating the two processes within a single network. 
A Short-Term Memory Architecture for the Learning of Morphophonemic Rules 607 
3 ARCHITECTURE 
The type of network we work with is shown in Figure 1. Both its inputs and 
(cun'nt phon ) (stem { tns ) 
FORM MEANING 
Figure 1: Network Architecture 
outputs include FORM, that is, an individual phone, and what we'll call MEANING, 
that is, a pattern representing the stem of the word to be recognized or produced 
and a single unit representing a grammatical feature such as PAST or PRESENT. 
In fact, the meaning patterns have no real semantics, but like real meanings, they 
are arbitrarily assigned to the various morphemes and thus convey nothing about 
the phonological realization of the stem and grammatical feature. The network is 
trained both to auto-associate the current phone and predict the next phone. 
The word recognition task corresponds to being given phone inputs (together with a 
default pattern on the meaning side) and generating meaning outputs. The meaning 
outputs are copied to the input meaning layer on each time step. While networks 
trained in this way can learn to recognize the words they are trained on, we have 
not been able to get them to generalize well. Networks which are expected only to 
output the grammatical feature, however, do generalize, as we shall see. 
The word production task corresponds to being given a constant meaning input 
and generating form output. Following an initial default phone pattern, the phone 
input is what was predicted on the last time step. Again, however, though such a 
network does fine on the training set, it does not generalize well to novel inputs. 
We have had more success with a version using teacher forcing. Here the correct 
current phone is provided on the input at each time step. 
4 SIMULATIONS 
4.1 STIMULI 
We conducted a set of experiments to test the effectiveness of this architecture for 
the learning of morphophonemic rules. Input words were composed of sequences of 
phones in an artificial language. Each of the 15 possible phones was represented 
by a pattern over a set of 8 phonetic features. For each simulation, a set of 20 
words was generated randomly from the set of possible words. Twelve of these were 
608 Gasser and Lee 
designated training words, 8 test words. 
For each of these basic words, there was an associated infiected form. For each 
simulation, one of a set of 9 rules was used to generate the inflected form: (1) suffix 
(+ assimilation) (g�p-*g�ps, g�b-*g�bz), (2) prefix (+ assimilation) 
k�pskip), (3) gemination (iga�gga), (4) initial deletion (gip�p), (5) medial 
deletion (6) final deletion (7) tone change 
(S) Pig Latin and (9) reversal 
In the two assimilation cases, the suffix or prefix agreed with the preceding or 
following phone on the voice feature. In the suffixing example, p is followed by 
s because it is voiceless, b by z because it is voiced. In the prefixing example, g 
is preceded by z because it is voiced, k by s because it is voiceless. Because the 
network is trained on prediction, these two rules are not symmetric. It would not 
be surprising if such a network could learn to generate a final phone which agrees 
in voicing with the phone preceding it. But in the prefixing case, the network must 
choose the correct prefix before it has seen the phone with which it is to agree in 
voicing. We thought this would still be possible, however, because the network also 
receives meaning input representing the stem of the word to be produced. 
We hoped that the network would succeed on rule types which are common in 
natural languages and fail on those which are rare or non-existent. Types 1-4 are 
relatively common, types 5-7 infrequent or rare, type 8 apparently known only in 
language games, and type 9 apparently non-occurring. 
For convenience, we will refer to the uninfiected form of a word as the present 
and the infiected form as the past tense of the word in question. Each input word 
consisted of a present or past tense form preceded and followed by a word boundary 
pattern composed of zeroes. Meaning patterns consisted of an arbitrary pattern 
across a set of 6 stem units, representing the meaning of the stem of one of the 
20 input words, plus a single bit representing the tense of the input word, that 
is, present or past. 
4.2 TRAINING 
During training each of the training words was presented in both present and past 
forms, while the test words appeared in the present form only. Each of the 32 
separate words was trained in both the recognition and production directions. 
For recognition training, the words were presented, one phone at a time, on the 
form input units. The appropriate pattern was also provided on the stem meaning 
units. Targets specified the current phone, next phone, and complete meaning. 
Thus the network was actually being trained to generate only the tense portion of 
the meaning for each word. The activation on 
