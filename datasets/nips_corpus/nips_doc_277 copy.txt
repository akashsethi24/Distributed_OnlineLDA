60 Nelson and Bower 
Computational E.ciency: 
A Common Organizing Principle for 
Parallel Computer Maps and Brain Maps 
Mark E. Nelson James M. Bower 
Computation and Neural Systems Program 
Division of Biology, 216-76 
California Institute of Technology 
Pasadena, CA 91125 
ABSTRACT 
It is well-known that neural responses in particular brain regions 
are spatially organized, but no general principles have been de- 
veloped that relate the structure of a brain map to the nature of 
the associated computation. On parallel computers, maps of a sort 
quite similar to brain maps arise when a computation is distributed 
across multiple processors. In this paper we will discuss the rela- 
tionship between maps and computations on these computers and 
suggest how similar considerations might also apply to maps in the 
brain. 
1 INTRODUCTION 
A great deal of effort in experimental and theoretical neuroscience is devoted to 
recording and interpreting spatial patterns of neural activity. A variety of map 
patterns have been observed in different brain regions and, presumably, these pat- 
terns reflect something about the nature of the neural computations being carried 
out in these regions. To date, however, there have been no general principles for 
interpreting the structure of a brain map in terms of properties of the associated 
computation. In the field of parallel computing, analogous maps arise when a com- 
putation is distributed across multiple processors and, in this case, the relationship 
Computational Efficiency 61 
between maps and computations is better understood. In this paper, we will at- 
tempt to relate some of the mapping principles from the field of parallel computing 
to the organization of brain maps. 
2 MAPS ON PARALLEL COMPUTERS 
The basic idea of parallel computing is to distribute the computational workload 
for a single task across a large number of processors (Dongarra, 1987; Fox and 
Messina, 1987). In principle, a parallel computer has the potential to deliver com- 
puting power equivalent to the total computing power of the processors from which 
it is constructed; a 100 processor machine can potentially deliver 100 times the 
computing power of a single processor. In practice, however, the performance that 
can be achieved is always less efficient than this ideal. A perfectly efficient imple- 
mentation with N processors would give a factor N speed up in computation time; 
the ratio of the actual speedup a to the ideal speedup N can serve as a measure of 
the efficiency c of a parallel implementation. 
e = -- (1) 
N 
For a given computation, one of the factors that most influences the overall perfor- 
mance is the way in which the computation is mapped onto the available processors. 
The efficiency of any particular mapping can be analyzed in terms of two principal 
factors: load-balance and communication overhead. Load-balance is a measure of 
how uniformly the computational work load is distributed among the available pro- 
cessors. Communication overhead, on the other hand, is related to the cost in time 
of communicating information between processors. 
On parallel computers, the load imbalance A is defined in terms of the average 
calculation time per processor Tav and the maximum calculation time required by 
the busiest processor Tma: 
(2) 
The communication overhead r/is defined in terms of the maximum calculation time 
Trnax and the maximum communication time Tooturn: 
Vomm 
'l= Tm,,, + Tcomm (3) 
Assuming that the calculation and communication phases of a computation do not 
overlap in time, as is the case for many parallel computers, the relationship between 
efficiency e, load-imbalance A, and communication overhead r/is given by (Fox et 
a1.,1988): 
62 Nelson and Bower 
e = 1 -I- , (4) 
When both load-imbalance A and communication overhead r/ are small, the inef- 
ficiency is approximately the sum of the contributions from load-imbalance and 
communication overhead: 
e  1-(r/+A) 
(5) 
When attempting to achieve maximum performance from a parallel computer, a 
programmer tries to find a mapping that minimizes the combined contributions of 
load-imbalance and communication overhead. In some cases this is accomplished by 
applying simple heuristics (Fox et al., 1988), while in others it requires the explicit 
use of optimization techniques like simulated annealing (Kirkpatrick et al., 1983) 
or even artificial neural network approaches (Fox and Furmanski, 1988). In any 
case, the optimal tradeoff between load imbalance and communication overhead 
depends on certain properties of the computation itself. Thus different types of 
computations give rise to different kinds of optimal maps on parallel computers. 
2.1 AN EXAMPLE 
In order to illustrate how different mappings can give rise to different computational 
efficiencies, we will consider the simulation of a single neuron using a multicompart- 
ment modeling approach (Segev et al., 1989). In such a simulation, the model neu- 
ron is divided into a large number of compartments, each of which is assumed to be 
isopotential. Each compartment is represented by an equivalent electric circuit that 
embodies information about the local membrane properties. In order to update the 
voltage of an individual compartment, it is necessary to know the local properties 
as well as the membrane voltages of the neighboring compartments. Such a model 
gives rise to a system of differential equations of the following form: 
Crn dt = Eg(� - E) + gi-z,i(�-z - �) + gi+z,i(Vi+z - �) 
k 
(6) 
where C, is the membrane capacitance, V/is the membrane voltage of compartment 
i, g} and E} are the local conductances and their reversal potentials, and gi+l,i are 
coupling conductances to neighboring compartments. 
When carrying out such a simulation on a parallel computer, where there are more 
compartments than processors, each processor is assigned responsibility for updating 
a subset of the compartments (Nelson et al., 1989). If the compartments represent 
equivalent computational loads, then the load-imbalance will be proportional to 
the difference between the maximum and the average number of compartments per 
processor. If the computer processors are fully interconnected by communication 
channels, then the communication overhead will be proportional to the number 
of interprocessor messages providing the voltages of neighboring compartments. If 
Computational Efficiency 63 
A 
B C 
),, = 0.2 ),, = 0.01 
q=O.04 i q 0.0'7 
Figure 1: Tradeoffs between load-imbalance X and communication overhead r/, 
giving rise to different efficiencies e for different mappings of a multicompart- 
ment neuron model. (A) a minimum-cut mapping that minimizes communication 
overhead but suffers from a significant load-imbalance, (B) a scattered mapping 
that minimizes load-imbalance but has a large communication overhead, and (C) 
a near-optimal mapping that simultaneously minimizes both load-imbalance and 
communication overhead. 
neighboring compartments are mapped to the same processor, then this information 
is available without any interprocessor communication and thus no communication 
overhead is incurred. 
Fig. i shows three different ways of mapping a 155 compartment neuron model 
onto a group of 4 processors. In each case the load-imbalance and communication 
overhead are calculated using the assumptions listed above and the computational 
efficiency is computed using eq. 4. The map in Fig. 1A minimizes the communication 
overhead of thd mapping by making a minimum number of cuts in the dendritic 
tree, but is rather inefficient because a significant load-imbalance remains even 
after optimizing the location of each cut. The map is Fig. lB, on the other hand, 
minimizes the load-imbalance, by using a scattered mapping technique (Fox et al., 
1988), but is inefficient because of a large communication overhead. The map in 
Fig. 1C strikes a balance between load-imbalance and communication overhead that 
results in a high computational efficiency. Thus this particular mapping makes the 
best use of the available computing resources for this particular computational task. 
64 Nelson and Bower 
A B C 
Figure 2: Three classes of map topologies found in the brain (of the rat). (A) 
continuous map of tactile inputs in somatosensory cortex (B) patchy map of tactile 
inputs to cerebellar cortex and (C) scattered mapping of olfactory inputs to olfactory 
cortex as represented by the unstructured pattern of 2DG uptake in a single section 
of this cortex. 
3 MAPS IN THE BRAIN 
Since some parallel computer maps are clearly more efficient than others for partic- 
ular problems, it seems natural to ask whether a similar relationship might hold for 
brain maps and neural computations. Namely, for a given computational task, does 
one particular brain map topology make more efficient use of the available neural 
computing resources than another? If so, does this impose a significant constraint 
on the evolution and development of brain map topologies? 
It turns out that there are striking similarities between the kinds of maps that 
arise on parallel computers and the types of maps that have been observed in 
the brain. In both cases, the map patterns can be broadly grouped into three 
categories: continuous maps, patchy maps, and scattered (non-topographic) maps. 
Fig. 2 shows examples of brain maps that fall into these categories. Fig. 2A shows 
an example of a smooth and continuous map representing the pattern of afferent 
tactile projections to the primary somatosensory cortex of a rat (Welker, 1971). 
The patchy map in Fig. 2B represents the spatial pattern of tactile projections to 
the granule cell layer of the rat cerebellar hemispheres (Shambes et al., 1978; Bower 
and Woolston, 1983). Finally, Fig. 2C represents an extreme case in which a brain 
region shows no apparent topographic organization. This figure shows the pattern 
of metabolic activity in one se
