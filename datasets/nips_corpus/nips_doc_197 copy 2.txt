702 Obradovic and Parberry 
Analog Neural Networks of Limited Precision I: 
Computing with Multilinear Threshold Functions 
(Preliminary Version) 
Zoran Obradovic and Ian Parberry 
Department of Computer Science, 
Penn State University, 
University Park, Pa. 16802. 
ABSTRACT 
Experimental evidence has shown analog neural networks to be ex- 
tremely fault-tolerant; in particular, their performance does not ap- 
pear to be significantly impaired when precision is limited. Analog 
neurons with limited precision essentially compute k-ary weighted 
multilinear threshold functions, which divide R n into k regions with 
k-1 hyperplanes. The behaviour of k-ary neural networks is investi- 
gated. There is no canonical set of threshold values for k>3, 
although they exist for binary and ternary neural networks. The 
weights can be made integers of only O ((z +k)log (z +k)) bits, where 
z is the number of processors, without increasing hardware or run- 
ning time. The weights can be made +1 while increasing running 
time by a constant multiple and hardware by a small polynomial in z 
and k. Binary neurons can be used if the running time is allowed to 
increase by a larger constant multiple and the hardware is allowed to 
increase by a slightly larger polynomial in z and k. Any symmetric 
k-ary function can be computed in constant depth and size 
0 (n'-/(k-2)!), and any k-ary function can be computed in constant 
depth and size O (nk). The alternating neural networks of Olafsson 
and Abu-Mostafa, and the quantized neural networks of Fleisher are 
closely related to this model. 
Analog Neural Networks of Limited Precision I 703 
1 INTRODUCTION 
Neural networks are typically circuits constructed from processing units which com- 
pute simple functions of the form f (w x,...,w, ):R -->S where So_R, wiR for l__n, 
and 
f (Wl,...,W.)(Xl, .... 
i=1 
for some output function g:R-->S. There are two choices for the set S which are 
currently popular in the literature. The first is the discrete model, with S=B (where B 
denotes the Boolean set {0,1}). In this case, g is typically a linear threshold function 
g(x)=l iff x_>O, and f is called a weighted linear threshold function. The second is 
the analog model, with S=[0,1] (where [0,1] denotes {reRI0_r<l}). In this case, g 
is typically a monotone increasing function, such as the sigmoid function 
g(x)=(l+c-X) - for some constant c e R. The analog neural network model is popular 
because it is easy to construct processors with the required characteristics using a few 
transistors. The digital model is popular because its behaviour is easy to analyze. 
Experimental evidence indicates that analog neural networks can produce accurate 
computations when the precision of their components is limited. Consider what actu- 
ally happens to the analog model when the precision is limited. Suppose the neurons 
can take on k distinct excitation values (for example, by restricting the number of di- 
gits in their binary or decimal expansions). Then S is isomorphic to Zk={0,...,k-1}. 
We will show that g is essentially the multilinear threshold function 
g (h,h2,...,hk_O:R-->Z defined by 
g (x)=i iff hi <_x <hi+. 
Here and throughout this paper, we will assume that h<h2<...<h_, and for conveni- 
ence define h0=-oo and h=oo. We will call f a k-ary weighted multilinear threshold 
function when g is a multilinear threshold function. 
We will study neural networks constructed from k-ary multilinear threshold functions. 
We will call these k-ary neural networks, in order to distinguish them from the stan- 
dard 2-ary or binary neural network. We are particularly concerned with the resources 
of time, size (number of processors), and weight (sum of all the weights) of k-ary 
neural networks when used in accordance with the classical computational paradigm. 
The reader is referred to (Parberry, 1990) for similar results on binary neural networks. 
A companion paper (Obradovic & Parberry, 1989b) deals with learning on k-ary neur- 
al networks. A more detailed version of this paper appears in (Obradovic & Parberry, 
1989a). 
2 A K-ARY NEURAL NETWORK MODEL 
A k-ary neural network is a weighted graph M=(V,E ,w ,h), where V is a set of pro- 
cessors and E cVxV is a set of connections between processors. Function 
w:VxV-->R assign weights to interconnections and h:V-->R-assign a set of k-1 
thresholds to each of the processors. We assume that if (u,v)&E, w (u,v)=0. The 
size of M is defined to be the number of processors, and the weight of M is 
704 Obradovic and Parberry 
Iw (u,v)l. 
,vV 
The processors of a k-ary neural network are relatively limited in computing power. 
A k-ary function is a function f :Z-->Zk. Let F denote the set of all n-input k-ary 
functions. Define O:R'*--->F by O'(w ,...,wn ,h ,...,hk_0:R-->Z, where 
O�(w ,h )=i 
i=l 
The set of k-ary weighted multilinear threshold functions is the union, over all n  N, 
of the range of OF. Each processor of a k-ary neural network can compute a k-ary 
weighted multilinear threshold function of its inputs. 
Each processor can be in one of k states, 0 through k-1. Initially, the input proces- 
sors of M are placed into states which encode the input. If processor v was updated 
during interval t, its state at time t-1 was i and output was j, then at time t its state 
will be j. A k-ary neural network computes by having the processors change state un- 
til a stable configuration is reached. The output of M are the states of the output pro- 
cessors after a stable state has been reached. A neural network M 2 is said to be f (t)- 
equivalent to M iff for all inputs x, for every computation of M on input x which 
terminates in time t there is a computation of M 2 on input x which terminates in time 
f(t) with the same output. A neural network M2 is said to be equivalent to M iff it 
is t-equivalent to it. 
3 ANALOG NEURAL NETWORKS 
Let f be a function with range [0,1]. Any limited-precision device which purports to 
compute f must actually compute some function with range the k rational values 
R={i/k-lli e Z,0<_/<k } (for some keN). This is sufficient for all practical purposes 
provided k is large enough. Since Rk is isomorphic to Z, we will formally define 
the limited precision variant of f to be the function f:X-->Z defined by 
f  (x )=round (f (x).(k-1)), where round:R-->N is the natural rounding function defined 
by round(x)=n iff n-0.5.x<n+0.5. 
Theorem 3.1: Letf(w,...,w,,):R-->[O,1] where wieR for ln, be defined by 
f (w,...,w,)(x,... ,x,g(wixi) 
i=l 
where g :R-->[0,1] is monotone increasing and invertible. Then f (w 1,...,w,)k :R n -->Z 
is a k-ary weighted multilinear threshold function. 
Proof: It is easy to verify that f(wi,...,w,)=O(wi,...,w,,hi,...,h_O, where 
hi=g-i((2i-1)/2(k-1)). [] 
Thus we see that analog neural networks with limited precision are essentially k-ary 
neural networks. 
Analog Neural Networks of Limited Precision I 705 
4 CANONICAL THRESHOLDS 
Binary neural networks have the advantage that all thresholds can be taken equal to 
zero (see, for example, Theorem 4.3.1 of Parberry, 1990). A similar result holds for 
ternary neural networks. 
Theorem 4.1: For every n-input ternary weighted multilinear threshold function there 
is an equivalent (n+l)-input ternary weighted multilinear threshold function with 
threshold values equal to zero and one. 
Proof: Suppose w=(w,...,wn)eR n, h,h2eR. 
h l<h2. Define �=(�1 ..... �n+l) R ' +l by 
�,+i=-h/(h2-hO. It can be demonstrated by 
Z�, 
Without loss of generality assume 
�i=wi/(h2-h) for 1.(d._n, and 
a simple case analysis that for all 
O'(w ,h ,h 2)(x )=O'+ (� ,0,1)(x  ,...,x, ,1). 
The choice of threshold values in Theorem 4.1 was arbitrary. Unfortunately there is 
no canonical set of thresholds for k >3. 
Theorem 4.2: For every k>3, n>2, m_>0, h,...,hk_eR, there exists an n-input k-ary 
weighted multilinear threshold function 
such that for all (n +m)-input k-ary weighted multilinear threshold functions 
O+m(� , . . , �,,h � 
� ,...,h_0.Zt -->Zk 
and y 1,...,Ym  R, there exists x=(x,...,x,,) Z such that 
O(w b...,wn ,t ,...,t-O(x):O (w  ..... �n+a ,h ,...,h_0(x b...,x, ,y b...,Ya). 
Proof (Sketch): Suppose that t l,...,t/_l R is a canonical set of thresholds, and w.l.o.g. 
assume n=2. Let h=(h,...,h,_O, where h=h2=2, h3--4, hi=5 for 4<i<k, and 
f =O2(1,1,h ). 
By hypothesis there exist w  ,. .. ,w, +2 and y=(y ,...,y,) R such that for all x  Z 2, 
f (x)=-Of+2(w 1,...,w +2,t 1,...,t-l)(x ,y ). 
Let S =EWi+2Yi. 
i=l 
Since f (1,0)--0, f (0,1)=0, f (2,1)=2, f (1,2)=2, it follows that 
2(w +w 2+S)<t l+t 3. 
(1) 
Since f (2,0)=2, f (1,1)=2, and f (0,2)=2, it follows that 
706 Obradovic and Parberry 
W i+W 2+5 ?t 2. (2) 
Inequalities (1) and (2) imply that 
2t2<tl+t3. (3) 
By similar arguments from g=O2(1,1,1,3,3,4,...,4) we can conclude that 
2t2>tl+t3. (4) 
But (4) contradicts (3). [] 
5 NETWORKS OF BOUNDED WEIGHT 
Although our model allows each weight to take on an infinite number of possible 
values, there are only a finite number of threshold functions (since there are only a 
finite number of k-ary functions) with a fixed number of inputs. Thus the number of 
n-input threshold functions is bounded above by some function in n and k. In fact, 
something stronger can be shown. All weights can be made integral, and 
0 ((n +k)log (n +k)) bits are sufficient to describe each one. 
Theorem 5.1: For every k-ary neural network M x of size z there exists an equivalent 
k-ary neural network M 2 of size z and weight ((k-1)/2)Z (z + l) (z+k)/2+�O) with integer 
weights. 
Proof (Sketch): It is sufficient to prove that for every weighted threshold function 
f(wi,...,w,,hi,...,hk_i):Z-->Z for some nN, there is an equivalent weighted thres- 
hold function g(w,...,w,h,...,h_i) such that Iwi*l<((k-1)/2)n(n+l) (n')/2+�(O for 
l<i..n. By extending the techniques used by Muroga, Toda and
