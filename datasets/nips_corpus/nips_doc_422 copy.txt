Back Propagation is Sensitive to Initial Conditions 
John E Kolen Jordan B. Pollack 
Laboratory for Artificial Intelligence Research 
The Ohio State University 
Columbus, OH 43210, USA 
kolen-j@ cis.ohio-state.edu 
pollack@cis.ohio-state.edu 
Abstract 
This paper explores the effect of initial weight selection on feed-forward 
networks leaming simple functions with the back-propagation 
technique. We first demonstrate, through the use of Monte Carlo 
techniques, that the magnitude of the initial condition vector (in weight 
space) is a very significant parameter in convergence time variability. In 
order to further understand this result, additional deterministic 
experiments were performed. The results of these experiments 
demonstrate the extreme sensitivity of back propagation to initial weight 
configuration. 
I INTRODUCTION 
Back Propagation (Rumelhart et al., 1986) is the network training method of choice for 
many neural network projects, and for good reason. Like other weak methods, it is simple 
to implement, faster than many other general approaches, well-tested by the field, and 
easy to mold (with domain knowledge encoded in the learning environment) into very 
specific and efficient algorithms. 
Rumelhart et al. made a confident statement: for many tasks, the network rarely gets 
stuck in poor local minima that are significantly worse than the global mlnima.(p. 536) 
According to them, initial weights of exactly 0 csnnot be used, since symmetries in the 
environment are not sufficient to break symmetries in initial weights. Since their paper 
was published, the convention in the field has been to choose initial weights with a 
uniform distribution between plus and minus p, usually set to 0.5 or less. 
The convergence claim was based solely upon their empirical experience with the back 
propagation technique. Since then, Minsky & Paperr (1988) have argued that there exists 
no proof of convergence for the technique, and several researchers (e.g. Judd 1988) have 
found that the convergence time must be related to the difficulty of the problem, otherwise 
an unsolved computer science question (P  NP ) would finally be answered. We do not 
wish to make claims about convergence of the technique in the limit (with vanishing step- 
860 
Back Propagation is Sensitive to Initial Conditions 861 
size), or the relationship between task and performance, but wish to talk about a pervasive 
behavior of the technique which has gone unnoticed for several years: the sensitivity of 
back propagation to initial conditions. 
2 THE MONTE-CARLO EXPERIMENT 
Initially, we performed empirical studies to determine the effect of learning rate, 
momenttun rate, and the range of initial weights on t-convergence (Kolen and Goel, to 
appear). We use the term t-convergence to refer to whether or not a network, starting at a 
precise initial configuration, could learn to separate the input patterns according to a 
boolean function (correct outputs above or below 0.5) within t epochs. The experiment 
consisted of training a 2-2-1 network on exclusive-or while varying three independent 
variables in 114 combinations: leaming rate, rl, equal to 1.0 or 2.0; momentum rate, 
equal to 0.0, 0.5, or 0.9; and initial weight range, p, equal to 0.1 to 0.9 in 0.1 increments, 
and 1.0 to 10.0 in 1.0 increments. Each combination of parameters was used to initialize 
and train a number of networks. I Figure I plots the percentage of t-convergent (where 
t  50, 000 epochs of 4 presentations) initial conditions for the 2-2-1 network trained on 
the exclusive-or problem. From the figure we thus conclude the choice of p < 0.5 is more 
than a convenient symmetry-breaking default, but is quite necessary to obtain low levels 
of nonconvergent behavior. 
8o.oo - 
% 70.00 - ?, - LZO-:=o.o 
Non Lz'ZO M=0.5 
Convergence 60.00- ' - 
After ' L=2.0 M=0.9 
50.00 - .:,, ,,, - 
50,000 
Trials 40.00 - :' - 
30.00 
20.00 - 
10.00 
0.00- 
0.00 2.00 4.00 6.00 8.00 10.00 
p 
Figure 1' Percentage T-Convergence vs. Initial Weight Range 
3 SCENES FROM EXCLUSIVE-OR 
Why do networks exhibit the behavior illustrated in Figure 17 While some might argue 
that very high initial weights (i.e. p > 10.0) lead to very long convergence times since the 
derivative of the semi-linear sigmoid function is effectively zero for large weights, this 
1. Numbers ranged from 8 to 8355, depending on availability of computational resources. 
Those data points calculated with small samples were usually surrounded by data points 
with larger samples. 
862 Kolen and Pollack 
7 8 
Figure 2: 
(Schematic Network) 
:i:3-$?.4S'.. ...... :-:..'..: 
Figure 3: 
(-5-3+3+6Y-I-6+7X) 
'q=3.25 o--0.40 
Figure 4: 
(+4-7+6+0-3Y+lX+l) 
q=2.75 o--0.00 
Figure 5: 
(-5+5+1-6+3XY+8+3) 
1=2.75 o-'-0.80 
Figure 8: 
(-6-4XY-6-6+9-4-9) 
q=3.00 a,--0.50 
Figure 11: 
(+7+4-9-9-5Y-3+9X) 
1=3.00 0--0.70 
Figure 6: 
(YX-3+6+8+3+1+7-3) 
1=3.25 a,--0.00 
:: 8i:!:!:::i:::::::;:::;:::::::': ' :::: 
� :::.': :!-:::i:::: %:::ii:!:i!iii':(.g_:_4: 
Figure 9: 
(-2+I+9-1X-3+8Y-4) 
'q=2.75 o--0.20 
Figure 12: 
(-9.0,-1.8) 
step 0.018 
Figure 7: 
(Y+3-9-2+6+7-3X+7) 
=3.25 o---0.60 
Figure 10: 
(+l+8-3-6X-l+I+8Y) 
1=3.50 o--0.90 
Figure 13: 
(-6.966,-0.500) 
step 0.004 
Back Propagation is Sensitive to Initial Conditions 863 
does not explain the fact that when p is between 2.0 and 4.0, the non-t-convergence rate 
varies from 5 to 50 percent. 
Thus, we decided to utilize a more deterministic approach for eliciting the structure of 
initial conditions giving rise to t-convergence. Unfortunately, most networks have many 
weights, and thus many dimensions in initial-condition space. We can, however, examine 
2-dimensional slices through the space in great detail. A slice is specified by an origin and 
two orthogonal directions (the X and Y axes). In the figures below, we vary the initial 
weights regularly throughout the plane formed by the axes (with the origin in the lower 
left-hand comer) and collect the results of running back-propagation to a particular time 
limit for each initial condition. The map is displayed with grey-level linearly related to 
time of convergence: black meaning not t-convergent and white representing the fastest 
convergence time in the picture. Figure 2 is a schematic representation of the networks 
used in this and the following experiment. The numbers on the links and in the nodes will 
be used for identification purposes. Figures 3 through 11 show several interesting slices 
of the the initial condition space for 2-2-1 networks trained on exclusive-or. Each slice is 
compactly identified by its 9-dimensional weight vector and associated learning/ 
momentum rates. For instance, the vector (-3+2+7-4X+5-2-6Y) describes a network with 
an initial weight of -0.3 between the left hidden unit and the left input unit. Likewise, 
+5 in the sixth position represents an initial bias of 0.5 to the right hidden unit. The 
letters X and Y indicate that the corresponding weight is varied along the X- or Y- 
axis from -10.0 to +10.0 in steps of 0.1. All the figures in this paper contain the results of 
40,000 runs of back-propagation (i.e.200 pixels by 200 pixels) for up to 200 epochs 
(where an epoch consists of 4 training examples). 
Figures 12 and 13 present a closer look at the sensitivity of back-propagation to initial 
conditions. These figures zoom into a complex region of Figure 11; the captions list the 
location of the origin and step size used to generate each picture. 
Sensitivity behavior can also be demonstrated with even simpler functions. Take the case 
of a 2-2-1 network learning the or function. Figure 14 shows the effect of learning or on 
networks (+5+5-1X+5-1Y+3-1) and varying weights 4 (X-axis) and 7 (Y-axis) from -20.0 
to 20.0 in steps of 0.2. Figure 15 shows the same region, except that it partitions the 
display according to equivalent solution networks after t-convergence (200 epoch limit), 
rather than the time to convergence. Two networks are considered equivalent2if their 
weights have the same sign. Since there are 9 weights, there are 512 ($2 sup 95) possible 
network equivalence classes. Figures 16 through 25 show successive zooms into the 
central swirl identified by the XY coordinate of the lower-left comer and pixel step size. 
After 200 iterations, the resulting networks could be paxtitioned into 37 (both convergent 
and nonconvergent) classes. Obviously, the smooth behavior of the t-convergence plots 
can be deceiving, since two initial conditions, arbitrarily alike, can obtain quite different 
final network configuration. 
Note the triangles appearing in Figures 19, 21, 23 and the mosaic in Figure 25 
corresponding to the area which did not converge in 200 iterations in Figure 24. The 
triangular boundaries are similar to fractal structures generated under iterated function 
systems (Barnsley 1988): in this case, the iterated function is the back propagation 
2. For rendering purposes only. It is extremely difficult to know precisely the equivalence 
classes of solutions, so we approximated. 
864 Kolen and Pollack 
Figure 14: 
(-20.00000, -20.00000) 
Step 0.200000 
Figure 17' 
Solution Networks 
i!' : ::':i,::!:!,ii!i! ! !:!'!:i:i:!:i:! i:! !.!' 
i -: ::':i'i:: !:i:!:i 
: . ,.::: ,::. ::::: ::::::::::::::::::::::::::::::::::::::::::::::::::: 
::::::::::::::::::::::::::::::::::::::::: :.'::: ::: -'_':,�::;: ::I:: :;:  ::' ::::: 
:::::::::::::::::::::::::::::::::: :::::: :.: :.:::'i.. ..:.q :::: :..: :,::: :.:.:.:.:.: 
Figure 20' 
(- 1.536000, -1.197000) 
Step 0.000780 
Figure 23' 
Solution Networks 
.-.-..-.------L.-L--------------..------..--.:.:.:::.::::::.:::.:.:.::...:.::.:.:.:::.:: 
'--:----::.----:----:.:;.;:.:.;:':.::.:::.;.:;;:.-4. 
:--c:-_--:-:..:i:i:i:i:!:i::i:i:i::i:i:i::i::i:i:: 
Figure 15' 
Solution Networks 
.... ...:: ::::::::::::::::::::::::::::::::::::::::::::::::: 
::::::::W:::':::: 
