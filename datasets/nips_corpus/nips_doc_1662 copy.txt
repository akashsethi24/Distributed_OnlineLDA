Bayesian Transduction 
Thore Graepel, Ralf Herbrich and Klaus Obermayer 
Department of Computer Science 
Technical University of Berlin 
Franklinstr. 28/29, 10587 Berlin, Germany 
{ graepe12, ralfh, oby} @cs. tu-berlin. de 
Abstract 
Transduction is an inference principle that takes a training sam- 
ple and aims at estimating the values of a function at given points 
contained in the so-called working sample as opposed to the whole 
of input space for induction. Transduction provides a confidence 
measure on single predictions rather than classifiers -- a feature 
particularly important for risk-sensitive applications. The possibly 
infinite number of functions is reduced to a finite number of equiv- 
alence classes on the working sample. A rigorous Bayesian analysis 
reveals that for standard classification loss we cannot benefit from 
considering more than one test point at a time. The probability 
of the label of a given test point is determined as the posterior 
measure of the corresponding subset of hypothesis space. We con- 
sider the PAC setting of binary classification by linear discriminant 
functions (perceptrons) in kernel space such that the probability of 
labels is determined by the volume ratio in version space. We 
suggest to sample this region by an ergodic billiard. Experimen- 
tal results on real world data indicate that Bayesian Transduction 
compares fa-vourably to the well-known Support Vector Machine, 
in particular if the posterior probability of labellings is used as a 
confidence measure to exclude test points of low confidence. 
I Introduction 
According to Vapnik [9], when solving a given problem one should avoid solving a 
more 9eneral problem as an intermediate step. The reasoning behind this principle is 
that in order to solve the more general task resources may be wasted or compromises 
may have to be made which would not have been necessary for the solution of the 
problem at hand. A direct application of this common-sense principle reduces the 
more general problem of inferring a functional dependency on the whole of input 
space to the problem of estimating the values of a function at given points (working 
sample), a paradigm referred to as transductive inference. More formally, given a 
probability measure Px� on the space of data l' x y = 2c' x {-1, +1}, a training 
sample $ = {(x, y),..., (xt, yt)} is generated i.i.d. according to Px�. Additional 
m data points W - {xl+,... ,xl+,} are drawn: the working sample. The goal 
is to label the objects of the working sample W using a fixed set 7/ of functions 
Bayesian Transduction 457 
f : A'  {-1,+1} so as to minimise a predefined loss. In.contrast, inductive 
inference, aims at choosing a single function ft 7-I best suited to capture the 
dependency expressed by the unknown Px�. Obviously, if we have a transductive 
algorithm .A (W, S, 7/) that assigns to each working sample W a set of labels given 
the training sample $ and the set 7/of functions, we can define a function fs: 2  
{- 1, + 1 } by fs (x) = A ({x}, $, 7/) as a result of the transduction algorithm. There 
are two crucial differences to induction, however: i) A ({x}, S, 7/) is not restricted 
to select a single decision function f E 7/for each x, ii) a transduction algorithm 
can give performance guarantees on particular labellings instead of functions. In 
practical applications this difference may be of great importance. 
After all, in risk sensitive applications (medical diagnosis, financial and critical 
control applications) it often matters to know how confident we are about a given 
prediction. In this case a general confidence measure of the classifier w.r.t. the 
whole input distribution would not provide the desired warranty at all. Note that 
for linear classifiers some guarantee can be obtained by the margin [7] which in 
Section 4 we will demonstrate to be too coarse a confidence measure. The idea of 
transduction was put forward in [8], where also first algorithmic ideas can be found. 
Later [1] suggested an algorithm for transduction based on linear programming and 
[3] highlighted the need for confidence measures in transduction. 
The paper is structured as follows: A Bayesian approach to transduction is formu- 
lated in Section 2. In Section 3 the function class of kernel perceptrons is introduced 
to which the Bayesian transduction scheme is applied. For the estimation of volumes 
in parameter space we present a kernel billiard as an efficient sampling technique. 
Finally, we demonstrate experimentally in Section 4 how the confidence measure 
for labellings helps Bayesian Transduction to achieve low generalisation error at a 
low rejection rate of test points and thus to outperform Support Vector Machines 
(SVMs). 
2 Bayesian Transductive Classification 
Suppose we are given a training sample $ = { (Xl, Vl),..., (xl, yl) ) drawn i.i.d. from 
Px� and a working sample W = {Xt+l,... ,xt+,} drawn i.i.d. from Px. Given 
a prior PH over the set 7 of functions and a likelihood P(x�)tlH=! we obtain a 
def 
posterior probability PHi(X�)t=S = PHIS by Bayes' rule. This posterior measure 
induces a probability measure on labellings b E {-1, +1}  of the working sample 
by I 
PY'IS,W (b) dej PHIS ({f' Vx+i  W f (x+i) = hi}) � 
(1) 
For the sake of simplicity let us assume a PAC style setting, i.e. there exists a 
function f* in the space 7 such that P�1x=x (Y) = 5 (y - f* (x)). In this case one 
can define the so-called version-space as the set of functions that is consistent with 
the training sample 
V($)={f:V(xi,yi)eS f(xi)=yi}, (2) 
outside which the posterior Psis vanishes. Then PYlS,W (b) represents the prior 
measure of functions consistent with the training sample $ and the labelling b 
on the working sample W normalised by the prior measure of functions consistent 
with $ alone. The measure PH can be used to incorporate prior knowledge into 
Note that the number of different labellings b implementable by 7/is bounded above 
by the value of the growth function IIt (IWl) [8, p. 321]. 
458 T. Graepel, R. Herbrich and K. Obermayer 
the inference process. If no such knowledge is available, considerations of symmetry 
may lead to uninformative priors. 
Given the measure P�,qs,w over labellings, in order to arrive at a risk minimal 
decision w.r.t. the labelling we need to define a loss function I: ym x ym  IR+ 
between labellings and minimise its expectation, 
/ (b, $, W) - EYlS,W It (b, Y)] =  l (b, b') PYlS,W (b'), (3) 
{b'} 
where the summation runs over all the 2  possible labellings b  of the working 
sample. Let us consider two scenarios: 
1. A 0-I-loss on the exact labelling b, i.e. for two labellings b and b  
/�(b,b')=l-l-IS(bi-b)  R�(b,S,W)= l-P�.s,w(b) . (4) 
i=1 
In this case choosing the labelling be = argmin b Re (b, $, W) of the highest 
joint probability P�lS,W (b) minimises the risk. This non-labelwise loss is 
appropriate if the goal is to exactly identify a combination of labels, e.g. the 
combination of handwritten digits defining a postal zip code. Note that 
classical SVM transduction (see, e.g. [8, 1]) by maximising the margin on 
the combined training and working sample approximates this strategy and 
hence does not minimise the standard classification risk on single instances 
as intended. 
2. A 0-i-loss on the single labels bi, i.e. for two labellings b and b  
m 
/,(b,b') = ly(1-(bi-b)), (5) 
i=1 
m 
(b,S, W) = � - (0, - o',)) (b') 
i=1 {b,} 
- 
i=1 
Due to the independent treatment of the loss at working sample points the 
risk R, (b, S, W) is minimised by the labelling of highest marginal proba- 
bility of the labels, i.e. 
b = argmaxvey P.lS ({f: f (xt+i) = y)). 
Thus in the case of the labelwise loss (5) a working sample of m > 1 
point does not offer any advantages over larger working samples w.r.t. the 
Bayes-optimal decision. Since this corresponds to the standard classifica- 
tion setting, we will restrict ourselves to working samples of size m = 1, 
i.e. to one working point xt+l. 
3 Bayesian Transduction by Volume 
3.1 The Kernel Perceptron 
We consider transductive inference for the class of kernel perceptrons. The decision 
functions are given by 
f (x) = sign ((w,b (x))j,) = sign ctik (xi,x) w =  climb (xi) e ', 
i=1 i=1 
Bayesian Transduction 459 
Figure 1: Schematic view of data space (left) and parameter space (right) for a 
classification toy example. Using the duality given by (w, b (x)}y = 0 data points 
on the left correspond to hyperplanes on the right, while hyperplanes on the left 
can be thought of as points on the right. 
where the mapping b: X  Y maps from input space X to a feature space  
completely determined by the inner product function (kernel) k : 2 x 2d   
(see [9, 10]). Given a training sample $ = {(xi,yi)}i=l we can define the version 
space -- the set of all perceptrons compatible with the training data -- as in (2) 
having the additional constraint Ilwll - i ensuring uniqueness. In order to obtain 
a prediction on the label bl of the working point Xl+l we note that Xl+l may 
bisects the volume V of version space into two sub-volumes V + and V-, where the 
perceptrons in V + would classify Xl+l as b = +1 and those in V- as b = -1. 
The ratio p+ = V+/V is the probability of the labelling b = +1 given a uniform 
prior P, over w and the class of kernel perceptrons, accordingly for bl = -1 (see 
Figure 1). Already Vapnik in [8, p. 323] noticed that it is troublesome to estimate 
sub-volumes of version space. As the solution to this problem we suggest to use a 
billiard algorithm. 
3.2 Kernel Billiard for Volume Estimation 
The method of playing billiard in version space was first introduced by Rujan [6] 
for the purpose of estimating its centre of mass and consequently refined and ex- 
tended to kernel spaces by [4]. For Bayesian Transduction the idea is to bounce 
the billiard ball in version space and to record how much time it spends in each
