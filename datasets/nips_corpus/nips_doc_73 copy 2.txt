592 
A Trellis-Structured Neural Network* 
Thomas Petsche t and Bradley W. Dickinson 
Princeton University, Department of Electrical Engineering 
Princeton, NJ 08544 
Abstract 
We have developed a neural network which consists of cooperatively inter- 
connected Grossberg on-center off-surround subnets and which can be used to 
optimize a function related to the log likelihood function for decoding convolu- 
tional codes or more general FIR signal deconvolution problems. Connections in 
the network are confined to neighboring subnets, and it is representative of the 
types of networks which lend themselves to VLSI implementation. Analytical and 
experimental results for convergence and stability of the network have been found. 
The structure of the network can be used for distributed representation of data 
items while allowing for fault tolerance and replacement of faulty units. 
1 Introduction 
In order to study the behavior of locally interconnected networks, we have focused 
on a class of trellis-structured networks which are similar in structure to multilayer 
networks [5] but use symmetric connections and allow every neuron to be an output. 
We are studying such. locally interconnected neural networks because they have the 
potential to be of great practical interest. Globally interconnected networks, e.g., 
Hopfield networks [3], are difficult to implement in VLSI because they require many 
long wires. Locally connected networks, however, can be designed to use fewer and 
shorter wires. 
In this paper, we will describe a subclass of trellis-structured networks which op- 
timize a function that, near the global minimum, has the form of the log likelihood 
function for decoding convolutional codes or more general finite impulse response sig- 
nMs. Convolutional codes, defined in section 2, provide an alternative representation 
scheme which can avoid the need for global connections. Our network, described in 
section 3, can perform maximum likelihood sequence estimation of convolutional coded 
sequences in the presence of noise. The performance of the system is optimal for low 
error rates. 
The specific application for this network was inspired by a signal decomposition 
network described by Hopfield and Tank [6]. However, in our network, there is an 
emphasis on local interconnections and a more complex neural model, the Grossberg 
on-center off-surround network [2], is used. A modified form of the Gorssberg model 
is defined in section 4. Section 5 presents the main theoretical results of this paper. 
Although the deconvolution network is simply a set of cooperatively interconnected 
*Supported by the Office of Naval Research through grant N00014-83-K-0577 and by the National 
Science Foundation through grant ECS84-05460. 
tPermanent address: Siemens Corporate Research and Support, Inc., 105 College Road East, 
Princeton, NJ 08540. 
American Institute of Physics 1988 
593 
on-center off-surround subnetworks, and absolute stability for the individual subnet- 
works has been proven [1], the cooperative interconnections between these subnets 
make a similar proof difficult and unlikely. We have been able, however, to prove 
equiasymptotic stability in the Lyapunov sense for this network given that the gain 
of the nonlinearity in each neuron is large. Section 6 will describe simulations of the 
network that were done to confirm the stability results. 
2 Convolutional Codes and MLSE 
In an error correcting code, an input sequence is transformed from a b-dimensional 
input space to an M-dimensional output space, where M _ b for error correction 
and/or detection. In general, for the b-bit input vector U - (u,..., ub) and the M- 
bit output vector V - (v,..., VM), we can write V -- F(u,..., ub). A convolutional 
code, however, is designed so that relatively short subsequences of the input vector 
are used to determine subsequences of the output vector. For example, for a rate 1/3 
convolutional code (where M  3b), with input subsequences of length 3, we can write 
the output, V = (v,..., vb) for vi - (vi,, vi,2, vi,3), of the encoder as a convolution 
of the input vector U -- (u,..., ub, 0, 0) and three generator sequences 
go=(1 i 1) g=(1 10) g2 =(0 i 1). 
This convolution can be written, using modulo-2 addition, as 
i 
vi --  ukgi-k (1) 
k=max(1,i-2) 
In this example, each 3-bit output subsequence, vi, of V depends only on three 
bits of the input vector, i.e., vi - f(ui-2, ui_, ui). In general, for a rate 1/n code, the 
coastraiat leagth, K, is the number of bits of the input vector that uniquely determine 
each a-bit output subsequence. In the absence of noise, any subsequences in the 
input vector separated by more than K bits (i.e., that do not overlap) will produce 
subsequences in the output vector that are independent of each other. 
If we view a convolutional code as a special case of block coding, this rate 1/3, 
K - 3 code converts a b-bit input word into a codeword of length 3(b + 2) where 
the 2 is added by introducing two zeros at the end of every input to zero-out the 
code. Equivalently, the coder can be viewed as embedding 2  memories into a 2 3(+)- 
dimensional space. The minimum distance between valid memories or codewords in 
this space is the free distaace of the code, which in this example is 7. This implies 
that the code is able to correct a minimum of three errors in the received signal. 
For a convolutional code with constraint length K, the encoder can be viewed as 
a finite state machine whose state at time i is determined by the K - I input bits, 
ui-k,. �., ui_. The encoder can also be represented as a trellis graph such as the one 
shown in figure i for a K - 3, rate 1/3 code. In this example, since the constraint 
length is three, the two bits ui_ and ui- determine which of four possible states the 
encoder is in at time i. In the trellis graph, there is a set of four nodes arranged in a 
vertical column, which we call a stage, for each time step i. Each node is labeled with 
the associated values of ui_ and ui_l. In general, for a rate 1/n code, each stage of 
the trellis graph contains 2 K-1 nodes, representing an equal number of possible states. 
A trellis graph which contains S stages therefore fully describes the operation of the 
encoder for time steps i through S. The graph is read from left to right and the upper 
edge leaving the right side of a node in stage i is followed if ui is a zero; the lower edge 
594 
stage i-2 stage i-1 stage i stage i+1 stage i+2 
0-- 000'- 000 0,/( 000000 state 1 
111 4 111X 111 4 -- 11 4  
X,._. 0 k,./O k,...O  00/ state 2 
 001 x)lu, f  001 ,10) 0-.01 10)g (01 (, 10) state3 
1)  010' 010'-- 010 010- state4 
Figure 1: Part of the trellis-code representation for a rate 1/3, K = 3 convolutional 
code, 
if ui is a one. The label on the edge determined by ui is vi, the output of the encoder 
given by equation i for the subsequence ui-2, ui_, ui. 
Decoding a noisy sequence that is the output of a convolutional coder plus noise 
is typicaJly done using a maximum likelihood sequence estimation (MLSE) decoder 
which is designed to accept as input a possibly noisy convolutional coded sequence, P, 
and produce as output the maximum likelihood estimate, ', of the originM sequence, 
V. If the set of possible n(b+ 2)-bit encoder output vectors is {Xm : m = 1, ..., 2'(b+2)} 
and Xm, i is the ith n-bit subsequence of Xm and ri is the ith n-bit subsequence of P 
then 
b 
Z = argmax 1-I P(ri ] Xm,i) (2) 
Xm i-1 
That is, the decoder chooses the Xm that maximizes the conditional probability, given. 
Xm, of the received sequence. 
A binary symmetric channel (BSC) is an often used transmission channel model in 
which the decoder produces output sequences formed from an alphabet containing two 
symbols and it is assumed that the probability of either of the symbols being affected 
by noise so that the other symbol is received is the same for both symbols. In the 
case of a BSC, the log of the conditional probability, P(ri [ Xm,i) , is a linear function 
of the Hamming distance between ri and Xm, i SO that maximizing the right side of 
equation 2 is equivaJent to choosing the X, that has the most bits in common with 
P. Therefore, equation 2 can be rewritten as 
9 -- argm_.a3(   [r,j(g6m,i,l) (3) 
� ,K-m i----1 /----1 
where Xm,i, l is the /th bit of the ith subsequence of X m and �a(b) is the indicator 
function: I,(b) -- 1 if and only if a equals b. 
For the generaJ case, maximum likelihood sequence estimation is very expensive 
since the number of possible input sequences is exponential in b. The Viterbi algo- 
rithm [7], fortunately, is able to take advantage of the structure of convolutional codes 
and their trellis graph representations to reduce the complexity of the decoder so that 
595 
it is only exponential in K (in general K << b). An optimum version of the Viterbi al- 
gorithm examines all b stages in the trellis graph, but a more practical and very nearly 
optimum version typically examines approximately 5K stages, beginning at stage i, 
before making a decision about ui. 
3 A Network for MLSE Decoding 
The structure of the network that we have defined strongly reflects the structure of a 
trellis graph. The network usually consists of 5K subnetworks, each containing 2 K-1 
neurons. Each subnetwork corresponds to a stage in the trellis graph and each neuron 
to a state. Each stage is implemented as an on-center off-surround competitive 
network [2], described in more detail in the next section, which produces as output a 
contrast enhanced version of the input. This contrast enhancement creates a winner 
take all situation in which, under normal circumstances, only one neuron in each 
stage --the neuron receiving the input with greatest magnitude -- will be on. The 
activation pattern of the network after it reaches equihbrium indicates the decoded 
sequence as a sequence
