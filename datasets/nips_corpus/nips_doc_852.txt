Temporal Dynamics of Generalization in 
Neural Networks 
Changfeng Wang 
Department of Systems Engineering 
University Of Pennsylvania 
Philadelphia, PA 19104 
wangpender. ee. upenn. edu 
Santosh S. Venkatesh 
Department of Electrical Engineering 
University Of Pennsylvania 
Philadelphia, PA 19104 
venkat eshee. upenn. edu 
Abstract 
This paper presents a rigorous characterization of how a general 
nonlinear learning machine generalizes during the training process 
when it is trained on a random sample using a gradient descent 
algorithm based on reduction of training error. It is shown, in 
particular, that best generalization performance occurs, in general, 
before the global minimum of the training error is achieved. The 
different roles played by the complexity of the machine class and 
the complexity of the specific machine in the class during learning 
are also precisely demarcated. 
1 INTRODUCTION 
In learning machines such as neural networks, two major factors that affect the 
'goodness of fit' of the examples are network size (complexity) and training time. 
These are also the major factors that affect the generalization performance of the 
network. 
Many theoretical studies exploring the relation between generalization performance 
and machine complexity support the parsimony heuristics suggested by Occam's ra- 
zor, to wit that amongst machines with similar training performance one should opt 
for the machine of least complexity. Multitudinous numerical experiments (cf. [5]) 
suggest, however, that machines of larger size than strictly necessary to explain the 
data can yield generalization performance similar to that of smaller machines (with 
264 Changfeng Wang, Santosh S. Venkatesh 
similar empirical error) if learning is optimally stopped at a critical point before 
the global minimum of the training error is achieved. These results seem to fly in 
contradiction with a learning theoretic interpretation of Occam's razor. 
In this paper, we ask the following question: How does the gradual reduction of 
training error affect the generalization error when a machine of given complexity is 
trained on a finite number of examples? Namely, we study the simultaneous effects 
of machine size and training time on the generalization error when a finite sample 
of examples is available. 
Our major result is a rigorous characterization of how a given learning machine 
generalizes during the training process when it is trained using a learning algorithm 
based on minimization of the empirical error (or a modification of the empirical 
error). In particular, we are enabled to analytically determine conditions for the 
existence of a finite optimal stopping time in learning for achieving optimal general- 
ization. We interpret the results in terms of a time-dependent, effective machine size 
which forms the link between generalization error and machine complexity during 
learning viewed as an evolving process in time. 
Our major results are obtained by introducing new theoretical tools which allow us 
to obtain finer results than would otherwise be possible by direct applications of the 
uniform strong laws pioneered by Vapnik and Cervonenkis (henceforth refered to as 
VC-theory). The different roles played by the complexity of the machine class and 
the complexity of the specific machine in the class during learning are also precisely 
demarcated in our results. 
Since the generalization error is defined in terms of an abstract loss function, the 
results find wide applicability including but not limited to regression (square-error 
loss function) and density estimation (log-likelihood loss) problems. 
2 THE LEARNING PROBLEM 
We consider the problem of learning from examples a relation between two vectors 
x and y determined by a fixed but unknown probability distribution P(x, y). This 
model includes, in particular, the input-output relation described by 
y:g(x,), (1) 
where g is some unknown function of x and , which are random vectors on the same 
probability space. The vector x can be viewed as the input to an unknown system, 
 a random noise term (possibly dependent on x), and y the system's output. 
The hypothesis class from which the learning procedure selects a candidate function 
(hypothesis) approximating g is a parametric family of functions 7-/a: { f(x, O) � 
0  Oa C_ 3 a } indexed by a subset Oa of d-dimensional Euclidean space. For 
example, if x 6 3 rn and y is a scalar, 7-/a can be the class of functions computed 
by a feedforward neural network with one hidden layer comprised of h neurons and 
activation function b, viz., 
h rn 
Temporal Dynamics of Generalization in Neural Networks 265 
In the above, d = (rn + 2)h denotes the number of adjustable parameters. 
The goal of learning within the hypothesis class 7-/a is to find the best approx- 
imation of the relation between x and y in 7-/a from a finite set of n examples 
Dn = { (Xl, yl),..., (xn, yn)} drawn by independent sampling from the distribu- 
tion P(x, y). A learning algorithm is simply a map which, for every sample 
(n >_ 1), produces a hypothesis in 7-/a. 
In practical learning situations one first selects a network of fixed structure 
fixed hypothesis class 7-/a), and then determines the best weight vector 0* (or 
equivalently, the best function f(x, 0') in this class) using some training algorithm. 
The proximity of an approximation f(x,O) to the target function g(x,) at each 
point x is measured by a loss function q � (f(x,0),g(x,))  3 +. For a given 
hypothesis class, the function f(.,O) is completely determined by the parameter 
vector 0. With g fixed, the loss function may be written, with a slight abuse of 
notation, as a map q(x, y, 0) from 3 m x 3 x O into 3 + . Examples of the forms of loss 
functions are the familiar square-law loss function q(x, y, O) = (g(x, ) - f(z, 0)) 2 
commonly used in regression and learning in neural networks, and the Kulback- 
Leibler distance (or relative entropy) q(x, y, 0) = In p(yl.(*,0)) for density estimation, 
p(ylg(,)) 
where p(y]i(x,O)) denotes the conditional density function of y given i(x,). 
The closeness of f(., ) to g(.) is measured by the expected (ensemble) loss or error 
�(O,d)  / q(x,y,O)P(dx, dy). 
The optimal approximation f(., 0') is such that �(0', d) = min0eoa �(0, d). In 
similar fashion, we define the corresponding empirical loss (or training error) by 
f o) 
�n(O,d)- q(x,y,O)Pn(dx, dy) =  . 
i--1 
where Pndenotes the joint empirical distribution of input-output pairs (x,y). 
The global minimum of the empirical error over Oa is denoted by t, namely, 
t = arg min0eo �n0, d). An iterative algorithm for minimizing �n0, d) (or a modi- 
fication of it) over Oa generates at each epoch t a random vector Ot'l)n - Oa. The 
quantity �(Ot,d) = E f q(x, Ot)P(dx, d) is referred to as the generalization error 
of 0t. We are interested in the properties of the process { 0t 't = 1,2,...}, and the 
time-evolution of the sequence { �(0t, d) 't = 1, 2,... }. 
Note that each 0t is a functional of Pn When P = Pn learning reduces to an opti- 
mization problem. Deviations from optimality arise intrinsically as a consequence 
of the discrepancy between Pnand P. The central idea of this work is to analyze 
the consequence of the deviation Ang Pn- P on the generalization error. 
To simplify notation, we henceforth suppress d and write simply O, �(0) and �n(O) 
instead of Oa, �(O,d), and �n(O, d), respectively. 
2.1 Regularity Conditions 
We will be interested in the local behavior of learning algorithms. Consequently, we 
assume that O is a compact set, and 0* is the unique global minimum of �(0) on O. 
266 Changfeng Wang, Santosh S. Venkatesh 
It can be argued that these assumptions are an idealization of one of the following 
situations: 
� A global algorithm is used which is able to find the global minimum of 
�n (0), and we are interested in the stage of training when 0t has entered a 
region 0 where 0* is the only global minimum of �(0); 
� A local algorithm is used, and the algorithm has entered a region 0 which 
contains 0* as the unique global minimum of �(0) or as a unique local 
minimum with which we are content. 
In the sequel, we write 0/00 to denote the gradient operator with respect to the 
r �2 I a 
vector 0, and likewise write 02/002 to denote the matrix of operators [ooioo d J i,j=l' 
In the rest of the development we assume the following regularity conditions: 
A1. The loss function q(x, y, .) is twice continuously differential for all 0  0 and 
for almost all (x, y); 
A2. P(x, y) has compact support; 
A3. The optimal network 0* is an interior point of O; 
A4. The matrix (0') = �2 era. 
,:ov ) is nonsingular. 
These assumptions are typically satisfied in neural network applications. We will 
also assume that the learning algorithm converges to the global minimum of �nO) 
over ) (note that t may not be a true global minimum, so the assumption applies to 
gradient descent algorithms which converge locally). It is easy to demonstrate that 
for each such algorithm, there exists an algorithm which decreases the empirical 
error monotonically at each step of iteration. Thus, without loss of generality, we 
also assume that all the algorithms we consider have this monotonicity property. 
3 GENERALIZATION DYNAMICS 
3.1 First Phase of Learning 
The quality of learning based on the minimization of the empirical error depends 
on the value of the quantity sure I�nO) - �(0)1. Under the above assumptions, it 
is shown in [3] that 
�(0)-'�n(0)%0 llnn 
k, Vrj and �(0)=�(0')+ 0 1 ---n-n 
Therefore, for any iterative algorithm for minimizing �nO), in the initial phase of 
learning the reduction of training error is essentially equivalent to the reduction of 
generalization error. It can be further shown that this situation persists until the 
estimates Ot enter an n -6' neighborhood of t, where 5n-- 1/2. 
The basic tool we have used in arriving at this
