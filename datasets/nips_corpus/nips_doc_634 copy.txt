The Power of Approximating: a 
Comparison of Activation Functions 
Bhaskar DasGupta 
Department of Computer Science 
University of Minnesota 
Minneapolis, MN 55455-0159 
email: dasguptacs. uam. edu 
Georg Schnitger 
Department of Computer Science 
The Pennsylvania State University 
University Park, PA 16809 
mail: gorg�s. psu. du 
Abstract 
We compare activation functions in terms of the approximation 
power of their feedforward nets. We consider the case of analog as 
well as boolean input. 
I Introduction 
We consider efficient approximations of a given multivariate function f: [- 1, 1] m - 
T by feedforward neural networks. We first introduce the notion of a feedforward 
net. 
Let F be a class of real-valued functions, where each function is defined on some 
subset of 7Z. A F-net C is an unbounded fan-in circuit whose edges and vertices 
are labeled by real numbers. The real number assigned to an edge (resp. vertex) 
is called its weight (resp. its threshold). Moreover, to each vertex v an activation 
function % 6 F is assigned. Finally, we assume that C has a single sink w. 
The net C computes a function fc : [-1, 1]  - 7Z as follows. The components 
of the input vector x = (xl,...,Xm) 6 [--1, 1] m are assigned to the sources of C. 
Let vl,..., Vn be the immediate predecessors of a vertex v. The input for v is then 
sv(x) = Y'.=i wiyi-iv, where wi is the weight of the edge (vi, v), tv is the threshold 
of v and yl is the value assigned to vi. If v is not the sink, then we assign the value 
7(s(x)) to v. Otherwise we assign s(x) to v. 
Then f� = sw is the function computed by C where w is the unique sink of C. 
615 
616 DasGupta and Schnitger 
A great deal of work has been done showing that nets of two layers can approximate 
(in various norms) large function classes (including continuous functions) arbitrar- 
ily well (Arai, 1989; Carrol and Dickinson, 1989; Cybenko, 1989; Funahashi, 1989; 
Gallant and White, 1988; Hornik et al. 1989; Irie and Miyake,1988; Lapades and 
Farber, 1987; Nielson, 1989; Poggio and Girosi, 1989; Wet et al., 1991). Various 
activation functions have been used, among others, the cosine squasher, the stan- 
dard sigmoid, radial basis functions, generalized radial basis functions, polynomials, 
trigonometric polynomials and binary thresholds. Still, as we will see, these func- 
tions differ greatly in terms of their approximation power when we only consider 
efficient nets; i.e. nets with few layers and few vertices. 
Our goal is to compare activation functions in terms of efficiency and quality of 
approximation. We measure efficiency by the size of the net (i.e. the number of 
vertices, not counting input units) and by its number of layers. Another resource 
of interest is the Lipschitz-bound of the net, which is a measure of the numerical 
stability of the net. We say that net C has Lipschitz-bound L if all weights and 
thresholds of C are bounded in absolute value by L and for each vertex v of C and 
for all inputs x, y G [-1, 1] , 
[%(s.(x)) - %(s.(y)) I _< L. Is.(x) - s.(y)l. 
(Thus we do not demand that activation function % has Lipschitz-bound L, but 
only that % has Lipschitz-bound L for the inputs it receives.) We measure the 
quality of an approximation of function f by function fc by the Chebychev norm; 
i.e. by the maximum distance between f and fc over the input domain [-1, 1] m. 
Let r be a class of activation functions. We are particularly interested in the 
following two questions. 
� Given a function f: [-1, 1]  - 7�, how well can we approximate f by a r-net 
with d layers, size s, and Lipschitz-bound L? Thus, we are particularly interested 
in the behavior of the approximation error e(s, d) as a function of size and number 
of layers. This set-up allows us to investigate how much the approximation error 
decreases with increased size and/or number of layers. 
� Given two classes of activation functions P and r2, when do r-nets and 
nets have essentially the same approximation power with respect to some error 
function e(s, d)? 
We first formalize the notion of essentially the same approximation power. 
Definition 1.1 Let e � Af 2 -- 7+ be a function. F and F2 are classes of activation 
functions. 
(a). We say that F simulates P with respect to e if and only if there is a constant 
k such that for all functions f � [-1, 1]  - 7 with Lipschitz-bound 1/e(s,d), 
if f can be approximated by a F-net with d layers, size s, Lipschitz- 
bound 2  and approximation error e(s, d), then f can also be ap- 
proximated with error e(s,d) by a Fl-net with k(d+ 1) layers, size 
(s + 1) k and Lipschitz-bound 2 sk. 
(b). We say that Pl and r2 are equivalent with respect to e if and only if P 
simulates F with respect to e and F simulates F with respect to e. 
The Power of Approximating: a Comparison of Activation Functions 617 
In other words, when comparing the approximation power of activation functions, 
we allow size to increase polynomially and the number of layers to increase by a 
constant factor, but we insist on at least the same approximation error. Observe 
that we have linked the approximation error e(s, d) and the Lipschitz-bound of the 
function to be approximated. The reason is that approximations of functions with 
high Lipschitz-bound tend to have an inversely proportional approximation error. 
Moreover observe that the Lipschitz-bounds of the involved nets are allowed to be 
exponential in the size of the net. We will see in section 3, that for some activation 
functions far smaller Lipschitz-bounds suffice. 
Below we discuss our results. In section 2 we consider the case of tight approx- 
imations, i.e. e(s, d) = 2 -s. Then in section 3 the more relaxed error model 
e(s, d) = s -a is discussed. In section 4 we consider the computation of boolean 
functions and show that sigmoidal nets can be far more efficient than threshold- 
nets. 
2 Equivalence of Activation Functions for Error e(s, d) = 2 -s 
We obtain the following result. 
Theorem 2.1 The following activation functions are equivalent with respect to er- 
ror e(s, d) -- 2 - . 
� the standard sigmoid a(x) =  
1 +exp(--x) ' 
� any rational function which is not a polynomial, 
� any root x , provided o is not a natural number, 
� the logarithm (for any base b > 1), 
� the gaussian e -2, 
� the radial basis functions (1 q- x) , a ( 1, a  0 
Notable exceptions from the list of functions equivalent to the standard sigmoid are 
polynomials, trigonometric polynomials and splines. We do obtain an equivalence 
to the standard sigmoid by allowing splines of degree s as activation functions for 
nets of size s. (We will always assume that splines are continuous with a single knot 
only.) 
Theorem 2.2 Assume that e(s,d)= 2 -s. Then splines (of degree s for nets of size 
s) and the standard sigmoid are equivalent with respect to e(s, d). 
Remark 2.1 
(a) Of course, the equivalence of spline-nets and {a}-nets also holds for binary 
input. Since threshold-nets can add and multiply m m-bit numbers with constantly 
many layers and size polynomial in m (Reif, 1987), threshold-nets can efficiently 
approzimate polynomials and splines. 
618 DasGupta and Schnitger 
Thus, we obtain that {r}-nets with d layers, size s and �ipschitz-bound � can 
be simulated by nets of binary thresholds. The number of layers of the simulat- 
ing threshold-net will increase by a constant factor and its size will increase by a 
polynomial in (s -{- n)log(�), where n is the number of input bits. (The inclusion 
of n accounts for the additional increase in size when approximately computing a 
weighted sum by a threshold-net.) 
(b) If we allow size to increase by a polynomial in s -t- n, then threshold-nets and 
(r)-nets are actually equivalent with respect to error bound 2 -. This follows, since 
a threshold function can easily be implemented by a sigmoidal gate (Maass et al., 
1991). 
Thus, if we allow size to increase polynomially (in s + n) and the number of layers 
to increase by a constant factor, then (r)-nets with weights that are at most expo- 
nential (in s + n) can be simulated by (r)-nets with wei9hts of size polynomial in 
S. 
{r}-nets and threshold-nets (respectively nets of linear thresholds) are not equiva- 
lent for analog input. The same applies to polynomials, even if we allow polynomials 
of degree s as activation function for nets of size s: 
Theorem 2.3 
(a) Let sq(x) = x 2. If a net of linear splines (with d layers and size s) approximates 
sq(x) over the interval [-1, 1], then its approximation error will be at least s -�(a). 
(b) Let abs(x) =1 x 1. If a polynomial net with d layers and size s approximates 
abs(x) over the interval [-1, 1], then the approximation error will be at least s -�(a). 
We will see in Theorem 2.5 that the standard sigmoid (and hence any activation 
function listed in Theorem 2.1) is capable of approximating sq(x) and abs(x) with 
error at most 2 -s by constant-layer nets of size polynomial in s. Hence the standard 
sigmoid is properly stronger than linear splines and polynomials. Finally, we show 
that sine and the standard sigmoid are inequivalent with respect to error 2 -s. 
Theorem 2.4 The function sine(Ax) can be approximated by a (r)-net C.4 with d 
layers, size s = A �O/a) and error at most s �(-a) On the other hand, every 
with d layers which approximates sine(Ax) with error at most �, has to have size 
at least An(/a). 
Below we sketch the proof of Theorem 2.1. The proof itself will actually be more 
instructive than the statement of Theorem 2.1. In particular, we will obtain a 
general criterion that allows us to decide whether a given activation function (or 
class of activation functions) has at least the approximation power of splines. 
2.1 Activation Punctions with the Approximation Power of Splines 
Obviously, any activation function which can efficiently approximate polynomials 
and the binary threshold will be able to efficiently appr
