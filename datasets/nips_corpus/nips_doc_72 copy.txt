387 
Neural Net and Traditional Classifiers  
William Y. Huang and Richard P. Lippmann 
MIT Lincoln Laboratory 
Lexington, MA 02173, USA 
Abstract
Previous work on nets with continuous-valued inputs led to generative 
procedures to construct convex decision regions with two-layer percepttons (one hidden 
layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). 
Here we demonstrate that two-layer perceptton classifiers trained with back propagation 
can form both convex and disjoint decision regions. Such classifiers are robust, train 
rapidly, and provide good performance with simple decision regions. When complex 
decision regions are required, however, convergence time can be excessively long and 
performance is often no better than that of k-nearest neighbor classifiers. Three neural 
net classifiers are presented that provide more rapid training under such situations. 
Two use fixed weights in the first one or two layers and are similar to classifiers that 
estimate probability density functions using histograms. A third feature map classifier 
uses both unsupervised and supervised training. It provides good performance with 
little supervised training in situations such as speech recognition where much unlabeled 
training data is available. The architecture of this classifier can be used to implement 
a neural net k-nearest neighbor classifier. 
1. INTRODUCTION 
Neural net architectures can be used to construct many different types of classi- 
tiers [7]. In particular, multi-layer perceptron classifiers with continuous valued in- 
puts trained with back propagation are robust, often train rapidly, and provide perfor- 
mance similar to that provided by Gaussian classifiers when decision regions are convex 
[12,7,5,8]. Generative procedures demonstrate that such classifiers can form convex deci- 
sion regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions 
with three-layer perceptrons (two hidden layers) [7,2,9]. More recent work has demon- 
strated that two-layer perceptrons can form non-convex and disjoint decision regions. 
Examples of hand crafted two-layer networks which generate such decision regions are 
presented in this paper along with Monte Carlo simulations where complex decision 
regions were generated using back propagation training. These and previous simula- 
tions [5,8] demonstrate that convergence time with back propagation can be excessive 
when complex decision regions are desired and performance is often no better than that 
obtained with k-nearest neighbor classifiers [4]. These results led us to explore other 
neural net classifiers that might provide faster convergence. Three classifiers called, 
fixed weight, hypercube, and feature map classifiers, were developed and eval- 
uated. All classifiers were tested on illustrative problems with two continuous-valued 
inputs and two classes (A and B). A more restricted set of classifiers was tested with 
vowel formant data. 
2. CAPABILITIES OF TWO LAYER PERCEPTRONS 
Multi-layer perceptron classifiers with hard-limiting nonlinearities (node outputs 
of 0 or 1) and continuous-valued inputs can form complex decision regions. Simple 
constructive proofs demonstrate that a three-layer perceptron (two hidden layers) can 
 This work was sponsored by the Defense Advanced Research Projects Agency and the Department 
of the Air Force. The views expressed are those of the authors and do not reflect the policy or position 
of the U.S. Government. 
American Institute of Physics 1988 
388 
b I b2 
Xl x 2 
DECISION REGION FOR CLASS A 
x I bl b2 
 b6 ........ 
-2 
I I 
I 
 I 
I 
o I 2 
3 4 
Xl 
FIG. 1. A two-layer perceptton that forms disjoint decision re9ions for class A (shaded areas). Connec- 
tion weights and node offsets are shown in the left. Hyperplanes formed by all hidden nodes are drawn 
as dashed lines with node labels. Arrows on these lines point to the half plane where the hidden node 
output is high 
form arbitrary decision regions and a two-layer perceptron (one hidden layer) can form 
single convex decision regions [7,2,9]. Recently, however, it has been demonstrated that 
two-layer perceptrons can form decision regions that are not simply convex [14]. Fig. 1, 
for example, shows how disjoint decision regions can be generated using a two-layer 
perceptron. The two disjoint shaded areas in this Fig. represent the decision region 
for class A (output node has a high output, y = 1). The remaining area represents 
the decision region for class B (output node has a low output, y = 0). Nodes in 
this Fig. contain hard-limiting nonlinearities. Connection weights and node offsets are 
indicated in the left diagram. Ten other complex decision regions formed using two-layer 
perceptrons are presented in Fig. 2. 
The above examples suggest that two-layer perceptrons can form decision regions 
with arbitrary shapes. We, however, know of no general proof of this capability. A 
1965 book by Nilson discusses this issue and contains a proof that two-layer nets can 
divide a finite number of points into two arbitrary sets ([10] page 89). This proof 
involves separating M points using at most M - 1 parallel hyperplanes formed by first- 
layer nodes where no hyperplane intersects two or more points. Proving that a given 
decision region can be formed in a two-layer net involves testing to determine whether 
the Boolean representations at the output of the first layer for all points within the 
decision region for class A are linearly separable from the Boolean representations for 
class B. One test for linear separability was presented in 1962 [13]. 
A problem with forming complex decision regions with two-layer percepttons is that 
weights and offsets must be adjusted carefully because they interact extensively to form 
decision regions. Fig. 1 illustrates this sensitivity problem. Here it can be seen that 
weights to one hidden node form a hyperplane which influences decision regions in 
an entire halLplane. For example, small errors in first layer weights that results in a 
change in the slopes of hyperplanes b$ and b6 might only slightly extend the A region 
but completely eliminate the A2 region. This interdependence can be eliminated in 
three layer perceptrons. 
It is possible to train two-layer percepttons to form complex decision regions using 
back propagation and sigmoidal nonlinearities despite weight interactions. Fig. 3, for 
example, shows disjoint decision regions formed using back propagation for the problem 
of Fig. 1. In this and all other simulations, inputs were presented alternately from 
classes A and B and selected from a uniform distribution covering the desired decision 
region. In addition, the back propagation rate of descent term, r/, was set equal to the 
momentum gain term, a and r/= a = .01. Small values for r/and a were necessary to 
guarantee convergence for the difficult problems in Fig. 2. Other simulation details are 
389 
I) ) s) ) I 
FIG. 2. Ten complex decision regions formed by two-layer perceptrons. The numbers assigned to each 
case are the acasenumbers used in the rest of this paper. 
as in [5,8]. Also shown in Fig. 3 are hyperplanes formed by those first-layer nodes with 
the strongest connection weights to the output node. These hyperplanes and weights 
are similar to those in the networks created by hand except for sign inversions, the 
occurrence of multiple similar hyperplanes formed by two nodes, and the use of node 
offsets with values near zero. 
3. COMPARATIVE RESULTS OF TwO-LAYERS VS. THREE-LAYERS 
Previous results [5,8], as well as the weight interactions mentioned above, suggest 
that three-layer percepttons may be able to form complex decision regions faster with 
back propagation than two-layer percepttons. This was explored using Monte Carlo 
simulations for the first nine cases of Fig. 2. All networks have 32 nodes in the first 
hidden layer. The number of nodes in the second hidden layer was twice the number 
of convex regions needed to form the decision region (2, 4, 6, 4, 6, 6, 8, 6 and 6 for 
Cases I through 9 respectively). Ten runs were typically averaged together to obtain 
a smooth curve of percentage error vs. time (number of training trials) and enough 
trials were run (to a limit of 250,000) until the curve appeared to fiatten out with little 
improvement over time. The error curve was then low-pass filtered to determine the 
convergence time. Convergence time was defined as the time when the curve crossed a 
value 5 percentage points above the final percentage error. This definition provides a 
framework for comparing the convergence time of the different classifiers. It, however, is 
not the time after which error rates do not improve. Fig. 4 summarizes results in terms 
of convergence time and final percentage error. In those cases with disjoint decision 
regions, back propagation sometimes failed to form separate regions after 250,000 trials. 
For example, the two disjoint regions required in Case 2 were never fully separated with 
390 
FIG. 3. Decision regions formed using back propagation for Cases  of Fig. . Thick solid lines represent 
decision boundaries. Dashed lines and arrows have the same meaning as in Fig. 1. Only hyperplanes 
for hidden nodes with large weights to the output node are shown. Over 300,000 training trials were 
required to form separate regions. 
a two-layer perceptron but were separated with a three-layer perceptron. This is noted 
by the use of filled symbols in Fig. 4. 
Fig. 4 shows that there is no significant performance difference between two and 
three layer perceptrons when forming complex decision regions using back propagation 
training. Both types of classifiers take an excessively long time (> 100,000 trials) to 
form complex decision regions. A minor difference is that in Cases 2 and 7 the two-layer 
network failed to separate disjoint regions after 250,000 trial
