Phase Diagram and Storage Capacity of 
Sequence Storing Neural Networks 
A. Diiring 
Dept. of Physics 
Oxford University 
Oxford OX1 3NP 
United Kingdom 
a.during 1 @physics.oxford.ac.uk 
A. C. C. Coolen 
Dept. of Mathematics 
King's College 
London WC2R 2LS 
United Kingdom 
tcoolen @mth.kcl.ac.uk 
D. Sherrington 
Dept. of Physics 
Oxford University 
Oxford OX1 3NP 
United Kingdom 
d.sherrington 1 @physics.oxford.ac.uk 
Abstract 
We solve the dynamics of Hopfield-type neural networks which store se- 
quences of patterns, close to saturation. The asymmetry of the interaction 
matrix in such models leads to violation of detailed balance, ruling out an 
equilibrium statistical mechanical analysis. Using generating functional 
methods we derive exact closed equations for dynamical order parame- 
ters, viz. the sequence overlap and correlation and response functions, 
in the limit of an infinite system size. We calculate the time translation 
invariant solutions of these equations, describing stationary limit-cycles, 
which leads to a phase diagram. The effective retarded self-interaction 
usually appearing in symmetric models is here found to vanish, which 
causes a significantly enlarged storage capacity of acm 0.269, com- 
pared to acm 0.139 for Hopfield networks s[oring static patterns. Our 
results are tested against extensive computer simulations and excellent 
agreement is found. 
212 A. Diring, A. C. C. Coolen and D. Sherrington 
1 INTRODUCTION AND DEFINITIONS 
We consider a system of N neurons or(t) = {ai(t) = +1}, which can change their states 
collectively at discrete times (parallel dynamics). Each neuron changes its state with a 
l[1-tanh/ai(t)[j Jijo'j(t)+Oi(t)]], so that the transition matrix is 
probability Pi (t) --  
W[o-� + 1)1o'�)1 = 
N 
H e/rr'(s+l)[Y-;/v--1 d''rr3(s)+O'(s)]-ln2c�sh(fl[Y= 
i=1 
(1) 
with the (non-symmetric) interaction strengths Jij chosen as 
1 P 
Jij = NZ +1� 
-- i sj, (2) 
=1 
The ' represent components of an ordered sequence of patterns to be stored 1. The gain 
parameter/ can be interpreted as an inverse temperature governing the noise level in the 
dynamics (1) and the number of patterns is assumed to scale as N, i.e. p - oN. If 
the interaction matrix would have been chosen symmetrically, the model would be acces- 
sible to methods originally developed for the equilibrium statistical mechanical analysis 
of physical spin systems and related models [1, 2], in particular the replica method. For 
the nonsymmetric interaction matrix proposed here this is ruled out, and no exact solution 
exists to our knowledge, although both models have been first mentioned at the same time 
and an approximate solution compatible with the numerical evidence at the time has been 
provided by Amari [3]. The difficulty for the analysis is that a system with the interactions 
(2) never reaches equilibrium in the thermodynamic sense, so that equilibrium methods 
are not applicable. One therefore has to apply dynamical methods and give a dynamical 
meaning to the notion of the recall state. Consequently, we will for this paper employ the 
dynamical method of path integrals, pioneered for spin glasses by de Dominicis [4] and 
applied to the Hopfield model by Rieger et al. [5]. 
We point out that our choice of parallel dynamics for the problem of sequence recall is 
deliberate in that simple sequential dynamics will not lead to stable recall of a sequence. 
This is due to the fact that the number of updates of a single neuron per time unit is not 
a constant for sequential dynamics. Schemes for using delayed asymmetric interactions 
combined with sequential updates have been proposed (see e.g. [6] for a review), but are 
outside the scope of this paper. 
Our analysis starts with the introduction of a generating functional Z[b] of the form 
z[p]=  p[o'(0),...,o-(t)le-iE<, (s)'(s), (3) 
,(o) ..., (t) 
which depends on real fields {i(t)}. These fields play a formal role only, allowing for the 
identification of interesting order parameters, such as 
Upper (pattern) indices are understood to be taken modulo p unless otherwise stated. 
Phase Diagram and Storage Capacity of Sequence-Storing Neural Networks 213 
for the average activation, response and correlation functions, respectively. Since this func- 
tional involves the probability p[cr(0),... , or(t)] of finding a 'path' of neuron activations 
{or(0),... , rr(t)}, the task of the analysis is to express this probability in terms of the 
macroscopic order parameters itself to arrive at a set of closed macroscopic equations. 
The first step in rewriting the path probability is to realise that (1) describes a one- 
step Markov process and the path probability is therefore just the product of the 
single-time transition probabilities, weighted by the probability of the initial state: 
p[cr(O),.. cr(t)] p(cr (0))t- 
� , = I-Is=o W[cr(s + 1)let(s)]. Furthermore, we will in the 
course of the analysis frequently isolate interesting variables by introducing appropriate 
5-functions, such as 
i----1 j=l 
f dh(s) dfa(s) N 
i=1 
The variable hi(t) can be interpreted as the local field (or presynaptic potential) at site i 
and time t and their introduction transforms z[Ip] into 
Z[Ip] =  p(cr(0))/ 
(o)...(t) 
t--1 
dh dfa [e3a(s+l).h(s)_i 
II 
In 2 cosh(hi(s)) 
ei(h(s).h(s)-N- X:. 3 h,(s) X:, ,+ya,(s)-h(s).O(s)-p(s).a(s))]. 
(4) 
This expression is the last general form of z[Ip] we consider. To proceed with the analysis, 
we have to make a specific ansatz for the system behaviour. 
2 DYNAMIC MEAN FIELD THEORY 
As sequence recall is the mode of operation we are most interested in, we make the 
ansatz that, for large systems, we have an overlap of order (9 (N �) between the pattern 
s at time s, and that all other patterns are overlapping with order (9 (N -/2) at most. 
Accordingly, we introduce the macroscopic order parameters for the condensed pattern 
re(s) = N -1 Y-i ]o'i(s) and for the quantity k(s) = N - Y-i ]i(s), and their noncon- 
densed equivalents yU(s) = N -1/2 Y-i o'i($) and x(s) = N -1/2 Y,i i($) (I   $), 
where the scaling ansatz is reflected in the normalisation constants. Introducing these ob- 
jects using 5 functions, as with the local fields hi(s), removes the product of two patterns 
in the last line of eq. (4), so that the exponent will be linear in the pattern bits. 
Because macroscopic observables will in general not depend on the microscopic realisation 
of the patterns, the values of these observables do not change if we average z[Ip] over the 
realisations of the patterns. Performing this average is complicated by the occurrence of 
some patterns in both the condensed and the noncondensed overlaps, depending on the 
current time index, which is an effect not occurring in the standard Hopfield model. Using 
some simple scaling arguments, this difficulty can be removed and we can perform the 
average over the noncondensed patterns. The disorder averaged Z[Ip] acquires the form 
Z[b] = / dmdriadk dc dqd/ldQ d( dK d e N('I'[']+a't']+at'])+�(N/) (5) 
214 A. Daring, A. C. C. Coolen and D. Sherrington 
where we have introduced the new observables q(s, s') -- 1IN Y']i o' (s)o'i (s'), Q( s, s') = 
1IN 5-i hi(s)hi(st), and K(s,s') = 1IN 5-i O'i($)hi($t) , and their corresponding conju- 
gate variables. The functions in the exponent turn out to be 
[m, fn, k, fc, q,(:l, Q, (, K,] = i E [rh(s)m(s)+ [c(s)k(s)-m(s)k(s)] + 
i E [O(s, s')q(s, s') + O(s, s')Q(s, s')+ t(s, s')K(s, s')l, (6) 
1 [ 
� [m,k,l,Q,]= . In 
.(o)...(t) 
Pi(a(O)) /l<t [dh(s)2(s) ] 
eel<, [fia(s+l)h(s)-In2cosh(2h(s))] X 
e - E.,,< [(s,s ),(s),(s )+q(s,s )h(s)h(')+R(,'),()hl')] x 
e i Z,<, (*)[a(s)-o,(,)-i(s)C +*] -i Z<, -(s)[m(*)C +,(s)] ], 
(7) 
and 
[q, Q, Q] =  In [ ((v 
--.>t  [uo(s)Q(s,s)uo(s)+u(s)K(s',s)vo(s')+vo(s)K(s,s)uo(s)+v.(s)q(s,s')v.(s')]. 
(8) 
The first of these expressions is just a result of the introduction of 5 functions, while the 
second will turn out to represent a probability measure given by the evolution of a single 
neuron under prescribed fields and the third reflects the disorder contribution to the local 
fields in that single neuron measure 2. We have thus reduced the original problem involving 
N neurons in a one-step Markov process to one involving just a single neuron, but at the 
cost of introducing two-time observables. 
3 DERIVATION OF SADDLE POINT EQUATIONS 
The integral in (5) will be dominated by saddle points, in our case by a unique saddle 
point when causality is taken into account. Extremising the exponent with respect to all 
occurring variables gives a number of equations, the most important of which give the 
physical meanings of three observables: q($, s') = C($, $'), K($, $') = iG($, $'), 
re(s) = lira 1 
m--,oc  E <oh($)> (9) 
with 
C($,$')= lim 1 
N'-'o  E <O'i(8)O'i(st)) G(8 s')= lim 1 
' 00i($') ' 
(lO) 
2We have assumed p(rr(0)) = 1-I, p,(a,(O)). 
Phase Diagram and Storage Capacity of Sequence-Storing Neural Networks 215 
which are the single-site correlation and response functions, respectively. The overline... 
is taken to represent disorder averaged values. Using also additional equations arising from 
the normalisation Z[0] = 1, we can rewrite the single neuron measure (I, as 
/i<t [dh(s)dt(s)' [rr(s+l,h,s,-In cosh,/3h(s,,] 
(f[{er}]), = E i i' P(a(O))f[{er}]eEs<' 2 
o...a(t) 
x e i  <, h() [a()-0()-m(s)] -  a E, ,, <, R(s,' )h(s)h(') ( 11 ) 
with the short-hand R = t=o GttCGt' To simplify notation, we have here assumed 
that the initial probabilities p(ai(O)) are uniform and that the external fields Oi(s) are 
so-called staggered ones, i.e. Oi(s) = 0( +, which makes the single neuron measure 
i 
site-independent. This single neuron measure (11) represents the essential result of our 
calculations and is already properly normalised (i.e. (1). = 1). 
When one compares the present fo of the single neu
