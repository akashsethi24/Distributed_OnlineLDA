Algebraic Analysis for Non-Regular 
Learning Machines 
Sumio Watanabe 
Precision and Intelligence Laboratory 
Tokyo Institute of Technology 
4259 Nagatsuta, Midori-ku, Yokohama 223 Japan 
swatanabpi. titech. ac.jp 
Abstract 
Hierarchical learning machines are non-regular and non-identifiable 
statistical models, whose true parameter sets are analytic sets with 
singularities. Using algebraic analysis, we rigorously prove that 
the stochastic complexity of a non-identifiable learning machine 
is asymptotically equal to Allogn- (ml - 1)loglogn q-const., 
where n is the number of training samples. Moreover we show that 
the rational number A1 and the integer ml can be algorithmically 
calculated using resolution of singularities in algebraic geometry. 
Also we obtain inequalities 0  /1  d/2 and I _< ml _< d, where d 
is the number of parameters. 
I Introduction 
Hierarchical learning machines such as multi-layer perceptrons, radial basis func- 
tions, and normal mixtures are non-regular and non-identifiable learning machines. 
If the true distribution is almost contained in a learning model, then the set of 
true parameters is not one point but an analytic variety [4][9][3][10]. This paper 
establishes the mathematical foundation to analyze such learning machines based 
on algebraic analysis and algebraic geometry. 
Let us consider a learning machine represented by a conditional probability density 
p(xlw ) where x is an M dimensional vector and w is a d dimensional parameter. We 
assume that n training samples X n -- {Xi; i = 1, 2, ..., n} are independently taken 
from the true probability distribution q(x), and that the set of true parameters 
*Vo = {w c ,v ; p(xlw)= q(x) (a.s. q(x)) } 
is not empty. In Bayes statistics, the estimated distribution p(xIX n) is defined by 
/ in 
p(xlX n) = p(xlw ) pn(w)dw, pn(W) -- n IIP(Xilw) 
i----1 
where (w) is an a priori probability density on R a, and Zn is a normalizing con- 
stant. The generalization error is defined by 
f q(x) x 
K(n) = Ex,{ q(x) logp(xlX,) 
Algebraic Analysis for Non-regular Learning Machines 357 
where Ex, {.} shows the expectation value over all training samples X '. One of 
the main purposes in learning theory is to clarify how fast K(n) converges to zero 
as n tends to infinity. Using the log-loss function h(x, w) = log q(x) - logp(x, w), 
we define the Kullback distance and the empirical one, 
I  h(Xi, w). 
Z(w) ---- h(x, w)q(x)dx, Z(w,X n) -- n i--1 
Note that the set of true parameters is equal to the set of zeros of H(w), W0 = 
{w E W; H(w) = 0}. If the true parameter set W0 consists of only one point, the 
learning machine p(xlw ) is called identifiable, if otherwise non-identifiable. It should 
be emphasized that, in non-identifiable learning machines, W0 is not a manifold 
but an analytic set with singular points, in general. Let us define the stochastic 
complexity by 
F(n) = -Ex.{log / exp(-nH(w, Xn)p(w)dw}. 
(1) 
Then we have an important relation between the stochastic complexity F(n) and 
the generalization error K(n) 
K(n) = F(n + 1)- F(n), 
which represents that K(n) is equal to the increase of F(n) [1]. In this paper, we 
show the rigorous asymptotic form of the stochastic complexity F(n) for general 
non-identifiable learning machines. 
2 Main Results 
We need three assumptions upon which the main results are proven. 
(A.1) The probability density q(w) is infinite times continuously differentiable and 
its support, W = supp q, is compact. In other words, q E C. 
(A.2) The log loss function, h(x, w) = log q(x) - log p(x, w), is continuous for x in 
the support Q = suppq, and is analytic for w in an open set W  D W. 
(A.3) Let {rj(x, w*);j = 1,2,..., d} be the associated convergence radii of h(x, w) 
at w*, in other words, Taylor expansion of h(x, w) at w* = (w, ..., w), 
h(x,w) -- 
kl,..,kd=O 
aklk2...kl(x)(w 1 -- w)kl(w2 -- w)k2 ...(W d -- W) kd 
absolutely converges in Iwj - 
j = 1,2,...,d. 
Assume inf inf 
xEQw*EW 
rj(x, w*) > 0 for 
Theorem 1 Assume (A.1),(A.2), and (A.3). Then, there exist a rational number 
A1 > O, a natural number ml, and a constant C, such that 
IF(n)-/1 logn q-(ml -- 1)loglogn I < C 
holds for an arbitrary natural number n. 
Remarks. (1) If q(x) is compact supported, then the assumption (A.3) is automat- 
ically satisfied. (2) Without assumptions (A.1) and (A.3), we can prove the upper 
bound, F(n) _</1 logn - (ml - 1) loglogn q- const. 
358  anabe 
From Theorem 1, if the generalization error K(n) has the asymptotic expansion, 
then it should be 
hi ml -- 1 1 
K(n) - +o(--). 
n n log n n log n 
As is well known, if the model is identifiable and has the positive definite Fisher 
information matrix, then /1 - d/2 (d is the dimension of the parameter space) and 
ml = 1. However, hierarchical learning models such as multi-layer percepttons, 
radial basis functions, and normal mixtures have smaller/1 and larger ral, in other 
words, hierarchical models are better learning machines than regular ones if Bayes 
estimation is applied. Constants 1 and ral are characterized by the following 
theorem. 
Theorem 2 Assume the same conditions as theorem 1. Let e > 0 be a sufficiently 
small constant. The holomorphic function in Re(z)  O, 
(z) = fu I(w)Z(w)dw' 
()< 
can be analytically continued to the entire complex plane as a meromorphic function 
whose poles are on the negative part of the real axis, and the constants -A1 and ml 
in theorem i are equal to the largest pole of J(z) and its multiplicity, respectively. 
The proofs of above theorems are explained in the following section. Let w = g(u) 
is an arbitrary analytic function from a set U C R d to W. Then J(z) is invariant 
under the mapping, 
{H(w), (w)}  (H(g(u)), 
where [g'(u)l = [det(Owi/Ouj) I is Jacobian. This fact shows that /1 and ml are in- 
variant under a bi-rational mapping. In section 4, we show an algorithm to calculate 
/1 and ral by using this invariance and resolution of singularities. 
3 Mathematical Structure 
In this section, we present an outline of the proof and its mathematical structure. 
3.1 Upper bound and b-function 
For a sufIiciently small constant e > 0, we define F* (n) by 
F*(n) = -log/u exp(-nH(w)) (w) dw. 
()< 
Then by using the Jensen's inequality, we obtain F(n) _< F* (n). To evaluate F* (n), 
we need the b-function in algebraic analysis [6][7]. Sato, Bernstein, Bj6rk, and 
Kashiwara proved that, for an arbitrary analytic function H(w), there exist a dif- 
ferential operator D(w, Ow, z) which is a polynomial for z, and a polynomial b(z) 
whose zeros are rational numbers on the negative part of the real axis, such that 
D(w, Ow, z)H(w) z+l -- b(z)H(w)  
(2) 
for any z C C and any w c Wc = {w  W; H(w) < e}. By using the relation eq.(2), 
the holomorphic function J(z) in Re(z) > 0, 
Ju 1 fu H w Z+lD* 
J(z) -- H(w) =(w)dw = bz) ( ) wo(w)dw' 
(w)<c ()< 
,41gebraic ,4nalysis for Non-regular Learning Machines 359 
can be analytically continued to the entire complex plane as a meromorphic func- 
tion whose poles are on the negative part of the real axis. The poles, which are 
rational numbers and ordered from the origin to the minus infinity, are referred to 
as -A1, -A2, -As, ..., and their multiplicities are also referred to as ml, m2, m3, ... 
Let Ckm be the coefficient of the m-th order of Laurent expansion of J(z) at -Ak. 
Then, 
K 
Ckm 
k=l m=l 
is holomorphic in Re(z) > -A:+i, and 
Let us define a function 
(t) = f (t - S(w))(w)dw 
for 0 < t < e and I(t) = 0 for e _< t _< 1. Then I(t) connects the function F*(n) 
with J(z) by the relations, 
(z) - t z (t) dr, 
1 
F*(n) = -log f0 exp(-nt) I(t) dt. 
The inverse Laplace transform gives the asymptotic expansion of I(t) as t - O, 
o mk 
Ckm tX-I (_ log t) m-1 
I(t)=   (m-l)! ' 
k----1 m=l 
resulting in the asymptotic expansion of F*(n), 
fo n t dt 
F*(n) = -log exp(-t) I() n 
= A11ogn--(m1 -- 1)1oglogn +O(1), 
which is the upper bound of F(n). 
3.2 Lower Bound 
We define a random variable 
A(X n) -- sup I nl/2(H(w,X n) - H(w)) / H(w) 1/2 I. (a) 
wEW 
Then, we prove in Appendix that there exists a constant co which is independent 
of n such that 
sx {A(X) 2} < co. 
By using an inequality ab _< (a 2 + b2)/2, 
TI, H(w,X n) _ Tl, H(w) -- A(xn)(TI, H(w)) 1/2 _ {TtH(w)- A(xn)2}, 
which derives a lower bound, 
_> -Ex {log /exp(--{nH(w)- A(Xn)2})(w)dw} 
-1E {A(Xn) 2} - log/exp(nil(w) 
= 2 x 2 )(w)w 
F(n) 
(5) 
(6) 
360 S. Watanabe 
The first term in eq.(6) is bounded. Let the second term be F,(n), then 
F,(n) = -log(Z1 4- Z) 
Z1 --- /H exp(nil(w) 
(,)< 2 
Z2 = /H exp(nil(w) 
(w)>e 2 ) 99(w)dw -< exp(--), 
which proves the lower bound of F(n), 
F(n) _> A1 logn- (ml - 1) loglogn +const. 
) 99(w)dw  const. n -A1 (log n) ml-1 
4 Resolution of Singularities 
In this section, we construct a method to calculate A1 and mi. First of all, we cover 
the compact set 14/0 with a finite union of open sets W a. In other words, 14/0 C 
OaW a. Hironaka's resolution of singularities [5][2] ensures that, for an arbitrary 
analytic function H(w), we can algorithmically find an open set U a C R d (U a 
contains the origin) and an analytic function ga � U a -4 W a such that 
H(go(/)) m a() 1 2 . . . d (  U ) (7) 
where a(u) > 0 is a positive function and k  0 (1  i  d) are even integers (a(u) 
and ki depend on Ua). Note that Jacobian [g(u)l = 0 if and only if u  gl(W0). 
finite 
(9(=))19(=)1 Cpl,P2 .... ,Pd ' ' + (8) 
= � 
(p,p .... ,p) 
By combining eq.(7) with eq.(8), we obtain 
J,(z)  /w H(w)Z99(w) 
---- /Uc a(Tl) {?Jl 1 U2 k2 ... ,dkd} Z 
For real z, max Jo,(z) _< J(z) _< -'.o, J(z), 
?,1 ,12o.. Ud dul du2... dud. 
/1 : min min min Pq + 1 
a (Pl .... ,pd) l_<q_<d kq 
and ml is equal to the number of q which attains the minimum, min. 
l _< q_< d 
Remark. In a neighborhood of w0 c W0, the analytic function H(w) is equivalent 
to a polynomial Hwo(W), in other words, there exists constants Cl, c > 
