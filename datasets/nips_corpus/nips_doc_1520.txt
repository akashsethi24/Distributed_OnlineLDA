Learning Lie Groups for Invariant Visual Perception* 
Rajesh P. N. Rao and Daniel L. Ruderman 
Sloan Center for Theoretical Neurobiology 
The Salk Institute 
La Jolla, CA 92037 
{rao, ruderman}@salk. edu 
Abstract 
One of the most important problems in visual perception is that of visual in- 
variance: how are objects perceived to be the same despite undergoing transfor- 
mations such as translations, rotations or scaling? In this paper, we describe a 
Bayesian method for learning invariances based on Lie group theory. We show 
that previous approaches based on first-order Taylor series expansions of inputs 
can be regarded as special cases of the Lie group approach, the latter being ca- 
pable of handling in principle arbitrarily large transformations. Using a matrix- 
exponential based generative model of images, we derive an unsupervised al- 
gorithm for learning Lie group operators from input data containing infinites- 
imal transformations. The on-line unsupervised learning algorithm maximizes 
the posterior probability of generating the training data. We provide experimen- 
tal results suggesting that the proposed method can learn Lie group operators for 
handling reasonably large 1-D translations and 2-D rotations. 
1 INTRODUCTION 
A fundamental problem faced by both biological and machine vision systems is the recognition 
of familiar objects and patterns in the presence of transformations such as translations, rotations 
and scaling. The importance of this problem was recognized early by visual scientists such as J. J. 
Gibson who hypothesized that constant perception depends on the ability of the individual to de- 
tect the invariants [6]. Among computational neuroscienfists, Pitts and McCulloch were perhaps 
the first to propose a method for perceptual invariance (knowing universals) [12]. A number of 
other approaches have since been proposed [5, 7, 10], some relying on temporal sequences of input 
patterns undergoing transformations (e.g. [4]) and others relying on modifications to the distance 
metric for comparing input images to stored templates (e.g. [15]). 
In this paper, we describe a Bayesian method for learning invariances based on the notion of contin- 
uous transformations'and Lie group theory. We show that previous approaches based on first-order 
Taylor series expansions of images [1, 14] can be regarded as special cases of the Lie group ap- 
proach. Approaches based on first-order models can account only for small transformations due 
to their assumption of a linear generative model for the transformed images. The Lie approach on 
the other hand utilizes a matrix-exponential based generative model which can in principle handle 
arbitrarily large transformations once the correct transformation operators have been learned. Us- 
ing Bayesian principles, we derive an on-line unsupervised algorithm for learning Lie group opera- 
tors from input data containing infinitesimal transformations. Although Lie groups have previously 
*This research was supported by the Alfred P. Sloan Foundation. 
Learning Lie Groups 
been used in visual perception [2], computer vision [16] and image processing [9], the question of 
whether it is possible to learn these groups directly from input data has remained open. Our pre- 
liminary experimental results suggest that in the two examined cases of 1-D translations and 2-D 
rotations, the proposed method can learn the corresponding Lie group operators with a reasonably 
high degree of accuracy, allowing the use of these learned operators in transformation-invariant 
vision. 
2 CONTINUOUS TRANSFORMATIONS AND LIE GROUPS 
Suppose we have a point (in general, a vector) I0 which is an element in a space F. Let TIo denote a 
transformation of the point I0 to another point, say I. The transformation operator T is completely 
specified by its actions on all points in the space F. Suppose T belongs to a family of operators 
7-. We will be interested in the cases where 7- is a group i.e. there exists a mapping f: 7- x 7- -4 
7- from pairs of transformations to another transformation such that (a) f is associative, (b) there 
exists a unique identity transformation, and (c) for every T E 7-, there exists a unique inverse 
transformation of T. These properties seem reasonable to expect in general for transformations on 
images. 
Continuous transformations are those which can be made infinitesimally small. Due to their favor- 
able properties as described below, we will be especially concerned with continuous transforma- 
tion groups or Lie groups. Continuity is associated with both the transformation operators T and 
the group 7-. Each T  7- is assumed to implement a continuous mapping from F -4 F. To be 
concrete, suppose T is parameterized by a single real number x. Then, the group 7- is continu- 
ous if the function T(x) : R -4 T is continuous i.e. any T  T is the image of some x  R 
and any continuous variation of x results in a continuous variation of T. Let T(0) be equivalent 
to the identity transformation. Then, as x -4 0, the transformation T(x) gets arbitrarily close to 
identity. Its effect on I0 can be written as (to first order in x): T(x)Io  (1 + xG)Io for some 
matrix (7 which is known as the generator of the transformation group. A macroscopic transfor- 
mation I1 = I(x) = T(x)Io can be produced by chaining together a number of these infinitesimal 
transformations. For example, by dividing the parameter x into N equal parts and performing each 
transformation in turn, we obtain: 
I(x) = (1 + (x/N)6) v lo 
In the limit N -4 c, this expression reduces to the matrix exponential equation: 
I(x) = eXGIo 
(1) 
(2) 
where I0 is the initial or reference input. Thus, each of the elements of our one-parameter Lie 
group can be written as: T(x) = &G. The generator G of the Lie group is related to the derivative 
--aT = (7T. This suggests an alternate way of deriving Equation 2. 
of T(x) with respect to x: aa: 
Consider the Taylor series expansion of a transformed input I(x) in terms of a previous input I(0): 
dI(O) x dI(O) x2 
I(x) = I(O) +  + dx  2 +' (3) 
where x denotes the relative transformation between I(x) and I(0). Defining --aI = GI for some 
dx 
operator matrix G, we can rewrite Equation 3 as: I(x) = &alo which is the same as equation 2 
with I0 = I(0). Thus, some previous approaches based on first-order Taylor series expansions 
[1, 14] can be viewed as special cases of the Lie group model. 
3 LEARNING LIE TRANSFORMATION GROUPS 
Our goal is to learn the generators (7 of particular Lie transformation groups directly from input data 
containing examples of infinitesimal transformations. Note that learning the generator of a trats- 
formation effectively allows us to remain invariant to that transformation (see below). We assume 
that during natural temporal sequences of images containing transformations, there are small im- 
age changes corresponding to deterministic sets of pixel changes that are independent of what the 
812 R. P. N. Rao and D. L. Ruderman 
(a) 
! (x)  
I(0) x 
I/k __ 
xkG k 
( --. ) i(o) 
(c) 
Nttwork 1: 
Esthnation of 
Object !dtntity 
Network 2: 
1on of 
Transformation 
Figure 1: Network Architecture and Interpolation Function. (a) An implementation of the proposed ap- 
proach to invariant vision involving two cooperating recurrent networks, one estimating transformations and 
the other estimating object features. The latter supplies the reference image I(0) to the transformation net- 
work. (b) A locally recurrent elaboration of the transformation network for implementing Equation 9. The 
network computes e=CI(0) = I(0) + k (zkG&/k!)I(O) ï¿½ (c) The interpolation function Q used to generate 
training data (assuming periodic, band-limited signals). 
actual pixels are. The rearrangements themselves are universal as in for example image transla- 
tions. The question we address is: can we learn the Lie group operator (7 given simply a series of 
before and after images? 
Let the n x 1 vector I(0) be the before image and I(x) the after image containing the infinites- 
imal transformation. Then, using results from the previous section, we can write the following 
stochastic generative model for images: 
I(x) = exGI(0) + n (4) 
where n is assumed to be a zero-mean Gaussian white noise process with variance a 2. Since learn- 
ing using this full exponential generafive model is difficult due to multiple local minima, we restrict 
ourselves to transformations that are infinitesimal. The higher order terms then become negligible 
and we can rewrite the above equation in a more tractable form: 
/xI = xGI(0) + n (5) 
where AI = I(x) - I(0) is the difference image. Note that although this model is linear, the gener- 
ator (7 learned using infinitesimal transformations is the same matrix that is used in the exponential 
model. Thus, once learned, this matrix can be used to handle larger tr9nsformations as well (see 
experimental results). 
Suppose we are given M image pairs as data. We wish to find the n x n matrix G and the trans- 
formations x which generated the data set. To do so, we take a Bayesian maximum a posteriori 
approach using Gaussian priors on x and (7. The negative log of the posterior probability of gen- 
erating the data is given by: 
1 (Ai_xGi(O))r(Ai_xGi(O))+2_=x + grC_g (6) 
E = -logP[G, xlI(x),I(O)] = 2a  
where er is the variance of the zero-mean Gaussian prior on x, g is the n  x 1 vector form of G 
and C is the covariance matrix associated with the Gaussian prior on G. Extending this equation 
Learning Lie Groups 813 
to multiple image data is accomplished straightforwardly by summing the data-driven term over 
the image pairs (we assume (7 is fixed for all images although the transformation x may vary). For 
the experiments, r, r x and 6, were chosen to be fixed scalar values but it may be possible to speed 
up learning and improve accuracy by choosing 6, based on some knowledge of what w
