Active Learning for Function 
Approximation 
Kah Kay Sung 
(sung@ai.mit.edu) 
Massachusetts Institute of Technology 
Artificial Intelligence Laboratory 
545 Technology Square 
Cambridge, MA 02139 
Partha Niyogi 
(pn@ai.mit.edu) 
Massachusetts Institute of Technology 
Artificial Intelligence Laboratory 
545 Technology Square 
Cambridge, MA 02139 
Abstract 
We develop a principled strategy to sample a function optimally for 
function approximation tasks within a Bayesian framework. Using 
ideas from optimal experiment design, we introduce an objective 
function (incorporating both bias and variance) to measure the de- 
gree of approximation, and the potential utility of the data points 
towards optimizing this objective. We show how the general strat- 
egy can be used to derive precise algorithms to select data for two 
cases: learning unit step functions and polynomial functions. In 
particular, we investigate whether such active algorithms can learn 
the target with fewer examples. We obtain theoretical and empir- 
ical results to suggest that this is the case. 
1 INTRODUCTION AND MOTIVATION 
Learning from examples is a common supervised learning paradigm that hypothe- 
sizes a target concept given a stream of training examples that describes the concept. 
In function approximation, example-based learning can be formulated as synthesiz- 
ing an approximation function for data sampled from an unknown target function 
(Poggio and Girosi, 1990). 
Active learning describes a class of example-based learning paradigms that seeks out 
new training examples from specific regions of the input space, instead of passively 
accepting examples from some data generating source. By judiciously selecting ex- 
594 Kah Kay Sung, Partha Niyogi 
ampies instead of allowing for possible random sampling, active learning techniques 
can conceivably have faster learning rates and better approximation results than 
passive learning methods. 
This paper presents a Bayesian formulation for active learning within the function 
approximation framework. Specifically, here is the problem we want to address: Let 
Dn = {(xi,yi)li -' 1,...,n} be a set of n data points sampled from an unknown 
target function g, possibly in the presence of noise. Given an approximation func- 
tion concept class, f', where each f E f' has prior probability P:=[f], one can use 
regularization techniques to approximate g from Dn (in the Bayes optimal sense) 
by means of a function  E f'. We want a strategy to determine at what input 
location one should sample the next data point, (XN+l, YN+I), in order to obtain 
the best possible Bayes optimal approximation of the unknown target function g 
with our concept class :. 
The data sampling problem consists of two parts: 
1) Defining what we mean by the best possible Bayes optimal ap- 
proximation of an unknown target function. In this paper, we propose an 
optimality criterion for evaluating the goodness of a solution with respect to an 
unknown target function. 
2) Formalizing precisely the task of determining where in input space 
to sample the next data point. We express the above mentioned optimality 
criterion as a cost function to be minimized, and the task of choosing the next 
sample as one of minimizing the cost function with respect to the input space 
location of the next sample point. 
Earlier work (Cohn, 1991; MacKay, 1992) have tried to use similar optimal experi- 
ment design (Fedorov, 1972) techniques to collect data that would provide maximum 
information about the target function. Our work differs from theirs in several re- 
spects. First, we use a different, and perhaps more general, optimality criterion for 
evaluating solutions to an unknown target function, based on a measure of function 
uncertainty that incorporates both bias and variance components of the total output 
generalization error. In contrast, MacKay and Cohn use only variance components 
in model parameter space. Second, we address the important sample complexity 
question, i.e., does the active strategy require fewer examples to learn the target 
to the same degree of uncertainty? Our results are stated in PAC-style (Valiant, 
1984). After completion of this work, we learnt that Sollich (1994) had also recently 
developed a similar formulation to ours. His analysis is conducted in a statistical 
physics framework. 
The rest of the paper is organized as follows: Section 2, develops our active sampling 
paradigm. In Sections 3 and 4, we consider two classes of functions for which active 
strategies are obtained, and investigate their performance both theoretically and 
empirically. 
2 THE MATHEMATICAL FRAMEWORK 
In order to optimally select examples for a learning task, one should first have a 
clear notion of what an ideal learning goal is for the task. We can then measure 
an example's utility in terms of how well the example helps the learner achieve the 
Active Learning for Function Approximation 595 
goal, and devise an active sampling strategy that selects examples with maximum 
potential utility. In this section, we propose one such learning goal -- to find an 
approximation function 0  :T' that best estimates the unknown target function 
g. We then derive an example utility cost function for the goal and finally present 
a general procedure for selecting examples. 
2.1 EVALUATING A SOLUTION TO AN UNKNOWN TARGET 
THE EXPECTED INTEGRATED SQUARED DIFFERENCE 
Let g be the target function that we want to estimate by means of an approximation 
function .0 6 Y. If the target function g were known, then one natural measure of 
how well (or badly) � approximates g would be the Integrated Squared Difference 
(ISD) of the two functions: 
5(O,g) = (g(x) - O(x))2dx. (1) 
In most function approximation tks, the target g is unknown, so we clearly can- 
not express the quality of a learning result, , in terms of g. We can, however, 
obtain an expected integrated squared difference (EISD) between the unknown tar- 
get, g, and its estimate, , by treating the unknown target g  a random vari- 
able from the approximation function concept cls . Taking into account the 
n data points, Dn, seen so far, we have the following a-posteriori likelihood for 
g: P(g[Dn)  [g]P(Dn[g). The expected integrated squared difference (EISD) 
between an unknown target, g, and its estimate, , given Dn, is thus: 
E[5(O,g)Dn] =  P(gDn)5(O,g)dg =  [g]P(Dng)5(O,g)dg. (2) 
2.2 SELECTING THE NEXT SAMPLE LOCATION 
We can now express our learning goal  minimizing the expected integrated squared 
difference (EISD) between the unknown target g and its estimate . A rexonahie 
sampling strategy would be to choose the next example from the input location that 
minimizes the EISD between g and the new estimate .+1. How does one predict 
the new EISD that results from sampling the next data point at location X.+l v 
Suppose we also know the target output value (possibly noisy), y.+, at x.+. 
The EISD between g and its new estimate .+ would then be E[5(.+1, g)D. U 
(x.+,y.+l)], where + can be recovered from D. U (x.+,y.+) via regular- 
ization. In reality, we do not know y.+, but we can derive for it the following 
conditional probability distribution: 
(3) 
This leads to the following expected value for the new EISD, if we sample our next 
data point at x.+: 
= 
Clearly, the optimal input location to sample next is the location that minimizes e 
cost function in Equation 4 (henceforth referred to  the total output unceainty), 
i.. 
+ = argmin(g.+lD.,x.+). (5) 
596 Kah Kay Sung, Partha Niyogi 
2.3 SUMMARY OF ACTIVE LEARNING PROCEDURE 
We summarize the key steps involved in finding the optimal next sample location: 
1) Compute P(glD,). This is the a-posteriori likelihood of the different functions 
g given Dn, the n data points seen so far. 
2) Fix a new point x,+ to sample. 
3) Assume a value y,+ for this x,+. One can compute P(g[D, U (x,+l,y,+l)) 
and hence the expected integrated squared difference between the target and its new 
estimate. This is given by E:[6(.n+,g)IDn U (xn+,yn+)]. See also Equation 2. 
4) At the given x,+, y,+ has a probability distribution given by Equation 3. 
Averaging over all Yn+l'S, we obtain the total output uncertainty for x,+, given by 
bl(.O,+lD,,x,+) in Equation 4. 
5) Sample at the input location that minimizes the total output uncertainty cost 
function. 
3 EXAMPLE 1: UNIT STEP FUNCTIONS 
To demonstrate the usefulness of the above procedure, let us first consider the 
following simple class of indicator functions parameterized by a single parameter a 
which takes values in [0, 1]. Thus 
:r - {l[,llO < a _< 1} 
We obtain a prior P(g - l[a,])) by assuming that a has an a-priori uniform distri- 
bution on [0, 1]. Assume that data, = i - 1, ..n} consistent with some 
unknown target function l[a,q (which the learner is to approximate) has been ob- 
tained. We are interested in choosing a point z E [0, 1] to sample which will provide 
us with maximal information. Following the general procedure outlined above we 
go through the following steps. 
For ease of notation, let zR be the right most point belonging to Dn whose y value 
is 0, i.e., zR = maxi=,..,{zilyi = 0}. Similarly let zL = mini=,..,{zilyi = 1} and 
let x: - xR ---- w. 
1) We first need to get P(gID,). It is easy to show that 
1 
P(g -- l[a,]lDn) - -- if a � [xt, x�]; 0 otherwise 
w 
2) Suppose we sample next at a particular x � [0, 1], we would obtain y with the 
distribution 
P(y - OlD,,x) - (x - x) _ (x - x) if x � [xt, x;]; 1 if x _ xt;0 otherwise 
r L -- r R w 
For a particular y, the new data set would be Dn+i = Dn U (x, y) and the corre- 
sponding EISD can be easily obtained using the distribution P(glD,+i). Averaging 
this over P(ylDn, x) as in step 4 of the general procedure, we obtain 
U(.O,+ID,,x) = ( w2/12 
(1/12w)((xL - x) 3 + (x- xn) ) 
if x _< x or x >_ x/; 
otherwise 
Ac
