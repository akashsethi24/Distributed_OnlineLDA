Data-Dependent Structural Risk 
Minimisation for Perceptron Decision 
Trees 
John Shawe-Taylor 
Dept of Computer Science 
Royal Holloway, University of London 
Egham, Surrey TW20 0EX, UK 
Eraall: jstOdcs.rhbnc.ac.uk 
ble!!o Cristi,,n!ni 
Dept of Engineering Mathematics 
University of Bristol 
Bristol BS8 1TR, UK 
Eraall: nello.cristianiniObristol.ac.uk 
Abstract 
Perceptron Decision Trees (a! known as Linear Machine DTs, 
etc.) are analysed in order that data-dependent Structural Risk 
Minlml,ation can be applied. Data-dependent analysis is per- 
formed which indicates that choosing the maximal margin hyper- 
plancs at the decision nodes will improve the generalization. The 
analysis uses a novel technique to bound the generalization error in 
terms of the margins at individual nodes. Experiments performed 
on real data sets confirm the validity of the approach. 
Introduction 
Neural network researchers have traditionally tackled classification problems by 
sembling perceptton or sigmoid nodes into feedforward neural networks. In this 
paper we consider a less common approach where the percepttons are used as deci- 
sion nodes in a decision tree structure. The approach has the advantage that more 
efficient heuristic algorithms exist for these structures, while the advantages of in- 
herent parallelism are if anything greater as all the percepttons can be evaluated in 
pardel, with the path through the tree determined in a very fast post-processing 
phase. 
Classical Decision Trees (DTs), like the ones produced by popular packages as 
CART [5] or C4.5 [9], partition the input space by means of axis-parallel hyperplanes 
(one at each internal node), hence inducing categories which are represented by 
(axis-parallel) hyperrectangles in such a space. 
A natural extension of that hypothesis space is obtained by associating to each 
internal node hyperplanes in general position, hence partitioning the input space 
by means of polygonal (polyhedral) categories. 
Data-Dependent SRM for Perceptron Decision Trees 337 
This approach has been pursued by many researchers, often with different motiva- 
tions, and hence the resulting hypothesis space has been given a number of different 
names: multivariate DTs [6], oblique DTs [8], or DTs using linear combinations of 
the attributes [5], Linear Machine DTs, Neural Decision Trees [12], Perceptton Trees 
[13], etc. 
We will call them Perceptton Decision Trees (PDTs), as they can be regarded as 
binary trees having & simple perceptton associated to each decision node. 
Different algorithms for Top-Down induction of PDTs from data have been pro- 
posed, based on different principles, [10], [5], [8], 
Experimental study of learning by means of PDTs indicates that their performances 
are sometimes better than those of traditional decision trees in terms of generaliz&- 
tion error, and usually much better in terms of tree-size [8], [6], but on some data 
set PDTs can be outperformed by normal DTs. 
We investigate an alternative strategy for improving the generaliz&tion of these 
structures, namely placing m,dmal margin hyperplanes at the decision nodes. By 
use of & novel analysis we are able to demonstrate that improved generalization 
bounds can be obtained for this approach. Experiments confirm that such a method 
delivers more accurate trees in all tested databases. 
2 Generalized Decision Trees 
Definition 2.1 Generalized Decision Trees (GDT). 
Given a space X and a set of boolean functions 
Y = {f: X - {0, 1}}, the class GDT(Y) of Generalized Decision Trees over Y are 
functions which can be implemented using a binary tree where each internal node 
is labeled with an element of Y, and each leag is labeled with either 1 or 0. 
To evaluate a particular tree T on input z {5 X, All the boolean functions associated 
to the nodes are assigned the same argument z {5 X, which is the argument ofT(z). 
The values assumed by them determine a unique path from the root to a leaf: at 
each internal node the left (respectively right) edge to a child is taken if the output 
of the function associated to that internal node is 0 (respectively 1). The value of 
the function at the assignment of a z {5 X is the value associated to the leag reached. 
We say that input z reaches a node of the tree, if that node is on the evaluation 
path for z. 
In the following, the nodes are the internal nodes of the binary tree, and the leaves 
are its external ones. 
Examples. 
� Given X = {0, 1}, a Boolean Decision Tree (BDT) is & GDT over 
BDT = {: J(x) = xl, �x {5 X} 
� Given X = R , a C$.5-1ike Decision e [CDT]   GDT over 
T d of deion rees defined on  CongeRoRs 8pe e he oRpR of 
common gohms llke C4.b d CART, d we   hem - for shorg 
- CDTs. 
� Given X = R , a Peepn Decision e (PDT)   GDT over 
PDT = {x:  6 R+}, 
where we have umM that the puts have bn auent th a coor- 
dinate of constt ue, hence implementg a tMholded perceptton. 
338 J. Shawe-Taylor and N. Cristianini 
3 Data-dependent SRM 
We begin with the definition of the fat-shattering dimension, which was first intro- 
duced in [7], and has been used for several problems in learning since [1, 4, 2, 3]. 
Definition 3.1 Let Y be a set of real valued functions. We say that a set of points 
X is 7-shattered by Y relative to r = (r)Ex if there are real numbers r indexed 
bit z  X such that for all binarid vectors b indexed bit X, there is a function fb  Y 
satis filing 
>r+7 ifb-I 
fb (z)  r. - 7 othervoise. 
The fat shattering dimension fat j, of the set Y is a function from the positive real 
numbers to the integers which maps a value 7 to the size of the largest 7-shattered 
set, if this is finite, or infinitit otherwise. 
As an example which will be relevant to the subsequent analysis consider the class: 
We quote the following result from [11]. 
Corollary 3.2 [11] Let ;Piin be restricted to points in a ball of n dimensions of 
radius R about the origin and with thresholds [01 _ R. Then 
fa%in('r) < + + 
The following theorem bounds the generalization of a classifier in terms of the 
fat shattering dimension rather than the usual Vapniir-Chervonenkis or Pseudo 
dimension. 
Let To denote the threshold function at O: T0: -- {0, 1}, To(a) = 1 itf a > O. For 
a class of functions Y, To(Y) = {To(f): f 
Theorem 3.3 [11] Consider a real valued function class Y having fat shattering 
function bounded above blt the function a. fat: ] - N which is continuous from 
the right. Fix O  . If a learner correctlit classifies m independentlit generated 
examples � with h = To(l) To(Y) such that er.(h) = 0 and 'r = ml. I.t(,d - ol, 
then with confidence 1 -- I the expected error of h is bounded from above by 
e(m,k,') = m2----- (klog (8--) log(32m) + log ($?) ) , 
where k = afar(7/8 ). 
The importance of this theorem is that it can be used to explain how a classifier 
can give better generalization than would be predicted by a classical analysis of its 
VC dimension. Essentially expanding the margin performs an automatic capacity 
control for function classes with small fat shattering dimensions. The theorem shows 
that when a large margin is achieved it is as if we were working in a lower VC class. 
We should stress that in general the bounds obtained should be better for cases 
where a large margin is observed, but that a priori there is no guarantee that such 
a margin will occur. Therefore a priori only the classical VC bound can be used. In 
view of corresponding lower bounds on the generalization error in terms of the VC 
dimension, the a posteriori bounds depend on a favourable probability distribution 
making the actual learning task easier. Hence, the result will only be useful if 
the distribution is favourable or at least not adversarial. In this sense the result 
is a distribution dependent result, despite not being distribution dependent in the 
Data-Dependent SRM fo r Perceptron Decision Trees 339 
traditional sense that assumptions about the distribution have had to be made in 
its derivation. The benign behaviour of the distribution is automatically estimated 
in the learning process. 
In order to perform a similar analysis for perceptton decision trees we will consider 
the set of margins obtained at each of the nodes, bounding the generalization as a 
function of these values. 
4 Generalisation analysis of the Tree Class 
It turns out that bounding the fat shattering dimension of PDT's viewed as real 
function classifiers is difficult. We will therefore do a direct generalization analysis 
mlmlclrig the proof of Theorem 3.3 but taking into account the margins at each of 
the decision nodes in the tree. 
Definition 4.1 Let (X, d) be a (pseudo-) metric space, let A be a subset of X and 
� > O. A set B C_ X is an e-cover for A if, for ever3t a  A, there exists b  B such 
that d(a, b) < e. The e-covering number of A, 3f(e, A), is the minimal cardinalitt 
of an e-cover for A (if there is no such finite cover then it is defined to be oo). 
We write Jq'(e, Y, x) for the e-covering number of Y with respect to the too pseudo- 
metric measuring the mvimum discrepancy on the sample x. These numbers are 
bounded in the following Lemma. 
lemma 4.2 (Alon eta/. [1]) Let Y be a class of functions X -- [0, 1] and P a 
distribution over X. Choose 0 < � < 1 and let d = faty(e/4). Then 
[ 4m  los(a,,,/()) 
E(Jq'(e,Y,x))_< 2 --j , 
vhere the expectation E is taken u.r.t. a sample x  X rn drawn according to p,n. 
Corollary 4.3 [11] Let  be a class of functions X -- [a, b] and P a distribution 
over X. Choose 0 < � < 1 and let d = faty(e/4). Then 
< 2 , 
vhere the expectation E is over samples x  X 'n dravn according to p,n. 
We are now in a position to tackle the main lemma which bounds the probability 
over a double sample that the first half has zero error and the second error greater 
than an appropriate e. Here, error is interpreted as being differently classified at 
the output o
