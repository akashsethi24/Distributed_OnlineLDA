Globally Optimal On-line Learning Rules 
Magnus Rattray*and David Saadt 
Department of Computer Science & Applied Mathematics, 
Aston University, Birmingham B4 7ET, UK. 
Abstract 
We present a method for determining the globally optimal on-line 
learning rule for a soft committee machine under a statistical me- 
chanics framework. This work complements previous results on 
locally optimal rules, where only the rate of change in general- 
ization error was considered. We maximize the total reduction in 
generalization error over the whole learning process and show how 
the resulting rule can significantly outperform the locally optimal 
rule. 
I Introduction 
We consider a learning scenario in which a feed-forward neural network model (the 
student) emulates an unknown mapping (the teacher), given a set of training exam- 
ples produced by the teacher. The performance of the student network is typically 
measured by its generalization error, which is the expected error on an unseen ex- 
ample. The aim of training is to reduce the generalization error by adapting the 
student network's parameters appropriately. 
A common form of training is on-line learning, where training patterns are pre- 
sented sequentially and independently to the network at each learning step. This 
form of training can be beneficial in terms of both storage and computation time, 
especially for large systems. A frequently used on-line training method for networks 
with continuous nodes is that of stochastic gradient descent, since a differentiable 
error measure can be defined in this case. The stochasticity is a consequence of 
the training error being determined according to only the latest, randomly cho- 
sen, training example. This is to be contrasted with batch learning, where all the 
training examples would be used to determine the training error leading to a de- 
terministic algorithm. Finding an effective algorithm for discrete networks is less 
straightforward as the error measure is not differentiable. 
* rattraym@aston.ac.uk 
t saadd@aston.ac.uk 
Globally Optimal On-line Learning Rules 323 
Often, it is possible to improve on the basic stochastic gradient descent algorithm 
and a number of modifications have been suggested in the literature. At late times 
one can use on-line estimates of second order information (the Hessian or its eigen- 
values) to ensure asymptotically optimal performance (e.g., [1, 2]). A number of 
heuristics also exist which attempt to improve performance during the transient 
phase of learning (for a review, see [3]). However, these heuristics all require the 
careful setting of parameters which can be critical to their performance. Moreover, 
it would be desirable to have principled and theoretically well motivated algorithms 
which do not rely on heuristic arguments. 
Statistical mechanics allows a compact description for a number of on-line learning 
scenarios in the limit of large input dimension, which we have recently employed to 
propose a method for determining globally optimal learning rates for on-line gradi- 
ent descent [4]. This method will be generalized here to determine globally optimal 
on-line learning rules for both discrete and continuous machines. That is, rules 
which provide the maximum reduction in generalization error over the whole learn- 
ing process. This provides a natural extension to work on locally optimal learning 
rules [5, 6], where only the rate of change in generalization error is optimized. In 
fact, for simple systems we sometimes find that the locally optimal rule is also glob- 
ally optimal. However, global optimization seems to be rather important in more 
complex systems which are characterized by more degrees of freedom and often re- 
quire broken permutation symmetries to learn perfectly. We will outline our general 
formalism and consider two simple and tractable learning scenarios to demonstrate 
the method. 
It should be pointed out that the optimal rules derived here will often require knowl- 
edge of macroscopic properties related to the teacher's structure which would not 
be known in general. In this sense these rules do not provide practical algorithms as 
they stand, although some of the required macroscopic properties may be evaluated 
or estimated on the basis of data gathered as the learning progresses. In any case 
these rules provide an upper bound on the performance one could expect from a 
real algorithm and may be instrumental in designing practical training algorithms. 
2 The statistical mechanics framework 
For calculating the optimal on-line learning rule we employ the statistical mechanics 
description of the learning process. Under this framework, which may be employed 
for both smooth [7, 8] and discrete systems (e.g. [9]), the learning process is captured 
by a small number of self-averaging statistics whose trajectory is deterministic in the 
limit of large input dimension. In this analysis the relevant statistics are overlaps 
between weight vectors associated with different nodes of the student and teacher 
networks. The equations of motion for the evolution of these overlaps can be written 
in closed form and can be integrated numerically to describe the dynamics. 
We will consider a general two-layer soft committee machine x. The desired teacher 
mapping is from an N-dimensional input space   R N onto a scalar   R, 
which the student models through a map a(J, ) = Y'=x g(Ji '), where g(x) is the 
activation function for the hidden layer, J = {Ji}l<_i_<K is the set of input-to-hidden 
adaptive weights for the K hidden nodes and the hidden-to-output weights are set 
to 1. The activation of hidden node i under presentation of the input pattern  is 
denoted x = Ji' 
The general result presented here also applies to the discrete committee machine, but 
we will limit our discussion to the soft-committee machine. 
324 M. Rattray and D. Saad 
Training examples are of the form ((, ) where u = 1, 2,..., P. The components 
of the independently drawn input vectors ( are uncorrelated random variables 
with zero mean and unit variance. The corresponding output  is given by a de- 
terministic teacher of a similar configuration to the student except for a possible 
difference in the number M of hidden units and is of the form  M 
- g(B. 
where B _---- {Bn)l(n(M is the set of input-to-hidden adaptive weights. The acti- 
vation of hidden node n under presentation of the input pattern  is denoted 
y - B,-. We will use indices i,j, k,l... to refer to units in the student net- 
work and n, m,... for units in the teacher network. We will use the commonly used 
quadratic deviation e(J, ) --  [ a(J, ) -  , as the measure of disagreement be- 
tween teacher and student. The most basic learning rule is to perform gradient 
descent on this quantity. Performance on a typical input defines the generalization 
error eg(J) -- (e(J, ))() through an average over all possible input vectors . 
The general form of learning rule we will consider is, 
j+l  __ , 
i =Ji + F(x  ) (1) 
where F -- {Fi} depends only on the student activations and the teacher's output, 
and not on the teacher activations which are unobservable. Note that gradient 
descent on the error takes this general form, as does Hebbian learning and other 
training algorithms commonly used in discrete machines. The optimal F can also 
depend on the self-averaging statistics which describe the dynamics, since we know 
how they evolve in time. Some of these would not be available in a practical appli- 
cation, although for some simple cases the unobservable statistics can be deduced 
from observable quantities. This is therefore an idealization rather than a practical 
algorithm and provides a bound on the performance of a real algorithm. 
The activations are distributed according to a multivariate Gaussian with covari- 
ances: (xixk) = Ji.Jk = Qin, (xiy,) = Ji-B, _= Ri,, and (y,y,) = B.B, -- T,, 
measuring overlaps between student and teacher vectors. Angled brackets denote 
averages over input patterns. The covariance matrix completely describes the state 
of the system and in the limit of large N we can write equations of motion for each 
macroscopic (the T,m are fixed and define the teacher): 
dRi, dQi 
da = (Fiy,) da = (Fix, + F,xi + FiF,) , (2) 
where angled brackets now denote the averages over activations, replacing the av- 
erages over inputs, and a = pin plays the role of a continuous time variable. 
3 The globally optimal rule 
Carrying out the averaging over input patterns one obtains an expression for the 
generalization error which depends exclusively on the overlaps R,Q and T. Using 
the dependence of their dynamics (Eq. 2) on F one can easily calculate the locally 
optimal learning rule [5] by taking the functional derivative of deg (F)/da to zero, 
looking for the rule that will maximize the reduction in generalization error at the 
present time step. This approach has been shown to be successful in some training 
scenarios but is likely to fail where the learning process is characterized by several 
phases of a different natures (e.g., multilayer networks). 
The globally optimal learning rule is found by minimizing the total change in gen- 
eralization error over a fixed time window, 
d% da = �(F, a) (3) 
A%(F)= o da o ' 
Globally Optimal On-line Learning Rules 325 
This is a functional of the learning rule which we minimize by a variational approach. 
First we can rewrite the integrand by expanding in terms of the equations of motion, 
each constrained by a Lagrange multiplier, 
(4) 
The expression for � still involve two multidimensional integrations over x and y, 
so taking variations in F, which may depend on x and  but not on y, we find an 
expression for the optimal rule in terms of the Lagrange multipliers: 
I x 
F = -x - x y (5) 
where , = [vq] and X = [Am]. We define y to be the teacher's expected field given 
the teacher's o
