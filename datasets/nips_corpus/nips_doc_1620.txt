Building Predictive Models from Fractal 
Representations of Symbolic Sequences 
Peter Tifio Georg Dorffner 
Austrian Research Institute for Artificial Intelligence 
Schottengasse 3, A- 1010 Vienna, Austria 
{petert, g eorg } @ ai. univie. ac. at 
Abstract 
We propose a novel approach for building finite memory predictive mod- 
els similar in spirit to variable memory length Markov models (VLMMs). 
The models are constructed by first transforming the n-block structure of 
the training sequence into a spatial structure of points in a unit hypercube, 
such that the longer is the common suffix shared by any two n-blocks, 
the closer lie their point representations. Such a transformation embodies 
a Markov assumption - n-blocks with long common suffixes are likely 
to produce similar continuations. Finding a set of prediction contexts is 
formulated as a resource allocation problem solved by vector quantizing 
the spatial n-block representation. We compare our model with both the 
classical and variable memory length Markov models on three data sets 
with different memory and stochastic components. Our models have a 
superior performance, yet, their construction is fully automatic, which is 
shown to be problematic in the case of VLMMs. 
1 Introduction 
Statistical modeling of complex sequences is a prominent theme in machine learning due 
to its wide variety of applications (see e.g. [5]). Classical Markov models (MMs) of finite 
order are simple, yet widely used models for sequences generated by stationary sources. 
However, MMs can become hard to estimate due to the familiar explosive increase in the 
number of free parameters when increasing the model order. Consequently, only low or- 
der MMs can be considered in practical applications. Some time ago, Ron, Singer and 
Tishby [4] introduced at this conference a Markovian model that could (at least partially) 
overcome the curse of dimensionality in classical MMs. The basic idea behind their model 
was simple: instead of fixed-order MMs consider variable memory length Markov models 
(VLMMs) with a deep memory just where it is really needed (see also e.g. [5][7]). 
The size of VLMMs is usually controlled by one or two construction parameters. Unfor- 
tunately, constructing a series of increasingly complex VLMMs (for example to enter a 
model selection phase on a validation set) by varying the construction parameters can be 
646 P Tiao and G. Dorffner 
a troublesome task [ 1]. Construction often does not work smoothly with varying the 
parameters. There are large intervals of parameter values yielding unchanged VLMMs in- 
terleaved with tiny parameter regions corresponding to a large spectrum of VLMM sizes. 
In such cases it is difficult to fully automize the VLMM construction. 
To overcome this drawback, we suggest an alternative predictive model similar in spirit 
to VLMMs. Searching for the relevant prediction contexts is reformulated as a resource 
allocation problem in Euclidean space solved by vector quantization. A potentially pro- 
hibitively large set of all length-L blocks is assigned to a much smaller set of prediction 
contexts on a suffix basis. To that end, we first transform the set of L-blocks appearing in 
the training sequence into a set of points in Euclidean space, such that points corresponding 
to blocks sharing a long common suffix are mapped close to each other. Vector quantiza- 
tion on such a set partitions the set of L-blocks into several classes dominated by common 
suffixes. Quantization centers play the role of predictive contexts. A great advantage of our 
model is that vector quantization can be performed on a completely self-organized basis. 
We compare our model with both classical MMs and VLMMs on three data sets repre- 
senting a wide range of grammatical and statistical structure. First, we train the models on 
the Feigenbaum binary sequence with a very strict topological and metric organization of 
allowed subsequences. Highly specialized, deep prediction contexts are needed to model 
this sequence. Classical Markov models cannot succeed and the full power of admitting a 
limited number of variable length contexts can be exploited. The second data set consists of 
quantized daily volatility changes of the Dow Jones Industrial Average (DJIA). Predictive 
models are used to predict the direction of volatility move for the next day. Financial time 
series are known to be highly stochastic with a relatively shallow memory structure. In this 
case, it is difficult to beat low-order classical MMs. One can perform better than MMs only 
by developing a few deeper specialized contexts, but that, on the other hand, can lead to 
overfitting. Finally, we test our model on the experiments of Ron, Singer and Tishby with 
language data from the Bible [5]. They trained classical MMs and a VLMM on the books 
of the Bible except for the book of Genesis. Then the models were evaluated on the bases 
of negative log-likelihood on an unseen text from Genesis. We compare likelihood results 
of our model with those of MMs and VLMMs. 
2 Predictive models 
We consider sequences S = ss2... over a finite alphabet `4 = {1,2, ..., A} generated by 
stationary sources. The set of all sequences over `4 with exactly n symbols is denoted by 
`4n. 
An information source over .4 = {1, 2, ..., A} is defined by a family of consistent prob- 
ability measures P on.4 , n = 0, 1,2,..., ,Et P+(ws) = P(w), for all w 
(.40 = {A} and P0(A) = 1, A denotes the empty string). 
In applications it is useful to consider probability functions P that are easy to handle. 
This can be achieved, for example, by assuming a finite source memory of length at most 
L, and formulating the conditional measures P(s[w) = P/`+(ws)/P/`(w), w 
using a function c: .4/'  C, from L-blocks over .4 to a (presumably small) finite set C of 
prediction contexts: 
P(slw ) - P(slc(w)). (1) 
In Markov models (MMs) of order n _< L, for all L-blocks w 6 .4�, c(w) is the length-n 
Predictive Models from Fractal Representations of Sequences 647 
suffix of w, i.e. c(uv) = v, v  A, u  A t'-. 
In variable memory length Markov models (VLMMs), the suffices c(w) of L-blocks w G 
A t' can have different lengths, depending on the particular L-block w. For strategies of 
selecting and representing the prediction contexts through prediction suffix trees and/or 
probabilistic suffix automata see, for example, [4][5]. VLMM construction is controlled by 
one, or several parameters regulating selection of candidate contexts and growing/pruning 
decisions. 
Prediction context function c � A t' --> C in Markov models of order n < L, can be 
interpreted as a natural homomorphism c � A t' --> A t' le corresponding to the equivalence 
relation � C_ A t. x .A t' on L-blocks over .A: two L-blocks u, v are in the same class, i.e. 
(u, v) 6 �, if they share the same suffix of length n. The factor set .At. {e = C = .A 
consists of all n-blocks over .A. Classical MMs define the equivalence � on the suffix 
bases, but regardless of the suffix structure present in the training data. Our idea is to keep 
the Markov-motivated suffix strategy for constructing �, but at the same time take into an 
account the data suffix structure. 
Vector quantization on a set of B points in a Euclidean space positions N < < B codebook 
vectors (CVs), each CV representing a subset of points that are closer to it than to any other 
CV, so that the overall error of substituting CVs for points they represent is minimal. In 
other words, CVs tend to represent points lying close to each other (in a Euclidean metric). 
In order to use vector quantization for determining relevant predictive contexts we need to 
do two things: 
1. Define a suitable metric in the sequence space that would correspond to Markov 
assumptions: 
(a) two sequences are close if they share a common suffix 
(b) the longer is the common suffix the closer are the sequences 
2. Define a uniformly continuous map from the sequence metric space to the Eu- 
clidean space, i.e. sequences that are close in the sequence space (i.e. share a long 
common suffix) are mapped close to each other in the Euclidean space. 
In [6] we rigorously study a class of such spatial representations of symbolic structures. 
Specifically, a family of distances between two L-blocks u = uu2...ut._ut. and v = 
vv2...vt._vt. over .A = {1,2, ..., A}, expressed as 
t. 1 
d(u, v) = (ui, k < 7' (2) 
i=1 
with $(i, j) = 1 if i = j, and $(i, j) = 0 otherwise, correspond to Markov assumption. 
The parameter k influences the rate of forgetting the past. We construct a map from 
the sequence metric space to the Euclidean space as follows: Associate with each symbol 
i .A amap 
i(z)=kz+(1-k)ti, ti6{O, 1) �, z610,1] � (3) 
operating on a unit D-dimensional hypercube [0, 1] D. Dimension of the hypercube should 
be large enough so that each symbol i is associated with a unique vertex, i.e. D = [log 2 A] 
and ti k tj whenever i  j. The map e: .At, ._> [0, 1] D, from L-blocks vv2 ...vt. over .A 
to the unit hypercube, 
rr(vlv2...VL) = VL(VL-i(...(V2(V(X*)))...)) = (vt. ovL_ o...ov2ov)(X*), (4) 
648 P Tiao and G. Dorffner 
where z* = {�}D is the center of the hypercube, is uniformly continuous. Indeed, 
whenever two sequences u, v share a common suffix of length Q, the Euclidean distance 
between their point representations (u) and (v) is less than vkQ. Strictly speaking, 
for a mathematically correct treatment of uniform continuity, we would need to consider 
infinite sequences. Finite blocks of symbols would then correspond to cylinder sets (see 
[6]). For sake of simplicity we only deal with finite sequences. 
As with classical Markov models, we define the prediction context function c: `4  C 
via an equivalence � on L-blocks over `4: two L-blocks u, v are in the same class if their 
images under the map  are represented by the same codebook vector. In this case, the set 
of prediction contex
