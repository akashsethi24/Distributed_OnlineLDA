Time Warping Invariant Neural Networks 
Guo-Zheng Sun, Hsing-Hen Chen and Yee-Chun Lee 
Institute for Advanced Computer Studies 
and 
Laboratory for Plasma Research, 
University of Maryland 
College Park, MD 20742 
Abstract 
We proposed a model of Time Warping Invariant Neural Networks (TWINN) 
to handle the time warped continuous signals. Although TWINN is a simple modifica- 
tion of well known recurrent neural network, analysis has shown that TWINN com- 
pletely removes time warping and is able to handle difficult classification problem. It 
is also shown that TWINN has certain advantages over the current available sequential 
processing schemes: Dynamic Programming(DP)[1], Hidden Markov Model(- 
HMM)[2], Time Delayed Neural Networks(TDNN) [3] and Neural Network Finite 
Automata(NNFA)[4]. 
We also analyzed the time continuity employed in TWINN and pointed out that 
this kind of structure can memorize longer input history compared with Neural Net- 
work Finite Automata (NNFA). This may help to understand the well accepted fact 
that for learning grammatical reference with NNFA one had to start with very short 
strings in training set. 
The numerical example we used is a trajectory classification problem. This 
problem, making a feature of variable sampling rates, having internal states, continu- 
ous dynamics, heavily time-warped data and deformed phase space trajectories, is 
shown to be difficult to other schemes. With TWINN this problem has been learned in 
100 iterations. For benchmark we also trained the exact same problem with TDNN and 
completely failed as expected. 
I. INTRODUCTION 
In dealing with the temporal pattern classification or recognition, time warping of input sig- 
nals is one of the difficult problems we often encounter. Although there are a number of 
schemes available to handle time warping, e.g. Dynamic Programming (DP) and Hidden Mark- 
ov Model(HMM), these schemes also have their own shortcomings in certain aspects. More de- 
pressing is that, as far as we know, there are no efficient neural network schemes to handle time 
warping. In this paper we proposed a model of Time Warping Invariant Neural Networks 
(TWINN) as a solution. Although TWINN is only a simple modification to the well known neu- 
ral net structure, analysis shows that TWINN has the built-in ability to remove time warping 
completely. 
The basic idea of TWINN is straightforward. If one plots the state trajectories of a continuous 
180 
Time Warping Invariant Neural Networks 181 
dynamical system in its phase space, these trajectory curves are independent of time warping 
because time warping can only change the time duration when traveling along these trajectories 
and does not affect their shapes and structures. Therefore, if we normalize the time dependence 
of the state variables with respect to any phase space variable, say the length of trajectory, the 
neural network dynamics becomes time warping invariant. 
To illustrate the power of the TWlNN we tested it with a numerical example of trajectory 
classification. This problem, chosen as a typical problem that the TWlNN could handle, has the 
following properties: (1). The input signals obey a continuous time dynamics and are sampled 
with various sampling rates. (2). The dynamics of the de-warped signals has internal states. (3). 
The temporal patterns consist of severely time warped signals. 
To our knowledge there have not been any neural network schemes which can deal with this 
case effectively. We tested it with TDNN and failed to learn. 
In the next section we will introduce the TWINN and prove its time warping invariance. In 
Section III we analyze its features and identify the advantages over other schemes. The numer- 
ical example of the trajectory classification with TWlNN is presented in Section IV. 
II. TIME WARPING INVARIANT NEURAL NETWORKS (TWINN) 
To process temporal signals, we consider a fully recurrent network, which consists of two 
groups of neurons: the state neurons (or recurrent units) represented by vector S(t) and the input 
neurons that are clamped to the external input signals {l(t), t = 0, 1, 2, ...... , T-I). The Time 
Warping Invariant Neural Networks (TWINN) is simply defined as.' 
S(t+ 1) = S(t) +l(t)F(S(t), W,l(t)) (1) 
where W is the weight matrix, l(t) is the distance between two consecutive input vectors defined 
by the norm 
l(t) = Ill(t+ 1) -l(t)II (2) 
and the mapping function F is a nonlinear function usually referred as neural activity function. 
For example of first order networks, it could take the form: 
Fi(S (t ), W,l (t) ) = Tanh (Wij(S (t )  l (t) )j) (3) 
J 
where Tanh(x) is Hyperbolic Tangent function and symbol  stands for the vector concatena- 
tion. 
For the purpose of classification (or recognition), we assign the target final state S k, 
(k=l,2,3,...K), for each category of pattems. After we feed into the TWlNN the whole sequence 
{I(0), I(1), I(2), ...... ,I(T-1)}, the state vector S(t) will reach the final state S(T). We then need to 
compare S(T) with the target final state Sk for each category k, (k=l,2,3,...K), and calculate the 
error: 
ek = ilSCT)_Skil 2 (4) 
The one with minimal error will be classified as such. The ideal error is zero. 
For the purpose of training, we are given a set of training examples for each category. We 
then minimize the error functions given by Eq. (4) using either back-propagation[7] or forward 
propagation algorithm[8]. The training process can be terminated when the total error reach its 
minimum. 
The formula of TWINN as shown in Eq. (1) does not look like new. The subtle difference 
from wildly used models is the introduction of normalization factor l(t) as in Eq. (1). The main 
advantage by doing this lies in its built-in time warping ability. This can be directly seen from 
its continuous version. 
As Eq. (1) is the discrete implementation of continuous dynamics, we can easily convert it 
into a continuous version by replacing t +1 by t+At and let At --> 0. By doing so, we get 
182 Sun, Chen, and Lee 
S (t + At) -S (t) dS 
lim = (5) 
at--}olll(t+at ) -l(t)1[ dL 
where L is the input trajectory length, which can be expressed as an integral 
t 
d: 
or summation (as in discrete version) 
t 
L(t) - o111(+ 1) -'I(:) II (7) 
For deterministic dynamics, the distance L(t) is a single-valued function. Therefore, we can 
make a unique mapping from t to L, I'I: t -> L, and any function of t can be transformed into a 
function of L in terms of this mapping. For instance, the input trajectory l(t) and the state tra- 
jectory S(t) can be transformed into I(L) and S(L). By doing so, discrete dynamics of Eq. (1) 
becomes, in the continuous limit, 
as  (s (�) w,  (�)) (8) 
dL 
It is obvious that there is no explicit time dependence in Eq. (8) and therefore the dynamics rep- 
resented by Eq. (8) is time warping independent. 
To be more specific, if we draw the trajectory curves off(t) and S(t) in their phase spaces re- 
spectively, these two curves would not be deformed if we only change the time duration when 
traveling along the curves. Therefore, if we generate several input sequences {l(t)} using dif- 
ferent time warping functions and feed them into TWINN, represented by Eq. (8) or Eq. (1), the 
induced state dynamics of S(L) would be the same. Meanwhile, the final state is the solo crite- 
rion for classification. Therefore, any time warped signals would be classified by the TWINN 
as the same. This is the so called time warping invariant. 
III. ANALYSIS OF TWINN VS. OTHER SCHEMES 
We emphasize two points in this section. First, we would analyze the advantages of the 
TWINN over the other neural network structures, like TDNN, and other mature and well known 
algorithms for time warping, such as HMM and Dynamics Programming. Second, we would an- 
alyze the memory capacity of input history for both the continuous dynamical networks as il- 
lustrated in Eq. (1) and its discrete companion, Neural Network Finite Automata used in 
grammatical inference by Liu [3], Sun [4] and Giles [5]. And, we will show by mathematical 
estimation that the continuity employed in TWINN increases the power of memorizing history 
compared with NNFA 
The Time Delayed Neural Networks (TDNN)[3] has been a useful neural network structure 
in processing temporal signals and achieves successes in several applications, e.g. speech rec- 
ognition. The traditional neural network structures are either feedforward or recurrent. The 
TDNN is something in between. The power of TDNN is in its dynamic combination of the spa- 
tial processing (as in a feedforward net) and sequential processing (as in a recurrent net with 
short time memory). Therefore, the TDNN could detect the local features within each windowed 
frame and store their voting scores into the short time memory neurons, and then make a final 
decision at the end of input sequence. This technique is suitable for processing the temporal pat- 
terns where the classification is decided by the integration of local features. But, it could not 
handle the long time correlation across time frames like a state machine. It also does not tolerate 
time warping effectively. Each of time warped patterns will be treated as a new feature. There- 
fore, TDNN would not be able to handle the numerical example given in this paper which has 
both the severe time warping and the internal states (long time correlation). The benchmark test 
has been performed and it proved our prediction. Actually, it can be seen later that in our exam- 
Time Warping Invariant Neural Networks 183 
ples, no matter which category they belong to, all windowed frames would contain similar local 
features, the simple integration of local features do not contribute directly to the final classifi- 
cation, rather the whole sinal history will decide the classification. 
As for the Dynamic Programming, it is to date the most efficient way to cope with time warp- 
ing problem. The most imp
