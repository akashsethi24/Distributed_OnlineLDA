The Learning Dynamics of 
a Universal Approximator 
Ansgar H. L. West 1'2 David Saad 1 Ian T. Nabney 1 
A. H. L. West aston. ac. uk D. Saadast on. ac. uk I.T. Nabneyaston. ac. uk 
1Neural Computing Research Group, University of Aston 
Birmingham B4 7ET, U.K. 
http://www. ncrg. aston. ac. uk/ 
2Department of Physics, University of Edinburgh 
Edinburgh EH9 3JZ, U.K. 
Abstract 
The learning properties of a universal approximator, a normalized 
committee machine with adjustable biases, are studied for on-line 
back-propagation learning. Within a statistical mechanics frame- 
work, numerical studies show that this model has features which 
do not exist in previously studied two-layer network models with- 
out adjustable biases, e.g., attractive suboptimal symmetric phases 
even for realizable cases and noiseless data. 
1 INTRODUCTION 
Recently there has been much interest in the theoretical breakthrough in the un- 
derstanding of the on-line learning dynamics of multi-layer feedforward perceptrons 
(MLPs) using a statistical mechanics framework. In the seminal paper (Saad & 
Solla, 1995), a two-layer network with an arbitrary number of hidden units was 
studied, allowing insight into the learning behaviour of neural network models whose 
complexity is of the same order as those used in real world applications. 
The model studied, a soft committee machine (Biehl & Schwarze, 1995), consists of 
a single hidden layer with adjustable input-hidden, but fixed hidden-output weights. 
The average learning dynamics of these networks are studied in the thermodynamic 
limit of infinite input dimensions in a student-teacher scenario, where a student 
network is presented serially with training examples (, ) labelled by a teacher 
network of the same architecture but possibly different number of hidden units. 
The student updates its parameters on-line, i.e., after the presentation of each 
example, along the gradient of the squared error on that example, an algorithm 
usually referred to as back-propagation. 
Although the above model is already quite similar to real world networks, the ap- 
proach suffers from several drawbacks. First, the analysis of the mean learning 
dynamics employs the thermodynamic limit of infinite input dimension -- a prob- 
lem which has been addressed in (Barber et al., 1996), where finite size effects have 
been studied and it was shown that the thermodynamic limit is relevant in most 
The Learning Dynamcis of a Universal Approxirnator 289 
cases. Second, the hidden-output weights are kept fixed, a constraint which has 
been removed in (Riegler & Biehl, 1995), where it was shown that the learning 
dynamics are usually dominated by the input-hidden weights. Third, the biases of 
the hidden units were fixed to zero, a constraint which is actually more severe than 
fixing the hidden-output weights. We show in Appendix A that soft committee 
machines are universal approximators provided one allows for adjustable biases in 
the hidden layer. 
In this paper, we therefore study the model of a normalized soft committee machine 
with variable biases following the framework set out in (Saad & Solla, 1995). We 
present numerical studies of a variety of learning scenarios which lead to remarkable 
effects not present for the model with fixed biases. 
2 DERIVATION OF THE DYNAMICAL EQUATIONS 
The student network we consider is a normalized soft committee machine of K 
hidden units with adjustable biases. Each hidden unit i consists of a bias Oi and a 
weight vector W/which is connected to the N-dimensional inputs . All hidden units 
are connected to a linear output unit with arbitrary but fixed gain ' by couplings 
of fixed strength. The activation of any unit is normalized by the inverse square 
root of the number of weight connections into the unit, which allows all weights to 
be of O(1) magnitude, independent of the input dimension or the number of hidden 
units. The implemented mapping is therefore fw()- (7/v)E/K=I g(ui--Oi), 
where ui -- W/./V and g(.) is a sigmoidal transfer function. The teacher net- 
work to be learned is of the same architecture except for a possible difference in 
the number of hidden units M and is defined by the weight vectors Bn and bi- 
ases Pn (n = 1,...,M). Training examples are of the form (t,,t,), where the 
input vectors ' are drawn form the normal distribution and the outputs are 
�' = 
Yn=l g(v - pn), where v = B- /x/-. 
The weights and biases are updated in response to the presentation of an example 
(, ), along the gradient of the squared error measure e = �[ - fw()] 2 
- r/0  
W//+1 - W// = fiwS  and Oil+l -- Oi = (i (1) 
with 5' =-[ - fw(e)la'(u' -od. The two learning rates are r/w for the weights 
and r/0 for the biases. In order to analyse the mean learning dynamics resulting 
from the above update equations, we follow the statistical mechanics framework in 
(Saad & Solla, 1995). Here we will only outline the main ideas and concentrate on 
the results of the calculation. 
As we are interested in the typical behaviour of our training algorithm we average 
over all possible instances of the examples . We rewrite the update equations (1) 
in W/ as equations in the order parameters describing the overlaps between pairs 
of student nodes Qij- Wi.Wj/N, student and teacher nodes Rin---Wi.Bn/N, 
and teacher nodes Tnm = Bn'Bm/N. The generalization error eg, measuring the 
typical performance, can be expressed solely in these variables and the biases 0 i and 
Pn. The order parameters Qij, Rin and the biases Oi are the dynamical variables. 
These quantities need to be self-averaging with respect to the randomness in the 
training data in the thermodynamic limit (N - c), which enforces two necessary 
constraints on our calculation. First, the number of hidden units K << N, whereas 
one needs K 00(N) for the universal approximation proof to hold. Second, one 
can show that the updates of the biases have to be of O(1/N), i.e., the bias learning 
rate has to be scaled by l/N, in order to make the biases self-averaging quantities, 
a fact that is confirmed by simulations [see Fig. 1]. If we interpret the normalized 
290 A. H. L. West, D. Saad and I. T. Nabney 
example number a = !/N as a continuous time variable, the update equations for 
the order parameters and the biases become first order coupled differential equations 
dQij 
da 
dRi, dO i 
da = n (Siv,) , d da = -n (Si) ' (2) 
Choosing g(x) = eff(x/) as the sigmoidal trsfer, most integrations in Eqs. [2) 
can be performed alytically, but for single Gaussian integrals remaining for - 
terms and the generization error. The exact form of the resulting dynamical 
equations is quite complicated and will be presented elsewhere. Here we only re- 
mark, that the gain 7 of the linear output unit, which determines the output scale, 
merely rescales the learning rates with 7 2 and can therefore be set to one without 
loss of generality. Due to the numerical integrations required, the differential equa- 
tions can only be solved accurately in moderate times for smaller student networks 
(K  5) but any teacher size M. 
3 ANALYSIS OF THE DYNAMICAL EQUATIONS 
The dynamical evolution of the overlaps Qij, Rin and the biases 0 i follows from 
integrating the equations of motion (2) from initial conditions determined by the 
(random) initialization of the student weights W/ and biases Oi. For random ini- 
tialization the resulting norms Qii of the student vector will be order O(1), while 
the overlaps Qij between different student vectors, and student-teacher vectors Rin 
will be only order O(1/v/). A random initialization of the weights and biases can 
therefore be simulated by initializing the norms Qii, the biases Oi and the normalized 
overlaps Oij -- Qij/v/QiiQjj and in -- Rin/x/QiiTnn from uniform distributions 
in the [0, 1], [- 1, 1], and [- 10 -2, 10 -12] intervals respectively. 
We find that the results of the numerical integration are sensitive to these ran- 
dom initial values, which has not been the case to this extent for fixed biases. 
Furthermore, the dynamical behaviour can become very complex even for realiz- 
able cases (K = M) and networks with three or four hidden units. For sake of 
simplicity, we will therefore restrict our presentation to networks with two hidden 
units (K - M = 2) and uncorrelated isotropic teachers, defined by Tnm -- 5nm, al- 
though larger networks and graded teacher scenarios were investigated extensively 
as well. We have further limited our scope by investigating a common learning 
rate (r/o = r/0 -- r/w) for biases and weights. To study the effect of different weight 
initialization, we have fixed the initial values of the student-student overlaps Qij 
and biases Oi, as these can be manipulated freely in any learning scenario. Only the 
initial student-teacher overlaps Rin are randomized as suggested above. 
In Fig. I we compare the evolution of the overlaps, the biases and the generalization 
error for the soft committee machine with and without adjustable bias learning a 
similar realizable teacher task. The student denoted by � lacks biases, i.e., Oi -- O, 
and learns to imitate an isotropic teacher with zero biases (Pn -- 0). The other 
student features adjustable biases, trained from an isotropic teacher with small 
biases (Pl,2 -- :F0.1). For both scenarios, the learning rate and the initial conditions 
were judiciously chosen to be r/o = 2.0, Qll = 0.1, Q22 = 0.2, in -- 012 = 
U[-10 -x2, 10 -12] with 01 = 0.0 and 02 = 0.5 for the student with adjustable biases. 
In both cases, the student weight vectors (Fig. la) are drawn quickly from their 
initial values into a suboptimal symmetric phase, characterized by the lack of spe- 
cialization of the student hidden units on a particular teacher hidden unit, as can 
be depicted from the similar values of Rin in Fig. lb. This symmetry is broken 
The Learning Dynamcis of a Universal Approximator 29
