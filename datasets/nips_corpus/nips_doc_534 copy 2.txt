Improving the Performance of Radial Basis 
Function Networks by Learning Center Locations 
Dietrich Wettschereck 
Department of Computer Science 
Oregon State University 
Corvallis, OR 97331-3202 
Thomas Dietterich 
Department of Computer Science 
Oregon State University 
Corvallis, OR 97331-3202 
Abstract 
Three methods for improving the performance of (gaussian) radial basis 
function (RBF) networks were tested on the NETtalk task. In RBF, a 
new example is classified by computing its Euclidean distance to a set of 
centers chosen by unsupervised methods. The application of supervised 
learning to learn a non-Euclidean distance metric was found to reduce the 
error rate of RBF networks, while supervised learning of each center's vari- 
ance resulted in inferior performance. The best improvement in accuracy 
was achieved by networks called generalized radial basis function (GRBF) 
networks. In GRBF, the center locations are determined by supervised 
learning. After training on 1000 words, RBF classifies 56.5% of letters 
correct, while GRBF scores 73.4% letters correct (on a separate test set). 
From these and other experiments, we conclude that supervised learning 
of center locations can be very important for radial basis function learning. 
I Introduction 
Radial basis function (RBF) networks are 3-layer feed-forward networks in which 
each hidden unit a computes the function 
IIx-x112 
f(x) = e- -: , 
and the output units compute a weighted sum of these hidden-unit activations: 
N 
a=l 1133 
1134 Wettschereck and Dietterich 
In other words, the value of f*(x) is determined by computing the Euclidean dis- 
tance between x and a set of N centers, xa. These distances are then passed 
through Gaussians (with variance rr 2 and zero mean), weighted by ca, and summed. 
Radial basis function networks (RBF networks) provide an attractive alternative 
to sigmoid networks for learning real-valued mappings: (a) they provide excellent 
approximations to smooth functions (Poggio & Girosi, 1989), (b) their centers are 
interpretable as prototypes, and (c) they can be learned very quickly, because the 
center locations (xa) can be determined by unsupervised learning algorithms and 
the weights (ca) can be computed by pseudo-inverse methods (Moody and Darken, 
1989). 
Although the application of unsupervised methods to learn the center locations 
does yield very efficient training, there is some evidence that the generalization 
performance of RBF networks is inferior to sigmoid networks. Moody and Darken 
(1989), for example, report that their RBF network must receive 10 times more 
training data than a standard sigmoidal network in order to attain comparable 
generalization performance on the Mackey-Glass time-series task. 
There are several plausible explanations for this performance gap. First, in sigmoid 
networks, all parameters are determined by supervised learning, whereas in RBF 
networks, typically only the learning of the output weights has been supervised. 
Second, the use of Euclidean distance to compute IIx- xall assumes that all input 
features are equally important. In many applications, this assumption is known to 
be false, so this could yield poor results. 
The purpose of this paper is twofold. First, we carefully tested the performance 
of RBF networks on the well-known NETtalk task (Sejnowski & Rosenberg, 1987) 
and compared it to the performance of a wide variety of algorithms that we have 
previously tested on this task (Dietterich, Hild, & Bakiri, 1990). The results confirm 
that there is a substantial gap between RBF generalization and other methods. 
Second, we evaluated the benefits of employing supervised learning to learn (a) 
the center locations xa, (b) weights wi for a weighted distance metric, and (c) 
2 for each center. The results show that supervised learning of the 
variances er a 
center locations and weights improves performance, while supervised learning of 
the variances or of combinations of center locations, variances, and weights did 
not. The best performance was obtained by supervised learning of only the center 
locations (and the output weights, of course). 
In the remainder of the paper we first describe our testing methodology and review 
the NETtalk domain. Then, we present results of our comparison of RBF with other 
methods. Finally, we describe the performance obtained from supervised learning 
of weights, variances, and center locations. 
2 Methodology 
All of the learning algorithms described in this paper have several parameters (such 
as the number of centers and the criterion for stopping training) that must be spec- 
ified by the user. To set these parameters in a principled fashion, we employed the 
cross-validation methodology described by Lang, Hinton & Waibel (1990). First, as 
Improving the Performance of Radial Basis Function Networks by Learning Center Locations 1135 
usual, we randomly partitioned our dataset into a training set and a test set. Then, 
we further divided the training set into a subtraining set and a cross-validation set. 
Alternative values for the user-specified parameters were then tried while training 
on the subtraining set and testing on the cross-validation set. The best-performing 
parameter values were then employed to train a network on the full training set. The 
generalization performance of the resulting network is then measured on the test 
set. Using this methodology, no information from the test set is used to determine 
any parameters during training. 
We explored the following parameters: (a) the number of hidden units (centers) 
N, (b) the method for choosing the initial locations of the centers, (c) the variance 
er 2 (when it was not subject to supervised learning), and (d) (whenever supervised 
training was involved) the stopping squared error per example. We tried N = 
50, 100, 150, 200, and 250; er 2 = 1, 2, 4, 5, 10, 20, and 50; and three different 
initialization procedures: 
(a) Use a subset of the training examples, 
(b) Use an unsupervised version of the IB2 algorithm of Aha, Kibler & Albert 
(1991), and 
(c) Apply k-means clustering, starting with the centers from (a). 
For all methods, we applied the pseudo-inverse technique of Penrose (1955) followed 
by Gaussian elimination to set the output weights. 
To perform supervised learning of center locations, feature weights, and variances, 
we applied conjugate-gradient optimization. We modified the conjugate-gradient 
implementation of backpropagation supplied by Barnard & Cole (1989). 
3 The NETtalk Domain 
We tested all networks on the NETtalk task (Sejnowski & Rosenberg, 1987), in 
which the goal is to learn to pronounce English words by studying a dictionary of 
correct pronunciations. We replicated the formulation of Sejnowski &; Rosenberg in 
which the task is to learn to map each individual letter in a word to a phoneme and 
a stress. 
Two disjoint sets of 1000 words were drawn at random from the NETtalk dictionary 
of 20,002 words (made available by Sejnowski and Rosenberg): one for training 
and one for testing. The training set was further subdivided into an 800-word 
subtraining set and a 200-word cross-validation set. 
To encode the words in the dictionary, we replicated the encoding of Sejnowski 
&; Rosenberg (1987): Each input vector encodes a 7-letter window centered on the 
letter to be pronounced. Letters beyond the ends of the word are encoded as blanks. 
Each letter is locally encoded as a 29-bit string (26 bits for each letter, i bit for 
comma, space, and period) with exactly one bit on. This gives 203 input bits, seven 
of which are i while all others are 0. 
Each phoneme and stress pair was encoded using the 26-bit distributed code devel- 
oped by Sejnowski & Rosenberg in which the bit positions correspond to distinctive 
features of the phonemes and stresses (e.g., voiced/unvoiced, stop, etc.). 
1136 Wettschereck and Dietterich 
4 RBF Performance on the NETtalk Task 
We began by testing RBF on the NETtalk task. Cross-validation training deter- 
mined that peak RBF generalization was obtained with N = 250 (the number of 
centers), rr 2 = 5 (constant for all centers), and the locations of the centers computed 
by k-means clustering. Table i shows the performance of RBF on the 1000-word 
test set in comparison with several other algorithms: nearest neighbor, the decision 
tree algorithm ID3 (Quinlan, 1986), sigmoid networks trained via backpropagation 
(160 hidden units, cross-validation training, learning rate 0.25, momentum 0.9), 
Wolpert's (1990) HERBIE algorithm (with weights set via mutual information), 
and ID3 with error-correcting output codes (ECC, Dietterich &: Bakiri, 1991). 
Table 1: Generalization performance on the NETtalk task. 
 correct (1000-word test set) 
Algorithm Word Letter Phoneme Stress 
Nearest neighbor 3.3 53.1 61.1 74.0 
RBF 3.7 57.0***** 65.6***** 80.3***** 
ID3 9.6***** 65.6***** 78.7***** 77.2***** 
Back propagation 13.6'* 70.6***** 80.8**** 81.3'**** 
Wolpert 15.0 72.2* 82.6***** 80.2 
ID3 + 127-bit ECC 20.0*** 73.7* 85.6***** 81.1 
Prior row different, p < .05* .01'* .005*** .002**** .0111'**** 
Performance is shown at several levels of aggregation. The stress column indicates 
the percentage of stress assignments correctly classified. The phoneme column 
shows the percentage of phonemes correctly assigned. A letter is correct if the 
phoneme and stress are correctly assigned, and a word is correct if all letters in 
the word are correctly classified. Also shown are the results of a two-tailed test for 
the difference of two proportions, which was conducted for each row and the row 
preceding it in the table. 
From this table, it is clear that RBF is performing substantially below virtually all 
of the algorithms except nearest neighbor. There is certainly room for supervised 
learning of RBF parameters to improve on this. 
5 Supervised Learning of Additional RBF Parameters 
In this section, we 
