Analog VLSI Implementation of 
Multi-dimensional Gradient Descent 
David B. Kirk Douglas Kerns Kurt Fleischer Alan H. Barr 
California Institute of Technology 
Beckman Institute 350-74 
Pasadena, CA 91125 
E-mail: dkegg. gg. caltech. edu 
Abstract 
We describe an analog VLSI implementation of a multi-dimensional 
gradient estimation and descent technique for minimizing an on- 
chip scalar function f(). The implementation uses noise injec- 
tion and multiplicative correlation to estimate derivatives, as in 
[Anderson, Kerns 92]. One intended application of this technique 
is setting circuit parameters on-chip automatically, rather than 
manually [Kirk 91]. Gradient descent optimization may be used 
to adjust synapse weights for a backpropagation or other on-chip 
learning implementation. The approach combines the features of 
continuous multi-dimensional gradient descent and the potential 
for an annealing style of optimization. We present data measured 
from our analog VLSI implementation. 
1 Introduction 
This work is similar to [Anderson, Kerns 92], but represents two advances. First, we 
describe the extension of the technique to multiple dimensions. Second, we demon- 
strate an implementation of the multi-dimensional technique in analog VLSI, and 
provide results measured from the chip. Unlike previous work using noise sources 
in adaptive systems, we use the noise as a means of estimating the gradient of a 
function f(__y), rather than performing an annealing process [Alspector 88]. We also 
estimate gradients continuously in position and time, in contrast to [Umminger 89] 
and [Jabri 91], which utilize discrete position gradient estimates. 
789 
790 Kirk, Kerns, Fleischer, and Barr 
It is interesting to note the existence of related algorithms, also presented in this 
volume [Cauwenberghs 93] [Alspector 93] [Flower 93]. The main difference is that 
our implementation operates in continuous time, with continuous differentiation 
and integration operators. The other approaches realize the integration and differ- 
entiation processes as discrete addition and subtraction operations, and use unit 
perturbations. [Cauwenberghs 93] provides a detailed derivation of the convergence 
and scaling properties of the discrete approach, and a simulation. [Alspector 93] 
provides a description of the use of the technique as part of a neural network hard- 
ware architecture, and provides a simulation. [Flower 93] derived a similar discrete 
algorithm from a node perturbation perspective in the context of multi-layered feed- 
forward networks. Our work is similar in spirit to [Dembo 90] in that we don't make 
any explicit assumptions about the model that is embodied in the function/(). 
The function may be implemented as a neural network. In that case, the gradient 
descent is on-chip learning of the parameters of the network. 
We have fabricated a working chip containing the continuous-time multi- 
dimensional gradient descent circuits. This paper includes chip data for individ- 
ual circuit components, as well as the entire circuit performing multi-dimensional 
gradient descent and annealing. 
2 The Gradient Estimation Technique 
y(t) f(-)  d/dt 
n(t)  d/dt 
Figure 1: Gradient estimation technique from [Anderson, Kerns 92] 
Anderson and Kerns [Anderson, Kerns 92] describe techniques for one-dimensional 
gradient estimation in analog hardware. The gradient is estimated by correlating 
(using a multiplier) the output of a scalar function f(v(t)) with a noise source 
n(t), as shown in Fig. 1. The function input y(t) is additively contaminated by 
the noise n(t) to produce v(t) = y(t) + n(t). A scale factor B is used to set the 
scale of the noise to match the function output, which improves the signal-to-noise 
ratio. The signals are high-pass filtered to approximate differentiation (shown 
as d/dt operators in Fig. l) directly before the multiplication. The results of the 
multiplication are low-pass filtered to approximate integration. 
The gradient estimate is integrated over time, to smooth out some of the noise and 
to damp the response. This smoothed estimate is compared with a zero reference, 
using an amplifier A, and the result is fed back to the input, as shown in Fig. 2. 
Th, contents of Fig. 1 are represented by the Gradient Estimation box in Fig. 2. 
We have chosen to implement the multi-dimensional technique in analog VLSI. We 
Analog VLSI Implementation of Multi-dimensional Gradient Descent 791 
n(t) I 
Gradient 
Estimation 
zero 
Figure 2: Closing the loop: performing gradient descent using the gradient estimate. 
will not reproduce here the one-dimensional analysis from [Anderson, Kerns 92], 
but summarize some of the more important results, and provide a multi-dimensional 
derivation. [Anderson 92] provides a more detailed theoretical discussion. 
3 Multi-dimensional Derivation 
The multi-dimensional gradient descent operation that we are approximating can 
be written as follows: 
yt(t) - -kVf(y(t)) (1) 
where y and yt are vectors, and the solution is obtained continuously in time t, 
rather than at discrete ti. The circuit described in the block diagram in Fig. 1 
computes an approximation to the gradient: 
vf = 0yi  Z f (t) + (t)) n(t) at () 
We approximate the operations of differentiation and integration in time by realiz- 
able high-pass and low-pass filters, respectively. To see that Eq. 2 is valid, and that 
this result is useful for approximating Eq. l, we sketch an N-dimensional extension 
of [Anderson 92]. Using the chain rule, 
 , Of () 
d f (t) + E(t)) =  (y(t) + rid(t)) Oyj 
Assuming ntj(t)  y(t), the rhs is approximated to produce 
d  Of 
d f ((t) + E(t)) =  rid(t)  (4) 
Multiplying both sides by n(t), and taking the expectation integral operator El] 
of each side, 
when i  j, nd the sum on the right hs  contribution only when i = j, 
E [n',(t)zf (t) + = (t)n,(t)j (6) 
792 Kirk, Kerns, Fleischer, and Barr 
E n;(t) f ((t) -t- n(t)  c Oyi 
The expectation operator E[] can be used to smooth random variations of the noise 
ni(t). So, we have 
of ni(t) f + (8) 
Since the descent rte k is rbitmry, we cn bsorb  into k. Using equation 8, we 
cn pproximte the gradient descent technique as follows: 
y(t)  - E [n(t) f ((t) + (t)) ] (9) 
4 Elements of the Multi-dimensional Implementation 
We have designed, fabricated, and tested a chip which allows us to test these ideas. 
The chip implementation can be decomposed into six distinct parts: 
noise source(s): an analog VLSI circuit which produces a noise function. An in- 
dependent, correlation-free noise source is needed for each input dimension, 
designated ni(t). The noise circuit is described in [Alspector 91]. 
target function: a scalar function f(Yl, Y2,'' ', YN) of N input variables, bounded 
below, which is to be minimized 
dimensional variant of the bump 
general case, this f() can be any 
by some circuit. Specifically, the 
input signal(s): the inputs yi(t) to the 
chip values, or real-world inputs. 
[Kirk 91]. The circuit in this case is a 4- 
circuit described in [Delbriick 91]. In the 
scalar function or error metric, computed 
function may be a neural network. 
function f0- These will typically be on- 
multiplier circuit(s): the multiplier computes the correlation between the noise 
values and the function output. Offsets in the multiplication appear as 
systematic errors in the gradient estimate, so it is important to compensate 
for the offsets. Linearity is not especially important, although monotonicity 
is critical. Ideally, the multiplication will also have a tanh-like character, 
limiting the output range for extreme inputs. 
integrator: an integration over time is approximated by a low-pass filter 
differentiator: the time derivatives of the noise signals and the function are ap- 
proximated by a high-pass filter. 
The N inputs, y(t), are additively contaminated with the noise signals, ni(t), by 
capacitive coupling, producing vi(t) = yi(t) + hi(t), the inputs to the function f(). 
The function output is differentiated, as are the noise functions. Each differentiated 
noise signal is correlated with the differentiated function output, using the multi- 
pliers. The results are low-pass filtered, providing N partial derivative estimates, 
for the N input dimensions, shown for 4 dimensions in Fig. 3. 
The function f0 is implemented as an 4-dimensional extension of Delbriick's 
[Do. lbriick 91] bump circuit. Details of the N-dimensionalbump circuit can be found 
in [Kirk 93]. For learning and other applications, the function f0 can implement 
some other error metric to be minimized. 
Analog VLSI Implementation of Multi-dimensional Gradient Descent 793 
nl(t) 
n2(t) 
n3(t) 
n4(t) 
yl(t) 
y2(t) 
y3(t) 
y4(t) 
( 
i vl(t) [' 
I v2(t) lf(.) I 
) v3(t) l I 
Figure 3: Block diagram for a 4-dimensionM gradient estimation circuit. 
5 Chip Results 
We have tested chips implementing the gradient estimation and gradient descent 
techniques described in this paper. Figure 4 shows the gradient estimate, without 
the closed loop descent process. Figure 5 shows the trajectories of two state variables 
during the 2D gradient descent process. Figure 6 shows the gradient descent process 
in operation on a 2D bump surface, and Fig. 7 shows how, using appropriate choice 
of noise scale, we can perform annealing using the gradient estimation hardware. 
06 
04 
m -03 
04 
o 
03 
04 
Tme 
Figure 4: Measured Chip Data: 1D Gradient Estimate. Upper curves are 1D bump 
output as the input y(t) is a slow triangle wave. Lower curves are gradient estimates. 
(left) raw data, and (right) average of 1024 runs. 
794 Kirk, Kerns, Fleischer, and Barr 
0 03 0.04 
Tmle (secoa:b) 
-3 I 
-32 
002 003 gM 0,0 
Tare (secoad) 
Figure 5: Measured Chip Data: 2D Gradient Descent. The curves above show the 
function optimization by gradient descent for 2 variables. Each curve represents 
the path of one of the 
