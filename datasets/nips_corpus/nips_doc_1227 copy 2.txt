Learning with Noise and Regularizers in 
Multilayer Neural Networks 
David Saad 
Dept. of Comp. Sci. & App. Math. 
Aston University 
Birmingham B4 7ET, UK 
D.Saad@aston.ac.uk 
Sara A. Solla 
AT&T Research Labs 
Holmdel, NJ 07733, USA 
solla@research.att.com 
Abstract 
We study the effect of noise and regularization in an on-line 
gradient-descent learning scenario for a general two-layer student 
network with an arbitrary number of hidden units. Training ex- 
amples are randomly drawn input vectors labeled by a two-layer 
teacher network with an arbitrary number of hidden units; the ex- 
amples are corrupted by Gaussian noise affecting either the output 
or the model itself. We examine the effect of both types of noise 
and that of weight-decay regularization on the dynamical evolu- 
tion of the order parameters and the generalization error in various 
phases of the learning process. 
I Introduction 
One of the most powerful and commonly used methods for training large layered 
neural networks is that of on-line learning, whereby the internal network parameters 
{J} are modified after the presentation of each training example so as to minimize 
the corresponding error. The goal is to bring the map fj implemented by the 
network as close as possible to a desired map ] that generates the examples. Here 
we focus on the learning of continuous maps via gradient descent on a differentiable 
error function. 
Recent work [1]-[4] has provided a powerful tool for the analysis of gradient-descent 
learning in a very general learning scenario [5]: that of a student network with N 
input units, K hidden units, and a single linear output unit, trained to implement a 
continuous map ?ore an N-dimensional input space  onto a scalar (. Examples of 
the target task f are in the form of input-output pairs (, (). The output labels 
( to independently drawn inputs  are provided by a teacher network of similar 
Learning with Noise and Regularizers in Multilayer Neural Networks 261 
architecture, except that its number M of hidden units is not necessarily equal to 
K. 
Here we consider the possibility of a noise process p that corrupts the teacher 
output. Learning from corrupt examples is a realistic and frequently encountered 
scenario. Previous analysis of this case have been based on various approaches: 
Bayesian [6], equilibrium statistical physics [7], and nonequilibrium techniques for 
analyzing learning dynamics [8]. Here we adapt our previously formulated tech- 
niques [2] to investigate the effect of different noise mechanisms on the dynamical 
evolution of the learning process and the resulting generalization ability. 
2 The model 
We focus on a soft committee machine [1], for which all hidden-to-output weights 
are positive and of unit strength. Consider the student network: hidden unit i 
receives information from input unit r through the weight Jir, and its activation 
under presentation of an input pattern  - (El,... ,Es) is xi = Ji � , with Ji = 
(Jil, � �., JiN) defined as the vector of incoming weights onto the i-th hidden unit. 
The output of the student network is cr(J,) = E/K=i g (Ji. ), where g is the 
activation function of the hidden units, taken here to be the error function g(x) -- 
erf(x/V), and J -- {Ji}l<i<K is the set of input-to-hidden adaptive weights. 
The components of the input vectors  are uncorrelated random variables with zero 
mean and unit variance. Output labels ( are provided by a teacher network of sim- 
ilar architecture: hidden unit n in the teacher network receives input information 
through the weight vector B = (Bnl,..., BnN), and its activation under presenta- 
tion of the input pattern  is y = B � . In the noiseless case the teacher output 
is given by (0  M (B ). Here we concentrate on the architecturally 
-- En:l g ' 
matched case M = K, and consider two types of Gaussian noise: additive output 
noise that results in ( = p + -K= g (n. ), and model noise introduced as 
fluctuations in the activations y of the hidden units, ( K 
= -=1 g (P + n .). 
The random variables p and pn  are taken to be Gaussian with zero mean and 
variance a 2 . 
The error made by a student with weights J on a given input  is given by the 
quadratic deviation 
e(J,) --  [ G(J,) - (0 = g(xi)--g(yn) , 
i--1 n--1 
(1) 
measured with respect to the noiseless teacher (it is also possible to measure 
performance as deviations with respect to the actual output ( provided by the 
noisy teacher). Performance on a typical input defines the generalization error 
eg(J)  < e(J,) >{], through an average over all possible input vectors  to 
be performed implicitly through averages over the activations x = (Xl,. �., XK) and 
Y = (Yl,..., YK). These averages can be performed analytically [2] and result in a 
compact expression for eg in terms of order parameters: Qik ---- Ji 'Jk, Rin = Ji 'Sn, 
and Tm -- B. Bin, which represent student-student, student-teacher, and teacher- 
teacher overlaps, respectively. The parameters Tm are characteristic of the task to 
be learned and remain fixed during training, while the overlaps Qik among student 
hidden units and Rin between a student and a teacher hidden units are determined 
by the student weights J and evolve during training. 
A gradient descent rule on the error made with respect to the actual output provided 
262 D. Saad and S. A. Solla 
by the noisy teacher results in j,+x = j + N 5'  for the update of the student 
weights, where the learning rate /has been scaled with the input size N, and 
depends on the type of noise. The time evolution of the overlaps Ri, and Qi can 
be written in terms of similar difference equations. We consider the large N limit, 
and introduce a normalized number of examples a = It/N to be interpreted as a 
continuous time variable in the N   limit. The time evolution of Ri, and Qi 
is thus described in terms of first-order differential equations. 
3 Output noise 
The resulting equations of motion for the student-teacher and student-student over- 
laps are given in this case by: 
dRi,, 
= < >, (2) 
da 
dQit: 
= r I < 5 x > +r I < 5 x > +rl 2 < 5 5 > +riCo ' < g'(xi) g'(x) >, 
da 
where each term is to be averaged over all possible ways in which an example 
could be chosen at a given time step. These averages have been performed using 
the techniques developed for the investigation of the noiseless case [2]; the only 
difference due to the presence of additive output noise is the need to evaluate the 
fourth term in the equation of motion for Qi, proportional to both r/ and a . 
We focus on isotropic uncorrelated teacher vectors: T,rn = T 6,m, and choose T = 1 
in our numerical examples. The time evolution of the overlaps lin and Qik follows 
from integrating the equations of motion (2) from initial conditions determined by 
a random initialization of the student vectors {Ji}l<i<K. Random initial norms 
Qii for the student vectors are taken here from a uniform distribution in the [0, 0.5] 
interval. Overlaps Qik between independently chosen student vectors Ji and J, 
or tin between Ji and an unknown teacher vector B, are small numbers of order 
1/x/- for N >> K, and taken here from a uniform distribution in the [0, 10 -x2] 
interval. 
We show in Figures 1.a and 1.b the evolution of the overlaps for a noise variance 
a 2 = 0.3 and learning rate r/= 0.2. The example corresponds to M = K = 3. The 
qualitative behavior is similar to the one observed for M = K in the noiseless case 
extensively analyzed in [2]. A very short transient is followed by a long plateau 
characterized by lack of differentiation among student vectors: all student vectors 
have the same norm Qii -- Q, the overlap between any two different student vectors 
takes a unique value Qik -- C for i - k, and the overlap lin between an arbitrary 
student vector i and a teacher vector n is independent of i (as student vectors are 
indistinguishable in this regime) and of n (as the teacher is isotropic), resulting 
in Iin - R. This phase is characterized by an unstable symmetric solution; the 
perturbation introduced through the nonsymmetric initialization of the norms 
and overlaps lin eventually takes over in a transition that signals the onset of 
specialization. 
This process is driven by a breaking of the uniform symmetry of the matrix of 
student-teacher overlaps: each student vector acquires an increasingly dominant 
overlap R with a specific teacher vector which it begins to imitate, and a gradually 
decreasing secondary overlap $ with the remaining teacher vectors. In the example 
of Figure 1.b the assignment corresponds to i = 1 - n = 1, i = 2 - n = 3, and 
i = 3 - n = 2. A relabeling of the student hidden units allows us to identify R 
with the diagonal elements and $ with the off-diagonal elements of the matrix of 
student-teacher overlaps. 
Learning with Noise and Regularizers in Multilayer Neural Networks 263 
(a) 
1.0- I ..... 
0.5- 
0.0- 
0 
Q,, -- Q,2 ..... Q,, 
..... Q ........... Q2, ...... 
I I I 
1.2 
1.0 t 
0.8 
(b) 
0.6- 
0.4- 
0.2- 
0.0- 
0 
I I I 
2000 4000 6000 
0.03- 
0.025 - 
0.02 - 
0.015- 
0.01 
0.005 
0.0 
(c) 
....... a. 0, \\\ 
O'.0 3 
I I t [ 1 
0 500 1000 1500 2000 2500 3000 
ot 
(d) 
........ Zo 1 
......... Z 
0.03 - 
0.025 - 
0.02- 
0.015 
0.01 
0.005 
4'10 3 6'10 3 8'103 
Figure 1: Dependence of the overlaps and the generalization error on the normal- 
ized number of examples ct for a three-node student learning corrupted examples 
generated by an isotropic three-node teacher. (a) student-student overlaps Qi and 
(b) student-teacher overlaps tin for tr 2 = 0.3. The generalization error is shown in 
(c) for different values of the noise variance r 2, and in (d) for different powers of 
the polynomial learning rate decay, focusing on ct > ct0 (asymptotic regime). 
Asymptotically the secondary overlaps $ decay to zero, while Ri, --  indicates 
full alignment for
