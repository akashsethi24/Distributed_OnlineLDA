Correctness of belief propagation in Gaussian 
graphical models of arbitrary topology 
Yair Weiss 
Computer Science Division 
UC Berkeley, 485 Soda Hall 
Berkeley, CA 94720-1776 
Phone: 510-642-5029 
yweiss @ cs. berkeley. ed u 
William T. Freeman 
Mitsubishi Electric Research Lab 
201 Broadway 
Cambridge, MA 02139 
Phone: 617-621-7527 
freeman @ merl.com 
Abstract 
Local belief propagation rules of the sort proposed by Pearl [15] are 
guaranteed to converge to the correct posterior probabilities in singly 
connected graphical models. Recently, a number of researchers have em- 
pirically demonstrated good performance of 1oopy belief propagation- 
using these same rules on graphs with loops. Perhaps the most dramatic 
instance is the near Shannon-limit performance of Turbo codes, whose 
decoding algorithm is equivalent to loopy belief propagation. 
Except for the case of graphs with a single loop, there has been little theo- 
retical understanding of the performance of 1oopy propagation. Here we 
analyze belief propagation in networks with arbitrary topologies when 
the nodes in the graph describe jointly Gaussian random variables. We 
give an analytical formula relating the true posterior probabilities with 
those calculated using loopy propagation. We give sufficient conditions 
for convergence and show that when belief propagation converges it gives 
the correct posterior means for all graph topologies, not just networks 
with a single loop. 
The related max-product belief propagation algorithm finds the max- 
imum posterior probability estimate for singly connected networks. We 
show that, even for non-Gaussian probability distributions, the conver- 
gence points of the max-product algorithm in loopy networks are max- 
ima over a particular large local neighborhood of the posterior proba- 
bility. These results help clarify the empirical performance results and 
motivate using the powerful belief propagation algorithm in a broader 
class of networks. 
Problems involving probabilistic belief propagation arise in a wide variety of applications, 
including error correcting codes, speech recognition and medical diagnosis. If the graph 
is singly connected, there exist local message-passing schemes to calculate the posterior 
probability of an unobserved variable given the observed variables. Pearl [15] derived such 
a scheme for singly connected Bayesian networks and showed that this belief propagation 
algorithm is guaranteed to converge to the correct posterior probabilities (or beliefs). 
Several groups have recently reported excellent experimental results by running algorithms 
674 Y. Weiss and W T. Freeman 
equivalent to Pearl's algorithm on networks with loops [8, 13, 6]. Perhaps the most dramatic 
instance of this performance is for Turbo code [2] error correcting codes. These codes 
have been described as the most exciting and potentially important development in coding 
theory in many years [12] and have recently been shown [10, 11] to utilize an algorithm 
equivalent to belief propagation in a network with loops. 
Progress in the analysis of loopy belief propagation has been made for the case of networks 
with a single loop [17, 18, 4, 1]. For these networks, it can be shown that (1) unless 
all the compatabilities are deterministic, 1oopy belief propagation will converge. (2) The 
difference between the loopy beliefs and the true beliefs is related to the convergence rate 
of the messages -- the faster the convergence the more exact the approximation and (3) If 
the hidden nodes are binary, then the loopy beliefs and the true beliefs are both maximized 
by the same assignments, although the confidence in that assignment is wrong for the loopy 
beliefs. 
In this paper we analyze belief propagation in graphs of arbitrary topology, for nodes de- 
scribing jointly Gaussian random variables. We give an exact formula relating the correct 
marginal posterior probabilities with the ones calculated using 1oopy belief propagation. 
We show that if belief propagation converges, then it will give the correct posterior means 
for all graph topologies, not just networks with a single loop. We show that the covari- 
ance estimates will generally be incorrect but present a relationship between the error in 
the covariance estimates and the convergence speed. For Gaussian or non-Gaussian vari- 
ables, we show that the max-product algorithm, which calculates the MAP estimate in 
singly connected networks, only converges to points that are maxima over a particular large 
neighborhood of the posterior probability of loopy networks. 
1 Analysis 
To simplify the notation, we assume the graphical model has been preprocessed into an 
undirected graphical model with pairwise potentials. Any graphical model can be con- 
verted into this form, and running belief propagation on the pairwise graph is equivalent 
to running belief propagation on the original graph [ 18]. We assume each node zi has a 
local observation .i. In each iteration of belief propagation, each node zi sends a message 
to each neighboring zj that is based on the messages it received from the other neighbors, 
its local observation y and the pairwise potentials ij (xi, x j) and ï¿½ ii (xi, yi). We assume 
the message-passing occurs in parallel. 
The idea behind the analysis is to build an unwrapped tree. The unwrapped tree is the 
graphical model which belief propagation is solving exactly when one applies the belief 
propagation rules in a loopy network [9, 20, 18]. It is constructed by maintaining the same 
local neighborhood structure as the 1oopy network but nodes are replicated so there are no 
loops. The potentials and the observations are replicated from the loopy graph. Figure 1 (a) 
shows an unwrapped tree for the diamond shaped graph in (b). By construction, the belief 
at the root node a is identical to that at node zx in the loopy graph after four iterations of 
belief propagation. Each node has a shaded observed node attached to it, omitted here for 
clarity. 
Because the original network represents jointly Gaussian variables, so will the unwrapped 
tree. Since it is a tree, belief propagation is guaranteed to give the correct answer for the 
unwrapped graph. We can thus use Gaussian marginalization formulae to calculate the 
true mean and variances in both the original and the unwrapped networks. In this way, we 
calculate the accuracy of belief propagation for Gaussian networks of arbitrary topology. 
We assume that the joint mean is zero (the means can be added-in later). The joint distri- 
Correctness of Belief Propagation 675 
xl 
x2 
x5 
Figure 1: Left: A Markov network with multiple loops. Right: The unwrapped network 
corresponding to this structure. 
bution of z = is given by P(z) = ae- where V = . It 
y ' vvv 
is straightforward to construct the inverse covariance matrix V of the joint Gaussian that 
describes a given Gaussian graphical model [3]. 
Writing out the exponent of the joint and completing the square shows that the mean t of 
x, given the observations y, is given by: 
= - Vvy , (1) 
and the covariance matrix C'i v of x given y is: C=i v: g x. We will denote by C,iy the 
ith row of Clv so the marginal posterior variance of zi given the data is a 2 (i) = C, Iv (i). 
We will use ~ for unwrapped quantities. We scan the tree in breadth first order and denote by 
. the vector of values in the hidden nodes of the tree when so scanned. Simlarly, we denote 
by 0 the observed nodes scanned in the same order and P, Pv the inverse covariance 
matrices. Since we are scanning in breadth first order the last nodes are the leaf nodes and 
we denote by L the number of leaf nodes. By the nature of unwrapping, (1) is the mean 
of the belief at node zx after t iterations of belief propagation, where t is the number of 
unwrappings. Similarly 2(1) -- 'xlv(1) is the variance of the belief at node x after t 
iterations. 
Because the data is replicated we can write 0 = Oy where O(i, j) = 1 if 0i is a replica ofyj 
and 0 otherwise. Since the potentials I'(zi, Yi) are replicated, we can write PvO = OVer. 
Since the I, (zi, z j) are also replicated and all non-leaf :i have the same connectivity as 
the corresponding zi, we can write lzO = OVzz + E where E is zero in all but the last 
L rows. When these relationships between the loopy and unwrapped inverse covariance 
matrices are substituted into the loopy and unwrapped versions of equation 1, one obtains 
the following expression, true for any iteration [ 19]: 
(1) =/(1) + zlve 
(2) 
where e is a vector that is zero everywhere but the last L components (corresponding to the 
leaf nodes). Our choice of the node for the root of the tree is arbitrary, so this applies to 
all nodes of the 1oopy network. This formula relates, for any node of a network with loops, 
the means calculated at each iteration by belief propagation with the true posterior means. 
Similarly when the relationship between the loopy and unwrapped inverse covariance ma- 
trices is substituted into the loopy and unwrapped definitions of Ci v we can relate the 
676 Y. Weiss and W. T. Freeman 
0.5 
0.4 
0.3 
0.2 
0.1 
0 
--0.1 
--0.2 
0 40 60 80 
node 
oo 
Figure 2: The conditional correlation between the root node and all other nodes in the 
unwrapped tree of Fig. I after eight iterations. Potentials were chosen randomly. Nodes 
are presented in breadth first order so the last elements are the correlations between the root 
node and the leaf nodes. We show that if this correlation goes to zero, belief propagation 
converges and the loopy means are exact. Symbols plotted with a star denote correlations 
with nodes that correspond to the node :r in the loopy graph. The sum of these correlations 
gives the correct variance of node :ca while loopy propagation uses only the first correlation. 
marginalized covariances calculated by belief propagation to the true ones [ 19]: 
2(1) = a2(1) + 
