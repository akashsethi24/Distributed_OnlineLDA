Dynamics of On-Line Gradient Descent 
Learning for Multilayer Neural Networks 
David Saad* 
Dept. of Comp. Sci. & App. Math. 
Aston University 
Birmingham B4 7ET, UK 
Sara A. Solla t 
CONNECT, The Niels Bohr Institute 
Blegdamsdvej 17 
Copenhagen 2100, Denmark 
Abstract 
We consider the problem of on-line gradient descent learning for 
general two-layer neural networks. An analytic solution is pre- 
sented and used to investigate the role of the learning rate in con- 
trolling the evolution and convergence of the learning process. 
Learning in layered neural networks refers to the modification of internal parameters 
{J} which specify the strength of the interneuron couplings, so as to bring the map 
fj implemented by the network as close as possible to a desired map f. The 
degree of success is monitored_through the generalization error, a measure of the 
dissimilarity between fj and f. 
Consider maps from an N-dimensional input space  onto a scalar (, as arise in 
the formulation of classification and regression tasks. Two-layer networks with an 
arbitrary number of hidden units have been shown to be universal approximators 
[1] for such N-to-one dimensional maps. Information about the desired map f is 
provided through independent examples (,(), with ( = f() for all/. The 
examples are used to train a student network with N input units, K hidden units, 
and a single linear output unit; the target map f is defined through a teacher 
network of similar architecture except for the number M of hidden units. We 
investigate the emergence of generalization ability in an on-line learning scenario 
[2], in which the couplings are modified after the presentation of each example so 
as to minimize the corresponding error. The resulting changes in {J} are described 
as a dynamical evolution; the number of examples plays the role of time. 
In this paper we limit our discussion to the case of the soft-committee machine 
[2], in which all the hidden units are connected to the output unit with positive 
couplings of unit strength, and only the input-to-hidden couplings are adaptive. 
* D.Saad@aston.ac.uk 
/On leave from AT&T Bell Laboratories, Holmdel, NJ 07733, USA 
Dynamics of On-line Gradient Descent Learning for Multilayer Neural Networks 303 
Consider the student network: hidden unit i receives information from input unit 
r through the weight Jir, and its activation under presentation of an input pattern 
 = ((,...,(N) is zi = Ji' , with Ji: (Jii,..., Jilv) defined as the vector of 
incoming weights onto the i-th hidden unit. The output of the student network is 
cr(J,) = -/K=l g (ji. ), where g is the activation function of the hidden units, 
taken here to be the error function g(z) -- erf(z/v), and J _= {Ji}i<i<K is the set 
of input-to-hidden adaptive weights. 
Training examples are of the form (, (). The components of the independently 
drawn input vectors  are uncorrelated random variables with zero mean and 
unit variance. The corresponding output ( is given by a deterministic teacher 
whose internal structure is the same as for the student network but may differ in 
the number of hidden units. Hidden unit n in the teacher network receives input 
information through the weight vector B, = (B, 1,..., B,y), and its activation 
under presentation of the input pattern  is y = B, � . The corresponding 
output is ( = -,M__l  (B,. ). We will use indices i,j,k,l... to refer to units 
in the student network, and n, m,... for units in the teacher network. 
The error made by a student with weights J on a given input  is given by the 
quadratic deviation 
1 ]2 1 [ K M] 2 
e(a,)_-- [cr(a,)-( =  yg(xi)-yg(y,) (1) 
i=1 n=l 
Performance on a typical input defines the generalization error ca(J ) -- 
< e(J,) >{e) through an average over all possible input vectors , to be per- 
formed implicitly through averages over the activations x = (x,..., x:) and 
Y = (Y,..., YM). Note that both < xi >=< y >= 0; second order correlations are 
given by the overlaps among the weight vectors associated with the various hidden 
units: < xi xk > = Ji � Jk ---- Qik, < xi Yn > = Ji ' Bn ---- lrin, and < y, y, > = 
B � B. -- T,. Averages over x and y are performed using the resulting multivari- 
ate Gaussian probability distribution, and yield an expression for the generalization 
error in terms of the parameters Qi, Rir, and Tm [3]. For g(x) -_- erf(x/v) the 
result is: 
eg(J) = 1 {/ arcsin Qi Trm 
7 v/1 q- Qii x/1 + Q +  arcsin 41 + T x/1 + 
nm 
-2 y arcsin v/1 + Qii x/1 + T,, ' (2) 
in 
The parameters T, are characteristic of the task to be learned and remain fixed. 
The overlaps Qit: and ]in, which characterize the correlations among the various 
student units and their degree of specialization towards the implementation of the 
desired task, are determined by the student weights J and evolve during training. 
A gradient descent rule for the update of the student weights results in J'+ = 
J' + � ' , where the learning rate r/has been scaled with the input size N, and 
r--1 
is defined in terms of both the activation function g and its derivative gr. The time 
evolution of the overlaps ]in and Qit: can be explicitly written in terms of similar 
304 D. SAAD, S. A. SOLLA 
difference equations. In the large N limit the normalized number of examples 
 =/tin can be interpreted as a continuous time variable, leading to the equations 
of motion 
dRin 
= r/<Si y >{} , 
da 
dQi 
= r/< 5i xk >{} +r/< 5k xi >{} +r/2 < 5 5 >{} , (4) 
da 
to be averaged over all possible ways in which an example can be chosen at a given 
time step. The dependence on the current input  is only through the activations 
x and y; the corresponding averages can be performed analytically for g(x) = 
erf(x/v), resulting in a set of coupled first-order differential equations [3]. These 
dynamical equations are exact, and provide a novel tool used here to analyze the 
learning process for a general soft-committee machine with an arbitrary number K 
of hidden units, trained to implement a task defined through a teacher of similar 
architecture except for the number M of hidden units. In what follows we focus on 
uncorrelated teacher vectors of unit length, T, = 
The time evolution of the overlaps tiin and Qit follows from integrating the equa- 
tions of motion (4) from initial conditions determined by a random initialization of 
the student vectors {Ji}l<i<K. Random initial norms Qii for the student vectors 
are taken here from a uniform distribution in the [0, 0.5] interval. Overlaps Qit 
between independently chosen student vectors Ji and J, or tiin between Ji and 
an unknown teacher vector B are small numbers, of order 1/v/-ff for N >> K, M, 
and taken here from a uniform distribution in the [0, 10 -2] interval. 
We show in Fig. la-c the evolution of the overlaps and generalization error for a 
realizable case: K = M = 3 and r/ = 0.1. This example illustrates the succes- 
sive regimes of the learning process. The system quickly evolves into a symmetric 
subspace controlled by an unstable suboptimal solution which exhibits no differenti- 
ation among the various student hidden units. Trapping in the symmetric subspace 
prevents the specialization needed to achieve the optimal solution, and the gener- 
alization error remains finite, as shown by the plateau in Fig. lc. The symmetric 
solution is unstable, and the perturbation introduced through the random initializa- 
tion of the overlaps tiin eventually takes over: the student units become specialized 
and the matrix R of student-teacher overlaps tends towards the matrix T, except 
for a permutational symmetry associated with the arbitrary labeling of the student 
hidden units. The generalization error plateau is followed by a monotonic decrease 
towards zero once the specialization begins and the system evolves towards the 
optimal solution. The evolution of the overlaps and generalization error for the un- 
realizable case K < M is characterized by qualitatively similar stages, except that 
the asymptotic behavior is controlled by a suboptimal solution which reflects the 
differences between student and teacher architectures. 
Curves for the time evolution of the generalization error for different values of 
shown in Fig. ld for K = M -- 3 identify trapping in the symmetric subspace 
as a small r/ phenomenon. We therefore consider the equations of motion (4) in 
the small r/ regime. The term proportional to r/2 is neglected and the resulting 
truncated equations of motion are used to investigate a phase characterized by 
students of similar norms: Qii - Q for all 1 _< i _< K, similar correlations among 
themselves: Qik -- C for all i - k, and similar correlations with the teacher vectors: 
Rin - / for all 1 < i < K, 1 _< n _< M. The resulting dynamical equations exhibit 
a fixed point solution at 
Q* = C* M M - K 2 + x/K 4 - K 
= K 2 2M-1 and R*= (5) 
Dynamics of On-line Gradient Descent Learning for Multilayer Neural Networks 305 
(a) 
0.8-- 
- 0.6- 
0.4- 
o.o/ 
I I 
0 2000 4000 
6000 8000 
(b) 
0.8' 
0.6- 
0.4- 
0.2- 
0.0 
0 
I I 
2000 4000 6000 
8000 
(c) 
(d) 
0.08- 
0.06- 
0.04- 
0.02- 
0.0 
0 
I I I 
2000 4000 6000 8000 
0.08 ........... ?'/o.  
0 2000 4000 6000 
0.06- 
Figure 1: Dependence of the overlaps and the generalization error on the normal- 
ized number of examples a for a three-node student learning a three-node teacher 
characterized by Trm -- 8.. Results for r/= 0.1 are shown for (a) student-student 
overlaps Qik and (b) student-teacher overlaps tin. The generalization error is shown 
in (c), and again in (d) for different values of the learning rate. 
for the general case, which reduces to 
Q* = C* = 1 and R* = QV/' 1 
1 = x/K(5)c- 1) (6) 
in the realizable case (K = M), where the corresponding generalization error is 
given by 
ca- r -Karcsin (7) 
A simple geometrical picture explains the relation Q* = C'* = M(
