9 
Stochastic Learning Networks and their Electronic Implementation 
Joshua Alspector*, Robert B. Allen, Victor Hut, and Srinagesh Satyanarayana 
Bell Communications Research, Morristown, NJ 07960 
ABSTRACT
We describe a family of learning algorithms that operate on a recurrent, symmetrically 
connected, neuromorphic network that, like the Boltzmann machine, settles in the 
presence of noise. These networks learn by modifying synaptic connection strengths on 
the basis of correlations seen locally by each synapse. We describe a version of the 
supervised learning algorilhm for a network with analog activation functions. We also 
demonstrate unsupervised competitive learning with this approach, where weight 
saturation and decay play an important role, and describe preliminary experiments in 
reinforcement !earning, where noise is used in the search procedure. We identify the 
above described phenomena as elements that can unify learning techniques at a physical 
microscopic level. 
These algorilhms were chosen for ease of implementation in vlsi. We have designed a 
CMOS test chip in 2 micron rules that can speed up the learning about a millionfold 
over an equivalent simulation on a VAX 11/780. The speedup is due to parallel analog 
computation for summing and multiplying weights and activations, and the use of 
physical processes for generating random noise. The components of the test chip are a 
noise amplifier, a neuron amplifier, and a 300 transistor adaptive synapse, each of which 
is separately testable. These components are also integrated into a 6 neuron and 15 
synapse network. Finally, we point out techniques for reducing the area of the 
electronic correlational synapse both in technology and design and show how the 
algorithm. we study can be implemented naturally in electronic systems. 
1. INTRODUCTION 
Vnere has been significant progress, in recent years, in modeling brain function as the collective 
havior of highly interconnected networks of simple model neurons. This paper focuses on the 
issue of !earning in these networks especially with regard to their implementation in an electronic 
system. Learning phenomena that have been studied include associative memory Ill, supervised 
learning by error correction [21 and by stochastic search 131, competitive learning 141 D! reinforcement 
!earning 16l, and other forms of nnsupervised !earning [71. From the point of view of neural 
plausibility as well as electronic implementation, we particularly like !earning algorithms that 
change synaptic connection strengths asynchronously and are based only on information 
available locally at the synapse. This is illustrated in Fig. 1, where a model synapse uses only the 
correlations of the neurons it connects and perhaps some weak global evaluation signal not 
specific to individual neurons to decide how to adjust its conductance. 
* Address for correspondence: I. Alspector, Bell Communications Research, 2E-378, 435 South St., Morristown, NS 
07960 / (201) 829-4342 / josh@bellcore.corn 
t Permanent address: University of Califomia, Berkeley, EE Department, Co T Hall, Berkeley, CA 94720 
f Permanent address: Columbia University, EE Department, S.W. Mudd Bldg., New York, blY 10027 
@ American Institute of Physics 1988 
10 
s 
S o 
<r> 
Hebb-type learning rule: 
I If Cij increases, 
glob. al scalar (perhaps in the presence of 
evaluation increment w ij 
signal 
r) 
Fig. 1. A local correlational synapse. 
We believe that a stochastic search procedure is most compatible with this viewpoint. Statistical 
procedures based on noise form the communication pathways by which global optimization can 
take place based only on the interaction of neurons. Search is a necessary part of any learning 
procedure as the network attempts to find a connection strength matrix that solves a particular 
problem. Some learning procedures attack the search directly by gradient following through error 
co.,rectionlSl 191 but electronic implementation requires specifying which neurons are input, 
htdden and output in advance and necessitates global control of the error correction 121 procedure 
m a way that requires specific connectivity and synchrony at the neural level. There is also the 
question of how such procedures would work with unsupervised methods and whether they might 
get stuck in local minima. Stochastic processes can also do gradient following but they are better 
at avoiding minima, are compatible with asynchronous updates and local weight adjustments, 
and, as we show in this paper, can generalize well to less supervised learning. 
The phenomena we studied are 1) analog activation, 2) noise, 3) semi-local Hebbian synaptic 
modification, and 4) weight decay and saturation. These techniques were applied to problems in 
supervised, unsupervised, and reinforcement learning. The goal of the study was to see if these 
diverse learning styles can be unified at the microscopic level with a small set of physically 
plausible and electronically implementable phenomena. The hope is to point the way for 
powerful electronic learning systems in the future by elucidating the conditions and the types of 
circuits that may be necessary. It may also be U'ue that the conditions for electronic learning may 
11 
have some bearing on the general principles of biological learning. 
$. LOCAL LEARNING AND STOCHASTIC SEARCH 
$.1 Supervised Learning in Recurrent Networks with Analog Activations 
We have previously shown !!�] how the supervised !earning procedure of the Boltzmann 
machine 131 can be implemented in an electronic system. This system works on a recurrent, 
symmelrically connected network which can be characterized as settling to a minimum in its 
Liapunov function [0il II. While this architecture may stretch our criterion of neural plausibility, it 
does provide for stability and analyzability. The feedback connectivity provides a way for a 
supervised !earning procedure to propagate information back through the network as the 
stochastic search proceeds. More plausible would be a randomly connected network where 
symmetry is a statistical approximation and inhibition damps oscillations, but symmetry is more 
efficient and well matched to our choice of learning rule and search procedure. 
We have extended our electronic model of the Boltzmann machine to include analog activations. 
Fig. 2 shows the model of the neuron we used and its tanh or sigmoid transfer function. The net 
input consists of the usual weighted sum of activations from other neurons but, in the case of 
Boltzmann machine learning, these are added to a noise signal chosen from a variety of 
distributions so that the neuron performs the physical computation: 
activation =f (netl )=f (wlj sj +noise )=tanh(gain*netl ) 
Instead of counting the number of on-on and off-off cooccurrences of neurons which a synapse 
connects, the correlation rule now defines the value of a cooccurrence as: 
Clj=f i * f  
where fi is the activation of neuron i which is a real value from -1 to 1. Note that this rule 
effectively counts both on-on and off-off cooccurrences in the high gain limit. In this limit, for 
Gaussian noise, the cumulative probability distribution for the neuron to have activation +1 (on) 
is close to sigmoidal. The effect of noise jitter is illustrated at the bottom of the figure. The 
weight change rule is still: 
if Cq + > Cq- then increment wij .... ele decrement 
where the plus phase clamps the output neurons in their desired states while the minus phase 
allows them to run free. 
A' mentioned, we have studied a variety of noise distributions other than those based on the 
Boltzmann distribution. The 2-2-1 XOR problem was selected as a test case since it has been 
shown !�! to be easily caught in local minima. The gain was manipulated in conditions with no 
noise or with noise sampled from one of three distributions. The Gaussian distribution is closest 
to true electronic thermal noise such as used in our implementation, but we also considered a 
cut-off uniform distribution and a Cauchy distribution with long noise tails for comparison. The 
inset to Fig. 3 shows a histogram of samples from the noise distributions used. The noise was 
multiplied by the temperature to 'jitter' the transfer function. Hence, the jitter decreased as the 
mealing schedule proceeded. 
12 
z Vnols e 
f/(: w,,,, + nois 
A Vl n . A rout y transfer function 
- 1.0 
f.(' 
I . 
high gain 
tranlfar function 
wth nola 'Jlttsr' 
 Wij Sj 
J 
Fig. 2. Electronic analog neuron. 
z Vin + z Vnois e 
or z w us +no/se= 
Fig. 3 shows average performance across 100 runs for the last 100 patterns of 2000 training 
pattern presentations. It can be seen that reducing the gain from a sharp step can improve 
learning in a small region of gain, even without noise. There seems to be an optimal gain level. 
However, the addition of noise for any distribution can substantially improve learning at all levels 
Gaussian 
Uniform 
Cauch� 
No Noise 
0.9 
o 
o 
0.8 
0.7 
0.64 
0.5 
-3 
10 
10 10 
Inverse Gain 
Fig. 3. Propordon correct vs. inverse gain. 
1 
lO 
13 
7..2 Stochastic Competitive Le-ning 
We have am how comfitive Ig 14111 c  accplish wi sthastic ! ts. 
ter e preenration of  Mput pattern, e network is led d e weight  Mcrsed 
twn e winning cluster t d e Mput  wMch e on. As shown M Fig. 4 is 
pmach wm nppli to e !e problem of Rumelh d Zipset. A 4x4 pixel may Mput 
yer co to n 2 t comprifive layer wi rent inhibito coections at e not 
ndjmt. e inhibito cofions pride e comfition by mns of a winner-e-all 
s  e netwo settles. e put pae  !  oy two put ts e ed 
 at h pattern prenmti d ey must  physic!y adjacent, eider vertically  
mHy.  s way, e network I aut e coteess of e space d eventually 
vid it Mto two eq! spatial regions wi ch of e cluster ts rndMg only to !es 
m one of e hv. Rmet 
