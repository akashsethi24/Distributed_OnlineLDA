632 
STATIC AND DYNAMIC ERROR PROPAGATION 
NETWORKS WITH APPLICATION TO SPEECH 
CODING 
A J Robinson, F Fallside 
Cambridge University Engineering Department 
Trumpington Street,, Cambridge, England 
Abstract 
Error propagation nets have been shown to be able to learn a variety of tasks in 
which a static input pattern is mapped onto a static output pattern. This paper 
presents a generalisation of these nets to deal with time varying, or dynanic 
patterns, and three possible architectures are explored. As an example, dynanic 
nets are applied to the problem of speech coding, in which a time sequence of 
speech data are coded by one net and decoded by another. The use of dynamic 
nets gives a better signal to noise ratio than that achieved using static nets. 
1. INTRODUCTION 
This paper is based upon the use of the error propagation algorithn of Runnelhart, Hinton 
and Williams  to train a connectionist net. The net is defined as a set of units, each with an 
activation, and weights between units which determine the activations. The algorithm uses a 
gradient descent technique to calculate the direction by which each weight should be changed 
in order to miniraise the summed squared difference between the desired outpnt and the actual 
output. Using this algorithm it is believed that a net can be trained to make an arbitrary 
non-linear mapping of the input units onto the output units if given enough intermediate 
units. This 'static' net can be used as part of a larger system with more complex behaviour. 
The static net has no memory for past inputs, but many problems require the context of 
the input in order to compute the answer. An extension to the static net is developed, the 
'dynamic' net, which feeds back a section of the output to the input, so creating some internal 
storage for context, and allowing a far greater class of problems to be learned. Previously this 
method of training time dependence into nets has suffered from a computational requirement 
which increases linearly with the time span of the desired context. The three architectures 
for dynamic nets presented here overcome this difficulty. 
To illustrate the power of these networks a general coder is developed and applied to the 
problem of speech coding. The non-linear solution found by training a dynamic net coder is 
compared with an established linear solution, and found to have an increased performance as 
measured by the signal to noise ratio. 
2. STATIC ERROR PROPAGATION NETS 
A static net is defined by a set of units and links between the units. Denoting oi as the value 
of the i th unit, and wi,j as the weight of the link between oi and oj, we may divide up the 
units into input units, hidden units and output units. If we assign o0 to a constant to forIn a 
� American Institute of Physics 1988 
633 
bias, the input units run from ox up to On,.,, followed by the hidden units to O.h,  and then 
the output units to o. ..... . The values of the input units are defined by the problem and the 
values of the remaining units are defined by: 
i-1 
net/ ---- 
=0 
oi = f(neti) (2.2) 
where f(x) is any continuous monotonic non-linear function and is known as the activation 
function. The function used the application is: 
2 
- 
These equations define a net which has the maxiinum number of interconnections. This 
arrangement is commonly restricted to a layered structure in which units are only connected 
to the immediately preceding layer. The architecture of these nets is specified by the number 
of input, output and hidden units. Diagrainmatically the static net is transformation of an 
input u, onto the output y, as in figure 1. 
uStatic  
figure 1 
The net is trained by using a gradient descent Mgorithm which minismises an energy 
term, E, defined as the summed squared error between the actual outputs, ol, and the target 
outputs, h. The algorithm also defines an error signal, 61, for each unit: 
E =   (ti-oi) 2 (2.4) 
6i = f'(neti)(ti -- oi) nhid < i _ nout (2.5) 
= ft(neti)  ilOd,i inp < i __ hid 
j=i+l 
where f'(x) is the derivative of f(t). The error signal and the activations of the units define 
the change in each weight, Awi,j. 
Au'i,i = ioj (2.7) 
where 71 is a constant of proportionality which determines the learning rate. The above 
equations define the error signal, i, for the input units as well as for the hidden units. Thus 
any number of static nets can be connected together, the values of i being passed from input 
units of one net to output units of the preceding net. It is this ability of error propagation 
nets to be 'glued' together in this way that enables the construction of dynamic nets. 
3. DYNAMIC ERROR PROPAGATION NETS 
The essential quality of the dynamic net is is that its behaviour is determined both by the 
external input to the net, and also by its own internal state. This state is represented by the 
634 
activation of a group of units. These units form part of the output of a static net and also 
part of the input to another copy of the same static net in the next tine period. Thus the 
state units link multiple copies of static nets over time to form a dynamic net. 
3.1. DEVELOPMENT FROM LINEAR CONTROL THEORY 
The analogy of a dynamic net in linear systems 2 may be stated as: 
xv+i = Ax v + Bu v (3.1.1) 
Yv = �xv (3.1.2) 
where us, is the input vector, xs, the state vector, and ys, the output vector at the integer time 
p. A, B and C are matrices. 
The structure of the linear systems solution may be implemented as a non-linear dynamic 
net by substituting the matrices A, B and C by static nets, represented by the non-linear 
functions A[.], B[.] and C[.]. The summation operation of Axs, and Bu s, could be achieved 
using a net with one node for each element in x and u and with unity weights from the two 
inputs to the identity activation function f(x) - x. Alternatively this net can be incorporated 
into the A[.] net giving the architecture of figure 2. 
A[.] 
Time 
Delay 
x(p+ y(p+l) 
- 'l I ' 
u(p) 
dynamic i 
net I 
figure 2 figure 3 
The three networks may be combined into one, as in figure 3. Simplicity of architecture 
is not just an aesthetic consideration. If three nets are used then each one must have enough 
computational power for its part of the task, combining the nets means that only the combined 
power must be sufficient and it allows common computations can be shared. 
The error signal for the output Ys,+x, can be calculated by comparison with the desired 
output. However, the error signal for the state units, %, is only given by the net at time p+ 1, 
which is not known at time p. Thus it is impossible to use a single backward pass to train 
this net. It is this difficulty which introduces the variation in the architectures of dynamic 
nets. 
3.2. THE FINITE INPUT DURATION ('FID) DYNAMIC NET 
If the output of a dynamic net, Ys,, is dependent on a finite number of previous inputs, try_/, 
to tts,, or if this assumption is a good approxination, then it is possible to formulate the 
635 
learning algorithm by expansion of the dynamic net for a finite time, as in figure 4. This 
formulation is sinliar to a restricted version of the recurrent net of Rumelhart, Hinton and 
Williams) 
dynamic 
(p-2) 
u(P-l) dynami c 
x (p,) net 
(p-l) 
figure 4 
u(P) 1 
x(p) 
dynamic 
(p) 
x(p+l) 
! 
Consider only the component of the error signal in past instantiations of the nets which 
is the result of the error signal at time t. The error signal for yp is calculated from the target 
output and the error signal for z r is zero. This cmnbined error signal is propagated back 
though the dynamic net at p to yield the error signals for up and x r. Similarly these error 
signals can then be propagated back through the net at t -p, and so on for all relevant inputs. 
The summed error signal is then used to change the weights as for a static net. 
Formalising the FID dynamic net for a general time q, q < p: 
ns is the nunber of state units 
oq,i is the output value of unit i at time q 
tq,i is the target value of unit i at time q 
6q,i is the error value of unit i at time q 
wi,j is the weight between oi and oj 
Awq,i,j is the weight change for this iteration at time q 
Awl,j is the total weight change for this iteration 
These values are calculated in the same way as in a static net, 
i-1 
netq,i - E wi,joq,j (3.2.1) 
oq,i = f(netq,i) (3.2.2) 
q,i -- f'(netq,i)(tq, i -- Oq,i) thi d + n s < i <_ nout (3.2.3) 
---- q+l,i_nh,,l+nmp_n, /1hi d < i  nhid + n, (3.2.4) 
= f'(netq,i)  6q,SwAi inp < i S hid (3..) 
ji+l 
Wq,i, j = lq,iOq,j (3.2.6) 
and the total weight change is given by the summation of the partial weight changes for all 
636 
previous times. 
A w i ,j 
p 
= (3.2.7) 
q=p-P 
P 
= (3.2.8) 
q=p-P 
Thus, it is possible to train a dynamic net to incorporate the information from any time 
period of finite length, and so learn any function which has a finite impulse response.* 
In some situations the approximation to a finite length may not be valid, or the storage 
and computational reluirements of such a net may not be feasible. In such situations another 
approach is possible, the infinite input duration dynamic net. 
3.3. THE INFINITE INPUT DURATION (IID) DYNAMIC NET 
Although the forward pass of the FID net of the previous section is a non-linear process, the 
backward pass computes the effect of small variations on the forward pass, and is a linear 
process. Thus the recursive learning procedure described in the previous section may be 
compressed into a single operation. 
Given the target values for the output of the net at time iv, equations (3.2.3) and (3.2.4) 
define values of e, at the outputs. If we denote this set of e, by D e then equation (3.2.5) 
states that any 6e,i in the net at time p is simply a linear transformation of D e. Writing the 
transformation matrix as S: 
(e,i ---- ,p,i Dp (3.3.1) 
In particular the set of 6e,i which is to be fed back int
