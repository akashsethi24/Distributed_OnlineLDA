A Model for Associative Multiplication 
G. BjSrn Christianson* 
Department of Psychology 
McMaster University 
Hamilton,Ont. L8S 4K1 
bjorn@caltech.edu 
Suzanna Becker 
Department of Psychology 
McMaster University 
Hamilton, Ont. L8S 4K1 
becker@mcmaster.ca 
Abstract 
Despite the fact that mental arithmetic is based on only a few hun- 
dred basic facts and some simple algorithms, humans have a diffi- 
cult time mastering the subject, and even experienced individuals 
make mistakes. Associative multiplication, the process of doing 
multiplication by memory without the use of rules or algorithms, 
is especially problematic. Humans exhibit certain characteristic 
phenomena in performing associative multiplications, both in the 
type of error and in the error frequency. We propose a model for 
the process of associative multiplication, and compare its perfor- 
mance in both these phenomena with data from normal humans 
and from the model proposed by Anderson et al (1994). 
1 INTRODUCTION 
Associative multiplication is defined as multiplication done without recourse to 
computational algorithms, and as such is mainly concerned with recalling the basic 
times table. Learning up to the ten times table requires learning at most 121 
facts; in fact, if we assume that normal humans use only four simple rules, the 
number of facts to be learned reduces to 39. In theory, associative multiplication is 
therefore a simple problem. In reality, school children find it difficult to learn, and 
even trained adults have a relatively high rate of error, especially in comparison 
to performance on associative addition, which is superficially a similar problem. 
There has been surprisingly little work done on the methods by which humans 
perform basic multiplication problems; an excellent review of the current literature 
is provided by McCloskey et al (1991). 
If a model is to be considered plausible, it must have error characteristics similar to 
*Author to whom correspondence should be addressed. Current address: Computation 
and Neural Systems, California Institute of Technology 139-74, Pasadena, CA 91125. 
18 G. B. Christianson and S. Becker 
those of humans at the same task. In arithmetic, this entails accounting for, at a 
minimum, two phenomena. The first is the problem size effect, as noted in various 
studies (e.g. Stazyk et al, 1982), where response times and error rates increase for 
problems with larger operands. Secondly, humans have a characteristic distribution 
in the types of errors made. Specifically, errors can be classified as one of the 
following five types, as suggested by Campbell and Graham (1985), Siegler (1988), 
McCloskey et al (1991), and Girelli et al (1996): operand, where the given answer is 
correct with one of the operands replaced (e.g. 4 x 7 = 21; this category accounts 
for 66.4% of all errors made by normal adults); close-miss, where the result is within 
ten percent of the correct response (4 x 7 = 29; 20.0%); table, where the result is 
correct for a problem with both operands replaced (4 x 7 = 25; 3.9%); non-table, 
where the result is not on the times table (4 x 7 = 17; 6.7%); or operation, where 
the answer would have been correct for a different arithmetic operation, such as 
addition (4 x 7- 11; 3.0%) 1. 
It is reasonable to assume that humans use at least two distinct representations 
when dealing with numbers. The work by Mandler and Shebo (1982) on modeling 
the performance of various species (including humans, monkeys, and pigeons) on 
numerosity judgment tasks suggests that in such cases a coarse coding is used. On 
the other hand, humans are capable of dealing with numbers as abstract symbolic 
concepts, suggesting the use of a precise localist coding. Previous work has either 
used only one of these coding ideas (for example, Sokol et al, 1991) or a single 
representation which combined aspects of both (Anderson et al, 1994). 
Warrington (1982) documented DRC, a patient who suffered dyscalculia following 
a stroke. DRC retained normal intelligence and a grasp of numerical and arithmetic 
concepts. When presented with an arithmetic problem, DRC was capable of rapidly 
providing an approximate answer. However, when pressed for a precise answer, he 
was incapable of doing so without resorting to an explicit computational algorithm 
such as counting. One possible interpretation of this case study is that DRC retained 
the ability to work with numbers in a magnitude-related fashion, but had lost the 
ability to treat numbers as symbolic concepts. This suggests the hypothesis that 
humans may use two separate, concurrent representations for numbers: both a 
coarse coding and a more symbolic, precise coding in the course of doing associative 
arithmetic in general, and multiplication in particular, and switch between the 
codings at various points in the process. This hypothesis will form the basis of our 
modeling work. To guide the placement of these transitions between representations, 
we assume the further constraint that the coarse coding is the preferred coding (as 
it is conserved across a wide variety of species) and will tend to be expressed before 
the precise coding. 
01245878901245878901234�67890124587890124 
Figure 1' The coarse coding for digits. Numbers along the left are the digit; numbers 
along the bottom are position numbers. Blank regions in the grid represent zero 
activity. 
1Data taken from Girelli et al (1996). 
.1 Model for lssociative Mu#it#cation 
2 METHODOLOGY 
19 
Following the work of Mandler and Shebo (1982), our coarse coding consists of a 
54-dimensional vector, with a sliding bump of ones corresponding to the magni- 
tude of the digit represented. The size of the bump decreases and the degree of 
overlap increases as the magnitude of the digit increases (Figure 1). Noise in this 
representation is simulated by the probability that a given bit will be in the wrong 
state. The precise representation, intended for symbolic manipulation of numbers, 
consists of a/O-dimensional vector with the value of the coded digit given by the 
dimension of greatest activity. Both of these representations are digit-based: each 
vector codes only for a number between 0 and 9, with concatenations of vectors 
used for numbers greater than 9. 
c 
D 
direct/on of flow 
Figure 2: Schematic of the network architecture. (A) The coarse coding. (B) The 
winner-take-all network. (C) The precise coding. (D) The feed-forward look-up 
table. See text for details. 
The model is trained in three distinct phases. A simple one-layer perceptton 
trained by a winner-take-all competitive learning algorithm is used to map the 
input operands from the original coarse coding into the precise representation. 
The network was trained for 10 epochs, each with a different set of 5 samples 
of noisy coarse-coded digits. At the end of training, the winner-take-all network 
performed at near-perfect levels. The translated operands are then presented to a 
two-layer feed-forward network with a logistic activation function trained by back- 
propagation. The number of hidden units was equal to the number of problems in 
the training set tin this case, 32) to force look-up table behaviour. The 
.table was trained independently for varying numbers of iterations, using look-up 
mg rate constant of 0.01 The output of the look-up table is coarse coded as in 
� a learn- 
Figure 1. In the final phase, the table output is translated by the winner-take-all 
network to provide the final answer in the precise coding. A schematic of the net- 
work architecture is given in Figure 2. The operarid vectors used for training of 
both networks had a noise parameter of 5%, while the vectors used in the analysis 
had 7.5% noise. Both the training and the testing problem set consisted of ten 
copies of each of the problems listed in Table 2, which are the problems used in 
20 G. B. Christianson and S. Becker 
Anderson et al (1994). Simulations were done in MATLAB v5.1 (Mathworks, Inc., 
24 Prime Park Way, Natick MA, 01760-1500). 
3 RESULTS 
9O 
8O 
7O 
6O 
5O 
4O 
3O 
2O 
lO 
o 
Normal humans (Girelli e/a/1996) 
Model of Anderson e/a/(1994) 
Model, 200 iterations training 
Model, 400 iterations training 
Model, 600 iterations training 
operand close-m iss table non-table operation 
Error Category 
Figure 3: Error distributions for human data (Girelli et al 1996), the model of 
Anderson et al (1994), and our model. 
Once a model has been trained, its errors on the training data can be categorized 
according to the error types listed in the Introduction section; a summary of the 
performance of our model is presented in Table 1. For comparison, we plot data 
generated by our model, the model of Anderson et al (1994), and human data from 
Girelli et al (1996) in Figure 3. In no case did the model generate an operation error. 
This is to be expected, as the model was only trained on multiplication, it should 
permit no way in which to make an operation error, other than by coincidence. A 
full set of results obtained from the model with 400 training iterations is presented 
in Table 22 . 
Table 1: Error rates generated by our model. A column for operation errors is not 
included, as in no instance did our model generate an operation error. 
Iterations Errors in Operand Close-miss Table Non-table 
320 trials (%) (%) (%) (%) 
200 114 61.4 21.0 8.8 8.8 
400 85 65.9 20.0 7.1 7.1 
600 65 63.7 16.9 9.2 10.8 
2As in Anderson et al (1994), we have set 8 x 9 - 67 deliberately so that it is not the 
only problem with an answer greater than 70. 
Model for Associative Multiplication 21 
Table 2: Results from ten trials run with the model after 400 training iterations. 
Errors are marked in boldface. 
Trial I 
Problem 1121al41516171s1911o 
2x2 4 4 4 4 4 4 4 4 4 4 
2x4 8 8 8 8 8 8 8 8 8 8 
2 x 5 10 10 10 10 10 10 10 10 10 10 
3 x 7 21 21 21 21 21 21 21 21 21 21 
3 x 8 24 24 24 64 24 24 21 24 24 21 
3 x 9 27 27 27 27 27 27 21 27 27
