Explorations with the Dynamic Wave 
Model 
Thomas P. Rebotier 
Department of Cognitive Science 
UCSD, 9500 Gilman Dr 
LA JOLLA CA 92093-0515 
rebotier@cogsci.ucsd.edu 
Jeffrey L. Elman 
Department of Cognitive Science 
UCSD, 9500 Gilman Dr 
LA JOLLA CA 92093-0515 
elman@cogsci.ucsd.edu 
Abstract 
Following Shrager and Johnson (1995) we study growth of logi- 
cal function complexity in a network swept by two overlapping 
waves: one of pruning, and the other of Hebbian reinforcement of 
connections. Results indicate a significant spatial gradient in the 
appearance of both linearly separable and non linearly separable 
functions of the two inputs of the network; the n.l.s. cells are much 
sparser and their slope of appearance is sensitive to parameters in 
a highly non-linear way. 
1 INTRODUCTION 
Both the complexity of the brain (and concomittant difficulty encoding that com- 
plexity through any direct genetic mapping), as well as the apparently high degree 
of cortical plasticity suggest that a great deal of cortical structure is nergent 
rather than pre-specified. Several neural models have explored the emergence of 
complexity. Von der Marlsburg (1973) studied the grouping of orientation selec- 
tivity by competitive Hebbian synaptic modification. Linsker (1986.a, 1986.b and 
1986.c) showed how spatial selection cells (off-center on-surround), orientation selec- 
tive cells, and finally orientation columns, emerge in successive layers from random 
input by simple, Hebbian-like learning rules. Miller (1992, 1994) studied the emer- 
gence of orientation selective columns from activity dependant competition between 
on-center and off-center inputs. 
Kerzsberg, Changeux and Dehaene (1992) studied a model with a dual-aspect learn- 
ing mechanism: Hebbian reinforcement of the connection strengths in case of cor- 
related activity, and gradual pruning of immature connections. Cells in this model 
were organized on a 2D grid, connected to each other according to a probability ex- 
ponentially decreasing with distance, and received inputs from two different sources, 
550 T.P. REBOTIER, J. L. ELMAN 
A and B, which might or might not be correlated. The analysis of the network re- 
vealed 17 different kinds of cells: those whose output after several cycles depended 
on the network's initial state, and the 16 possible logical functions of two inputs. 
Kerzsberg et al. found that learning and pruning created different patches of cells 
implementing common logical functions, with strong excitation within the patches 
and inhibition between patches. 
Shrager and Johnson (1995) extended that work by giving the network structure in 
space (structuring the inputs in intricated stripes) or in time, by having a Hebbian 
learning occur in a spatiotemporal wave that passed through the network rather 
than occurring everywhere simultaneously. Their motivation was to see if these 
learning conditions might create a cascade of increasingly complex functions. The 
approach was also motivated by developmental findings in humans and monkeys 
suggesting a move of the peak of maximal plasticity from the primary sensory 
and motor areas towards parietal and then frontal regions. Shrager and Johnson 
classified the logical functions into three groups: the constants (order 0), those that 
depend on one input only (order 1), those that depend on both inputs (order 2). 
They found that a slow wave favored the growth of order 2 cells, whereas a fast 
wave favored order i cells. However, they only varied the connection reinforcement 
(the growth Trophic Factor), so that the still diffuse pruning affected the rightmost 
connections before they could stabilize, resulting in an overall decrease which had 
to be compensated for in the analysis. 
In this work, we followed Shrager and Johnson in their study of the effect of a 
dynamic wave of learning. We present three novel features. Firstly, both the growth 
trophic factor (hereafter, TF) and the probability of pruning (by analogy, death 
factor, DF) travel in gaussian-shaped waves. Second, we classify the cells in 4, not 
3, orders: order 3 is made of the non-linearly separable logical functions, whereas 
the order 2 is now restricted to linearly separable logical functions of both inputs. 
Third, we use an overall measure of network performance: the slope of appearance 
of units of a given order. The density is neglected as a measure not related to the 
specific effects we are looking for, namely, spatial changes in complexity. Thus, each 
run of our network can be analyzed using 4 values: the slopes for units of order 0, 
1, 2 and 3 (See Table 1.). This extreme summarization of functional information 
allows us to explore systematically many parameters and to study their influence 
over how complexity grows in space. 
Table 1: Orders of logical complexity 
ORDER FUNCTIONS 
0 
1 
2 
3 
True False 
A !A B !B 
A.B !A.B A.!B !A.!B AvB !AvB Av!B !Av!B 
A xor B, A==B 
2 METHODS 
Our basic netxvork consisted of 4 columns of 50 units (one simulation verified the 
scaling up of results, see section 3.2). Internal connections had a gaussian band- 
width and did not wrap around. All initial connections were of weight 1, so that the 
connectivity weights given as parameters specified a number of labile connections. 
Early investigations were made with a set of manually chosen parameters (MAN- 
Explorations with the Dynamic Wave Model 551 
UAL). Afterwards, two sets of parameters were determined by a Genetic Algorithm 
(see Goldberg 1989): the first, SYM, by maximizing the slope of appearance of 
order 3 units only, the second, ASY, byoptimizing jointly the appearance of order 
2 and order 3 units (ASY). The SYMnetwork keeps a symmetrical rate of 
presentation between inputs A and B. In contrast, the ASY net presents input 
B much more often than input A. Parameters are specified in Table 1 and, are in 
natural units: bandwidths and distances are in cells apart, trophic factor is 
homogenous to a weight, pruning is a total probability. Initial values and prun- 
ing necessited random number generation. We used a linear congruence generator 
(see p284 in Press 1988), so that given the same seed, two different machines could 
produce exactly the same run. All the points of each Figure are means of several 
(usually 40) runs with different random seeds and share the same series of random 
seeds. 
Table 2: Default parameters 
MAN. SYM. ASY. name 
description 
8.5 6.20 12 Wae 
6.5 5.2 9.7 Wai 
8.5 8.5 13.4 Wbe 
6.5 6.5 14.1 Wbi 
5.0 6.5 9.9 Wne 
3.5 1.24 12.4 Wni 
0.2 0.20 0.28 DW 
7.0 1.26 0.65 Bne 
7.0 2.86 0.03 Bni 
0.? 0.68 0.98 Cdw 
1.5 3.0 -3.2 Ddw 
9.8]' 17.6 16.4 Wtf 
0.6 0.6 0.6 Btf 
3.5 1.87 3.3 Tst 
0.6 0.64 0.5 Bdf 
0.65 0.62 0.12 Pdf 
0.5 0.5 0.06 Pa 
0.5 0.5 0.81 Pb 
0.00 0.00 0.00 Pab 
mean ini. weight of A excitatory connections 
mean ini. weight of A inhibitory connections 
mean ini. weight of B excitatory connections 
mean ini. weight of B inhibitory connections 
m.ini. density of internal excitatory connections 
m.ini. density of internal inhibitory connections 
relative variation in initial weights 
bandwidth of internal excitatory connections 
bandwidth of internal inhibitory connections 
celerity of dynamic wave 
distance between the peaks of both waves 
base level of TF (=highest available weight) 
bandwidth of TF dynamic wave 
Threshold of stabilisation (pruning stop) 
bandwidth of DF dynamic wave 
base level of DF (total proba. of degeneration) 
probability of A alone in the stimulus set 
probability of B alone in the stimulus set 
probability of simultaneous s A and B 
3 RESULTS 
3.1 RESULTS FORMAT 
All Figures have the same format and summarize 40 runs per point unless otherwise 
specified. The top graph presents the mean slope of appearance of all 4 orders 
of complexity (see Table 1) on the y axis, as a function of different values of the 
experimentally manipulated parameter, on the x axis. The bottom left graph shows 
the mean slope for order 2, surrounded by a gray area one standard deviation below 
and above. The bottom right graph shows the mean slope for order 3, also with 
a 1-s.d. surrounding area. The slopes have not been normalized, and come from 
networks whose columns are 50 units high, so that a slope of 1.0 indicates that the 
number of such units increase in average by one unit per columns, ie, by 3 units 
552 T.P. REBOTIER, J. L. ELMAN 
across a 4-column network. Because there were very few if any undefined units, 
the slopes approximately sum to zero. Standard deviations are obtained pointwise, 
which gives a very conservative estimate of reliability. 
3.2 OVERALL ASPECT OF THE NETS 
Very few order 3 units were appearing. The mean slope with the SYM and 
ASY netwts was about 0.3 unit/column. As in Kertzsberg et al., units with 
identical function tended to appear in blocks. The SYM nets tend to have a 
general overlay of order 2 units, with sparse units of other orders appearing in the 
rightmost columns. The ASY nets manifest sharp transitions between order 0 
columns to the left and order 2 columns to the right, with sparse order 3 units 
almost exclusively in the rightmost column. 
3.3 CHANGE IN NETWORK WIDTH 
Figure 1 presents the results of 4000 runs for each of 4 networks with identical 
parameters (SYM) except for the number of columns, which varied from 4 to 
10. This Figure allows to see how the other results of this paper scale when wider 
networks are being used. All slopes decrease in a nearly hyperbolic manner. Since 
the mean network of width N is embedded as the beginning of the mean network 
of width N+P, this suggests that the effects of the growth reach a ceiling between 
width 4 and 6, and that the lesser slopes of networks of width 6, 8 and 10 is due 
to averaging between high slope in the first 4 columns and little afterwards. All 
parameter sets gave similar results. In essence, this justifies our using only 4 colum
