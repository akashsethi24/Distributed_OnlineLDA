Connectionist Models for 
Auditory Scene Analysis 
Richard O. Duda 
Department of Electrical Engineering 
San Jose State University 
San Jose, CA 95192 
Abstract 
Although the visual and auditory systems share the same basic 
tasks of informing an organism about its environment, most con- 
nectionist work on hearing to date has been devoted to the very 
different problem of speech recognition. We believe that the most 
fundamental task of the auditory system is the analysis of acoustic 
signals into components corresponding to individual sound sources, 
which Bregman has called auditory scene analysis. Computational 
and connectionist work on auditory scene analysis is reviewed, and 
the outline of a general model that includes these approaches is 
described. 
1 INTRODUCTION 
The primary task of any perceptual system is to tell us about the external world. 
The primary problem is that the sensory inputs provide too much data and too little 
information. A perceptual system must glean from the flood of incomplete, noisy, 
redundant and constantly changing streams of data those invariant properties that 
reveal important objects and events in the environment. For humans, the perceptual 
systems with the widest bandwidths are the visual system and the auditory system. 
There are many obvious similarities and differences between these modalities, and 
in addition to using them to perceive different aspects of the physical world, we also 
use them in quite different xvays to communicate with one another. 
1069 
1070 Duda 
The earliest neural-network models for vision and hearing addressed problems in 
pattern recognition, with optical character recognition and isolated word recogni- 
tion among the first engineering applications. However, about twenty years ago 
the research goals in vision and hearing began to diverge. In particular, the need 
for computers to perceive the external environment motivated vision researchers to 
seek the principles and procedures for recovering information about the physical 
world from visual data (Marr, 1982; Ballard and Brown, 1982). By contrast, the 
vast majority of work on machine audition remained focused on the communica- 
tion problem of speech recognition (Morgan and Scofield, 1991; Rabiner and Juang, 
1993). While this focus has produced considerable progress, the resulting systems 
are still not very robust, and perform poorly in uncontrolled environments. Fur- 
thermore, as Richards (1988) has noted, ... Speech, like writing and reading, is 
a specialized skill of advanced animals, and understanding speech need not be the 
best route to understanding how we interpret the patterns of natural sounds that 
comprise most of the acoustic spectrum about us. 
In recent years, some researchers concerned with modeling audition have begun to 
shift their attention from speech understanding to sound understanding. The inspi- 
ration for much of this activity has come from the work of Bregman, whose book 
on auditory scene analysis documents experimental evidence for important gestalt 
principles that summarize the ways that people group elementary events in fre- 
quency/time into sound objects or streams (Bregman, 1990). In this survey paper, 
we briefly review this activity and consider its implications for the development of 
connectionist models for auditory scene analysis. 
2 AUDITORY SCENE ANALYSIS 
In vision, Mart (1982) emphasized the importance of identifying the tasks of the 
visual system and developing a computational theory that is distinct from partic- 
ular algorithms or implementations. The computational theory had to specify the 
problems to be solved, the sensory data that is available, and the additional knowl- 
edge or assumptions required to solve the problems. Among the various tasks of 
the visual system, Mart believed that the recovery of the three-dimensional shapes 
of the surfaces of objects from the sensory image data was the most fundamental. 
The auditory system also has basic tasks that are more primitive than the recog- 
nition of speech. These include (1) the separation of different sound sources, (2) 
the localization of the sources in space (3) the suppression of echoes and reverber- 
ation, (4) the decoupling of sources from the environment, (5) the characterization 
of the sources, and (6) the characterization of the environment. Unfortunately, the 
relation between physical sound sources and perceived sound streams is not a sim- 
ple one-to-one correspondence. Distributed sound sources, echoes, and synthetic 
sounds can easily confuse auditory perception. Nevertheless, humans still do much 
better at these six basic tasks than any machine hearing system that exists today. 
From the standpoint of physics, the raw data available for performing these tasks 
is the pair of acoustic signals arriving at the two ears. From the standpoint of 
neurophysiology, the raw data is the activity on the auditory nerve. The nonlinear, 
mechano-neural spectral analysis performed by the cochlea converts sound pres- 
sure fluctuations into auditory nerve firings. For better or for worse, the cochlea 
Connectionist Models for Auditory Scene Analysis 1071 
decomposes the signal into many frequency components, transforming it into a fre- 
quency/time (or, more accurately, a place/time) spectrogram-like representation. 
The auditory system must find the underlying order in this dynamic flow of data. 
For a specific case, consider a simple musical mixture of several periodic signals. 
Within its limits of resolution, the cochlea decomposes each individual signal into 
its discrete harmonic components. Yet, under ordinary circumstances, we do not 
hear these components as separate sounds, but rather we fuse them into a single 
sound having, as musicians say, its particular timbre or tone color. However, if 
there is something distinctive about the different signals (such as different pitch or 
different modulation), we do not fuse all of the sounds together, but rather hear the 
separate sources, each with its own timbre. 
What information is available to group the spectral components into sound streams? 
Hartmann (1988) identifies the following factors that influence grouping: (1) com- 
mon onset/offset, (2) common harmonic relations, (3) common modulation, (4) 
common spatial origin, (5) continuity of spectral envelope, (6) duration, (7) sound 
pressure level, and (8) context. These properties are easier to name than to pre- 
cisely specify, and it is not surprising that no current model incorporates them all. 
However, several auditory scene analysis systems have been built that exploit some 
subset of these cues (Weintraub, 1985; Cooke, 1993; Mellinger, 1991; Brown, 1992; 
Brown and Cooke, 1993; Ellis, 1993). Although these are computational rather 
than connectionist models, most of them at least find inspiration in the structure 
of the mammalian auditory system. 
3 NEURAL AND CONNECTIONIST MODELS 
The neural pathways from the cochlea through the brainstem nuclei to the auditory 
cortex are complex, but have been extensively investigated. Although this system 
is far from completely understood, neurons in the brainstem nuclei are known to be 
sensitive to various acoustic features -- onsets, offsets and modulation in the dorsal 
cochlear nucleus, interaural time differences (ITD's) in the medial superior olive 
(MSO), interaural intensity differences (IID's) in the lateral superior olive (LSO), 
and spatial location maps in the inferior colliculus (Pickles, 1988). 
Both functional and connectionist models have been developed for all of these func- 
tions. Because it is both important and relatively well understood, the cochlea 
has received by far the most attention (Allen, 1985). As a result of this work, we 
now have real-time implementations for some of these models as analog VLSI chips 
(Lyon and Mead, 1988; Lazzaro et al., 1993). Connectionist models for sound local- 
ization have also been extensively explored. Indeed, one of the earliest of all neural 
network models was Jeffress's classic crosscorrelation model (Jefftess, 1948), which 
was hypothesized forty years before neural crosscorrelation structures were actually 
found in the barn owl (Cart and Konishi, 1988). Models have subsequently been 
proposed for both the LSO (Reed and Blum, 1990) and the MSO (Hah and Col- 
burn, 1991). Mathematically, both the ITD and IID cues for binaural localization 
are exposed by crosscorrelation. Lyon showed that crosscorrelation can also be used 
to separate as well as localize the signals (Lyon, 1983). VLSI crosscorrelation chips 
can provide this information in real time (Lazzaro and Mead, 1989; Bhadkamkar 
and Fowler, 1993). 
1072 Duda 
While interaural crosscorrelation can determine the azimuth to a sound source, 
full three-dimensional localization also requires the determination of elevation and 
range. Because of a lack of symmetry in the orientation of its ears, the barn owl can 
actually determine azimuth from the ITD and elevation from the IID. This at least 
in part explains why it has been such a popular choice for connectionist modeling 
(Spence et al., 1990; Moiseff et al., 1991; Palmieri et al., 1991; Rosen, Rumelhart 
and Knudsen, 1993). Unfortunately, the localization mechanisms used by humans 
are more complicated. 
It is well known that humans use monaural, spectral shape cues to estimate elevation 
in the median sagittal plane (Blauert, 1983; Middlebrooks and Green, 1991), and 
source localization models based on this approach have been developed (Neti, Young 
and Schneider, 1992; Zakarauskas and Cynander, 1993). The author has shown that 
there are strong binaural cues for elevation at short distances away from the median 
plane, and has used statistical methods to estimate both azimuth and elevation 
accurately from IID data alone (Duda, 1994). In addition, backprop models have 
been developed that can estimate azimuth and elevation from IID and
