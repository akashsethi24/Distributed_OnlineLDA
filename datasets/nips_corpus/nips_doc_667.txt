Synchronization and Grammatical Inference 
in an Oscillating Elman Net 
Bill Baird 
Dept Mathematics, 
U.C.Berkeley, 
Berkeley, Ca. 94720, 
baird@math.berkeley.edu 
Todd Troyer 
Dept Mathematics, 
U.C.Berkeley, 
Berkeley, Ca. 94720 
Yank Eeckman 
Lawrence Livermore 
National Laboratory, 
P.O. Box 808 (L-426), 
Livermore, Ca. 94551 
Abstract 
We have designed an architecture to span the gap between bio- 
physics and cognitive science to address and explore issues of how 
a discrete symbol processing system can arise from the continuum, 
and how complex dynamics like oscillation and synchronization can 
then be employed in its operation and affect its learning. We show 
how a discrete-time recurrent Elman network architecture can 
be constructed from recurrently connected oscillatory associative 
memory modules described by continuous nonlinear ordinary dif- 
ferential equations. The modules can learn connection weights be- 
tween themselves which will cause the system to evolve under a 
clocked machine cycle by a sequence of transitions of attractors 
within the modules, much as a digital computer evolves by transi- 
tions of its binary flip-flop attractors. The architecture thus em- 
ploys the principle of computing with attractors used by macro- 
scopic systems for reliable computation in the presence of noise. We 
have specifically constructed a system which functions as a finite 
state automaton that recognizes or generates the infinite set of six 
symbol strings that are defined by a Reber grammar. It is a symbol 
processing system, but with analog input and oscillatory subsym- 
bolic representations. The time steps (machine cycles) of the sys- 
tem are implemented by rhythmic variation (clocking) of a bifurca- 
tion parameter. This holds input and context modules clamped 
at their attractors while 'hidden and output modules change state, 
then clamps hidden and output states while context modules are 
released to load those states as the new context for the next cycle of 
input. Superior noise immunity has been demonstrated for systems 
with dynamic attractors over systems with static attractors, and 
synchronization (binding) between coupled oscillatory attractors 
in different modules has been shown to be important for effecting 
reliable transitions. 
236 
Synchronization and Grammatical Inference in an Oscillating Elman Net 237 
I Introduction 
Patterns of 40 to 80 Hz oscillation have been observed in the large scale ac- 
tivity (local field potentials) of olfactory cortex [Freeman and Baird, 1987] and 
visual neocortex [Gray and Singer, 1987], and shown to predict the olfactory 
[Freeman and Baird, 1987] and visual pattern recognition responses of a trained 
animal. Similar observations of 40 Hz oscillation in auditory and motor cortex (in 
primates), and in the retina and EMG have been reported. It thus appears that 
cortical computation in general may occur by dynamical interaction of resonant 
modes, as has been thought to be the case in the olfactory system. 
The oscillation can serve a macroscopic clocking function and entrain or bind 
the relevant microscopic activity of disparate cortical regions into a well defined 
phase coherent collective state or gestalt. This can overide irrelevant microscopic 
activity and produce coordinated motor output. There is further evidence that 
although the oscillatory activity appears to be roughly periodic, it is actually chaotic 
when examined in detail. 
If this view is correct, then oscillatory/chaotic network modules form the actual cor- 
tical substrate of the diverse sensory, motor, and cognitive operations now studied 
in static networks. It must then be shown how those functions can be accomplished 
with oscillatory and chaotic dynamics, and what advantages are gained thereby. It 
is our expectation that nature makes good use of this dynamical complexity, and 
our intent is to search here for novel deign principles that may underly the superior 
computational performance of biological systems over man made devices in many 
task domains. These principles may then be applied in artificial systems to engi- 
neering problems to advance the art of computation. We have therefore constructed 
a parallel distributed processing architecture that is inspired by the structure and 
dynamics of cerebral cortex, and applied it to the problem of grammatical inference. 
The construction assumes that cortex is a set of coupled oscillatory associative 
memories, and is also guided by the principle that attractors must be used by 
macroscopic systems for reliable computation in the presence of noise. Present day 
digital computers are built of flip-flops which, at the level of their transistors, are 
continuous dissipative dynamical systems with different attractors underlying the 
symbols we call 0 and 1. 
2 Oscillatory Network Modules 
The network modules of this architecture were developed previously as models of 
olfactory cortex, or caricatures of patchesof neocortex [Baird, 1990a]. A partic- 
ular subnetwork is formed by a set of neural populations whose interconnections 
also contain higher order synapses. These synapses determine attractors for that 
subnetwork independent of other subnetworks. Each subnetwork module assumes 
only minimal coupling justified by known olfactory anatomy. An N node module 
can be shown to function as an associative memory for up to N/2 oscillatory and 
N/3 chaotic memory attractors [Baird, 1990b, Baird and Eeckman, 1992b]. Single 
modules with static, oscillatory, and three types of chaotic attractors - Lorenz, 
Roessler, Ruelle-Takens - have been sucessfully used for recognition of handwritten 
characters [Baird and Eeckman, 1992b]. 
We have shown in these modules a superior stability of oscillatory attractors over 
static attractors in the presence of additive Gaussian noise perturbations with 
the 1/f spectral character of the noise found experimentally by Freeman in the 
brain[Baird and Eeckman, 1992a]. This may be one reason why the brain uses 
dynamic attractors. An oscillatory attractor acts like a a bandpass filter and is 
238 Baird, Troyer, and Eeckman 
effectively immune to the many slower macroscopic bias perturbations in the theta- 
alpha-beta range (3 - 25 Hz) below its 40 -80 Hz passband, and the more microscopic 
perturbations of single neuron spikes in the 100 - 1000 Hz range. 
The mathematical foundation for the construction of network modules is contained 
in the normal form projection algorithm[Baird, 1990b]. This is a learning algo- 
rithm for recurrent analog neural networks which allows associative memory storage 
of analog patterns, continuous periodic sequences, and chaotic attractors in the same 
network. A key feature of a net constructed by this algorithm is that the underly- 
ing dynamics is explicitly isomorphic to any of a class of standard, well understood 
nonlinear dynamical systems - a normal form [Guckenheimer and Holmes, 1983]. 
This system is chosen in advance, independent of both the patterns to be stored 
and the learning algorithm to be used. This control over the dynamics permits the 
design of important aspects of the network dynamics independent of the particu- 
lar patterns to be stored. Stability, basin geometry, and rates of convergence to 
attractors can be programmed in the standard dynamical system. 
By analyzing the network in the polar form of these normal form coordinates, 
the amplitude and phase dynamics have a particularly simple interaction. When 
the input to a module is synchronized with its intrinsic oscillation, the amplitudes 
of the periodic activity may be considered separately from the phase rotation, and 
the network of the module may be viewed as a static network with these amplitudes 
as its activity. We can further show analytically that the network modules we have 
constructed have a strong tendency to synchronize as required. 
3 Oscillatory Elman Architecture 
Because we work with this class of mathematically well-understood associative mem- 
ory networks, we can take a constructive approach to building a cortical computer 
architecture, using these networks as modules in the same way that digital com- 
puters are designed from well behaved continuous analog flip-flop circuits. The 
architecture is such that the larger system is itself a special case of the type of 
network of the submodules, and can be analysed with the same tools used to design 
the subnetwork modules. 
Each module is described in normal form or mode coordinates as a k-winner- 
take-all network where the winning set of units may have static, periodic or chaotic 
dynamics. By choosing modules to have only two attractors, networks can be built 
which are similar to networks using binary units. There can be fully recurrent con- 
nections between modules. The entire super-network of connected modules, how- 
ever, is itself a polynomial network that can be projected into standard network 
coordinates. The attractors within the modules may then be distributed patterns 
like those described for the biological model [Baird, 1990a], and observed exper- 
imentally in the olfactory system [Freeman and Baird, 1987]. The system is still 
equivalent to the architecture of modules in normal form, however, and may easily 
be designed, simulated, and theoretically evaluated in these coordinates. In this 
paper all networks are discussed in normal form coordinates. 
As a benchmark for the capabilities of the system, and to create a point of contact 
to standard network architectures, we have constructed a discrete-time recurrent 
Elman network tElman, 1991] from oscillatory moduleï¿½ defined by ordinary dif- 
ferential equations. We have at present a system which functions as a finite state 
automaton that perfectly recognizes or generates the infinite set of strings defined by 
the Reber grammar described in Cleeremans et. al. [Cleeremans et al., 1989]. The 
connections for this netw
