On the Use of Evidence in Neural Networks 
David H. Wolpert 
The Santa Fe Institute 
1660 Old Pecos Trail 
Santa Fe, NM 87501 
Abstract 
The Bayesian evidence approximation has recently been employed to 
determine the noise and weight-penalty terms used in back-propagation. 
This paper shows that for neural nets it is far easier to use the exact result 
than it is to use the evidence approximation. Moreover, unlike the evi- 
dence approximation, the exact result neither has to be re-calculated for 
every new data set, nor requires the running of computer code (the exact 
result is closed form). In addition, it tums out that the evidence proce- 
dure's MAP estimate for neural nets is, in toto, approximation error. An- 
other advantage of the exact analysis is that it does not lead one to incor- 
rect intuition, like the claim that using evidence one can evaluate differ- 
ent priors in light of the data. This paper also discusses sufficiency 
conditions for the evidence approximation to hold, why it can sometimes 
give reasonable results, etc. 
1 THE EVIDENCE APPROXIMATION 
It has recently become popular to consider the problem of training neural nets from a Baye- 
sian viewpoint (Buntine and Weigend 1991, MacKay 1992). The usual way of doing this 
starts by assuming that there is some underlying target function f from R n to R, parameter- 
ized by an N-dimensional weight vector w. We are provided with a training set L of noise- 
corrupted samples of f. Our goal is to make a guess for w, basing that guess only on L. Now 
assume we have i.i.d. additive gaussian noise resulting in P(L I w, 13) o exp(-13 %2)), where 
%2(w, L) is the usual sum-squared training set error, and [3 reflects the noise level. Assume 
further that P(w I cz) o exp(-czW(w)), where W(w) is the sum of the squares of the weights. 
If the values of cz and 13 are known and fixed, to the values a t and 13t respectively, then P(w) 
539 
540 Wolpert 
= P(w I at) and P(L I w) = P(L I w, 13t). Bayes' theorem then tells us that the posterior is 
proportional to the product of the likelihood and the prior, i.e., P(w I L) o P(L I w) P(w). 
Consequently, finding the maximum a posteriori (MAP) w - the w which maximizes 
P(w I L) - is equivalent to finding the w minimizing %2(w, L) + ( / 13t)W(w). This can be 
viewed as a justification for performing gradient descent with weight-decay. 
One of the difficulties with the foregoing is that we almost never know  and t in real- 
world problems. One way to deal with this is to estimate  and 13t, for example via a tech- 
nique like cross-validation. In contrast, a Bayesian approach to this problem would be to 
set priors over c and 13, and then examine the consequences for the posterior of w. 
This Bayesian approach is the starting point for the evidence approximation created by 
Gull (Gull 1989). One makes three assumptions, for P(w I T), P(L I w, T), and P(T). (For sim- 
plicity of the exposition, from now on the two hyperparameters c and 13 will be expressed 
as the two components of the single vector T.) The quantity of interest is the posterior: 
P(w I L): ( d5 P(w, 51 L) 
= J d5 [(P(w, 51L) / P(51L)} xP(51L)] (1) 
The evidence approximation suggests that if P(51 L) is sharply peaked about 5 = �, while 
the term in curly brackets is smooth about 5 = �, then one can approximate the w-depen- 
dence of P(w I L) as P(w, 5/I L) / P(� I L) o P(L I w, �) P(w I �). In other words, with the 
evidence approximation, one sets the posterior by taking P(w) = P(w I �) and P(L I w) = P(L 
I w, �), where T' is the MAP 5. P(L 15) = f dw [P(L I w, 5) P(w 15)] is known as theevidence 
for L given 5. For relatively smooth P(5), the peak of P(51 L) is the same as the peak of the 
evidence (hence the name evidence approximation). Although the current discussion will 
only explicitly consider using evidence to set hyperparameters like c and 13, most of what 
will be said also applies to the use of evidence to set other characteristics of the learner, like 
its architecture. 
MacKay has applied the evidence approximation to finding the posterior for the neural net 
P(w I (x) and P(L I w, 13) recounted above combined with a P(5) = P(c, 13) which is uniform 
over all c and 13 from 0 to +oo (MacKay 1992). In addition to the error introduced by the 
evidence approximation, additional error is introduced by his need to approximate �. 
MacKay states that although he expects his approximation for T' to be valid, it is a matter 
of further research to establish [conditions for] this approximation to be reliable. 
2 THE EXACT CALCULATION 
It is always true that the exact posterior is given by 
P(w) : I dyP(w 1 5) P(5), 
P(L I w) = Id5 {P(L I w, 5) x P(w 1 5) x P(5)} / P(w); 
P(wlL) : .fds{P(LIw, 5)xP(w I�)xP(5)} (2) 
where the proportionality constant, being independent of w, is irrelevant. 
Using the neural net P(w I a) and P(L I w, 13) recounted above, and MacKay's P(5), it is 
trivial to use equation 2 to calculate that P(w) : [W(w)] -(Na + 1), where N is the number of 
weights. Similarly, with m the number of pairs in L, P(L I w) : [%2(w, L)] '(m/2 + 1). (See 
(Wolpert 1992) and (Buntine and Weigend 1991), and allow the output values in L to range 
On the Use of Evidence in Neural Networks 541 
from _oo to +oo.) These two results give us the exact expression for the posterior P(w I L). 
In contrast, the evidence-approximated posterior o exp[-c'(L) W(w) - 13'(L) %2(w, L)]. 
It is illuminating to compare this exact calculation to the calculation based on the evidence 
approximation. A good deal of relatively complicated mathematics followed by some com- 
puter-based numerical estimation is necessary to arrive at the answer given by the evidence 
approximation. (This is due to the need to approximate ,/.) In contrast, to perform the exact 
calculation one only need evaluate a simple gaussian integral, which can be done in closed 
form, and in particular one doesn't need to perform any computer-based numerical estima- 
tion. In addition, with the evidence procedure �' must be re-evaluated for each new data set, 
which means that the formula giving the posterior must be re-derived every time one uses 
a new data set. In contrast, the exact calculation's formula for the posterior holds for any 
data set; no re-calculations are required. So as a practical tool, the exact calculation is both 
far simpler and quicker to use than the calculation based on the evidence approximation. 
Another advantage of the exact calculation, of course, is that it is exact. Indeed, consider 
the simple case where the noise is fixed, i.e., P(T) = P(T1) T2 - 13t), so that the only term 
we must deal with is T1 = c. Set all other distributions as in (MacKay 1992). For this case, 
the w-dependence of the exact posterior can be quite different from that of the evidence- 
approximated posterior. In particular, note that the MAP estimate based on the exact calcu- 
lation is w = 0. This is, of course, a silly answer, and reflects the poor choice of distributions 
made in (MacKay 1992). In particular, it directly reflects the un-normalizability of MacK- 
ay's P(c). However the important point is that this is the exactly correct answer for those 
distributions. On the other hand, the evidence procedure will result in an MAP estimate of 
argmin w [%2(w, L) + (c'/13')W(w)], where c' and 13' are derived from L. This answer will 
often be far from the correct answer ofw = 0. Note also that the evidence approximations's 
answer will vary, perhaps greatly, with L, whereas the correct answer is L-independent. Fi- 
nally, since the correct answer is w = 0, the difference between the evidence procedure's 
answer and the correct answer is equal to the evidence procedure's answer. In other words, 
although there exist scenarios for which the evidence approximation is valid, neural nets 
with flat P(�0 is not one of them; for this scenario, the evidence procedure's answer is in 
toto approximation error. (A possible reason for this is presented in section 4.) 
If one were to use a more reasonable P(c), uniform only from 0 to an upper cut-off Omax, 
the results would be essentially the same, for large enough %nax' The effect on the exact 
posterior, to first order, is to introduce a small region around w = 0 in which P(w) behaves 
like a decaying exponential in W(w) (the exponent being set by %nax) rather than like 
[W(w)]'(Nt2 +  ) (T. Wallstrom, private communication). For large enough Ctma x, the region 
is small enough so that the exact posterior still has a peak very close to 0. On the other hand, 
for large enough O[max, there is no change in the evidence procedure's answer. (Generically, 
the major effect on the evidence procedure of modifying P(T) is not to change its guess for 
P(w I L), but rather to change the associated error, i.e., change whether sufficiency condi- 
tions for the validity of the approximation are met. See below.) Even with a normalizable 
prior, the evidence procedure's answer is still essentially all approximation error. 
Consider again the case where the prior over both c and 13 is uniform. With the evidence 
approximation, the log of the posterior is -{ Z2(w, L) + (c' / 13')W(w) }, where c' and 13' 
are set by the data. On the other hand, the exact calculation shows that the log of the pos- 
542 Wolpert 
terior is really given by -{ ln[%2(w, L)] + (N+2/m+2) ln[W(w)] }. What's interesting about 
this is not simply the logarithms, absent from the evidence approximation's answer, but also 
the factor multiplying the term involving the weight penalty quantity W(w). In the evi- 
dence approximation, this factor is data-dependent, whereas in the exact calculation it only 
depends on the number of data. Moreover, the value of this factor in the exact calculation 
tells us that if the number of weights increases, or alternatively the number of training ex- 
amples decreases, the weight penalty term becomes more i
