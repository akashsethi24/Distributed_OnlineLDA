A Lagrangian Formulation For 
Optical Backpropagation Training In 
Kerr-Type Optical Networks 
James E. Steck 
Mechanical Engineering 
Wichita State University 
Wichita, KS 67260-0035 
Steven R. Skinner 
Electrical Engineering 
Wichita State University 
Wichita, KS 67260-0044 
Aivaro A. Cruz-Cabrara 
Electrical Engineering 
Wichita State University 
Wichita, KS 67260-0044 
Elizabeth C. Behrman 
Physics Department 
Wichita State University 
Wichita, KS 67260-0032 
Abstract 
A training method based on a form of continuous spatially distributed 
optical error back-propagation is presented for an aH optical network 
composed of nondiscrete neurons and weighted interconnections. The all 
optical network is feed-forward and is composed of thin layers of a Kerr- 
type self focusing/defocusing nonlinear optical material. The training 
method is derived from a Lagrangian formulation of the constrained 
minimization of the network error at the output. This leads to a 
formulation that describes training as a calculation of the distributed error 
of the optical signal at the output which is then reflected back through the 
device to assign a spatially distributed error to the internal layers. This 
error is then used to modify the internal weighting values. Results from 
several computer simulations of the training are presented, and a simple 
optical table demonstration of the network is discussed. 
772 Elizabeth C. Behrman 
I KERR TYPE MATERIALS 
Kerr-type optical networks utilize thin layers of Kerr-type nonlinear materials, in which the 
index of refraction can vary within the material and depends on the amount of light striking 
the material at a given location. The material index of refraction can be described by: 
n(x)=n0+n2I(x), where no is the linear index of refraction, n: is the nonlinear coefficient, and 
I(x) is the irradiance of a applied optical field as a function of position x across the material 
layer (Armstrong, 1962). This means that a beam of light (a signal beam carrying 
information perhaps) passing through a layer of Kerr-type material can be steered or 
controlled by another beam of light which applies a spatially varying pattern of intensity 
onto the Kerr-type material. Steering of light with a glass lens (having constance index of 
refraction) is done by varying the thickness of the lens (the amount of material present) as 
a function of position. Thus the Kerr effect can be loosely thought of as a glass lens whose 
geometry and therefore focusing ability could be dynamically controlled as a function of 
position across the lens. Steering in the Kerr matehal is accomplished by a gradient or 
change in the matehal index of refraction which is created by a gradient in applied light 
intensity. This is illustrated by the simple experiment in Figure 1 where a small weak probe 
beam is steered away from a straight path by the intensity gradient of a more powerful pump 
beam. 
I(x) 
mp 
x Probe 
n2<0 
Figure 1: Light Steering In Kerr Materials 
2 OPTICAL NETWORKS USING KERR MATERIALS 
The Kerr optical network, shown in Figure 2, is made up of thin layers of the Kerr- type 
nonlinear medium separated by thick layers of a linear medium (free space) (Skinner, 1995). 
The signal beam to be processed propagates optically in a direction z perpendicular to the 
layers, from an input layer through several alternating linear and nonlinear layers to an output 
layer. The Kerr material layers perform the nonlinear processing and the linear layers serve 
as connection layers. The input (I(x)) and the weights (Wl(x),W2(x) ... Wn(x)) are irradiance 
fields applied to the Kerr type layers, as functions of lateral position x, thus varying the 
A Lagrangian Formulation for Optical Backpropagation 773 
refractive index profile of the nonlinear medium. Basically, the applied weight irradiences 
steer the signal beam via the Kerr effect discussed above to produce the correct output. The 
advantage of this type of optical network is that both neuron processing and weighted 
connections are achieved by uniform layers of the Kerr material. The all optical nature 
eliminates the need to physically construct neurons and connections on an individual basis. 
I(x,y) W. (x,y) Wn(x,y) 
, 
Figure 2: Kerr Optical Neural Network Architecture 
If Ei(e ) is the light entering the i th nonlinear layer at lateral position e, then the effect of the 
nonlinear layer is given by 
O) 
where W(a) is the applied weight field. Transmission of light at lateral location  at the 
beginning of the i t linear layer to location 13 just before the i+l t nonlinear layer is given by 
E,.l([3)_ J2, F(a) eJC'O-')2do where C,- (2) 
 2AL. 
3 OPTICAL BACK-PROPAGATION TRAINING 
Traditional feed-forward artificial neural networks composed of afinite number of discrete 
neurons and weighted connections can be trained by many techniques. Some of the most 
successful techniques are based upon the well known training method called back- 
propagation which results from minimizing the network output error, with respect to the 
network weights by a gradient descent algorithm. The optical network is trained using a 
form of continuous optical back-propagation which is developed for a nondiscrete network. 
Gradient descent is applied to minimize the error over the entire output region of the optical 
network. This error is a continuous distribution of error calculated over the output region. 
774 Elizabeth C. Behrman 
Optical back-propagation is a specific technique by which this error distribution is optically 
propagated backward through the linear and nonlinear optical layers to produce error signals 
by which the light applied to the nonlinear layers is modified. Recall that this applied light 
W i controls what serves as connection weights in the optical network. Optical back- 
propagation minimizes the error Lo over an output region rio, a subdomain of the final or n th 
layer of the network, 
ro - ---[ o- o l 2 
2� 2 
where 
subject to the constraint that the propagated light, Ei(a), satisfies the equations of forward 
propagation (1) and (2). 003) = E,+l([t) and is the network output, � is a scaling factor on 
the output intensity. L0 then is the squared error between the desired output value D and the 
average intensity I0 of the output distribution O(3). 
This constrained minimization problem is posed in a Lagrange formulation similar to the 
work of (le Cunn, 1988) for conventional feedforward networks and (Pineda, 1987) for 
conventional recurrent networks; the difference being that for the optical network of this 
paper the Electric field E and the Lagrange multiplier are complex and also continuous in the 
spatial variable thus requiring the Lagrangian below. A Lagrangian is defined as; 
L= /+  f.. (a)[ Ei+l({Z ) - fF(f3)--Jc'(o .)a d[3]  
/=1 
i=1 
(4) 
Taking the variation of L with respect to Ei, the Lagrange multipliers Ji, and using gradient 
descent to minimize L with respect to the applied weight fields Wi gives a set of equations 
that amount to calculating the error at the output and propagating the error optically 
backwards through the network. The pertinent results are given below. The distributed 
assignment of error on the output field is calculated by 
X..l() = L o -() [ O - t 0 ] (S) 
This error is then propagated back through the n th or final linear optical layer by the equation 
jc. foX..l() 
8n([5 ) - e (6) 
which is used to update the weight light applied to the n th nonlinear layer. Optical back- 
propagation, through the i th nonlinear layer (giving .i(13)) followed by the linear layer 
(giving 8i4([3)) is performed according to the equations 
A Lagrangian Formulation for Optical Backpropagation 775 
+ ,ap2;'() 2  8) E,() e 
8,-1( [ ) jCi_t fq .(;x) e -.C,q(ll 
- do{ 
This gives the error signal 8i. 1([ ) used to update the weight light distribution Wi.([3) 
applied to the i-1 nonlinear layer. The weights are updated based upon these errors 
according to the gradient descent rule 
+tlO)koANLin2}Y,�O) 2 /M[ E,.(0 ) 6,(0) e 
(s) 
where rh(13 ) is a learning rate which can be, but usually is not a function of layer number i 
and spatial position [3. Figure 3 shows the optical network (thick linear layers and thin 
nonlinear layers) with the uniform plane wave E0, the input signal distribution I, forward 
propagation signals E l E 2 ... E,, the weighting light distributions at the nonlinear layers W 
W 2 ... W,. Also shown are the error signal 3,,+ at the output and the back-propagated error 
signals 8, ... 82 8 for updating the nonlinear layers. Common nonlinear materials exist for 
which the material constants are such that the second term in the first of Equations 7 
becomes small. Ignoring this second term gives an approximate form of optical back- 
propagation which amounts to calculating the error at the output of the network and then 
reversing its direction to optically propagate this error backward through the device. 
This can be easily seen by comparing Equations 6 and 7 (with the second term dropped) for 
optical back-propagation of the output error 3., with Equations 1 and 2 for the forward 
propagation of the signal Ei. This means that the optical back-propagation training 
calculations potentially can be implemented in the same physical device as the forward 
network calculations. Equation (8) then becomes; 
())' ] 
which may be able to be implemented optically. 
4 SIMULATION RESULTS 
To prove feasibility, the network was then trained and tested on several benchmark 
classification problems, two of which are discussed here. More details on these and other 
simulations of the optical network can be found in (Skinner, 1995). In the first (Using 
Nworks, 1991), iris species were classified into one of three categories: Setosa, 
Versicolor or Virginica. Classification was based upon length and width of the sepals and 
776 Elizabeth C. Behrman 
petals. The network cons
