Generalization Error and The Expected 
Network Complexity 
Chuanyi Ji 
Dept. of Elec., Cornpt. and Syst Engr. 
Rensselaer Polytechnic Inst, itule 
Troy, NY 12180-3590 
chuanyi@ecse.rpi.edu 
Abstract 
For two layer networks with n sigmoidal hidden units, the generalization error is 
shown to be bounded by 
0(: ) + O( (:C)d 
N 
where d and N are the input dimension and the number of training samples, re- 
spectively. E represents the expectation on random number I( of hidden units 
(1 _< X _< n). The proba,bility Pr(I( = k) (1 <_ k <_ n) is dctermined by a prior 
distribution of weights, which corresponds to a Gibbs distribtttion of a regularizer. 
This relationship makes it possible to characterize explicitly how a regularization 
term affects bias/variance of networks. The bound can be obta.ined analytically 
for a large c. lass of commonly used priors. It can also be applied to estimate the 
expected network complexity E.r in practice. The result provides a quantitative 
explanation on how large networks can generalize well. 
1 Introduction 
Pegularization (or weight-deca.y) methods are widely used in supervised learning by 
adding a regularization term to an energy function. Although it is well known that 
such a regularization term effectively reduces network complexity by introducing 
more bias and less variance[4] to the networks, it is not clear whether and how the 
information given by a regularization term can be used alone to characterize the 
effective network complexity and how the estimated effective network complexity 
relates to the generalization error. This research a. ttempts to provide answers to 
these questions for two layer feedforward networks with sigmoidal hidden units. 
367 
368 Ji 
Specifically, the effective network complexity is characterized by the expected 
bet of hidden units determined by a Gibbs dist, ribution corresponding to a regulat'- 
ization term. The generalization error can then be bounded by the expected network 
complexity, and thus be tighter than the original bound given by Barron[2]. The 
new bound shows explicitly, through a bigger approximation error and a smaller 
estimation error, how a regularization term introduces more bias and less variance 
to the networks� It therefore provides a quantitative explanation on how a network 
larger than necessary can also generalize well under certain conditions, which can 
not, be explained by the existing learning theory[9]. 
For a class of commonly-used regularizers, the expec'ced netxvork complexity can 
be obtained in a closed form. It is then used to estimate the expected network 
complexity for Gaussion mixture model[6]. 
2 Background and Previous Results 
A relationship has been developed bv Barron[2] between generalization error and 
network complexity for two la,yer networks nsed for function approximation. We 
will briefly describe this restlit in this section and give onr extension subsequently. 
Consider a cla.ss of two layer networks of fixed architecture with n sigmoidal hidden 
units and one (linear)output unit. Let .f,,(z; w)= w?)g(w)rz)be a net'work 
/:1 
fitnction, where w  O is the network weight vector comprising both w? and w?) 
for 1 5 l 5 . w ) and w? are the incoming weights to the/-th hidden unit 
the weight from the /-th hidden unit to the output, respectively.   R � is 
the weight space for n hidden nnits (and input dimension d). Each sigmoid unit 
g(z) is assumed to be oftanh type: g(z)  1 as z   for 1 5 I 5  
The input is z  D  Ra. Without loss of generality, D is assumed to be a unit 
hypercube in R d, i.e., all the components of x are in [--1, 1]. 
Let j'(x) be a. target function defined in the same domain D and satisfy some 
smoot. hness conditions [2]. Consider N [raining samples independently drawn from 
some distribntion It(a:): (z,./(x)), ...,(x2v, f(:t:v)). Define au energy function e, 
where e = c + h z'*'( L, (w) is a regularization term as a function of w 
Ar ,,N 
for a fixed ,. A is a constant. c 
N 
1 
fimction such that 'b minimizes the 
is a quadratic error function on N training 
))2. Let ./;,,(.v;'&) be the (optimal)network 
energy fimction e: b = arg rain �. The gert- 
eralization error E.q is defined to be the squared L 2 -- 
norm = f- II -- 
f(f(x) -- fn,N(X;tb))2d/t(x), vhere  is the expectation over a]] training sets of 
D 
size Ar drawn from the same distribution. Thus, the generalization error measures 
the mean squared distance between the unknoxvn function an,l the best network 
function that can be obtained for training sets of size A; The ,,e � ' ' 
� g me ahzgton error 
In the previous work by Barron, the sigmoidal hidden units arc, ,,(:)+l 
2 
show tha, t his results axe applica,ble to the class of .qt(z)'s we consider here. 
It is ea.sy to 
Generalization Error and the Expected Network Complexity 369 
Ea is shown[2] to be bounded as 
� _< o ( , ) , 
where ]?.,,�, called the index of resolvability [2], can be expressed as 
R,,N = rain {11/- I1-0+ (2) 
wO, N 
where .f, is the clipped fn(a:; w) (see [2]). The index of resolvability can be further 
bounded as 
bounded as 
7.([ T' 
where O(.3) and D(rtlo9N) are the bonnds for gpproximation error 
es[iamtion error (variance), respec[ively. 
In addRion, the bound br Eq can be minimized if an additiongl regulariza.tion [erm 
L () is used in the energy hmcion to minimize the number of hidden units, i.e., 
50(toT). 
R.,,,;v _< O(,)+ O( . Therefore, the generalization en'or is 
'- l o 9 N ) 
(3) 
(bias) and 
3 Open Questions and Motivations 
Two open questions, which can not be answered by the previous result, are of the 
primary interest of this work. 
1) How do large networks generalize? 
Tle large networks refer to those with a ratio [ to be somewhat big, where W 
and N are the total number of independently modifiable weights (W  cl, for 
 large) and the nnmber of training samples, respectively. Netvorks trained witIx 
regularization terms ntay fall into this category. Such large networks are found 
o }r. a10]r to generalize well sometimes. Ilowever, when ' is big, the bomd in 
Equation (3) is too loose to loound the actual generalization error meaningfully. 
Therefore. tbr the large networks, the total nnmber of hidden units l may no longer 
be a. good estimate fbr netsyork complexity. Efforts have been made to develop 
measures on effective network complexity both analytically and empirically[I][5][10]. 
These measures depend on training data as well as a regularization term in an 
implicit way which make it dicult to see direct effects of a regularization term on 
generalization error. This naturally leads to our second qnestion. 
2) Is it possi101e to characterize network complexity for a el:,,, of networks using 
only the information given by a regularization term2? Hmv to relate the estimated 
network complexity rigorously with generalization error? 
In practice, when a regularization term (L,,,a, (w)) is used to penalize the ma g,itude 
of weights, it effectively minimizes the number of hidden units as well even tl,,,tlh an 
additional regularization term Lsr0z ) is not used. This is due to the fact th:tt, some 
of the hidden nnits may only operate in the linear region of a sigmoid when their 
This was posed as an open problem by Solla et.al. [8] 
370 Ji 
incoming weights are small and inputs are bounded. Therefore, a L,,,v(w) term ca.n 
effectively act like a L;v(n) term that reduces the effective number of hidden units, 
and thus result in a degenerate parameter space whose degrees of freedom is fewer 
than rid. This fact was not taken into consideration in the previous work, and as 
shovn later in this work, will lead to a tighter bound on 
In vhat follovs, ve will first define the expected network complexity, then use it to 
bound the generalization error. 
4 The Expected Network Complexity 
For reasons that will become apparent, ve choose to define the effective complexity 
of a feed forward two layer network as the expected mtm10er of hidden units Elf 
(1 _< If <_ ,.) which are effectively nonlinear, i.e. operating outside the central 
linear regions of their sigmoid response fnnction g(z). We define the linear region 
as an interval [ z [< b with b a positive constant. 
Consider the presynaptic input _ = w'ZPz to a hidden unit g(z), where w' is the 
incoming weight vector for the unit. Then the unit is considered to be effectively 
linear if[ z {< b for all z  D. This will happen if [ z' {< b, where z' = w'Ta ' with 
x' being any vertex of the unit hypercube D. This is because {z {_< w'T:, where : 
is the vertex of D whose elements are the sgn functions of the elements of w'. 
Next, consider network veights as random variables with a distribution p(w) = 
Aexp(-L,,,A,(w)), which corresponds to a Gibbs distribntion of a regularization 
term with a normalizing consta. ut A. C, ousider the vector :' to 10e a random vector 
also with equally probable l's and -l's. Then ] z' [< b will be a random event. The 
probability for this hidden unit to be effectively nonlineal' equals to 1 - Pr(I z [< b), 
which can be completely determined by the distributions of weights p(w) and z' 
(equally pro10able). Let, If 10e the number of hidden units which are effectively 
nonlinear. Then the probability, Pt(If = k) (1 _< /c' _< n), can be determined 
through a joint probability of k hidden units that are operating beyond the central 
linear region of sigmoid functions. The expected network complexity, EK, can then 
be obtained through Pt(It = k), which is determined by the Gibbs distribution of 
Lz�,,, (w). The motivation on ntilizing such a Gibbs distri10ution comes from the fact 
that/k,N is independent of training samples but dependent of a regularization term 
vhich corresponds to a prior distril0ution of weights. Using such a formulation, as 
will be shown later, the effect of a regularization term on bias and variance can be 
chara
