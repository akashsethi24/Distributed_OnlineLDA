Classification of Electroencephalogram using 
Artificial Neural Networks 
A C Tsoi*, D S C So*, A Sergejew** 
*Department of Electrical Engineering 
**Department of Psychiatry 
University of Queensland 
St Lucia, Queensland 4072 
Australia 
Abstract 
In this paper, we will consider the problem of classifying electroencephalo- 
gram (EEG) signals of normal subjects, and subjects suffering from psychi- 
atric disorder, e.g., obsessive compulsive disorder, schizophrenia, using a 
class of artificial neural networks, viz., multi-layer perceptton. It is shown 
that the multilayer perceptton is capable of classifying unseen test EEG 
signals to a high degree of accuracy. 
1 Introduction 
The spontaneous electrical activity of the brain was first observed by Caton in 1875. 
Although considerable investigations on the electrical activity of the non-human 
brain have been undertaken, it was not until 1929 that a German neurologist Hans 
Berger first published studies on the electroencephalogram (EEG) recorded on the 
scalp of human. He lay the foundation of clinical and experimental applications of 
EEG between 1929 and 1938. 
Since then EEG signals have been used in both clinical and experimental work 
to discover the state which the brain is in (see e.g., Herrmann, 1982, Kolb and 
Whishaw, 1990, Lindsay and Holmes, 1984). It has served as a direct indication of 
any brain activities. It is routinely being used in clinical diagnosis of epilepsy (see 
e.g., Basar, 1980; Cooper, 1980). 
Despite advances in technology, the classification of EEG signals at present requires 
a trained personnel who either eyeballs the direct EEG recordings over time, 
1151 
1152 Tsoi, So, and Sergejew 
or studies the contour maps representing the potentials generated from the raw 
electrical signal (see e.g., Cooper, 1980). This is both a highly skillful job, as well as 
a laborious task for a neurologist. With the current advances in computers, a logical 
question to ask: can we use the computer to perform an automatic classification of 
EEG signals into different classes denoting the psychiatric states of the subjects? 
This type of classification studies is not new. In fact, in the late 1960's there were 
a number of attempts in performing the automatic classification using discriminant 
analysis techniques. However, this work was largely abandoned as most researchers 
concluded that classification based on discriminant techniques does not generalise 
well, i.e., while it has very good classification accuracies in classifying the data which 
is used to train the automatic classification system, it may not have high accuracy 
in classifying the unseen data which are not used to train the system in the first 
instance. 
Recently, a class of classification techniques, called artificial neural network (ANN), 
based on nonlinear models, has become very popular (see e.g., Touretzky, 1989, 
1990, Lippmann et al, 1991). This type of networks claims to be inspired by bi- 
ological neurons, and their many inter-connections. This type of artificial neural 
networks has limited pattern recognition capabilities. Among the many applications 
which have been applied so far are sonar signal classification (see e.g., Touretzky, 
1989), handwritten character recognition (see .e.g., Touretzky, 1990), facial expres- 
sion recognition (see e.g., Lippmann et al. 1991). 
In this paper, we will investigate the possibility of using an ANN for EEG classifica- 
tions. While it is possible to extract features from the time series using either time 
domain or frequency domain techniques, from some preliminary work, it is found 
that the time domain techniques give much better results. 
The structure of this paper is as follows: In section 2, we will give a brief discussion 
on a popular class of ANNs, viz., multi-layer percepttons (MLP). In section 3, we 
will discuss various feature extractions using time domain techniques. In section 4, 
we will present results in classifying a set of unseen EEG signals. 
2 Multi-layer Perceptrons 
Artificial neural network (ANN) consists of a number of artificial neurons inter- 
connected together by synaptic weights to form a network (see e.g, Lippmann, 
1987). Each neuron is modeled by the following mechanical model: 
y -- f(y wixi q- O) (1) 
i=1 
where y is the output of the neuron, wi, i = 1,2,..., n are the synaptic weights, 
xi,i = 1,2...,n are the inputs, and 0 is a threshold function. The nonlinear 
function f(.) can be a sigmoid function, or a hyperbolic tangent function. An ANN 
is a network of inter-connected neurons by synapses (Hertz, Krogh and Palmer, 
1991). 
There are many possible ANN architectures (Hertz, Krogh, Palmer, 1991). A pop- 
Classification of Electroencephalogram Using Artificial Neural Networks 1153 
ular architecture is the multi-layer perceptron (MLP) (see e.g., Lippmann, 1987). 
In this class of ANN, signal travels only in a forward direction. Hence it is also 
known as a feedforward network. Mathematically, it can be described as follows: 
y = f(Az + 
z = f(Bu + 
(2) 
where y is a m x I vector, representing the output of the output layer neurons; z 
is a p x I vector, representing the outputs of the hidden layer neurons; u is a n x 1 
vector, representing the input feature vector; 0y is a m x i vector, known as the 
threshold vector for the output layer neurons; O, is a p x I vector, representing 
the threshold vector for the hidden layer neurons; A and B are matrices of m x p 
and p x n respectively. The matrices A, and B are the synaptic weights connecting 
the hidden layer neuron to the output layer neuron; and the input layer neurons, 
and the hidden layer neurons respectively. For simplicity sake, we will assume the 
nonlinearity function to be a sigmoid function, i.e., 
1 
,f(a) - I q- e -' (4) 
The unknown parameters A, B, Oy, O, can be obtained by minimizing an error cri- 
terion: 
P 
i=1 
(5) 
where P is the total number of examplars, di, i = 1,2,..., P are the desired outputs 
which we wish the MLP to learn. 
By differentiating the error criterion J with respect to the unknown parameters, 
learning algorithms can be obtained. 
The learning rules are as follows: 
A new = A �Id q_ r/A(y)ez ' 
(6) 
where A new is the next estimate of the matrix A, T denotes the transpose of a 
vector or a matrix. r/ is a learning constant. A(y) is a m x rn diagonal matrix, 
whose diagonal elements are f'(yi),i = 1,2,...,m. The vector e is m x 1, and it is 
given by d z' = [(dr - y,),(d2 - y2),...,(din - ym)] T. 
The updating equation for the B matrix is given by the following 
B new = B old q_ rlA(z)Su T 
where  is a p x 1 vector, given by 
(7) 
1154 Tsoi, So, and Sergejew 
5 = A TA(y)e (8) 
and the other parameters are as defined above. 
The threshold vectors can be obtained as follows: 
and 
y 
(9) 
o, = o, �*d + 
(10) 
Thus it is observed that once a set of initial conditions for the unknown parameters 
are given, this algorithm will find a set of parameters which will converge to a value, 
representing possibly a local minimum of the error criterion. 
3 Pre-processing of the EEG signal 
A cursory glance at a typical EEG signal of a normal subject, or a psychiatrically ill 
subject would convince anyone that one cannot hope to distinguish the signal just 
from the raw data alone. Consequently, one would need to perform considerable 
feature extraction (data pre-processing) before classification can be made. There 
are two types of simple feature extraction techniques, viz., frequency domain and 
time domain (see e.g., Kay, 1988, Marple, 1987). In the frequency domain, one 
performs a fast Fourier transform (FFT) on the data. Often it is advantageous 
to modify the signal by a window function. This will reduce the sidelobe leakage 
(Kay, and Marpie, 1981, Harris, 1978). it is possible to use the average spectrum, 
obtained by averaging the spectrum over a number of frames, as the input feature 
vector to the MLP. 
In the time domain, one way to pre-process the data is to fit a parametric model to 
the underlying data. There are a number of parametric models, e.g., autoregressive 
(AR) model, an autoregressive moving average (ARMA) model (see e.g., Kay, 1988, 
Marple, 1987). 
The autoregressive model can be described as follows: 
N 
st = E �tist-i + {;t (11) 
j=l 
where st is the signal at time t; ct is assumed to be a zero mean Gaussian vari- 
able with variance a 2. The unknown parameters or1, j = 1,2,...,N describe the 
spectrum of the signal. They can be obtained by using standard methods, e.g., 
Yule-Walker equations, or Levinson algorithm (Kay, 1988, Marpie, 1987). 
The autoregressive moving average (ARMA) model can be seen as a parsimonious 
model for an AR model with a large N. Hence, as long as we are not concerned 
Classification of Electroencephalogram Using Artificial Neural Networks 1155 
about the interpretation of the AR model obtained, there is little advantage to 
use the more complicated ARMA model. Subsequently, in this paper, we will only 
consider the AR models. 
Once the AR parameters are determined, then they can be used as the input fea- 
tures to the MLP. It is known that the AR parametric model basically produces 
a smoothed spectral envelope (Kay, 1988, Marple, 1987). Thus, the model param- 
eters of AR is another way to convey the spectral information to the MLP. This 
information is different in quality to that given by the FFT technique in that the 
FFT transforms both signal and noise alike, while the parametric models tend to 
favor the signal more and is more effective in suppressing the noise effect. 
In some preliminary work, we find that the frequency domain extracted features do 
not give rise to good classification results using MLP. Henceforth we will consider 
only the AR parameters as input feature vectors. 
4 Classification Results 
In this section, we will summarise the results of the experiments in using the AR 
parametric method of feature extraction 
