Rule Induction through Integrated Symbolic and 
Subsymbolic Processing 
Clayton McMillan, Michael C. Mozer, Paul Smolensky 
Department of Computer Science and 
Institute of Cognitive Science 
University of Colorado 
Boulder, CO 80309-0430 
Abstract 
We describe a neural network, called RuleNet, that learns explicit, sym- 
bolic condition-action rules in a formal string manipulation domain. 
RuleNet discovers functional categories over elements of the domain, 
and, at various points during learning, extracts rules that operate on 
these categories. The rules are then injected back into RuleNet and 
training continues, in a process called iterative projection. By incorpo- 
rating rules in this way, RuleNet exhibits enhanced learning and gener- 
alization performance over alternative neural net approaches. By 
integrating symbolic rule learning and subsymbolic category learning, 
RuleNet has capabilities that go beyond a purely symbolic system. We 
show how this architecture can be applied to the problem of case-role 
assignment in natural language processing, yielding a novel rule-based 
solution. 
1 INTRODUCTION 
We believe that neural networks are capable of more than pattern recognition; they can 
also perform higher cognitive tasks which are fundamentally rule-governed. Further we 
believe that they can perform higher cognitive tasks better if they incorporate rules rather 
than eliminate them. A number of well known cognitive models, particularly of language, 
have been criticized for going too far in eliminating rules in fundamentally rule-governed 
domains. We argue that with a suitable choice of high-level, rule-governed task, represen- 
tation, processing architecture, and learning algorithm, neural networks can represent and 
learn rules involving higher-level categories while simultaneously learning those catego- 
ries. The resulting networks can exhibit better learning and task performance than neural 
networks that do not incorporate rules, have capabilities that go beyond that of a purely 
symbolic rule-learning algorithm. 
969 
970 McMillan, Mozer, and Smolensky 
We describe an architecture, called RuleNet, which induces symbolic condition-action 
rules in a string mapping domain. In the following sections we describe this domain, the 
task and network architecture, simulations that demonstrate the potential for this 
approach, and finally, future directions of the research leading toward more general and 
complex domains. 
2 DOMAIN 
We are interested in domains that map input strings to output strings. A string consists of n 
slots, each containing a symbol. For example, the string abed contains the symbol e in 
slot 3. The domains wc have studied are intrinsically rule-based, meaning that the map- 
ping function from input to output strings can be completely characterized by explicit, 
mutually exclusive condition-action rules. These rules are of the general form if certain 
symbols are present in the input then perform a certain mapping from the input slots to the 
output slots. The conditions do not operate directly on the input symbols, but rather on 
categories defined over thc input symbols. Input symbols can belong to multiple catego- 
ries. For example, the words boy and 9irl. are instances of the higher level category 
I-I. We denote instances with lowercase bold font, and categories with uppercase 
bold font. It should be apparent from context whether a letter string refers to a single 
instance, such as boy, or a string of instances, such as abed. 
Three types of conditions are allowed: 1) a simple condition, which states that an instance 
of some category must be present in a particular slot of the input string, 2) a conjunction of 
two simple conditions, and 3) a disjunction of two simple conditions. A typical condition 
might be that an instance of the category W must be present in slot 1 of the input string and 
an instance of category � must be present in slot 3. 
The action performed by a rule produces an output string in which the content of each slot 
is either a fixed symbol or a function of a particular input slot, with the additional con- 
straint that each input slot maps to at most one output slot. In the present work, this func- 
tion of the input slots is the identity function. A typical action might be to switch the 
symbols in slots 1 and 2 of the input, replace slot 3 with the symbol a, and copy slot 4 of 
the input to the output string unchanged, e.g., abed -- baad. 
We call rules of this general form second-order categorical permutation (SCP) rules. The 
number of rules grows exponentially with the length of the strings and the number of input 
symbols. An example of an SCP rule for strings of length four is: 
if (input is an instance of w and input3 is an instance of �) then 
(outpuQ = input2, output 2 = inputs, output3 = a, output4=input4) 
where input a and output s denote input slot c and output slot rS, respectively. As a short- 
hand for this rule, we write [^  �  2 la4], where the square brackets indicate this is 
a rule, the ^ denotes a conjunctive condition, and the _ denotes a wiMcard symbol. A 
disjunction is denoted by v. 
This formal string manipulation task can be viewed as an abstraction of several interesting 
cognitive models in the connectionist literature, including case-role assignment (McClel- 
land & Kawamoto, 1986), grapheme-phoneme mapping (Sejnowski & Rosenberg, 1987), 
and mapping verb stems to the past tense (Rumelhart & McClelland, 1986). 
Rule Induction through Integrated Symbolic and Subsymbolic Processing 971 
single unit 
layer of units 
complete connectivity 
gating connection 
m condition units 
n pools of v category units 
n pools of u hidden units 
output 
Figure 1: The RuleNet Architecture 
input 
3 TASK 
RuleNet's task is to induce a compact set of rules that accurately characterizes a set of 
training examples. We generate training examples using a predefined rule base. The rules 
are over strings of length four and alphabets which are subsets of {a, b, �, d, e, f, g, 
h, i, j, k, 1}. For example, the rule [v � W �h21] may be used to generate the 
exemplars: 
hedk - kheh, cldk-khlc, gbdj -j hbg, gdbk-*khdg 
where category w consists of a, b, c, d, i, and category � consists of f, g, h. Such 
exemplars form the corpus used to train RuleNet. Exemplars whose input strings meet the 
conditions of several rules are excluded. RuleNet's task is twofold: It must discover the 
categories solely based upon the usage of their instances, and it must induce rules based 
upon those categories. 
The rule bases used to generate examples are minimal in the sense that no smaller set of 
rules could have produced the examples. Therefore, in our simulations the target number 
of rules to be induced is the same as the number used to generate the training corpus. 
There are several traditional, symbolic systems, e.g., COBWEB (Fisher, 1987), that 
induce rules for classifying inputs based upon training examples. It seems likely that, 
given the correct representation, a system such as COBWEB could learn rules that would 
classify patterns in our domain. However, it is not clear whether such a system could also 
learn the action associated with each class. Classifier systems (Booker, et al., 1989) learn 
both conditions and actions, but there is no obvious way to map a symbol in slot ot of the 
input to slot f of the output. We have also devised a greedy combinatoric algorithm for 
inducing this type of rule, which has a number of shortcomings in comparison to RuleNet. 
See McMillan (1992) for comparisons of RuleNet and alternative symbolic approaches. 
4 ARCHITECTURE 
RuleNet can implement SCP rules of the type outlined above. As shown in Figure 1, 
RuleNet has five layers of units: an input layer, an output layer, a layer of category units, a 
layer of condition units, and a layer of hidden units. The operation of RuleNet can be 
divided into three functional components: categorization is performed in the mapping 
from the input layer to the category layer via the hidden units, the conditions are evaluated 
in the mapping from the category layer to the condition layer, and actions are performed in 
972 McMillan, Mozer, and Smolensky 
the mapping from the input layer to the output layer, gated by the condition units. 
The input layer is divided into n pools of units, one for each slot, and activates the cate- 
gory layer, which is also divided into n pools. Input pool ct maps to category pool ct. Units 
in category pool c represent possible categorizations of the symbol in input slot c. One or 
more category units will respond to each input symbol. The activation of the hidden and 
category units is computed with a logistic squashing function. There are rn units in the 
condition layer, one per rule. The activation of condition unit i, Pi, is computed as follows: 
logistic(net i) 
1ogistic(netj) 
The activation pi represents the probability that rule i applies to the current input. The nor- 
malization enforces a soft winner-take-all competition among condition units. To the 
degree that a condition unit wins, it enables a set of weights from the input layer to the out- 
put layer. These weights correspond to the action for a particular rule. There is one set of 
weights, A, for each of the m rules. The activation of the output layer, y, is calculated from 
the input layer, x, as follows: 
m 
y ,. PiAi x 
i 
Essentially, the transformation A i for rule each rule i is applied to the input, and it contrib- 
utes to the output to the degree that condition i is satisfied. Ideally, just one condition unit 
will be fully activated by a given input, and the rest will remain inactive. 
This architecture is based on the local expert architecture of Jacobs, Jordan, Nowlan, and 
Hinton (1991), but is independently motivated in our work by the demands of the task 
domain. RuleNet has essentially the same structure as the Jacobs network, 
