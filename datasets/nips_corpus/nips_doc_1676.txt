Rules and Similarity in Concept Learning 
Joshua B. Tenenbaum 
Department of Psychology 
Stanford University, Stanford, CA 94305 
jbt@psych. stanford. edu 
Abstract 
This paper argues that two apparently distinct modes of generalizing con- 
cepts - abstracting rules and computing similarity to exemplars - should 
both be seen as special cases of a more general Bayesian learning frame- 
work. Bayes explains the specific workings of these two modes - which 
rules are abstracted, how similarity is measured - as well as why gener- 
alization should appear rule- or similarity-based in different situations. 
This analysis also suggests why the rules/similarity distinction, even if 
not computationally fundamental, may still be useful at the algorithmic 
level as part of a principled approximation to fully Bayesian learning. 
1 Introduction 
In domains ranging from reasoning to language acquisition, a broad view is emerging of 
cognition as a hybrid of two distinct modes of computation, one based on applying abstract 
rules and the other based on assessing similarity to stored exemplars [7]. Much support for 
this view comes from the study of concepts and categorization. In generalizing concepts, 
people's judgments often seem to reflect both rule-based and similarity-based computations 
[9], and different brain systems are thought to be involved in each case [8]. Recent psycho- 
logical models of classification typically incorporate some combination of rule-based and 
similarity-based modules [ 1,4]. In contrast to this currently popular modularity position, I 
will argue here that rules and similarity are best seen as two ends of a continuum of possible 
concept representations. In [ 11,12], I introduced a general theoretical framework to account 
for how people can learn concepts from just a few positive examples based on the principles 
of Bayesian inference. Here I explore how this framework provides a unifying explanation 
for these two apparently distinct modes of generalization. The Bayesian framework not only 
includes both rules and similarity as special cases but also addresses several questions that 
conventional modular accounts do not. People employ particular algorithms for selecting 
rules and measuring similarity. Why these algorithms as opposed to any others? People's 
generalizations appear to shift from similarity-like patterns to rule-like patterns in system- 
atic ways, e.g., as the number of examples observed increases. Why these shifts? 
This short paper focuses on a simple learning game involving number concepts, in which 
both rule-like and similarity-like generalizations clearly emerge in the judgments of human 
subjects. Imagine that I have written some short computer programs which take as input a 
natural number and return as output either yes or no according to whether that number 
60 J. B. Tenenbaum 
satisfies some simple concept. Some possible concepts might be z is odd, z is between 
30 and 45, z is a power of 3, or z is less than 10. For simplicity, we assume that only 
numbers under 100 are under consideration. The learner is shown a few randomly chosen 
positive examples - numbers that the program says yes to - and must then identify the 
other numbers that the program would accept. This task, admittedly artificial, nonetheless 
draws on people's rich knowledge of number while remaining amenable to theoretical anal- 
ysis. Its structure is meant to parallel more natural tasks, such as word learning, that often 
require meaningful generalizations from only a few positive examples of a concept. 
Section 2 presents representative experimental data for this task. Section 3 describes a 
Bayesian model and contrasts its predictions with those of models based purely on rules or 
similarity. Section 4 summarizes and discusses the model's applicability to other domains. 
2 The number concept game 
Eight subjects participated in an experimental study of number concept learning, under es- 
sentially the same instructions as those given above [ 11 ]. On each trial, subjects were shown 
one or more random positive examples of a concept and asked to rate the probability that 
each of 30 test numbers would belong to the same concept as the examples observed. X 
denotes the set of examples observed on a particular trial, and n the number of examples. 
Trials were designed to fall into one of three classes. Figure 1 a presents data for two repre- 
sentative trials of each class. Bar heights represent the average judged probabilities that par- 
titular test numbers fall under the concept given one or more positive examples X, marked 
by *s. Bars are shown only for those test numbers rated by subjects; missing bars do not 
denote zero probability of generalization, merely missing data. 
On class I trials, subjects saw only one example of each concept: e.g., X = { 16} and X --- 
{60}. To minimize bias, these trials preceded all others on which multiple examples were 
given. Given only one example, people gave most test numbers fairly similar probabilities 
of acceptance. Numbers that were intuitively more similar to the example received slightly 
higher ratings: e.g., for X = {16}, 8 was more acceptable than 9 or 6, and 17 more than 
87; for X = {60}, 50 was more acceptable than 51, and 63 more than 43. 
The remaining trials each presented four examples and occured in pseudorandom order. 
On class II trials, the examples were consistent with a simple mathematical rule: X = 
{16, 8, 2, 64} or X = {60, 80, 10, 30}. Note that the obvious rules, powers of two and 
multiples often, are in no way logically implied by the data. Multiples of five is a pos- 
sibility in the second case, and even numbers or all numbers under 80 are possibilities 
in both, not to mention other logically possible but psychologically implausible candidates, 
such as all powers of two, except 32 or 4. Nonetheless, subjects overwhelmingly followed 
an all-or-none pattern of generalization, with all test numbers rated near 0 or 1 according to 
whether they satisified the single intuitively correct rule. These preferred rules can be 
loosely characterized as the most specific rules (i.e., with smallest extension) that include 
all the examples and that also meet some criterion of psychological simplicity. 
On class III trials, the examples satisified no simple mathematical rule but did have sim- 
ilar magnitudes: X = { 16, 23, 19, 20} and X = {60, 52, 57, 55}. Generalization now 
followed a similarity gradient along the dimension of magnitude. Probability ratings fell 
below 0.5 for numbers more than a characteristic distance  beyond the largest or smallest 
observed examples - roughly the typical distance between neighboring examples (~ 2 or 
3). Logically, there is no reason why participants could not have generalized according to 
Rules and Similarity in Concept Learning 61 
various complex rules that happened to pick out the given examples, or according to very 
different values off, yet all subjects displayed more or less the same similarity gradients. 
To summarize these data, generalization from a single example followed a weak similarity 
gradient based on both mathematical and magnitude properties of numbers. When several 
more examples were observed, generalization evolved into either an all-or-none pattern de- 
termined by the most specific simple rule, or, when no simple rule applied, a more articu- 
lated magnitude-based similarity gradient falling off with characteristic distance  roughly 
equal to the typical separation between neighboring examples. Similar patterns were ob- 
served on several trials not shown (including one with a different value of ) and on two 
other experiments in quite different domains (described briefly in Section 4). 
3 The Bayesian model 
In [12], I introduced a Bayesian framework for concept learning in the context of learn- 
ing axis-parallel rectangles in a multidimensional feature space. Here I show that the same 
framework can be adapted to the more complex situation of learning number concepts and 
can explain all of the phenomena of rules and similarity documented above. Formally, we 
observe n positive examples X = {z(),..., z ï¿½} of concept C and want to compute 
p(y E CIX), the probability that some new objet y belongs to C' given the observations 
X. Inductive leverage is provided by a hypothesis space 7-/of possible concepts and a prob- 
abilistic model relating hypotheses h to data X. 
The hypothesis space. Elements of7/correspond to subsets of the universe ofobjts that 
are psychologically plausible candidates for the extensions of concepts. Here the universe 
consists of numbers between 1 and 100, and the hypotheses correspond to subsets such as 
the even numbers, the numbers between 1 and 10, etc. The hypotheses can be thought of 
in terms of either rules or similarity, i.e., as potential rules to be abstracted or as features 
entering into a similarity computation, but Bayes does not distinguish these interpretations. 
Bause we can capture only a fraction of the hypotheses people might bring to this task, 
we would like an objetlye way to focus on the most relevant parts of people's hypothesis 
space. One such method is additive clustering (ADCLUS) [6,10], which extracts a set of fea- 
tures that best accounts for subjects' similarity judgments on a given set of objects. These 
features simply correspond to subsets of objects and are thus naturally identified with hy- 
potheses for concept learning. Applications of ADCLUS to similarity judgments for the 
numbers 0-9 reveal two kinds of subsets [6,10]: numbers sharing a common mathemati- 
cal property, such as {2, 4, 8} and {3, 6, 9}, and consecutive numbers of similar magnitude, 
such as {1, 2, 3, 4} and {2, 3, 4, 5, 6}. Applying ADCLUS to the full set of numbers from 
1 to 100 is impractical, but we can construct an analogous hypothesis space for this domain 
based on the two kinds of hypot
