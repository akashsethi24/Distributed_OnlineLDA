S-Map: A network with a simple 
self-organization algorithm for generative 
topographic mappings 
Kimmo Kiviluoto 
Laboratory of Computer and 
Information Science 
Helsinki University of Technology 
P.O. Box 2200 
FIN-02015 HUT, Espoo, Finland 
Ki mmo. Kiviluot oChut. f i 
Erkki Oja 
Laboratory of Computer and 
Information Science 
Helsinki University of Technology 
P.O. Box 2200 
FIN-02015 HUT, Espoo, Finland 
Erkki. Oj ahu. f i 
Abstract 
The S-Map is a network with a simple learning algorithm that com- 
bines the self-organization capability of the Self-Organizing Map 
(SOM) and the probabilistic interpretability of the Generative To- 
pographic Mapping (GTM). The simulations suggest that the S- 
Map algorithm has a stronger tendency to self-organize from ran- 
dom initial configuration than the GTM. The S-Map algorithm 
can be further simplified to employ pure Hebbian learning, with- 
out changing the qualitative behaviour of the network. 
I Introduction 
The self-organizing map (SOM; for a review, see [1]) forms a topographic mapping 
from the data space onto a (usually two-dimensional) output space. The SOM has 
been succesfully used in a large number of applications [2]; nevertheless, there are 
some open theoretical questions, as discussed in [1, 3]. Most of these questions 
arise because of the following two facts: the SOM is not a generative model, i.e. it 
does not generate a density in the data space, and it does not have a well-defined 
objective function that the training process would strictly minimize. 
Bishop et al. [3] introduced the generative topographic mapping (GTM) as a solution 
to these problems. However, it seems that the GTM requires a careful initialization 
to self-organize. Although this can be done in many practical applications, from a 
theoretical point of view the GTM does not yet offer a fully satisfactory model for 
natural or artificial self-organizing systems. 
550 K. Kiviluoto and E. Oja 
In this paper, we first briefly review the SOM and GTM algorithms (section 2); then 
we introduce the S-Map, which may be regarded as a crossbreed of SOM and GTM 
(section 3); finally, we present some simulation results with the three algorithms 
(section 4), showing that the S-Map manages to combine the computational sim- 
plicity and the ability to self-organize of the SOM with the probabilistic framework 
of the GTM. 
2 SOM and GTM 
2.1 The SOM algorithm 
The self-organizing map associates each data vector t with that map unit that has 
its weight vector closest to the data vector. The activations rl of the map units are 
given by 
r/ = {1' when IIt, i- '11 < IIt,j vj # i 
O, otherwise (1) 
where/z i is the weight vector of the ith map unit i, i = 1,...,K. Using these 
activations, the SOM weight vector update rule can be written as 
K 
t.j := + 5 * Q; * - (2) 
i=1 
Here parameter 5 t is a learning rate parameter that decreases with time. The neigh- 
borhood function h(i, j;/t) is a decreasing function of the distance between map 
units i and j;/t is a width parameter that makes the neighborhood function get 
narrower as learning proceeds. One popular choice for the neighborhood function 
is a Gaussian with inverse variance/t. 
2.2 The GTM algorithm 
In the GTM algorithm, the map is considered as a latent space, from which a 
nonlinear mapping to the data space is first defined. Specifically, a point  in the 
latent space is mapped to the point v in the data space according to the formula 
L 
v(�;M) = Mb(�) = E qbj(�)/zj (3) 
j=l 
where qb is a vector consisting of L Gaussian basis functions, and M is a D x L 
matrix that has vectors luj as its columns, D being the dimension of the data space. 
The probability density p(�) in the latent space generates a density to the manifold 
that lies in the data space and is defined by (3). If the latent space is of lower 
dimension than the data space, the manifold would be singular, so a Gaussian noise 
model is added. A single point in the latent space generates thus the following 
density in the data space: 
p(]; M,/) ()D/2 
= exp[-l]v(;M)-,] 2] (4) 
where/ is the inverse of the variance of the noise. 
The key point of the GTM is to approximate the density in the data space by 
assuming the latent space prior p(�) to consist of equiprobable delta functions that 
S-Map 551 
form a regular lattice in the latent space. The centers i of the delta functions are 
called the latent vectors of the GTM, and they are the GTM equivalent to the SOM 
map units. The approximation of the density generated in the data space is thus 
given by 
K 
1 
p({IM,/) =  EP({[i;M,/) (5) 
i=1 
The parameters of the GTM are determined by minimizing the negative log likeli- 
hood error 
�(M,/) - - Eln ip({tl&;M,/) (6) 
t=l i----1 
over the set of sample vectors {{t}. The batch version of the GTM uses the EM 
algorithm [4]; for details, see [3]. One may also resort to an on-line gradient descent 
procedure that yields the GTM update steps 
K 
/t+l .__ t 5t /t 
J '-- J q- E rl(Mt'/t)qJ(i)[{t - v(i;Mt)] (7) 
i----1 
/t+ :=/ +5 t v(Mt,/)ll{t-v(�i;Mt)ll 2 -. (8) 
where /(M,/) is the GTM counterpart to the SOM unit activation, the posterior 
probability p(&lt; M,/) of the latent vector i given data vector t. 
r/ (M,/) = p(&lt;M,/) 
p({tl&;M,/) 
-2iK,= p({tli, ; M, / ) (9) 
exp[-  IIv(&; M) - tl12] 
- Z=x exp[-llv(�i,; M) - tll 
2.3 Connections between SOM and GTM 
Let us consider a GTM that has an equal number of latent vectors and basis func- 
tions  , each latent vector i being the center for one Gaussian basis function bi(). 
Latent vector locations may be viewed as units of the SOM, and consequently the 
basis functions may be interpreted as connection strengths between the units. Let 
us use the shorthand notation b E qj(i). Note that b. -- , and assume that 
the basis functions be normalized so that E=i ' -- E/K=I (' '- 1. 
At the zero-noise limit, or when/ - c, the softmax activations of the GTM given 
in (9) approach the winner-take-all function (1) of the SOM. The winner unit c(t) 
for the data vector t is the map unit that has its image closest to the data vector, 
so that the index c(t) is given by 
c(t)=argminllv(&)-{tll=argminl(kc/.lj) -{tll (10) 
i i j----1 
Note that this choice serves the purpose of illustration only; to use GTM properly, 
one should choose much more latent vectors than basis functions. 
552 K. Kiviluoto and E. Oja 
The GTM weight update step (7) then becomes 
tq-1 t (j [ _ v(c(t);Mt)] 
J ..__ #j q_ (t (t) t 
(11) 
This resembles the variant of SOM, in which the winner is searched with the rule (10) 
and weights axe updated as 
tq-1 t j (t -- #j) 
J 1- lJ q_ (t (t) t 
(12) 
Unlike the original SOM rules (1) and (2), the modified SOM with rules (10) 
and (12) does minimize a well-defined objective function: the $OM distortion mea- 
sure [5, 6, 7, 1]. However, there is a difference between GTM and SOM learning 
rules (11) and (12). With SOM, each individual weight vector moves towards the 
data vector, but with GTM, the image of the winner latent vector v(�(t); M) moves 
towaxds the data vector, and all weight vectors/zj move to the same direction. 
For nonzero noise, when 0 </ < c, there is more difference between GTM and 
SOM: with GTM, not only the winner unit but activations from other units as well 
contribute to the weight update. 
3 S-Map 
Combining the softmax activations of the GTM and the learning rule of the SOM, 
we axrive at a new algorithm: the S-Map. 
3.1 The S-Map algorithm 
The S-Map resembles a GTM with an equal number of latent vectors and basis 
functions. The position of the ith unit on the map is is given by the latent vector 
i; the connection strength of the unit to another unit j is b-, and a weight vector 
/z i is associated with the unit. The activation of the unit is obtained using rule (9). 
The S-Map weights leaxn proportionally to the activation of the unit that the weight 
is associated with, and the activations of the neighboring units: 
:= uj + W- u}) (13) 
i=1 
which can be further simplified to a fully Hebbian rule, updating each weight pro- 
portionally to the activation of the corresponding unit only, so that 
/t+ t 
j :---- j q_ 5t]( t -- ) (14) 
The paxameter/ value may be adjusted in the following way: start with a small 
value, slowly increase it so that the map unfolds and spreads out, and then keep 
increasing the value as long as the error (6) decreases. The parameter adjustment 
scheme could also be connected with the topographic error of the mapping, as 
proposed in [9] for the SOM. 
Assuming normalized input and weight vectors, the dot-product metric form of 
the learning rules (13) and (14) may be written as 
i----1 
(15) 
S-Map 553 
and 
t (16) 
respectively; the matrix in the second penthesis keeps the weight vectors norm- 
ized to unit lenh, sumg a sml ue for the leaning rate paxmeter 5 t [8]. 
The dot-product metric form of a unit tivity is 
exp ] 
which appromates the posterior probability p(iIt;M,) that the data vector 
were generated by that specific unit. This is bed on the obsermtion that if the 
data vectors {t} e normized to unit lenh, the density generated in the data 
space (unit sphere in B ) becomes 
P(li; M, 3) = , constant ] x exp b}/j  
j=l 
(18) 
3.2 S-Map algorithm minimizes the GTM error function in 
dot-product metric 
The GTM error function is the negative log likelihood, which is given by (6) and is 
reproduced here: 
�(M,) = - In ip(ttli;M,) (19) 
When the weights axe updated using a batch version of (15), accumulating the 
updates for one epoch, the expected value of the error [4] for the unit i is 
T 
E(� ew) = - E p�ld(i!t;M, ) lnnew(i)pnew(t[i;M,])] 
t=l r/ta. t =i/K 
T (20) 
k]? ld't k i. new t 
= - . rkjpj ) + terms not involving the weight vectors 
t=l j=l 
The change of the error for the whole map after one e
