Bayesian modelling of fMRI time series 
Pedro A.d. E R. H0jen-S0rensen, Lars K. Hansen and Carl Edward Rasmussen 
Department of Mathematical Modelling, Building 321 
Technical University of Denmark 
DK-2800 Lyngby, Denmark 
phs, lkhansen, carlimm. dtu. dk 
Abstract 
We present a Hidden Markov Model (HMM) for inferring the hidden 
psychological state (or neural activity) during single trial fMRI activa- 
tion experiments with blocked task paradigms. Inference is based on 
Bayesian methodology, using a combination of analytical and a variety 
of Markov Chain Monte Carlo (MCMC) sampling techniques. The ad- 
vantage of this method is that detection of short time learning effects be- 
tween repeated trials is possible since inference is based only on single 
trial experiments. 
1 Introduction 
Functional magnetic resonance imaging (fMRI) is a non-invasive technique that enables 
indirect measures of neuronal activity in the working human brain. The most common 
fMRI technique is based on an image contrast induced by temporal shifts in the relative 
concentration of oxyhemoglobin and deoxyhemoglobin (BOLD contrast). Since neuronal 
activation leads to an increased blood flow, the so-called hemodynamic response, the mea- 
sured fMRI signal reflects neuronal activity. Hence, when analyzing the BOLD signal 
there are two unknown factors to consider; the task dependent neuronal activation and the 
hemodynamic response. Bandettini et al. [1993] analyzed the correlation between a bi- 
nary reference function (representing the stimulus/task sequence) and the BOLD signal. 
In the following we will also make reference to the binary representation of the task as 
the paradigm. Lange and Zeger [1997] discuss a parameterized hemodynamic response 
adapted by a least squares procedure. Multivariate strategies have been pursued in [Wors- 
ley et al. 1997, Hansen et al. 1999]. Several explorative strategies have been proposed 
for finding spatio-temporal activation patterns without explicit reference to the activation 
paradigm. McKeown et al. [1998] used independent component analysis and found sev- 
eral types of activations including components with transient task related response, i.e., 
responses that could not simply be accounted for by the paradigm. The model presented 
in this paper draws on the experimental observation that the basic coupling between the 
net neural activity and hemodynamic response is roughly linear while the relation between 
neuronal response and stimulus/task parameters is often nonlinear [Dale 1997]. We will 
represent the neuronal activity (integrated over the voxel and sampling time interval) by a 
binary signal while we will represent the hemodynamic response as a linear filter of un- 
known form and temporal extent. 
Bayesian Modelling of fMRI Time Series 755 
2 A Bayesian model of fMRI time series 
Let s = {st � t = 0,... , T - 1} be a hidden sequence of binary state variables st 
{0, 1}, representing the state of a single voxel over time; the time variable, t, indexes the 
sequence of fMRI scans. Hence, st is a binary representation of the neural state. The 
hidden sequence is governed by a symmetric first order Hidden Markov Model (HMM) 
with transition probability oz = P(St+l = jlSt = j). We expect the activation to mimic 
the blocked structure of the experimental paradigm so for this reason we restrict oz to be 
larger than one half. The predicted signal (noiseless signal) is given by tt = h. s + 00 + 01 t, 
where � denotes the linear convolution and h is the impulse response of a linear system of 
order Mr. The dc off-set and linear trend which are typically seen in fMRI time series 
are given by 00 and 01, respectively. Finally, it is assumed that the observable is given by 
2 The generative model 
zt = lt + t, where t is iid. Gaussian noise with variance a n. 
considered is therefore given by: 
p(stlst-i,oz) - oZCs,,s,_x + (1 - c0(1 - d,,,_i), 
(,) 
p(zls, an,O, Ms) AC(y, an2I), where y: {Yt}: HO, and z: {zt}. 
Furthermore, d,,_x is the usual Kronecker delta and H = [1, , 3'0s, 3'is, � � �, 3'Ms-is], 
where 1 = (1,...1)', =(1,...,T)'/T and % is a /-step shift operator, that is 3'is = 
(0,..., 0, so, Sl,..., ST-l-i)'. The linear parameters are collected in 0 = (00,0i, 
s 
S 2 
X 1 X 2 
S: ST_ 
-() 
X 3 XT_ 1 
The graphical model. The hidden states 
Xt = (St-l,St-2,...,St_(Ms_l)) have 
been introduced to make the model first or- 
der. 
3 Analytic integration and Monte Carlo sampling 
In this section we introduce priors over the model parameters and show how inference may 
be performed. The filter coefficients and noise parameters may be handled analytically, 
whereas the remaining parameters are treated using sampling procedures (a combination 
of Gibbs and Metropolis sampling). Like in the previous section explicit reference to the 
filter order M s may be omitted to ease the notation. 
The dc off-set 00 and the linear trend 01 are given (improper) uniform priors. The filter 
coefficients are given priors that are uniform on an interval of length  independently for 
each coefficient: 
p(h[Mf ) = {  
for Ihi[<-, O_<i_<Mf-1 
otherwise 
Assuming that all the values of 0 for which the associated likelihood has non-vanishing 
contributions lie inside the box where the prior for 0 has support, we may integrate out the 
filter coefficients via a Gaussian integral: 
f 
P(Zlan'S'Mf): P(ZlO'an'S'Mf)P(OlMI)dO -- M 
exp ( z'z- ' 
2On )' 
756 PA. d. E R. Hojen-Sorensen, L. K. Hansen and C. E. Rasmussen 
We have here defined the mean filter, ts = (HHs)-iHz and mean predicted signal, 
) = Hs, for given state and filter length. We set the interval-length,/ to be 4 times the 
standard deviation of the observed signal z. This is done, since the response from the filter 
should be able to model the signal, for which it is thought to need an interval of plus/minus 
two standard deviations. 
We now proceed to integrate over the noise parameter; using the (improper) non- 
informative Jeffreys prior, p(Crn) oc cr -1 , we get a Gamma integral: 
f T-2% 
p(zls, Mf): p(z[crn,S, MI)p(crn)dcrn = 2 
�(z,z- 
The remaining variables cannot be handled analytically, and will be treated using various 
forms of sampling as described in the following sections. 
3.1 Gibbs and Metropolis updates of the state sequence 
We use a flat prior on the states, p(st = O) = p(st = 1), together with the first order 
Markov property for the hidden states and Bayes' rule to get the conditional posterior for 
the individual states: 
p(st = jls\st,c,M) oc p(st = jlst_l,O)p(st+llSt = j,o)p(zls, Mf). 
These probabilities may (in normalized form) be used to implement Gibbs updates for the 
hidden state variables, updating one variable at a time and sweeping through all variables. 
However, it turns out that there are significant correlations between states which makes it 
difficult for the Markov Chain to move around in the hidden state-space using only Gibbs 
sampling (where a single state is updated at a time). To improve the situation we also 
perform global state updates, consisting of proposing to move the entire state sequence 
one step forward or backward (the direction being chosen at random) and accepting the 
proposed state using the Metropolis acceptance procedure. The proposed movements are 
made using periodic boundary conditions. The Gibbs sweep is computationally involved, 
since it requires computation of several matrix expressions for every state-variable. 
3.2 Adaptive Rejection Sampling for the transition probability 
The likelihood for the transition probability o is derived from the Hidden Markov Model: 
p(slc) 
T-1 
1 aE( ) (1 - a) 
H P($tlst-l'�)--  
t=l 
T-1 
where E(s) = E/=i 6st ,st-x is the number of neighboring states in s with identical values. 
The prior on the transition probabilities is uniform, but restricted to be larger than one half, 
since we expect the activation to mimic the blocked structure of the experimental paradigm. 
It is readily seen that p(o[s) x p(slo ), o E [21-, 1] is log-concave. Hence, we may use 
the Adaptive Rejection Sampling algorithm [Gilks and Wild, 1992] to sample from the 
distribution for the transition probability. 
3.3 Metropolis updates for the filter length 
In practical applications using real fMRI data, we do typically not know the necessary 
length of the filter. The problem of finding the right model order is difficult and has re- 
ceived a lot of attention. Here, we let the Markov Chain sample over different filter lengths, 
effectively integrating out the filter-length rather than trying to optimize it. Although the 
Bayesian Modelling of fMRI Time Series 757 
value of Mf determines the dimensionality of the parameter space, we do not need to use 
specialized sampling methodology (such as Reversible Jump MCMC [Green, 1995]), since 
those parameters are handled analytically in our model. We put a flat (improper) prior on 
Mf and propose new filter lengths using a Gaussian proposal centered on the current value, 
with a standard deviation of 3 (non-positive proposed orders are rejected). This choice of 
the standard deviation only effects the mixing rate of the Markov chain and does not have 
any influence on the stationary distribution. The proposed values are accepted using the 
Metropolis algorithm, using p(Mfls, y) oc p(yl s, Mf ). 
3.4 The posterior mean and uncertainty of the predicted signal 
Since 0 has a flat prior the conditional probability for the filter coefficients is proportional 
to the likelihood p( zlO , .) and by (,) we get: 
D 2  , 
p(Olz, s, an,Mf) ,,,.A/'( sZ, anDD), Ds = (HH)-H' . 
The posterior mean of the predicted signal, 9, is then readily computed as: 
= = = = 
where F -- HD. Here, the average over 0 and an is done analytically, and the average 
over the state and filter length is done using Monte Carlo. The uncertainty in the posterior, 
can also be estimated partly by analytical averaging, and p
