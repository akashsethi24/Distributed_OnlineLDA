The Clusteron: Toward a Simple Abstraction for 
a Complex Neuron 
Bartlett W. Mel 
Computation and Neural Systems 
Division of Biology 
Caltech, 216-76 
Pasadena, CA 91125 
mel@cns.caltech.edu 
Abstract 
Are single neocortical neurons as powerful as multi-layered networks? A 
recent compartmental modeling study has shown that voltage-dependent 
membrane nonlinearities present in a complex dendritic tree can provide 
a virtual layer of local nonlinear processing elements between synaptic in- 
puts and the final output at the cell body, analogous to a hidden layer 
in a multi-layer network. In this paper, an abstract model neuron is in- 
troduced, called a clusteron, which incorporates aspects of the dendritic 
cluster-sensitivity phenomenon seen in these detailed biophysical mod- 
eling studies. It is shown, using a clusteron, that a Hebb-type learning 
rule can be used to extract higher-order statistics from a set of train- 
ing patterns, by manipulating the spatial ordering of synaptic connections 
onto the dendritic tree. The potential neurobiological relevance of these 
higher-order statistics for nonlinear pattern discrimination is then studied 
within a full compartmental model of a neocortical pyramidal cell, using 
a training set of 1000 high-dimensional sparse random patterns. 
I INTRODUCTION 
The nature of information processing in complex dendritic trees has remained an 
open question since the origin of the neuron doctrine 100 years ago. With respect 
to learning, for example, it is not known whether a neuron is best modeled as 
36 Mel 
a pseudo-linear unit, equivalent in power to a simple Perceptron, or as a general 
nonlinear learning device, equivalent in power to a multi-layered network. In an at- 
tempt to characterize the input-output behavior of a whole dendritic tree containing 
voltage-dependent membrane mechanisms, a recent compartmental modeling study 
in an anatomically reconstructed neocortical pyramidal cell (anatomical data from 
Douglas et al., 1991; NEURON simulation package provided by Michael Hines 
and John Moore) showed that a dendritic tree rich in NMDA-type synaptic chan- 
nels is selectively responsive to spatially clustered, as opposed to diffuse, pattens 
of synaptic activation (Mel, 1992). For example, 100 synapses which were simulta- 
neously activated at 100 randomly chosen locations about the dendritic arbor were 
less effective at firing the cell than 100 synapses activated in groups of 5, at each of 
20 randomly chosen dendritic locations. The cooperativity among the synapses in 
each group is due to the voltage dependence of the NMDA channel: Each activated 
NMDA synapse becomes up to three times more effective at injecting synaptic cur- 
rent when the post-synaptic membrane is locally depolarized by 30-40 mV from the 
resting potential. When synapses are activated in a group, the depolarizing effects 
of each helps the others (and itself) to move into this more efficient voltage range. 
This work suggested that the spatial ordering of afferent synaptic connections onto 
the dendritic tree may be a crucial determinant of cell responses to specific input 
patterns. The nonlinear interactions among neighboring synaptic inputs further lent 
support to the idea that two or more afferents that form closely grouped synaptic 
connections on a dendritic tree may be viewed as encoding higher-order input-space 
features to which the dendrite is sensitive (Feldman & Ballard, 1982; Mel, 1990; 
Durbin & Rumelhart, 1990). The more such higher-order features are present in 
a given input pattern, the more the spatial distribution of active synapses will 
be clustered, and hence the more the post-synaptic cell will be inclined to fire in 
response. In a demonstration of this idea through direct manipulation of synaptic 
ordering, dendritic cluster-sensitivity was shown to allow the model neocortical 
pyramidal cell to reliably discriminate 50 training images of natural scenes from 
untrained control images (see Mel, 1992). Since all presented patterns activated the 
same number of synapses of the same strength, and with no systematic variation 
in their dendritic locations, the underlying dendritic discriminant function was 
necessarily nonlinear. 
A crucial question remains as to whether other, e.g. non-synaptic, membrane non- 
linearities, such as voltage-dependent calcium channels in the dendritic shaft mem- 
brane, could enhance, abolish, or otherwise alter the dendritic cluster-sensitivity 
phenomenon seen in the NMDA-only case. Some of the simulations presented in 
the remainder of this paper include voltage-dependent calcium channels and/or an 
anomalous rectification in the dendritic membrane. However, detailed discussions 
of these channels and their effects will be presented elsewhere. 
2 THE CLUSTERON 
2.1 MOTIVATION 
This paper deals primarily with an important extension to the compartmental mod- 
eling experiments and the hand-tuned demonstrations of nonlinear pattern discrimi- 
The Clusteron: Toward a Simple Abstraction for a Complex Neuron 37 
T=4 
'1'=3 
=2 
x. xj 
 =1 
D i Dj 
Figure 1: The Clusteron. Active inputs lines are designated by arrows; shading of 
synapses reflects synaptic activation ai when xi  {0, 1} and weights are set to 1. 
nation capacity presented in (Mel, 1992). If the manipulation of synaptic ordering is 
necessary for neurons to make effective use of their cluster-sensitive dendrites, then 
a learning mechanism capable of appropriately manipulating synaptic ordering must 
also be present in these neurons. An abstract model neuron called a clusteron is 
presented here, whose input-output relation was inspired by the idea of dendritic 
cluster-sensitivity, and whose learning rule is a variant of simple Hebbian learning. 
The clusteron is a far simpler and more convenient model for the study of cluster- 
sensitive learning than the full-scale compartmental model described in (Mel, 1992), 
whose solutions under varying stimulus conditions are computed through numerical 
integration of a system of several hundred coupled nonlinear differential equations 
(Hines, 1989). However, once the basic mathematical and algorithmic issues have 
been better understood, more biophysically detailed models of this type of learning 
in dendritic trees, as has been reported in (Brown et al., 1990), will be needed. 
2.2 INPUT-OUTPUT BEHAVIOR 
The clusteron is a particular second-order generalization of the thresholded linear 
unit (TLU), exemplified by the common Perceptton. It consists of a cell body 
where the globally thresholded output of the unit is computed, and a dendritic tree, 
which for present purposes will be visualized as a single long branch attached to the 
cell body (fig. 1). The dendritic tree receives a set of N weighted synaptic contacts 
from a set of afferent axons. All synaptic contacts are excitatory. The output of 
the dusteron is given by 
N 
y = g(Y. ai), (1) 
i--1 
where ai is the net excitatory input at synapse i and g is a thresholding nonlinearity. 
Unlike the TLU, in which the net input due to a single input line i is wixi, the net 
38 Mel 
input at a clusteron synapse i with weight wi is given by, 
a, = wjxj), (2) 
j 
where xi is the direct input stimulus intensity at synapse i, as for the TLU, and 
Di - {i- r,... i,..., i + r} represents the neighborhood of rdius r round synapse 
i. It my be noted that the weight on ech second-order erm is constrained to 
be the product of elemental weights wiwj, such that he clusteron hs only N 
underlying degrees of freedom s compared to N  possible in  full second-order 
model. For the simplest cse of xi  0, 1) nd 11 weights set to 1, equation 2 
sys tlmt the excitetory contribution of ech cive synapse is equl to he number 
of coctive synpses within its neighborhood. A synapse that is ciwted alone 
in its neighborhood thus provides a net excitetory input of ai  1; wo synpses 
ctivted near to ech other ech provide  net excitetory input of ai  aj  2, 
etc. The biophysicl inspiration for the mukiplicfive relation in (2) is that, 
the net injected current through  region of vokge-dependent dendrkic membrane 
cn, under mny circumstances, grow fster thn linearly with increasing synoptic 
input to that region. Unlike the dendritic membrane modeled t the biophysicl 
level, however, the clusteron in its current definition does not contain ny sturting 
nonlinerkies in the dendriVes. 
2.3 THE LEARNING PROBLEM 
The learning problem of present interest is that of two-category classification. A 
pattern is a sparse N-element vector, where each component is a boolean random 
variable equal to 1 with probability p, and 0 otherwise. Let T = {tl,t2,... ,tp} be 
a training set consisting of P randomly chosen patterns. The goal of the classifier 
is to respond with y = I to any pattern in T, and y = 0 to all other control 
patterns with the same average bit density p. Performance at this task is measured 
by the probability of correct classification on a test set consisting of equal numbers 
of training and control patterns. 
2.4 THE LEARNING RULE 
Learning in the clusteron is the process by which the ordering of synaptic connec- 
tions onto the dendrite is manipulated. Second-order features that are statistically 
prominent in the training set, i.e. pairs of pattern components that are coactivated 
in the training set more often than average, can become encoded in the clusteron 
as pairs of synaptic connections within the same dendritic neighborhood. 
Learning proceeds as follows. Each pattern in T is presented once to the clusteron 
in a random sequence, constituting one training epoch. At the completion of each 
training epoch, each synapse i whose activation averaged over the training set 
P 
1 a? ) 
p=l 
falls below threshold 0, is switched with another randomly chosen subthreshold 
N 
 'i=x < al >, i.e. 
synapse. The threshold can, for ex
