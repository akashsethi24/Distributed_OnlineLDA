Features as Sufficient Statistics 
D. Geiger * 
Department of Computer Science 
Courant Institute 
and Center for Neural Science 
New York University 
geigercs. nyu. edu 
A. Rudra ? 
Department of Computer Science 
Courant Institute 
New York University 
archi,cs. nyu. edu 
L. Maloney; 
Departments of Psychology and Neural Science 
New York University 
ltmcns. nyu. edu 
Abstract 
An image is often represented by a set of detected features. We get 
an enormous compression by representing images in this way. Fur- 
thermore, we get a representation which is little affected by small 
amounts of noise in the image. However, features are typically 
chosen in an ad hoc manner. We show how a good set of fea- 
tures can be obtained using sufficient statistics. The idea of sparse 
data representation naturally arises. We treat the 1-dimensional 
and 2-dimensional signal reconstruction problem to make our ideas 
concrete. 
I Introduction 
Consider an image, I, that is the result of a stochastic image-formation process. The 
process depends on the precise state, f, of an environment. The image, accordingly, 
contains information about the environmental state f, possibly corrupted by noise. 
We wish to choose feature vectors (I) derived from the image that summarize this 
information concerning the environment. We are not otherwise interested in the 
contents of the image and wish to discard any information concerning the image 
that does not depend on the environmental state f. 
*Supported by NSF grant 5274883 and AFOSR grants F 49620-96-1-0159 and F 49620- 
96-1-0028 
Partially supported by AFOSR grants F 49620-96-1-0159 and F 49620-96-1-0028 
tSupported by NIH grant EY08266 
Features as Sufficient Statistics 795 
We develop criteria for choosing sets of features (based on information theory and 
statistical estimation theory) that extract from the image precisely the information 
concerning the environmental state. 
2 Image Formation, Sufficient Statistics and Features 
As above, the image I is the realization of a random process with distribution 
Pznvronment(f). We are interested in estimating the parameters f of the environ- 
mental model given the image (compare [4]). We assume in the sequel that f, the 
environmental parameters, are themselves a random vector with known prior dis- 
tribution. Let b(I) denote a feature vector derived from the the image I. Initially, 
we assume that b(I) is a deterministic function of I. 
For any choice of random variables, X, Y, define[2] the mutual information of X 
and Y to be M(X;Y) = x,yP(X,Y)lï¿½g 
P(X)P(Y)' The information about 
the environmental parameters contained in the image is then M(f;I), while the 
information about the environmental parameters contained in the feature vector 
b(I) is then M(f; qb(I)). As a consequence of the data processing inequality[2], 
M(f;c(I)) _< M(f;I). 
A vector b(I), 'of features is defined to be su]ficient if the inequality above is an 
equality. We will use the terms feature and statistic interchangeably. The definition 
of a sufficient feature vector above is then just the usual definition of a set of jointly 
su.ficient statistics[2]. 
To summarize, a feature vector q(I) captures all the information about the envi- 
ronmental state parameters f precisely when it is sufficent. 1 
Graded Sufficiency: A feature vector either is or is not sufficient. For every 
possible feature vector b(I), we define a measure of its failure to be sufficent: 
Surf(b(I)) = M (f; I) - M (f; q(I)). This su.ficency measure is always non-negative 
and it is zero precisely when b is sufficient. We wish to find feature vectors b(I) 
where Suff(q(I)) is close to 0. We define b(I) to be e-sufficient if Suff((I)) _ e. In 
what follows, we will ordinarily say sufficient, when we mean e-sufficient. 
The above formulation of feature vectors as jointly sufficient statistics, maximiz- 
ing the mutual information, M(f, qb(I)), can be expressed as the Kullback-Leibler 
distance between the conditional distributions, P(f[I) and 
E[D(P(fII) II P(flqb(I)))] = M(f;I)- M(f;qb(I)), (1) 
where the symbol E denotes the expectation with respect to I, D denotes the 
Kullback-Leibler (K-L) distance, defined by D(fllg)- f(x)log(f(x)/g(x)) 
Thus, we seek feature vectors b(I) such that the conditional distributions, P(flI) 
and P(fl((I)) in the K-L sense, averaged across the set of images. However, this 
optimization for each image could lead to over-fitting. 
3 Sparse Data and Sufficient Statistics 
The notion of sufficient statistics may be described by how much data can be re- 
moved without increasing the K-L distance between P(f[4(I)) and P(f]I). Let us 
An information-theoretic framework has been adopted in neural networks by others; 
e.g., [5] [9][6] [1][8]. However, the connection between features and sufficiency is new. 
2We won't prove the result here. The proof is simple and uses the Markov chain 
property to say that P(f, I, qb(I)) = P(I, qb(I))P(flI, qb(I)) = P(I)P(flI). 
796 D. Geiser, A. Rudra and L. T. Maloney 
formulate the approach more precisely, and apply two methods to solve it. 
3.1 Gaussian Noise Model and Sparse Data 
We are required to construct P(f]I) and P(f]b(I)). Note that according to Bayes' 
rule P(f]b(I)) -- P(b(I)]f)P(f)/P(b(I)). We will assume that the form of 
the model P(f) is known. In order to obtain P(q3(I)]f) we write P(b(I)]f) - 
Computing P(fl q(I)): Let us first assume that the generative process of the image 
I, given the model f, is Gaussian i.i.d. , i.e., P(I]f) = 
where i = 0, 1, ..., N- 1 are the image pixel index for an image of size N. Fur- 
ther, P(Ii]fi) is a function of (Ii - fi) and Ii varies from -co to +co, so that the 
normalization constant does not depend on fi. Then, P(f]I) can be obtained by 
normalizing P(f)P(I]f). 
i 
where Z is the normalization constant. 
Let us introduce a binary decision variable si = O, 1, which at every image pixel i 
decides if that image pixel contains important information or not regarding the 
model f. Our statistic  is actually a (multivariate) random variable generated 
from I according to 
/(1 - si) e- 
This distribution gives i = Ii with probability 1 (Dirac delta function) when si = 0 
(data is kept) and gives bi uniformly distributed otherwise (si = 1, data is removed). 
We then have 
 - - 2.= (li-O) 
= II o. s,) -,-. 
i 2ro' e 
The conditional distribution of b on f satisfies the properties that we mentioned in 
connection with the posterior distribution of f on I. Thus, 
rs(f[b) = (1/Zs)r(f)(He-2'%A? (fi-ll)2(1-si)) (2) 
i 
where Z, is a normalization constant. 
It is also plausible to extend this model to non-Gaussian ones, by simply modifying 
the quadratic term (fi - Ii)  and keeping the sparse data coefficient (1 - si). 
3.2 Two Methods 
We can now formulate the problem of finding a feature-set, or finding a sufficient 
statistics, in terms of the variables si that can remove data. More precisely, we can 
find s by minimizing 
Features as Sufficient Statistics 797 
Z(s,I) = D(P(fl I) IIPs(fl(I))) + A (x - 
i 
(3) 
It is clear that the K-L distance is minimized when si = 0 everywhere and all the 
data is kept. The second term is added on to drive the solution towards a minimal 
sufficient statistic, where the parameter A has to be estimated. Note that, for A 
very large, all the data is removed (si = 1), while for A = 0 all the data is kept. 
We can further write (3) as 
Z(s,I) -  P(flI) log(P(flI)/Ps(fl(I))) + A (! - sd 
! 
( ----l--(!i-ll)2(1--(1--sl))) 
- _P(flI)log (Z/Z) 11 
_ , + 
! i 
s, 
= to9 _ ,[ _, (f, _ ,)2)] +  (x _ s,). 
i i 
where Ep[.] denotes the expectation taken with respect to the distribution P. 
If we let si be a continuous variable the minimum E(s, I) will occur when 
as _ (r,[(f _ i)2]_ r[(A - )]) - A (4) 
0 - Os'--. ' 
We note that the Hessian matrix 
c92E -- Ep.[(fi - /-i)2(fj _/-j)2]_ EPo[(fi - Ii)2]Ep.[(f - Ij)2], (5) 
Hs[i,j]- OsiOsy 
is a correlation matrix, i.e., it is positive semi-definite. Consequently, E(s) is convex. 
Continuation Method on A: 
In order to solve for the optimal vector s we consider the continuation method on 
the parameter A. We know that s = 0, for A = 0. Then, taking derivatives of (4) 
with respect to A, we obtain 
OE Os OE Osj 
E OSiOSj OA O/OSi -- 0 -- O/ -- E H-l[i'J]' 
It was necessary the Hessian to be invertible, i.e., the continuation method works 
because E is convex. The computations are expected to be mostly spent on esti- 
mating the Hessian matrix, i.e., on computing the averages Zp,[(fi- Ii)a(fj - I)2], 
Zp,[(fi- Ii)2], and Zpo[(fj -iy)2]. Sometimes these averages can be exactly com- 
puted, for example for one dimensional graph lattices. Otherwise these averages 
could be estimated via Gibbs sampling. 
The above method can be very slow, since these computations for Hs have to be 
repeated at each increment in A. We then investigate an alternative direct method. 
A Direct Method: 
Our approach seeks to find a large set of si = I and to maintain a distribution 
Ps(flb(I)) close to P(flI), i.e., to remove as many data points as possible. For this 
798 D. Geiser, A. Rudra and L T. Maloney 
o 
o 
,=, 0'4 h 
10 
3O 4O 5O 
& vdu# lot I, fred$4 
i 0.1s i 
60 0 10 
o ,o ,o ,o ,o (b) 
(a) 
20 30 4 0 0 
20 30 4 0 0 
 0.7  , 
0 10 20 30 40 50 60 
Figure 1: (a). Complete results for step edge showing the image, the effective 
variance and the computed s-value (using the continuation method). (b) Complete 
results for step edge with added noise. 
goal, we can investigate the marginal distribution 
where Pe//(fi) is an effective marginal distribution that depends on all the other 
values of I besides the one at pixel i. 
How to decide if si - 0 or si = I directly from this marginal distribution 
The entropy of the first term Hz (fi) - f dfiP&(fi)logP& (fi) indicates how much 
fi is conditioned by the data. The larger the entropy the less the data co
