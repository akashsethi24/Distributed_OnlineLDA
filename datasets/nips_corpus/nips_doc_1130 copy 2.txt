Gradient and Hamiltonian Dynamics 
Applied to Learning in Neural Networks 
James W. Howse 
Chaouki T. Abdallah 
Gregory L. Heileman 
Department of Electrical and Computer Engineering 
University of New Mexico 
Albuquerque, NM 87131 
Abstract 
The process of machine learning can be considered in two stages: model 
selection and parameter estimation. In this paper a technique is presented 
for constructing dynamical systems with desired qualitative properties. The 
approach is based on the fact that an rt-dimensional nonlinear dynamical 
system can be decomposed into one gradient and (rt - 1) Hamiltonian sys- 
tems. Thus, the model selection stage consists of choosing the gradient and 
Hamiltonian portions appropriately so that a certain behavior is obtainable. 
To estimate the parameters, a stably convergent learning rule is presented. 
This algorithm has been proven to converge to the desired system trajectory 
for all initial conditions and system inputs. This technique can be used to 
design neural network models which are guaranteed to solve the trajectory 
learning problem. 
I Introduction 
A fundamental problem in mathematical systems theory is the identification of dy- 
namical systems. System identification is a dynamic analogne of the functional ap- 
proximation problem. A set of input-output pairs (u(t), y(t)) is given over some time 
interval t E IT/, 7'f]. The problem is to find a model which for the given input sequence 
returns an approximation of the given output sequence. Broadly speaking, solving an 
identification problem involves two steps. The first is choosing a class of identifica- 
tion models which are capable of emulating the behavior of the actual system. The 
second is selecting a method to determine which member of this class of models best 
emulates the actual system. In this paper we present a class of nonlinear models and 
a learning algorithm for these models which are guaranteed to learn the trajectories 
of an example system. Algorithms to learn given trajectories of a continuous time 
system have been proposed in [6], [8], and [7] to name only a few. To our knowledge, 
no one has ever proven that the error between the learned and desired trajectories 
vanishes for any of these algorithms. In our trajectory learning system this error is 
guaranteed to vanish. Our models extend the work in [1] by showing that Cohen's 
systems are one instance of the class of models generated by decomposing the dynam- 
ics into a component normal to some surface and a set of components tangent to the 
same surface. Conceptually this formalism can be used to design dynamical systems 
with a variety of desired qualitative properties. Furthermore, we propose a provably 
convergent learning algorithm which allows the parameters of Cohen's models to be 
learned from examples rather than being programmed in advance. The algorithm is 
Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks 2 75 
convergent in the sense that the error between the model trajectories and the de- 
sired trajectories is guaranteed to vanish. This learning procedure is related to one 
discussed in [5] for use in linear system identification. 
2 Constructing the Model 
First some terminology will be defined. For a system of rt first order ordinary differ- 
ential equations, the phase space of the system is the a-dimensional space of all state 
components. A solution trajectory is a curve in phase space described by the differ- 
ential equations for one specific starting point. At every point on a trajectory there 
exists a tangent vector. The space of all such tangent vectors for all possible solution 
trajectories constitutes the vector field for this system of differential equations. 
The trajectory learning models in this paper are systems of first order ordinary dif- 
ferential equations. The form of these equations will be obtained by considering the 
system dynamics as motion relative to some surface. At each point in the state space 
an arbitrary system trajectory will be decomposed into a component normal to this 
surface and a set of components tangent to this surface. This approach was suggested 
to us by the results in [4], where it is shown that an arbitrary a-dimensional vector 
field can be decomposed locally into the sum of one gradient vector field and (rt - 1) 
Hamiltonian vector fields. The concept of a potential function will be used to de- 
fine these surfaces. A potential function V(x) is any scalar valued function of the 
system states a: = [x,x2,... ,xt] t which is at least twice continuously differentiable 
(i.e. V(a:) E C r � r >_ 2). The operation [.It denotes the transpose of the vector. If 
there are rt components in the system state, the function V(a:), when plotted with 
respect all of the state components, defines a surface in an (rt + 1)-dimensional space. 
There are two curves passing through every point on this potential surface which are 
of interest in this discussion, they are illustrated in Figure l(a). The dashed curve is 
- v.v()l.o =o 
0.. c 
2 
(a) (b) 
Figure 1: (a) The potential function V(x) = x (x -1) 2 +x plotted versus its two depen- 
dent variables x and x2. The dashed curve is called a level surface and is given 
by V(x) = 0.5. The solid curve follows the path of steepest descent through 0. 
(b) The partitioning of a 3-dimensional vector field at the point 0 into a 1- 
dimensional portion which is normal to the surface V() =/C and a 2-dimensional 
portion which is tangent to V () =/C. The vector -VV(x)I0 is the normal vec- 
tor to the surface V(a:) =/C at the point a:0. The plane (a:- a:0) t VV(a:)[ 0 = 0 
contains all of the vectors which are tangent to V (a:) =/C at a:0. Two linearly 
independent vectors are needed to form a basis for this tangent space, the pair 
Q2(a:) VV(a:)[ 0 and Qa(a:) VV(a:)[ 0 that are shown are just one possibility. 
referred to as a level surface, it is a surface along which V(a:) =/C for some constant 
K. Note that in general this level surface is an n-dimensional object. The solid curve 
2 76 J.W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN 
moves downhill along V(a) following the path of steepest descent through the point 
a:0. The vector which is tangent to this curve at a:0 is normal to the level surface 
at a:0. The system dynamics will be designed as motion relative to the level surfaces 
of V(a). The results in [4] require rt different local potential functions to achieve 
arbitrary dynamics. However, the results in [1] suggest that a considerable number 
of dynamical systems can be achieved using only a single global potential function. 
A system which is capable of traversing any downhill path along a given potential 
surface V(a:), can be constructed by decomposing each element of the vector field 
into a vector normal to the level surface of V(a:) which passes through each point 
and a set of vectors tangent to the level surface of V(a:) which passes through the 
same point. So the potential function V(a) is used to partition the rt-dimensional 
phase space into two subspaces. The first contains a vector field normal to some 
level surface V(a:) = /C for /C E 1 while the second subspace holds a vector field 
tangent to V() = /C. The subspace containing all possible norma] vectors to the 
rt-dimensional level surface at a given point, has dimension one. This is equivalent 
to the statement that every point on a smooth surface has a unique normal vector. 
Similarly, the subspace containing all possible tangent vectors to the level surface at 
a given point has dimension (rt - 1). An example of this partition in the case of a 
3-dimensional system is shown in Figure l(b). Since the space of all tangent vectors 
at each point on a level surface is (rt - 1)-dimensional, (rt - 1) linearly independent 
vectors are required to form a basis for this space. 
Mathematically, there is a straightforward way to construct dynamical systems which 
either move downhill along V() or remain at a constant height on V(). In this 
paper, dynamical systems which always move downhill along some potential surface 
are called gradient-like systems. These systems are defined by differential equations 
of the form 
 = -P(a:) X7=V(), (1) 
where P(a) is a matrix function which is symmetric (i.e. pt = p) and positive 
definite at every point a:, and where X7_V(a:) - [o�V ov ov It. These systems 
' 0z2 '' ' 
are similar to the gradient flows discussed in [2]. The trajectories of the system 
formed by Equation (1) always move downhill along the potential surface defined by 
V(x). This can be shown by taking the time derivative of V(x) which is /(x) = 
-[VV()] t P()[VV(x)] _( 0. Because P(x) is positive definite, /(x) can only be 
zero where 7=V(x) - 0, elsewhere (x) is negative. This means that the trajectories 
of Equation (1) always move toward a level surface of V(x) formed by slicing V(x) 
at a lower height, as pointed out in [2]. It is also easy to design systems which remain 
at a constant height on V(x). Such systems will be denoted Hamiltonjan-like systems. 
They are specified by the equation 
k = Q(x) V=V(x), (2) 
where Q(x) is a matrix function which is skew-symmetric (i.e. Qt = _Q) at every 
point a:. These systems are similar to the Hamiltonian systems defined in [2]. The 
elements of the vector field defined by Equation (2) are always tangent to some level 
surface of V (a:). Hence the trajectories of this system remain at a constant height on 
the potential surface given by .V(a). Again this is indicated by the time derivative 
of V(), which in this case is V() = [V=V()] t Q(x)[V=V()] = 0. This indicates 
that the trajectories of Equation (2) always remain on the level surface on which the 
system starts. So a model which can follow an arbitrary downhill path along the 
potential surface V(x) can be designed by combining the dynamics of Equations (1) 
and (2). The dynamics in the subspace normal t
