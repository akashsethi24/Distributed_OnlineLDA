Neural Network Modeling of Speech and Music 
Signals 
Axel R6bel 
Technical University Berlin, Einsteinufer 17, Sekr. EN-8, 10587 Berlin, Germany 
Tel: +49-30-314 25699, FAX: +49-30-314 21143, email: roebel@kgw. tu-berlin.de 
Abstract 
Time series prediction is one of the major applications of neural net- 
works. After a short introduction into the basic theoretical foundations 
we argue that the iterated prediction of a dynamical system may be in- 
terpreted as a model of the system dynamics. By means of RBF neural 
networks we describe a modeling approach and extend it to be able to 
model instationary systems. As a practical test for the capabilities of the 
method we investigate the modeling of musical and speech signals and 
demonstrate that the model may be used for synthesis of musical and 
speech signals. 
1 Introduction 
Since the formulation of the reconstruction theorem by Takens [10] it has been clear that 
a nonlinear predictor of a dynamical system may be directly derived from a systems time 
series. The method has been investigated extensively and with good success for the pre- 
diction of time series of nonlinear systems. Especially the combination of reconstruction 
techniques and neural networks has shown good results [ 12]. 
In our work we extend the ideas of predicting nonlinear systems by the more demanding 
task of building system models, which are able to resynthesize the systems time series. In 
the case of chaotic or strange attractors the resynthesis of identical time series is known to 
be impossible. However, the modeling of the underlying attractor leads to the possibility 
to resynthesis time series which are consistent with the system dynamics. Moreover, the 
models may be used for the analysis of the system dynamics, for example the estimation 
of the Lyapunov exponents [6]. In the following we investigate the modeling of music and 
speech signals, where the system dynamics are known to be instationary. Therefore, we 
780 A. Riibel 
develop an extension of the modeling approach, such that we are able to handle instationary 
systems. 
In the following, we first give a short review concerning the state space reconstruction from 
time series by delay coordinate vectors, a method that has been introduced by Takens [ 10] 
and later extended by Sauer et al. [9]. Then we explain the structure of the neural net- 
works we used in the experiments and the enhancements necessary to be able to model 
instationary dynamics. As an example we apply the neural models to a �axophone tone 
and a speech signal and demonstrate that the signals may be resynthesized using the neural 
models. Furthermore, we discuss some of the problems and outline further developments 
of the application. 
2 Reconstructing attractors 
Assume an n-dimensional dynamical system f(.) evolving on an attractor A. A has frac- 
tal dimension d, which often is considerably smaller then n. The system state ' is ob- 
served through a sequence of measurements h(z-'), resulting in a time series of measure- 
ments yt = h((t)). Under weak assumptions concerning h(.) and f(.) the fractal embed- 
ding theorem[9] ensures that, for D > 2d, the set of all delayed coordinate vectors 
YD,T = {t > to: (Yt,Yt-T,...,Yt-(D-t)T)}, (1) 
with an arbitrary delay time T, forms an embedding of A in the D-dimensional recon- 
struction space. We call the minimal D, which yields an embedding of A, the embedding 
dimension D,. Because an embedding preserves characteristic features of A, especially it 
is one to one, it may be employed for building a system model. For this purpose the recon- 
struction of the attractor is used to uniquely identify the systems state thereby establishing 
the possibility of uniquely predicting the systems evolution. The prediction function may 
be represented by a hyperplane over the attractor in an (D + 1) dimensional space. By 
iterating this prediction function we obtain a vector valued system model which, however, 
is valid only at the respective attractor. 
For the reconstruction of instationary systems dynamics we confine ourselves to the case 
of slowly varying parameters and model the instationary system using a sequence of attrac- 
tors. 
3 RBF neural networks 
There are different topologies of neural networks that may be employed for time series 
modeling. In our investigation we used radial basis function networks which have shown 
considerably better scaling properties, when increasing the number of hidden units, than 
networks with sigmoid activation function [8]. As proposed by Verleysen et. al [11] we 
initialize the network using a vector quantization procedure and then apply backpropaga- 
tion training to finally tune the network parameters. The tuning of the parameters yields 
an improvement factor of about ten in prediction error compared to the standard RBF net- 
work approach [8, 3]. Compared to earlier results [7] the normalization of the hidden layer 
activations yields a small improvement in the stability of the models. 
The resulting network function for m-dimensional vector valued output is of the form 
exp 
(�) = y v7j y]iexp(_C_)2 ) + g, (2) 
Neural Network Modeling of Speech and Music Signals 781 
x(i+ 
k(i) 
Control 
! 
x(i-3T) x(i- T) 
x(i-2T) x(i) 
Fig. 1' Input/Output structure of the neural model. 
where o- 3 represents the standard deviation of the Gaussian, the input g and the centers 
E are -dimensional vectors and  and j are m-dimensional parameters of the network. 
Networks of the form eq. (2) with a finite number of hidden units are able to approximate 
arbitrary closely all continuous mappings R n --> R TM [4]. This universal approximation 
property is the foundation of using neural networks for time series modeling, where we 
denote them as neural models. In the context of the previous section the neural models are 
approximating the systems prediction function. 
To be able to represent instationary dynamics, we extend the network according to figure 1 
to have an additional input, that enables the control of the actual mapping 
This model is close to the Hidden Control Neural Network described in [2]. From the uni- 
versal approximation properties of the RBF-networks stated above it follows, that eq. (3) 
with appropriate control sequence k(i) is able to approximate any sequence of functions. 
In the context of time series prediction the value i represents the actual sample time. The 
control sequence may be optimized during training, as described in [2], The optimization 
of k (i) requires prohibitively large computational power if the number of different control 
values, the domain of k is large. However, as long as the systems instationarity is described 
by a smooth function of time, we argue that it is possible to select k(i) to be a fixed linear 
function of i. With the preselected k(i) the training of the network adapts the parameters 
tj and o'tj such that the model evolution closely follows the systems instationarity. 
4 Neural models 
As is shown in figure 1 we use the delayed coordinate vectors and a selected control se- 
quence to train the network to predict the sequence of the following T time samples. The 
782 A. Riibel 
vector valued prediction avoids the need for a further interpolation of the predicted sam- 
ples. Otherwise, an interpolation would be necessary to obtain the original sample fre- 
quency, but, because the Nyquist frequency is not regarded in choosing T, is not straight- 
forward to achieve. 
After training we initialize the network input with the first input vector (x-'o, k(O)) of the 
time series and iterate the network function shifting the network input and using the latest 
output unit to complete the new input. The control input may be copied from the training 
phase to resynthesize the training signal or may be varied to emulate another sequence of 
system dynamics. 
The question that has to be posed in this context is concerned with the stability of the 
model. Due to the prediction error of the model the iteration will soon leave the recon- 
structed attractor. Because there exists no training data from the neighborhood of the at- 
tractor the minimization of the prediction error of the network does not guaranty the sta- 
bility of the model [5]. Nevertheless, as we will see in the examples, the neural models are 
stable for at least some parameters D and T. 
Due to the high density of training data the method for stabilizing dynamical models pre- 
sented in [5] is difficult to apply in our situation. Another approach to increase the model 
stability is to lower the gradient of the prediction function for the directions normal to the 
attractor. This may be obtained by disturbing the network input during training with a 
small noise level. While conceptually straightforward, we found that this method is only 
partly successful. While the resulting prediction function is smoother in the neighborhood 
of the attractor, the prediction error for training with noise is considerably higher as ex- 
pected from the noise free results, such that the overall effect often is negative. To circum- 
vent the problems of training with noise further investigations will consider a optimization 
function with regularization that directly penalizes high derivatives of the network with 
respect to the input units [1]. The stability of the models is a major subject of further re- 
search. 
5 Practical results 
We have applied our method to two acoustic time series, a single saxophone tone, consist- 
ing of 16000 samples sampled at 32kHz and a speech signal of the word manna  . The latter 
time series consists of 23000 samples with a sampling rate of 44.1kHz. Both time series 
have been normalized to stay within the interval [- 1, 1]. The estimation of the dimension 
of the underlying attractors yields a dimension of about 2-3 in both cases. 
We chose the control input k(i) to be linear increasing from -0.8 to 0.8. Stable models 
we found for both ti
