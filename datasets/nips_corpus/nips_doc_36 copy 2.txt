5O5 
CONNECTING TO THE PAST 
Bruce A. MacDonald, Assistant Professor 
Knowledge Sciences Laboratory, Computer Science Department 
The University of Calgary, 2500 University Drive NW 
Calgary, Alberta T2N 1N4 
ABSTRACT 
lecently there has been renewed interest in neural-like processing systems, evidenced for ex- 
ample in the two volumes Parallel Distributed Processing edited by Pumelhart and McClelland, 
and discussed as parallel distributed systems, connectionist models, neural nets, value passing 
systems and multiple context systems. Dissatisfaction with symbolic manipulation paradigms 
for artificial intelligence seems partly responsible for this attention, encouraged by the promise 
of massively parallel systems implemented in hardware. This paper relates simple neural-like 
systems based on multiple context to some other well-known formalisms--namely production 
systems, k-length sequence prediction, finite-state machines and Turing machines--and presents 
earlier sequence prediction results in a new light. 
1 INTRODUCTION 
The revival of neural net research has been very strong, exemplified recently by Pumelhart 
and McClelland , new journals and a number of meetings �. The nets are also described as 
parallel distributed systems  , connectionist models 2, value passing systems 3 and multiple context 
learning systems 4,5,6,7,8.9. The symbolic manipulation paradigm for artificial intelligence does 
not seem to have been as successful as some hoped , and there seems at last to be real promise 
of massively parallel systems implemented in hardware. However, in the flurry of new work it 
is important to consolidate new ideas and place them solidly alongside established ones. This 
paper relates simple neural-like systems to some other well-known notions--namely production 
systems, k-length sequence prediction, finite-state machines and Turing machines--and presents 
earlier results on the abilities of such networks in a new light. 
The general form of a connectionist system � is simplified to a three layer net with binary 
fixed weights in the hidden layer, thereby avoiding many of the difficulties--and challenges-- 
of the recent work on neural nets. The hidden unit weights are regularly patterned using a 
template. Sophisticated, expensive learning algorithms are avoided, and a simple method is 
used for determining output unit weights. In this way we gain some of the advantages of multi- 
layered nets, while retaining some of the simplicity of two layer net training methods. Certainly 
nothing is lost in computational power--as I will explain--and the limitations of two layer 
nets are not carried over to the simplified three layer one. Biological systems may similarly 
avoid the need for learning algorithms such as the simulated annealing method commonly 
used in connectionist models u. For one thing, biological systems do not have the same clearly 
distinguished training phase. 
Briefly, the simplified net b is a production system implemented as three layers of neuron-like 
units; an output layer, an input layer, and a hidden layer for the productions themselves. Each 
hidden production unit potentially connects a predetermined set of inputs to any output. A 
k-length sequence predictor is formed once k levels of delay unit are introduced into the input 
layer. k-length predictors are unable to distinguish simple sequences such as ha... a and aa... a 
since after k or more characters the system has forgotten whether an a or b appeared first. If 
the k-length predictor is augmented with auxiliary actions, it is able to learn this and other 
regular languages, since the auxiliary actions can be equivalent to states, and can be inputs to 
aAmong them the 1st International Conference on Neural Nets, San Diego,CA, June 21-24, 1987, and this 
conference. 
bRoughly equivalent to a single context system in Andreae's multiple context system 4'5'6'7'$'9. See also 
MacDonald 12. 
@ American Institute of Physics 1988 
506 
Figure 1: The general form of a connectionist system �. 
(a) Form of a unit 
o 1 
0 2 
o i 
Oo 
(a) Operations within a unit 
inputs -. excitation 
weighm ' 
sum 
-- activation J- output 
Typical F Typical f 
the production units enabling predictions to depend on previous states 7. By combining several 
augmented sequence predictors a Turing machine tape can be simulated along with a finite-state 
controller 9, giving the net the computational power of a Universal Turing machine. Relatively 
simple neural-like systems do not lack computational ability. Previous implementations 7,9 of 
this ability are production system equivalents to the simplified nets. 
1.1 Organization of the paper 
The next section briefly reviews the general form of connectionist systems. Section 2 simplifies 
this, then section 3 explains that the result is equivalent to a production system dealing only 
with inputs and outputs of the net. Section 4 extends the simplified version, enabling it to learn 
to predict sequences. Section 5 explains how the computational power of the sequence predictor 
can be increased to that of a Turing machine if some input units receive auxiliary actions; in fact 
the system can learn to be a Turing machine. Section 6 discusses the possibility of a number of 
nets combining their outputs, forming an overall net with association areas. 
1.2 General form of a connectionist system 
Figure 1 shows the general form of a connectionist system unit, neuron or cell �. In the figure 
unit i has inputs, which are the outputs oj of possibly all units in the network, and an output of 
its own, oi. The net input excitation, neti, is the weighted sum of inputs, where wij is the weight 
connecting the output from unit j as an input to unit i. The activation, ai of the unit is some 
function Fi of the net input excitation. Typically Fi is semilinear, that is non~decreasing and 
differentiable a, and is the same function for all, or at least large groups of units. The output is 
a function fi of the activation; typically some kind of threshold function. I will assume that the 
quantities vary over discrete time steps, so for example the activation at time t + 1 is ai(t q- 1) 
and is given by Fi((neti(t)). 
In general there is no restriction on the connections that may be made between units. 
Units not connected directly to inputs or outputs are hidden units. In more complex nets 
than those described in this paper, there may be more than one type of connection. Figure 2 
shows a common connection topology, where there are three layers of units--input, hidden and 
output--with no cycles of connection. 
The net is trained by presenting it with input combinations, each along with the desired 
output combination. Once trained the system should produce the desired outputs given just 
507 
Figure 2: The basic structure of a three layer connectionist system. 
input units hidden output units 
units 
inputs. During training the weights are adjusted in some fashion that reduces the discrepancy 
between desired and actual output. The general method ism�: 
Awij = g(ai, ti) h(oj, Wij), 
where ti is the desired, training activation. Equation 1 is a general form of Hebb's classic 
rule for adjusting the weight between two units with high activations �. The weight adjustment 
is the product of two functions, one that depends on the desired and actual activations--often 
just the difference--and another that depends on the input to that weight and the weight itself. 
As a simple example suppose g is the difference and h as just the output o i. Then the weight 
change is the product of the output error and the input excitation to that weight: 
Aw 0 = rloj(ti - ai) 
where the constant r/determines the learning rate. This is the Widrow-Hoff or Delta rule which 
may be used in nets without hidden units. � 
The important contribution of recent work on connectionist systems is how to implement 
equation 1 in hidden units; for which there are no training signals ti directly available. The 
Boltzmann learning method iteratively varies both weights and hidden unit training activations 
using the controlled, gradually decreasing randomizing method simulated annealing4 Back- 
propagation 3 is also iterative, performing gradient descent by propagating training signal errors 
back through the net to hidden units. I will avoid the need to determine training signals for 
hidden units, by fixing the weights of hidden units in section 2 below. 
2 SIMPLIFIED SYSTEM 
Assume these simplifications are made to the general connectionist system of section 1.2: 
1. The system has three layers, with the topology shown in Figure 2 (ie no cycles) 
2. All hidden layer unit weights are fixed, say at unity or zero 
3. Each unit is a linear threshold unit �, which means the activation function for all units 
is the identity function, giving just neti, a weighted sum of the inputs, and the output 
function is a simple binary threshold of the form: 
T ouut 
threshol a  
activation 
508 
so that the output is binary; on or off. Hidden units will have thresholds requiring all 
inputs to be active for the output to be active (like an ,ND gate) while output units will 
have thresholds requiring only I or two active highly weighted inputs for an output to be 
generated (like an OR gate). This is in keeping with the production system view of the 
net, explained in section 3. 
4. Learning--which now occurs only at the output unit weights--gives weight adjustments 
according to: 
wlj -- 1 if a i '- oj -- 1 
Wij '-- 0 otherwise 
so that weights are turned on if their input and the unit output are on, and off otherwise. 
That is, Wij -- ai A oj. A simple example is given in Figure 3 in section 3 below. 
This simple form of net can be made probabilistic by replacing 4 with 4' below: 
4'. Adjust weights so that wij estimates the conditional probability of the unit i output being 
on when output j is on. That is, 
Wij -- estimate of 
