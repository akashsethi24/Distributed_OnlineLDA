Sample Size Requirements For 
Feedforward Neural Networks 
Michael J. Tarmort 
Cornell Univ. Electrical Engineering 
Ithaca, NY 14853 
mjt@ee.cornell.edu 
Terrence L. Fine 
Cornell Univ. Electrical Engineering 
Ithaca, NY 14853 
tlfine@ee.cornell.edu 
Abstract 
We estimate the number of training samples required to ensure that 
the performance of a neural network on its training data matches 
that obtained when fresh data is applied to the network. Existing 
estimates are higher by orders of magnitude than practice indicates. 
This work seeks to narrow the gap between theory and practice by 
transforming the problem into determining the distribution of the 
supremum of a random field in the space of weight vectors, which 
in turn is attacked by application of a recent technique called the 
Poisson clumping heuristic. 
I INTRODUCTION AND KNOWN RESULTS 
We investigate the tradeoffs among network complezity, training set size, and sta- 
tistical performance of feedforward neural networks so as to allow a reasoned choice 
of network architecture in the face of limited training data. Nets are functions 
/(x; w), parameterized by their weight vector w  42 C_ R a, which take as input 
points x  R . For classifiers, network output is restricted to {0, 1} while for fore- 
casting it may be any real number. The architecture of all nets under consideration 
is jV', whose complexity may be gauged by its Vapnik-Chervonenkis (VC) dimension 
v, the size of the largest set of inputs the architecture can classify in any desired way 
('shatter'). Nets r/6 A f are chosen on the basis of a training set T = 
These n samples are i.i.d. according to an unknown probability law P. Performance 
of a network is measured by the mean-squared error 
�(w) = E(/(x; w) - y)' (1) 
= P(/(x; w) / y) (for classifiers) (2) 
328 Michael Turmon, Terrence L. Fine 
and a good (perhaps not unique) net in the architecture is w � = arg minEw �(w). 
To select a net using the training set we employ the empirical error 
1 (,(,; ) _ ,). 
r(w) =  
i=1 
sustained by r/(-; w) on the training set T. A good choice for a classifier is then 
w* = arg min, oEw ,r(w). In these terms, the issue raised in the first sentence of the 
section can be restated as, How large must n be in order to ensure �(w*)-�(w �) _< 
e with high probability? 
For purposes of analysis we can avoid dealing directly with the stochastically chosen 
network w* by noting 
�(w*) - �(w �) 5 Ir(w *) - �(w*)l + Ir(w �) - �(w�)l _< 2 sup Ir(w) - �(w)l 
A bound on the last quantity is also useful in its own right. 
The best-known result is in (Vapnik, 1982), introduced to the neural network com- 
munity by (Baum &: Haussler, 1989): 
P( sup Ir(w) - �(w)l > ) _< 6 (2.,) e-'*/' (4) 
This remarkable bound not only involves no unknown constant factors, but holds 
independent of the data distribution P. Analysis shows that sample sizes of about 
nc = (4v/e ') log 3/e (5) 
are enough to force the bound below unity, after which it drops exponentially to 
zero. Taking e = .1, v = 50 yields nc = 68000, which disagrees by orders of 
magnitude with the experience of practitioners who train such simple networks. 
More recently, Talagrand (1994) has obtained the bound 
_ _ 
(up I-r(w) - (w)l > ) < x. 7 -, () 
w 
yielding a sufficient condition of order v/e 2, but the values of K and K2 are inac- 
cessible so the result is of no practical use. 
Formulations with finer resolution near E(w) = 0 are used. Vapnik (1982) bounds 
(.p I.r(w)-()l/(w)/  )--not (w)X/  (.r(w))X/ wh. 
E(w)  O--while Blumer et al. (1989) and Anthony and Biggs (1992) work with 
(-p I-r(w) - (w)l l{0(-r(w))  ). h htt otin th .mi.t o.di- 
tion 
., = (5.8/) log 2/ (7) 
for nets, if any, having yT(w) = 0. If one is guaranteed to do reonably well on 
the training set, a smaller order of dependence rults. 
Rults (rmon & Fine, 1993) for percepttons and P a Gaussian mixture imply 
that at let v/280e 2 samples are needed to force E(w*) -E(w �) < 2e with high 
probability. (Here w* is the best linear discriminant with weights estimated from 
the data.) Combining with Talagrand's result, we see that the general (not suming 
small yr(w)) functional dependence is vie . 
Sample Size Requirements for Feedforward Neural Networks 329 
2 APPLYING THE POISSON CLUMPING HEURISTIC 
We adopt a new approach to the problem. For the moderately large values of n 
we anticipate, the central limit theorem informs us that x/[v'(w)- �(w)] has 
nearly the distribution of a zero-mean Gaussian random variable. It is therefore 
reasonable x to suppose that 
P( sup Ivr(w) - �(w)l > ) - P( sup IZ(w)l > ) < 2P(sup z(w) > ,/) 
wEl wEl wEl 
where g(w) is a Gaussian process with mean zero and covariance 
R(w. v) = EZ(w)Z(v) = Cov((y - ,(x; w)) '. (y - ,(x; ))2) 
The problem about extrema of the original empirical process is equivalent to one 
about extrema of a corresponding Gaussian process. 
The Poisson clumping heuristic (PCH), introduced in the remarkable (Aldous, 
1989), provides a general tool for estimating such exceedance probabilities. Con- 
sider the excursions above level b(= evf >> 1) by a stochastic process Z(w). At 
left below, the set {w: Z(w) _> b} is seen as a group of clumps scattered in weight 
space YV. The PCH says that, provided Z has no long-range dependence and the 
level b is large, the centers of the clumps fall according to the points of a Poisson 
process on 142, and the clump shapes are independent. The vertical arrows (below 
right) illustrate two clump centers (points of the Poisson process); the clumps are 
the bars centered about the arrows. 
z(w) 
In fact, with pb(w) = P(Z(w) _> b), Cb(w) the size of a clump located at w, and 
Ab(w) the rate of occurrence of clump centers, the fundamental equation is 
pb(w) - b(w)EC(w). (8) 
The number of clumps in YV is a Poisson random variable N with parameter 
A(w)dw. The probability of a clump is P(N, > O)= 1-exp(-fwAb(w)dw ) - 
A,(w) dw where the approximation holds because our goal is to operate in a 
regime where this probability is near zero. Letting (b) = P(N(O, 1) > b) and 
a(w) = R(w, w), we have p,(w) = (b/a(w)). The fundamental equation becomes 
( p Z(w) > b) f (/(w)) w () 
 - - EC(w) 
It remains only to find the mean clump size EC,(w) in terms of the network archi- 
tecture and the statistics of (x, y). 
See ch. 7 of (Pollard, 1984) for treatment of some technical details in this limit. 
330 Michael Turmon, Terrence L. Fine 
3 POISSON CLUMPING FOR SMOOTH PROCESSES 
Assume Z(w) has two mean-square derivatives in w. (If the network activation 
functions have two derivatives in w, for example, Z(w) will have two almost sure 
derivatives.) Z then has a parabolic approximation about some w0 via its gradient 
G = VZ(w) and Hessian matrix H = VVZ(w) at w0. Provided z0 ) b, that is 
that there is a clump at w0, simple computations reveal 
((Zo - b) - 
where a is the volume of the unit ball in R a and [. [ is the determinant. The mean 
clump size is the expectation of this conditioned on Z(wo)  b. 
The same argument used to show that Z(w) is approximately normal shows that G 
and H are approximately normal too. In hct, 
z 
[nlZ(w0) = z] = 
(wo) = = w)l=o 
so that, since b (and hence z) is large, the second term in the numerator of (10) 
may be neglected. The expectation is then eily computed, resulting in 
Lemma 1 (Smooth process clump size) Let the network activation functions 
be twice continuously differenttable, and let b >> a(w). Then 
EC,(w)  (2) d/2 A(w) -Uz d 
Substituting into (9) yields 
,ew - a2(w) o'(-w) dw, (11) 
where use of the asymptotic expansion (z) _ (zx/) -x exp(-z2/2) is justified 
since (�w)b >> a(w) is necessary to have the individual P(Z(w) _ b) low--let alone 
the supremum. To go farther, we need information about the variance a2(w) of 
(y - ?(x; w)) 2. In general this must come from the problem at hand, but suppose 
for example the process has a unique variance maximum 2 at . Then, since 
the level b is large, we can use Laplace's method to approximate the d-dimensional 
integral. 
Laplace's method finds asymptotic expansions for integrals 
w g( w) exp(- f ( w) ' /2 ) dw 
when f(w) is C ' with a unique positive minimum at w0 in the interior of 14/C_ R d, 
and g(w) is positive and continuous. Suppose f(wo) >> 1 so that the exponential 
factor is decreasing much faster than the slowly varying g. Expanding f to second 
order about wo, substituting into the exponential, and performing the integral shows 
that 
wg(W exp(-f(w) '/2) dw _ exp(-f(w0)'/2) 
Satnple Size Requirements for Feedforward Neural Networks 331 
where K = VVf(w)lo, the Hessian of f. See (Wong_, 1989) for a proof. Applying 
this to (11) and using the asymptotic expansion for  in reverse yields 
Theorem 1 Let the network activation functions be twice continuously cliffetch- 
liable. Let the variance have a unique mazimum  at  in the interior of YV and 
the level b >> . Then the PCH estimate of ezceedance probability is given by 
P( sup Z(w) > b) _ IA()l/' 
Ew - la()- r()l / (b/a) (12 
w r() = vv(w, v)l==. ro, - r i positive-definte at ; 
it is -1/2 the Hessian of a(w). The leading constant thus stscfly eceeds unity. 
The above probability is just P(Z()  b) multiplied by a factor accounting for the 
other networks in the supremum. Letting b = e reveals 
 log(lA()l/lA() - r(w)l) (is) 
samples force P(sup Ivr(w)  S(w)l  e) below unity. If the variance mmum is 
not unique but occurs over a d-dimensional set within W, the sample size estimate 
becomes proportionM to /e . With  playing the role of VC dimension v, this 
is similar to Vapnik's bound although we retain dependence on P and . 
The above probability is determined by behavior near the mimum-variance point, 
which for example in clsification is where E(w) = 1/2. Such nets are uninterest- 
