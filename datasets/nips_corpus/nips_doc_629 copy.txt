Assessing and Improving Neural Network 
Predictions by the Bootstrap Algorithm 
Gerhard Paass 
German National Research Center for Computer Science (GMD) 
D-5205 Sankt Augustin, Germany 
e-mail: paassgmd. de 
Abstract 
The bootstrap algorithm is a computational intensive procedure to 
derive nonparametric confidence intervals of statistical estimators 
in situations where an analytic solution is intractable. It is ap- 
plied to neural networks to estimate the predictive distribution for 
unseen inputs. The consistency of different bootstrap procedures 
and their convergence speed is discussed. A small scale simulation 
experiment shows the applicability of the bootstrap to practical 
problems and its potential use. 
1 INTRODUCTION 
Bootstrapping is a strategy for estimating standard errors and confidence intervals 
for parameters when the form of the underlying distribution is unknown. It is 
particularly valuable when the parameter of interest is a complicated functional 
of the true distribution. The key idea first promoted by Efron (1979) is that the 
relationship between the true cumulative distribution function (cdf) F and the 
sample of size n is similar to the relationship between the empirical cdf a and 
a secondary sample drawn from it. So one uses the primary sample to form an 
estimate /a and calculates the sampling distribution of the parameter estimate 
under/a. This calculation is done by drawing many secondary samples and finding 
the estimate, or function of the estimate, for each. If .Ta is a good approximation 
of F, then Ha, the sampling distribution of the estimate under /a, is a generally 
good approximation to the sampling distribution for the estimate under F. Ha is 
196 
Assessing and Improving Neural Network Predictions by the Bootstrap Algorithm 197 
called the bootstrap distribution of the parameter. Introductory articles are Efron 
and Gong (1983) and Efron and Tibshirani (1986). For a survey of bootstrap results 
see Hinkley (1988) and DiCiccio and Romano (1988). 
A neural networks often may be considered as a nonlinear or nonparametric regres- 
sion model 
z = gp(y) q- e (1) 
which defines the relation between the vectors y and z of input and output variables. 
The ter:aa e can be interpreted as a random 'error' and the function g depends on 
some u kown parameter/3 which may have infinite dimension. Usually the network 
is used to determine a prediction zo - g(yo) for some new input vector Y0. If the 
data is a random sample, an estimate/ differs from the true value of/3 because of 
the sampling error and consequently the prediction g5(Yo) is different from the true 
prediction. In this paper the bootstrap approach is used to approximate a sampling 
distribution of the prediction (or a function thereof) and to estimate parameters 
of that distribution like its mean value, variance, percentties, etc. Bootstrapping 
procedures are closely related to other resampling methods like cross validation and 
jackknife (Efron 1982). The jackknife can be considered as a linear approximation 
to the bootstrap (Efron, Tibshirani 1986). 
In the next section different versions of the bootstrap procedure for feedforward 
neural networks are defined and their theoretical properties are reviewed. Main 
points are the convergence of the bootstrap distribution to true theoretical distri- 
bution and the speed of that convergence. In the following section the results of a 
simulation experiment for a simple backprop model are reported and the applica- 
tion of the bootstrap to model selection is discussed. The final section gives a short 
summary. 
2 
CONSISTENCY OF THE BOOTSTRAP FOR 
FEEDFORWARD NEURAL NETWORKS 
Assume X(n) := (Xl,..., xn) is the available independent, identically distributed 
(iid) sample from an underlying cdf F where xi = (zi,yi) and/n is the correspond- 
ing empirical cdf. For a given Y0 let r/= rl(gf(yo) ) be a parameter of interest of the 
prediction, e.g. the mean value of the prediction of a component of z for Y0. 
The pairwise bootstrap algorithm is an intuitive way to apply the bootstrap notion to 
regression. It was proposed by Efron (1982) and involves the independent repetition 
of following steps for b -- 1,..., B. 
1. A sample X(n) of size n is generated from P,. Notice that this amounts 
to the random selection of n elements from X(n) with replacement. 
2. An estimate f/b is determined from X(n). 
The resulting empirical cdf of the Ob, b -- 1,..., n is denoted by B and approxi- 
mates the sampling distribution for the estimate f/under/n. The standard devia- 
tion of HB is an estimate of the standard error of r/(/n), and [rx (c), rl(1-c)] 
is an approximate (1 - 2c) central confidence interval. 
198 Paass 
In general two conditions are necessary for the bootstrap to be consistent: 
� The estimator, e.g. b has to be consistent. 
� The functional which maps F to f/B has to be smooth. 
This requirement can be formalized by a uniform weak convergence condition (Di- 
Ciccio, Romano 1988). Using these concepts Freedman (1981) proved that for the 
parameters of a linear regression model the pairwise bootstrap procedure is con- 
sistent, i.e. yields the desired limit distribution for n, B -* cx. Mammen (1991) 
showed that this also holds for the pre(fictive distribution of a lineax model (i.e. 
linear contrasts). These results hold even if the errors are heteroscedastic, i.e. if 
the distribution of ei depends on the value of Yi. 
The performance of the bootstrap for linear regression is extensively discussed by 
Wu (1986). It turns out that the small sample properties can be different from 
the asymptotic relations and the bootstrap may exhibit a sizeable bias. Various 
procedures of bias correction have been proposed (DiCiccio, Romano 1988). Beran 
(1990) discusses a calibrated bootstrap prediction region containing the prediction 
g(yo)+e with prescribed probability a. It requires a sequence of nested bootstraps. 
Its coverage probability tends to c at a rate up to n -2. Note that this procedure can 
be applied to nonlinear regression models (1) with homoscedastic errors (Example 
3 in Beran (1990, p.718) can be extended to this case). 
Biases especially arise if the errors are heteroscedastic. Hinkley (1988) discusses the 
parametric modelling of dependency of the error distribution (or its variance) from 
y and the application of the bootstrap algorithm using this model. The problem is 
here to determine this parametric dependency from the data. As an alternative Wu 
(1986) and Liu (1988) take into account heteroscedasticity in a nonparametric way. 
They propose the following wild bootstrap algorithm which starts with a consistent 
estimate/ based on the sample X(n). Then the set of residuals (1,..-,g,) with 
i := zi - gfj (yi) is determined. The approach attempts to mimic the conditional 
distribution of z given yi in a very crude way by defining a distribution i whose 
first three moments coincide with the observed residual i: 
^2 = q (2) 
Two point distributions are used which are uniquely defined by this requirement 
(Mammen 1991, p.121). Then the following steps are repeated for b = 1,..., B: 
1. Independently generate residuals gi according to (i and generate observa- 
tions z := gf(yi) + gi for i = (1,... ,n). This yields a new sample X(n) 
of size n. 
2. An estimate ; is determined from X; (n). 
The resulting empirical cdf of the O is then taken as the bootstrap distribution 
rB which approximates the sampling distribution for the estimate  under fin. 
Mammen (1991, p.123) shows that this algorithm is consistent for the prediction of 
linear regression models if the least square estimator or M-estimators are used and 
discusses the convergence speed of the procedure. 
Assessing and Improving Neural Network Predictions by the Bootstrap Algorithm 199 
The bootstrap may also be applied to nonparameric regression models like kernel- 
type estimators of the form 
.0(Y) = [Ein----X Zi] (ILJ_)] (3) 
with kernel K and bandwidth h. These models are related to radial basis functions 
discussed in the neural network literature. For those models the pairwise bootstrap 
does not work (Hrdle, Mammen 1990) as the algorithm is not forced to perform 
local a raging. To account for heteroscedasticity in the errors of (1) Hirdle (1990, 
p.103) advocates the use of the wild bootstrap algorithm described above. Under 
some regularity conditions he shows the convergence of the bootstrap distribution 
of the kernel estimator to the correct limit distribution. 
To summarize the bootstrap often is used simply because an analytic derivation of 
the desired sampling distribution is too complicated. The asymptotic investigations 
offer two additional reasons: 
� There exist versions of the bootstrap algorithm that have a better rate of 
convergence than the usual asymptotic normal approximation. This effect 
has been extensively discussed in literature e.g. by Hall (1988), Beran 
(1988), DiCiccio and Romano (1988, p.349), Mammen (1991, p.74). 
� There are cases where the bootstrap works, even if the normal approxima- 
tion breaks down. Bickel and Freedman (1983) for instance show, that the 
bootstrap is valid for linear regression models in the presence of outliers and 
if the number of parameters changes with n. Their results are discussed 
and extended by Mammen (1991, p.88ff). 
3 SIMULATION EXPERIMENTS 
To demonstrate the performance of the bootstrap for real real problems we inves- 
tigated a small neural network. To get a nonlinear situation we chose a noisy 
version of the xor model with eight input units Yl,..., Y$ and a single output unit 
z. The input variables may take the values 0 and 1. The output unit of the true 
model is stochastic. It takes the values 0.1 and 0.9 with the following probabilities: 
p(y = 0.9) = 0.9 
p(y = 0.9)=0.1 
p(y = 0.9)=0.1 
p(y = 0.9)=0.9 
if Xl.t-x2.t-x3.t-x4<3 and 
if x+x2+xa+x,<3 and 
if x+x2+xa
