On Parallel Versus Serial Processing: 
A Computational Study of Visual Search 
Eyal Cohen 
Department of Psychology 
Tel-Aviv University Tel Aviv 69978, Israel 
eyalc@devil.tau.ac.il 
Eytan Ruppin 
Departments of Computer Science 8 Physiology 
Tel-Aviv University Tel Aviv 69978, Israel 
ruppin@math.tau.ac.il 
Abstract 
A novel neural network model of pre-attention processing in visual- 
search tasks is presented. Using displays of line orientations taken 
from Wolfe's experiments [1992], we study the hypothesis that the 
distinction between parallel versus serial processes arises from the 
availability of global information in the internal representations of 
the visual scene. The model operates in two phases. First, the 
visual displays are compressed via principal-component-analysis. 
Second, the compressed data is processed by a target detector mod- 
ule in order to identify the existence of a target in the display. Our 
main finding is that targets in displays which were found exper- 
imentally to be processed in parallel can be detected by the sys- 
tem, while targets in experimentally-serial displays cannot. This 
fundamental difference is explained via variance analysis of the 
compressed representations, providing a numerical criterion distin- 
guishing parallel from serial displays. Our model yields a mapping 
of response-time slopes that is similar to Duncan and Humphreys's 
search surface [1989], providing an explicit formulation of their 
intuitive notion of feature similarity. It presents a neural realiza- 
tion of the processing that may underlie the classical metaphorical 
explanations of visual search. 
On Parallel versus Serial Processing: A Computational Study of Visual Search 
1 Introduction 
11 
This paper presents a neural-model of pre-attentive visual processing. The model 
explains why certain displays can be processed very fast, in parallel, while others 
require slower, serial processing, in subsequent attentional systems. Our approach 
stems from the observation that the visual environment is overflowing with diverse 
info?mation, but the biological information-processing systems analyzing it have 
a limited capacity [1]. This apparent mismatch suggests that data compression 
should be performed at an early stage of perception, and that via an accompa- 
nying process of dimension reduction, only a few essential features of the visual 
display should be retained. We propose that only parallel displays incorporate 
global features that enable fast target detection, and hence they can be processed 
pre-attentively, with all items (target and distractors) examined at once. On the 
other hand, in serial displays' representations, global information is obscure and 
target detection requires a serial, attentional scan of local features across the dis- 
play. Using principal-component-analysis (PCA), our main goal is to demonstrate 
that neural systems employing compressed, dimensionally reduced representations 
of the visual information can successfully process only parallel displays and not se- 
rial ones. The source of this difference will be explained via variance analysis of the 
displays' projections on the principal axes. 
The modeling of visual attention in cognitive psychology involves the use of 
metaphors, e.g., Posner's beam of attention [2]. A visual attention system of a 
surviving organism must supply fast answers to burning issues such as detecting 
a target in the visual field and characterizing its primary features. An attentional 
system employing a constant-speed beam of attention [3] probably cannot perform 
such tasks fast enough and a pre-attentive system is required. Treisman's feature 
integration theory (FIT) describes such a system [4]. According to FIT, features 
of separate dimensions (shape, color, orientation) are first coded pre-attentively in 
a locations map and in separate feature maps, each map representing the values of 
a particular dimension. Then, in the second stage, attention glues the features 
together conjoining them into objects at their specified locations. This hypothesis 
was supported using the visual-searcti paradigm [4], in which subjects are asked 
to detect a target within an array of distractors, which differ on given physical di- 
mensions such as color, shape or orientation. As long as the target is significantly 
different from the distractors in one dimension, the reaction time (RT) is short and 
shows almost no dependence on the number of distractors (low FiT slope). This 
result suggests that in this case the target is detected pre-attentively, in parallel. 
However, if the target and distractors are similar, or the target specifications are 
more complex, reaction time grows considerably as a function of the number of 
distractors [5, 6], suggesting that the displays' items are scanned serially using an 
attentional process. 
FIT and other related cognitive models of visual search are formulated on the con- 
ceptual level and do not offer a detailed description of the processes involved in 
transforming the visual scene from an ordered set of data points into given values 
in specified feature maps. This paper presents a novel computational explanation 
of the source of the distinction between parallel and serial processing, progressing 
from general metaphorical terms to a neural network realization. Interestingly, we 
also come out with a computational interpretation of some of these metaphorical 
terms, such as feature similarity. 
12 E. Cohen and E. Ruppin 
2 The Model 
We focus our study on visual-search experiments of line orientations performed by 
Wolfe et. al. [7], using three set-sizes composed of 4, 8 and 12 items. The number of 
items equals the number of distractors + target in target displays, and in non-target 
displays the target was replaced by another distractor, keeping a constant set-size. 
Five experimental conditions were simulated: (A) - a 20 degrees tilted target among 
vertical distractors (homogeneous background). (B) - a vertical target among 20 
degrees tilted distractors (homogeneous background). (C) - a vertical target among 
heterogeneous background ( a mixture of lines with -t-20, :t:40 , :t:60 , :t:80 degrees 
orientations). (E) - a vertical target among two flanking distractor orientations (at 
:t:20 degrees), and (G) - a vertical target among two flanking distractor orientations 
(:t:40 degrees). The response times (lT) as a function of the set-size measured by 
Wolfe et. al. [7] show that type A, B and G displays are scanned in a parallel 
manner (1.2, 1.8, 4.8 msec/item for the [iT slopes), while type C and E displays are 
scanned serially (19.7, 17.5 msec/item). The input displays of our system were pre- 
pared following Wolfe's prescription: Nine images of the basic line orientations were 
produced as nine matrices of gray-level values. Displays for the various conditions 
of Wolfe's experiments were produced by randomly assigning these matrices into 
a 4x4 array, yielding 128x100 display-matrices that were transformed into 12800 
display-vectors. A total number of 2400 displays were produced in 30 groups (80 
displays in each group): 5 conditions (A, B, C, E, G ) x target/non-target x 3 
set-sizes (4, 8, 12). 
Our model is composed of two neural network modules connected in sequence as 
illustrated in Figure 1: a PCA module which compresses the visual data into a set 
of principal axes, and a Target Detector (TD) module. The latter module uses the 
compressed data obtained by the former module to detect a target within an array 
of distractors. The system is presented with line-orientation displays as described 
above. 
NO-TARGET =-1 TARGET=I 
TD OUTPUT LAYER (I UNIT) TARGET 
DETECTOR 
TD INTERMEDIATE LAYER 02 UIglTS) MODULE 
(70) 
IA oLrrPOT/TD INPUT LAYER ] DATA 
g UNITS) 
I COMPRESSION 
MODULE 
   DISPLAY 
Figure 1: General architecture of the model 
For the PCA module we use the neural network proposed by Sanger, with the 
connections' values updated in accordance with his Generalized Hebbian Algorithm 
(GHA) [8]. The outputs of the trained system are the projections of the display- 
vectors along the first few principal axes, ordered with respect to their eigenvalue 
magnitudes. Compressing the data is achieved by choosing outputs from the first 
On Parallel versus Serial Processing: A Computational Study of Visual Search 13 
few neurons (maximal variance and minimal information loss). Target detection in 
our system is performed by a feed-forward (FF) 3-1ayered network, trained via a 
standard back-propagation algorithm in a supervised-learning manner. The input 
layer of the FF network is composed of the first eight output neurons of the PCA 
module. The transfer function used in the intermediate and output layers is the 
hyperbolic tangent function. 
3 Results 
3.i Target Detection 
The performance of the system was examined in two simulation experiments. In 
the first, the PCA module was trained only with parallel task displays, and in the 
second, only with serial task displays. There is an inherent difference in the ability 
of the model to detect targets in parallel versus serial displays. In parallel task 
conditions (A, B, G) the target detector module learns the task after a comparatively 
small number (800 to 2000) of epochs, reaching performance level of almost 100%. 
However, the target detector module is not capable of learning to detect a target 
in serial displays (C, E conditions). Interestingly, these results hold (1) whether 
the preceding PCA module was trained to perform data compression using parallel 
task displays or serial ones, (2) whether the target detector was a linear simple 
perceptton, or the more powerful, non-linear network depicted in Figure 1, and (3) 
whether the full set of 144 principal axes (with non-zero eigenvalues) was used. 
3.2 Information Span 
To analyze the differences between parallel and serial tasks we examined the eig
