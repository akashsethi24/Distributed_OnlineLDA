Prior Knowledge in Support Vector Kernels 
Bernhard Sch/Jikopf *t, Patrice Simard*, Alex Smolat, & Vladimir Vapnik* 
* Max-Planck-Institut far biologische Kybernetik, Tbingen, Germany 
*GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany 
*AT&T Research, 100 Schulz Drive, Red Bank, NJ, USA 
bs@first. gmd.de 
Abstract 
We explore methods for incorporating prior knowledge about a problem 
at hand in Support Vector learning machines. We show that both invari- 
ances under group transformations and prior knowledge about locality in 
images can be incorporated by constructing appropriate kernel functions. 
1 INTRODUCTION 
When we are trying to extract regularities from data, we often have additional knowledge 
about functions that we estimate. For instance, in image classification tasks, there exist 
transformations which leave class membership invariant (e.g. local translations); moreover, 
it is usually the case that images have a local structure in that not all correlations between 
image regions carry equal amounts of information. 
The present study investigates the question how to make use of these two sources of know- 
ledge by designing appropriate Support Vector (SV) kernel functions. We start by giving 
a brief introduction to SV machines (Vapnik & Chervonenkis, 1979; Vapnik, 1995) (Sec. 
2). Regarding prior knowledge about invariances, we present a method to design kernel 
functions for invariant classification hyperplanes (Sec. 3). The method is applicable to 
invariances under the action of differentiable local 1-parameter groups of local transforma- 
tions, e.g. translational invariance in pattern recognition. In Sec. 4, we describe kernels 
which take into account image locality by using localized receptive fields. Sec. 5 presents 
experimental results on both types of kernels, followed by a discussion (Sec. 6). 
2 OPTIMAL MARGIN HYPERPLANES 
For linear hyperplane decision functions f(x) = sgn ((w. x) + b), the VC-dimension 
can be controlled by controlling the norm of the weight vector w. Given training data 
(xl, /1),..., (xe, /e), xi  R N, /i  {+1}, a separating hyperplane which generalizes 
Prior Knowledge in Support Vector Kernels 641 
well can be found by minimizing 
l[Iwl[ 2 subject to ti.((xi-w) +b) >_ 1 for/= 1,...,t, (1) 
the latter being the conditions for separating the training data with a margin. Nonseparable 
cases are dealt with by introducing slack variables (Cortes & Vapnik 1995), but we shall 
omit this modification to simplify the exposition. All of the following also applies for the 
nonseparable case. 
To solve the above convex optimization problem, one introduces a Lagrangian with multi- 
pliers ai and derives the dual form of the optimization problem: maximize 
1 y ritiaktk(xi' xk) subject to ci > 0, E riIi = 0. (2) 
i=1 i,k=l i=1 
It tums out that the solution vector has an expansion in terms of training examples, w - 
Ei=i �ilixi, where only those ci corresponding to constraints (1) which are met can 
become nonzero; the respective examples xi are called Support Vectors. Substituting this 
expansion for w yields the decision function 
f(x)=sgn(riyi(x.xi)+b). (3) 
i=1 
It can be shown that minimizing (2) corresponds to minimizing an upper bound on the VC 
dimension of separating hyperplanes, or, equivalently, to maximizing the separation margin 
between the two classes. In the next section, we shall depart from this and modify the dot 
product used such that the minimization of (2) corresponds to enforcing transformation 
invariance, while at the same time the constraints (1) still hold. 
3 INVARIANT HYPERPLANES 
Invariance by a self-consistency argument. We face the following problem: to express 
the condition of invariance of the decision function, we already need to know its coeffi- 
cients which are found only during the optimization, which in turn should already take into 
account the desired invariances. As a way out of this circle, we use the following ansatz: 
consider decision functions f = (sgn o g), where g is defined as 
g(Xj) : E otiYi(Bxj . Bxi) q- b, (4) 
with a matrix B to be determined below. This follows Vapnik (1995), who suggested to 
incorporate invariances by modifying the dot product used. Any nonsingular B defines a 
dot product, which can equivalently be written as (xj � Axi), with a positive definite matrix 
A= B-B. 
Clearly, invariance of g under local transformations of all x d is a sufficient condition for 
the local invariance of f, which is what we are aiming for. Strictly speaking, however, 
invariance of g is not necessary at points which are not Support Vectors, since these lie in 
a region where (sgn o g) is constant -- however, before training, it is hard to predict which 
examples will turn out to become SVs. In the Virtual SV method (Sch61kopf, Burges, & 
Vapnik, 1996), a first run of the standard SV algorithm is carried out to obtain an initial SV 
set; similar heuristics could be applied in the present case. 
Local invariance of g for each pattern xj under transformations of a differentiable local 
1-parameter group of local transformations �t, 
t lt=og(�txj) = O , (5) 
642 B. SchOlkopf, P. Simard, A. J. Smola and V. Vapnik 
can be approximately enforced by minimizing the regularizer 
-l-(lt=og(Etxj))2 (6) 
j=l 
Note that the sum may run over labelled as well as unlabelled data, so in principle one could 
also require the decision function to be invariant with respect to transformations of elements 
of a test set. Moreover, we could use different transformations for different patterns. 
For (4), the local invariance term (5) becomes 
using the chain rule. Here, 0 (B�0x. � Bxi) denotes the gradient of (x. y) with respect to 
x, evaluated at the point (x. y) = (B�0x. � Bxi). Substituting (7) into (6), using the facts 
that �0 = I and 0 (x, y) = y-, yields the regularizer 
e oiYi(Bxi)-l-B 0 �txj '-- E oiYiOkyk(Bxi' BCB-I-Bxk) (8) 
j=l i=1 Ot t=0 
where 
j=l 
We now choose B such that (8) reduces to the standard SV target function IIwll 2 in the form 
obtained by substituting the expansion w -- Y'4= �tiYiXi into it (cf. the quadratic term of 
(2)), utilizing the dot product chosen in (4), i.e. such that (Bxi � BCB n- Bxk) = (Bxi � 
Bx). Assuming that the xi span the whole space, this condition becomes Bn-BCB-B - 
B-rB, or, by requiring B to be nonsingular, i.e. that no information get lost during the 
preprocessing, BCB - = I. This can be satisfied by a preprocessing (whitening) matrix 
1 
B = C- (10) 
(modulo a unitary matrix, which we disregard), the nonnegative square root of the inverse 
of the nonnegative matrix C defined in (9). In practice, we use a matrix 
0 < A < 1, instead of C. As C is nonnegative, Cx is invertible. For A = 1, we recover the 
standard SV optimal hyperplane algorithm, other values of A determine the trade-off be- 
tween invariance and model complexity control. It can be shown that using Cx corresponds 
to using an objective function (w) = (1 - A) y'4(w � o � x 2 
By choosing the preprocessing matrix B according to (10), we have obtained a formulation 
of the problem where the standard SV quadratic optimization technique does in effect min- 
imize the tangent regularizer (6): the maximum of (2), using the modified dot product as in 
(4), coincides with the minimum of (6) subject to the separation conditions ti � 9(xi) _> 1, 
where 7 is defined as in (4). 
N6te that preprocessing with B does not affect classification speed: since (Bx. � Bxi) --- 
(x 5 � B-Bxi), we can precompute B-rBxi for all SVs xi and thus obtain a machine (with 
modified SVs) which is as fast as a standard SV machine (cf. (4)). 
Relationship to Principal Component Analysis (PCA). Let us now provide some inter- 
pretation of (10) and (9). The tangent vectors rko-lt_-o�tX 5 have zero mean, thus C' is a 
Prior Knowledge in Support Vector Kernels 643 
sample estimate of the covariance matrix of the random vector s � o__] {+1 
ot t=o�tx, s  } 
being a random sign. Based on this observation, we call C' (9) the Tangent Covariance 
Matrix of the data set {xi: i = 1,..., } with respect to the transformations 
Being positive definite, 1 C can be diagonalized, C' - SDS TM, with an orthogonal matrix 
S consisting of C's Eigenvectors and a diagonal matrix D containing the corresponding 
positive Eigenvalues. Then we can compute B = C- � = SD- � S -r, where D- � is 
the diagonal matrix obtained from D by taking the inverse square roots of the diagonal 
elements. Since the dot product is invariant under orthogonal transformations, we may 
drop the leading S and (4) becomes 
9(xj) = E aiJi(D-� STxj � D-� STxi) + b. (12) 
i=1 
A given pattern x is thus first transformed by projecting it onto the Eigenvectors of the 
tangent covariance matrix C, which are the rows of S -. The resulting feature vector is 
then rescaled by dividing by the square roots of Cs Eigenvalues. 2 In other words, the 
directions of main variance of the random vector  [t=o�tx are scaled back, thus more 
emphasis is put on features which are less variant under �t. For example, in image analysis, 
if the �t represent translations, more emphasis is put on the relative proportions of ink in 
the image rather than the positions of lines. The PCA interpretation of our preprocessing 
matrix suggests the possibility to regularize and reduce dimensionality by discarding part 
of the features, as it is common usage when doing PCA. 
In the present work, the ideas described in this section have only been tested in the linear 
case. More generally, SV machines use a nonlinear kernel function which can be shown 
to compute a dot product in a high-dimensional space F nonlinearly related to input space 
via some map , i.e. k(x, y) = ((x) � (y)). In that case, the above analysis leads to a 
tangent covariance matrix C' in F, and it can be shown that (12) can be evaluated in terms 
of the kernel function (SchOlkopf, 1997). To this end, one diagonalizes
