Learning a Continuous Hidden Variable 
Model for Binary Data 
Daniel D. Lee 
Bell Laboratories 
Lucent Technologies 
Murray Hill, NJ 07974 
ddlee�bell-labs. com 
Haim Sompolinsky 
Racah Institute of Physics and 
Center for Neural Computation 
Hebrew University 
Jerusalem, 91904, Israel 
haim�fiz. huj i. ac. il 
Abstract 
A directed generative model for binary data using a small number 
of hidden continuous units is investigated. A clipping nonlinear- 
ity distinguishes the model from conventional principal components 
analysis. The relationships between the correlations of the underly- 
ing continuous Gaussian variables and the binary output variables 
are utilized to learn the appropriate weights of the network. The 
advantages of this approach are illustrated on a translationally in- 
variant binary distribution and on handwritten digit images. 
Introduction 
Principal Components Analysis (PCA) is a widely used statistical technique for rep- 
resenting data with a large number of variables [1]. It is based upon the assumption 
that although the data is embedded in a high dimensional vector space, most of 
the variability in the data is captured by a much lower dimensional manifold. In 
particular for PCA, this manifold is described by a linear hyperplane whose char- 
acteristic directions are given by the eigenvectors of the correlation matrix with 
the largest eigenvalues. The success of PCA and closely related techniques such as 
Factor Analysis (FA) and PCA mixtures clearly indicate that much real world data 
exhibit the low dimensional manifold structure assumed by these models [2, 3]. 
However, the linear manifold structure of PCA is not appropriate for data with 
binary valued variables. Binary values commonly occur in data such as computer 
bit streams, black-and-white images, on-off outputs of feature detectors, and elec- 
trophysiological spike train data [4]. The Boltzmann machine is a neural network 
model that incorporates hidden binary spin variables, and in principle, it should be 
able to model binary data with arbitrary spin correlations [5]. Unfortunately, the 
516 D. D. Lee and H. Sornpolinsky 
Yl Y 
Wll 
s 1 s 2 SN_ 1 s N 
Figure 1: Generative model for N-dimensional binary data using a small number 
P of continuous hidden variables. 
computational time needed for training a Boltzmann machine renders it impractical 
for most applications. 
In these proceedings, we present a model that uses a small number of continuous 
hidden variables rather than hidden binary variables to capture the variability of 
binary valued visible data. The generative model differs from conventional PCA 
because it incorporates a clipping nonlinearity. The resulting spin configurations 
have an entropy related to the number of hidden variables used, and the resulting 
states are connected by small numbers of spin flips. The learning algorithm is par- 
ticularly simple, and is related to PCA by a scalar transformation of the correlation 
matrix. 
Generative Model 
Figure I shows a schematic diagram of the generafive process. As in PCA, the 
model assumes that the data is generated by a small number P of continuous hidden 
variables Yi. Each of the hidden variables are assumed to be drawn independently 
from a normal distribution with unit variance: 
P(Yi) = exp(-y /2) / v r. (1) 
The continuous hidden variables are combined using the feedforward weights Wij, 
and the N binary output units are then calculated using the sign of the feedforward 
activations' 
P 
x, = w,j yj (2) 
j=l 
si = sgn(xi). (3) 
Since binary data is commonly obtained by thresholding, it seems reasonable that 
a proper generafive model should incorporate such a clipping nonlinearity. The 
generative process is similar to that of a sigmoidal belief network with continuous 
hidden units at zero temperature. The nonlinearity will alter the relationship be- 
tween the correlations of the binary variables and the weight matrix W as described 
below. 
The real-valued Gaussian variables xi are exactly analogous to the visible variables 
of conventional PCA. They lie on a linear hyperplane determined by the span of 
the matrix W, and their correlation matrix is given by: 
C  = (xxr> = WW r. (4) 
Learning a Continuous Hidden Variable Model for Binary Data 517 
Y2 
/' x3_> 
.ooo?' Xl= Wljyj > 0 
 Yl 
', 
� >0 
, , ,, x 2- 
Figure 2: Binary spin configurations si in the vector space of continuous hidden 
variables yj with P = 2 and N - 3. 
By construction, the correlation matrix C xx has rank P which is much smaller 
than the number of components N. Now consider the binary output variables 
si = sgn(xi). Their correlations can be calculated from the probability distribution 
of the Gaussian variables xi: 
(Css)ij = (sisj) = / H dYk P(xk)sgn(xi)sgn(xj) 
k 
(5) 
where 
1 
P(x): (2)N/2 exp - x r(Cx)-x . (6) 
The integrals in Equation 5 can be done analytically, and yield the surprisingly 
simple result: 
(CSS)iJ = sin- /x � (7) 
V..ii vjj 
Thus, the correlations of the clipped binary variables C** are related to the corre- 
lations of the corresponding Gaussian variables C xx through the nonlinear arcsine 
function. The normalization in the denominator of the arcsine argument reflects the 
fact that the sign function is unchanged by a scale change in the Gaussian variables. 
Although the correlation matrix C ss and the generating correlation matrix C xx are 
easily related through Equation 7, they have qualitatively very different properties. 
In general, the correlation matrix C** will no longer have the low rank structure of 
C . As illustrated by the translationally invariant example in the next section, the 
spectrum of C** may contain a whole continuum of eigenvalues even though C x 
has only a few nonzero eigenvalues. 
PCA is typically used for dimensionality reduction of real variables; can this model 
be used for compressing the binary outputs si? Although the output correlations 
C** no longer display the low rank structure of the generating C x, a more appropri- 
ate measure of data compression is the entropy of the binary output states. Consider 
how many of the 2 v possible binary states will be generated by the clipping process. 
The equation xi = Yj Wijyj '- 0 defines a P- I dimensional hyperplane in the 
P-dimensional state space of hidden variables yj, which are shown as dashed lines 
in Figure 2. These hyperplanes partition the half-space where si - +1 from the 
518 D. D. Lee and H. Sompolinsky 
s i =+1 
10 3 
I S i =-1 0  
1 
I I 
 10  
I I 
I o 
i1o 
I I 10 
I I 
10' I 0o 
%_ 
CXX - -, 
10' 10 2 
I=ienvalue rank 
Figure 3: Translationally invariant binary spin distribution with N - 256 units. 
Representative samples from the distribution are illustrated on the left, while the 
eigenvalue spectrum of C ss and C xx are plotted on the right. 
region where si = -1. Each of the N spin variables will have such a dividing hyper- 
plane in this P-dimensional state space, and all of these hyperplanes will generically 
be unique. Thus, the total number of spin configurations si is determined by the 
number of cells bounded by N dividing hyperplanes in P dimensions. The number 
of such cells is approximately N p for N >> P, a well-known result from perceptrons 
[6]. To leading order for large N, the entropy of the binary states generated by this 
process is then given by $ = P log N. Thus, the entropy of the spin configurations 
generated by this model is directly proportional to the number of hidden variables 
P. 
How is the topology of the binary spin configurations si related to the PCA man- 
ifold structure of the continuous variables xi? Each of the generated spin states is 
represented by a polytope cell in the P dimensional vector space of hidden variables. 
Each polytope has at least P + 1 neighboring polytopes which are related to it by a 
single or small number of spin flips. Therefore, although the state space of binary 
spin configurations is discrete, the continuous manifold structure of the underlying 
Gaussian variables in this model is manifested as binary output configurations with 
low entropy that are connected with small Hamming distances. 
Translationally Invariant Example 
In principle, the weights W could be learned by applying maximum likelihood to 
this generarive model; however, the resulting learning algorithm involves analyti- 
cally intractable multi-dimensional integrals. Alternatively, approximations based 
upon mean field theory or importance sampling could be used to learn the appropri- 
ate parameters [7]. However, Equation 7 suggests a simple learning rule that is also 
approximate, but is much more computationally efficient [8]. First, the binary cor- 
relation matrix C s* is computed from the data. Then the empirical C ** is mapped 
into the appropriate Gaussian correlation matrix using the nonlinear transforma- 
tion: C x = sin(rC*S/2). This results in a Gaussian correlation matrix where the 
variances of the individual xi are fixed at unity. The weights W are then calculated 
using the conventional PCA algorithm. The correlation matrix C  is diagonalized, 
and the eigenvectors with the largest eigenvalues are used to form the columns of 
Learning a Continuous Hidden Variable Model for Binary Data 519 
W to yield the best low rank approximation C xx  WW T. Scaling the variables xi 
will result in a correlation matrix C xx with slightly different eigenvalues but with 
the same rank. 
The utility of this transformation is illustrated by the following simple example. 
Consider the distribution of N - 256 binary spins shown in Figure 3. Half of the 
spins are chosen to be positive, and the location of the positive bump is arbitrary 
under the periodic boundary conditions. Since the distribution is translationally 
invariant, the correlations Cf depend only on the relative distance between spins 
I i - Jl. The eigenvectors are the Fourier modes, and their eigenvalues correspond 
to their ov
