Active Data Clustering 
Thomas Hofmann 
Center for Biological and Computational Learning, MIT 
Cambridge, MA 02139, USA, hofmann@ai.mit.edu 
Joachim M. Buhmann 
Institut ffir Informatik III, UniversitKt Bonn 
RSmerstrafie 164, D-53117 Bonn, Germany, jb@cs.uni-bonn.de 
Abstract 
Active data clustering is a novel technique for clustering of proxim- 
ity data which utilizes principles from sequential experiment design 
in order to interleave data generation and data analysis. The pro- 
posed active data sampling strategy is based on the expected value 
of information, a concept rooting in statistical decision theory. This 
is considered to be an important step towards the analysis of large- 
scale data sets, because it offers a way to overcome the inherent 
data sparseness of proximity data. We present applications to unsu- 
pervised texture segmentation in computer vision and information 
retrieval in document databases. 
I Introduction 
Data clustering is one of the core methods for numerous tasks in pattern recognition, 
exploratory data analysis, computer vision, machine learning, data mining, and in 
many other related fields. Concerning the data representation it is important to 
distinguish between vectorial data and proximity data, cf. [Jain, Dubes, 1988]. In 
vectorial data each measurement corresponds to a certain 'feature' evaluated at an 
external scale. The elementary measurements of proximity data are, in contrast, 
(dis-)similarity values obtained by comparing pairs of entities from a given data set. 
Generating proximity data can be advantageous in cases where 'natural' similarity 
functions exist, while extracting features and supplying a meaningful vector-space 
metric may be difficult. We will illustrate the data generation process for two 
exemplary applications: unsupervised segmentation of textured images and data 
mining in a document database. 
Textured image segmentation deals with the problem of partitioning an image into 
regions of homogeneous texture. In the unsupervised case, this has to be achieved on 
Active Data Clustering 529 
the basis of texture similarities without prior knowledge about the occuring textures. 
Our approach follows the ideas of [Geman et al., 1990] to apply a statistical test to 
empirical distributions of image features at different sites. Suppose we decided to 
work with the gray-scale representation directly. At every image location p: (x, y) 
we consider a local sample of gray-values, e.g., in a squared neighborhood around p. 
Then, the dissimilarity between two sites pi and pj is measured by the significance of 
rejecting the hypothesis that both samples were generated from the same probability 
distribution. Given a suitable binning (t) <<R and histograms fi, fj, respectively, 
we propose to apply a x2-test, i.e., 
Dij --  (fi(t) - fij(t))  with fij(t) : fi(tk) + fj(tk) 
 fij(t) ' 2 (1) 
In fact, our experiments are based on a multi-scale Gabor filter representation in- 
stead of the raw data, cf. [Hofmann et al., 1997] for more details. The main advan- 
tage of the similarity-based approach is that it does not reduce the distributional 
information, e.g., to some simple first and second order statistics, before comparing 
textures. This preserves more information and also avoids the ad hoc specifica- 
tion of a suitable metric like a weighted Euclidean distance on vectors of extracted 
moment statistics. 
As a second application we consider structuring a database of documents for im- 
proved information retrieval. Typical measures of association are based on the 
number of shared index terms [Van Rijsbergen, 1979]. For example, a document 
is represented by a (sparse) binary vector B, where each entry corresponds to the 
occurrence of a certain index term. The dissimilarity can then be defined by the 
cosine measure 
Dij = 1 -(BBj/V/[Bi[[BjD 
Notice, that this measure (like many other) may violate the triangle inequality. 
2 Clustering Sparse Proximity Data 
In spite of potential advantages of similarity-based methods, their major drawback 
seems to be the scaling behavior with the number of data: given a dataset with N 
entities, the number of potential pairwise comparisons scales with (9(N2). Clearly, 
it is prohibitive to exhaustively perform or store all dissimilarities for large datasets, 
and the crucial problem is how to deal with this unavoidable data sparseness. More 
fundamentally, it is already the data generation process which has to solve the 
problem of experimental design, by selecting a subset of pairs (i, j) for evaluation. 
Obviously, a meaningful selection strategy could greatly profit from any knowledge 
about the grouping structure of the data. This observation leads to the concept of 
performing a sequential experimental design which interleaves the data clustering 
with the data acquisition process. We call this technique active data clustering, 
because it actively selects new data, and uses tentative knowledge to estimate the 
relevance of missing data. It amounts to inferring from the available data not 
only a grouping structure, but also to learn which future data is most relevant for 
the clustering problem. This fundamental concept may also be applied to other 
unsupervised learning problems suffering from data sparseness. 
The first step in deriving a clustering algorithm is the specification of a suitable 
objective function. In the case of similarity-based clustering this is not at all a 
trivial problem and we have systematically developed an axiomatic approach based 
on invariance and robustness principles [Hofmann et al., 1997]. Here, we can only 
530 T. Hofrnann and J. M. Buhmann 
give some informal justifications for our choice. Let us introduce indicator func- 
tions to represent data partitionings, Mir being the indicator function for entity oi 
belonging to cluster Cu. For a given number K of clusters, all Boolean functions 
are summarized in terms of an assignment matrix M E {0, 1} NXK. Each row of M 
is required to sum to one in order to guarantee a unique cluster membership. To 
distinguish between known and unknown dissimilarities, index sets or neighborhoods 
J- (Jl,..., JN) are introduced. Ifj  Af this means the value of Dij is available, 
otherwise it is not known. For simplicity we assume the dissimilarity measure (and 
in turn the neighborhood relation) to be symmetric, although this is not a necessary 
requ.irement. With the help of these definition the proposed criterion to assess the 
quality of a clustering configuration is given by 
N K 
i=1 
diu = 35uoi (3) 
7-/ additively combines contributions diu for each entity, where diu corresponds to 
the average dissimilarity to entities belonging to cluster C. In the sparse data case, 
averages are restricted to the fraction of entities with known dissimilarities, i.e., the 
subset of entities belonging to 
3 Expected Value of Information 
To motivate our active data selection criterion, consider the simplified sequential 
problem of inserting a new entity (or object) ON to a database of N - i entities 
with a given fixed clustering structure. Thus we consider the decision problem of 
optimally assigning the new object to one of the K clusters. If all dissimilarities 
between objects oi and object ON are known, the optimal assignment only depends 
on the average dissimilarities to objects in the different clusters, and hence is given 
7_:11 
, , MjuD2vj 
MNo* = 1  a* = argmindNu , where dNu = (4) 
u EJ__-i I Mju 
by 
For incomplete data, the total population averages dv u are replaced by point esti- 
mators dNu obtained by restricting the sums in (4) to AfN, the neighborhood of ON. 
Let us furthermore assume we want to compute a fixed number L of dissimilarities 
before making the terminal decision. If the entities in each cluster are not further 
distinguished, we can pick a member at random, once we have decided to sample 
from a cluster Cu. The selection problem hence becomes equivalent to the prob- 
lem of optimally distributing L measurements among K populations, such that the 
risk of making the wrong decision based on the resulting estimates dNu is minimal. 
More formally, this risk is given by 7g = dv  - dw. , where ( is the decision based 
on the subpopulation estimates {dNu} and a* is the true optimum. 
To model the problem of selecting an optimal experiment we follow the Bayesian 
approach developed by Raiffa & Schlaifer [Raiffa, Schlaifer, 1961] and compute the 
so-called Expected Value of Sampling Information (EVSI). As a fundamental step 
this involves the calculation of distributions for the quantities dNu. For reasons 
of computational efficiency we are assuming that dissimilarities resulting from a 
comparison with an object in cluster C are normally distributed I with mean dv  
and variance rrvu 2. Since the variances are nuisance parameters the risk func- 
tion 7g does not depend on, it suffices to calculate the marginal distribution of 
 Other computationally more expensive choices to model within cluster dissimilarities 
are skewed distributions like the Gamma-distribution. 
Active Data Clustering 531 
a) 
b) 
8oo 
-200 
-400 
-600 
-800 
0 
RANDOM 
ACTIVE 
50OO 100000 150OO 
# samples 
20O00O 
Figure 1: (a) Gray-scale visualization of the generated proximity matrix (N = 800). 
Dark/light gray values correspond to low/high dissimilarities respectively, Dij being 
encoded by pixel (i, j). (b) Sampling snapshot for active data clustering after 60000 
samples, queried values are depicted in white. (c) Costs evaluated on the complete 
data for sequential active and random sampling. 
dv u. For the class of statistical models we will consider in the sequel the empiri- 
cal mean dvu, the unbiased variance estimator rrv u and the sample size rnvu are 
a sufficient statistic. Depending on these enpirical quantities the marginal poste- 
rior distribution of dv  for uninformative priors is a Student t distrib
