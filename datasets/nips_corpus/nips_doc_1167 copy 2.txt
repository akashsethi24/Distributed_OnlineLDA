Radial Basis Function Networks and Complexity 
Regularization in Function Learning 
Adam Krzyak 
Department of Computer Science 
Concordia University 
Montreal, Canada 
krzy zak @ cs.concordia.ca 
Tamils Linder 
Dept. of Math. & Comp. Sci. 
Technical University of Budapest 
Budapest, Hungary 
linder@inf.bme.hu 
Abstract 
In this paper we apply the method of complexity regularization to de- 
rive estimation bounds for nonlinear function estimation using a single 
hidden layer radial basis function network. Our approach differs from 
the previous complexity regularization neural network function learning 
schemes in that we operate with random covering numbers and l metric 
entropy, making it possible to consider much broader families of activa- 
tion functions, namely functions of bounded variation. Some constraints 
previously imposed on the network parameters are also eliminated this 
way. The network is trained by means of complexity regularization in- 
volving empirical risk minimization. Bounds on the expected risk in 
terms of the sample size are obtained for a large class of loss functions. 
Rates of convergence to the optimal loss are also derived. 
1 INTRODUCTION 
Artificial neural networks have been found effective in learning input-output mappings from 
noisy examples. In this learning problem an unknown target function is to be inferred from a 
set of independent observations drawn according to some unknown probability distribution 
from the input-output space IR d x IR. Using this data set the learner tries to determine a 
function which fits the data in the sense of minimizing some given empirical loss function. 
The target function may or may not be in the class of functions which are realizable by the 
learner. In the case when the class of realizable functions consists of some class of artificial 
neural networks, the above problem has been extensively studied from different viewpoints. 
In recent years a special class of artificial neural networks, the radial basis function (RBF) 
networks have received considerable attention. RBF networks have been shown to be 
the solution of the regularization problem in function estimation with certain standard 
smoothness functionals used as stabilizers (see [5], and the references therein). Universal 
198 A. Krzyak and T. Linder 
convergence of RBF nets in function estimation and classification has been proven by 
Krzyak et al. [6]. Convergence rates of RBF approximation schemes have been shown to 
be comparable with those for sigmoidal nets by Girosi and Anzellotti [4]. In a recent paper 
Niyogi and Girosi [9] studied the tradeoff between approximation and estimation errors and 
provided an extensive review of the problem. 
In this paper we consider one hidden layer RBF networks. We look at the problem of 
choosing the size of the hidden layer as a function of the available training data by means 
of complexity regularization. Complexity regularization approach has been applied to 
model selection by Barron [ 1 ], [2] resulting in near optimal choice of sigmoidal network 
parameters. Our approach here differs from Barron's in that we are using l metric en- 
tropy instead of the supremum norm. This allows us to consider a more general class 
of activation function, namely the functions of bounded variation, rather than a restricted 
class of activation functions satisfying a Lipschitz condition. For example, activations with 
jump discontinuities are allowed. In our complexity regularization approach we are able 
to choose the network parameters more freely, and no discretization of these parameters is 
required. For RBF regression estimation with squared error loss, we considerably improve 
the convergence rate result obtained by Niyogi and Girosi [9]. 
In Section 2 the problem is formulated and two results on the estimation error of complexity 
regularized RBF nets are presented: one for general loss functions (Theorem 1) and a 
sharpened version of the first one for the squared loss (Theorem 2). Approximation bounds 
are combined with the obtained estimation results in Section 3 yielding convergence rates 
for function learning with RBF nets. 
2 PROBLEM FORMULATION 
The task is to predict the value of a real random variable Y upon the observation of an IR d 
valued random vector X. The accuracy of the predictor f � IR d  R is measured by the 
expected risk 
J(f) -- EL(f(X), Y), 
where L � IR x IR -- IR + is a nonnegative loss function. It will be assumed that there exists 
a minimizing predictor f* such that 
J(f*) = infJ(f). 
A good predictor f is to be determined based on the data (X, Yl), � �., (X, Y) which 
are i.i.d. copies of (X, Y). The goal is to make the expected risk EJ(f) as small as 
possible, while fr is chosen from among a given class Y of candidate functions. 
In this paper the set of candidate functions Y will be the set of single-layer feedforward 
neural networks with radial basis function activation units and we let  = tJk=k, where 
k is the family of networks with k hidden nodes whose weight parameters satisfy certain 
constraints. In particular, for radial basis functions characterized by a kernel K: IR +  IR, 
 is the family of networks 
k 
f(x): (Ix - - cd) + wo, 
i----I 
where w0, w..., w are real numbers called weights, c,..., c C IR , Ai are nonnegative 
definite d x d matrices, and x t denotes the transpose of the column vector x. 
The complexity regularization principle for the learning problem was introduced by Vapnik 
[10] and fully developed by Barron [1], [2] (see also Lugosi and Zeger [8]). It enables 
the learning algorithm to choose the candidate class Y automatically, from which is picks 
Radial Basis Function Networks and Complexity Regularization 199 
the estimate function by minimizing the empirical error over the training data. Complexity 
regularization penalizes the large candidate classes, which are bound to have small approx- 
imation error, in favor of the smaller ones, thus balancing the estimation and approximation 
errors. 
Let .T be a subset of a space A' of real functions over some set, and let p be a pseudometric 
on A'. For e > 0 the covering number N(e, Y, p) is defined to be the minimal number of 
closed e balls whose union cover .T. In other words, N(e, .T, p) is the least integer such 
that there exist f,..., fN with N = N(e, .T, p) satisfying 
sup min p( f , f i ) _ e. 
fEr l<i<N 
In our case,  is a family of real functions on IR ', and for any two functions f and g, p is 
given by 
1 lf(zi) - g(zi)l, 
p(f, g) = 
i----1 
where z,..., z are n given points in IR ' . In this case we will use the notation N (e, .T, p) - 
N(e, .T, z�), emphasizing the dependence of the metric p on z� = (Zl,..., z). Let us 
define the families of functions 7-/k, k = 1,2,... by 
7-gk = {L(f(-), .)-f G .T'). 
Thus each member of 7-/ maps IR d+ 1 into R. It will be assumed that for each k we are given 
a finite, almost sure uniform upper bound on the random covering numbers N(e, 7-/, Z�), 
where Z� = ((X, Y),..., (X,, Y,)). We may assume without loss of generality that 
N(e, 7-/) is monotone decreasing in . Finally, assume that L(.f(X), Y) is uniformly 
almost surely bounded by a constant B, i.e., 
P{L(f(X),Y)_<B)= 1, fG.Tk, k= 1,2,... 
The complexity penalty of the kth class for n training samples is a nonnegative number A  
satisfying 
A >_ /128B 2 log N(A/8, 7-/) + ck (2) 
where the nonnegative constants c satisfy -= e -ok _< 1. Note that since N(e, 7-[) is 
nonincreasing in e, it is possible to choose such A  for all k and n. The resulting complexity 
penalty optimizes the upper bound on the estimation error in the proof of Theorem 1 below. 
We can now define our estimate. Let 
n 
fen = argmin J,(f) = argmin 1  L(f(Xi),Yi), 
that is, fk, minimizes over .T the empirical risk for n training samples. The penalized 
empirical risk is defined for each f G .T as 
',(f) = Jr(f) + 
The estimate f is then defined as the f, minimizing the penalized empirical risk over all 
classes: 
f = argmin &(fk). (3) 
fn:k_> l 
We have the following theorem for the expected estimation error of the above complexity 
regularization scheme. 
200 A. Krzyak and T. Linder 
Theorem 1 For any n and k the complexity regularization estimate (3) satisfies 
EJ(f,)- J(f*) < rain (/k, + inf J(f)- J(f*)) 
-- k>_l fE.T  
where 
tgkn = rain (u + 9Be-nu2/(S12B2)) . 
u _ 4A: ,, 
Assuming without loss of generality that log N(e, 7-/) _> 1, it is easy to see that the choice 
A, = 128B 21�gN(B/x/'7t) + c (4) 
n 
satisfies (2). 
2.1 SQUARED ERROR LOSS 
For the special case when 
L(x, y) = (x - y)2 
we can obtain a better upper bound. The estimate will be the same as before, but instead of 
(2), the complexity penalty Ak, now has to satisfy 
A, _> C11�gN(A'/C2'y) + c (5) 
whereCt = 3499C 4, C2 = 256C 3, andC = max{B, 1}. Here N(e, .Tk)is a uniform upper 
bound on the random ll covering numbers N(e, .T, X). Assume that the class .T - 
is convex, and let  be the closure of .T in L2(/.t), where tt denotes the distribution of X. 
Then there is a unique f e  whose squared loss J(f) achieves inffe J(f). We have the 
following bound on the difference EJ (f,) - J(f). 
Theorem 2 Assume that Y = UYk is a convex set of functions, and consider the squared 
error loss. Suppose that If(x)l <_ B for all z c IR d and f  , and P(IYI > B) - o. 
Then complexity regularization estimate with complexity penalty satisfying (5) gives 
EJ(f)-J(f)<2min A+ inf J(f)-J(f) +2n. 
The proof of this result uses an idea of Barron [1 ] and a Bernstein-type uniform probability 
inequality recently obtained by Lee et al. [7]. 
3 RBF NETWORKS 
We will consider radial basis function (RBF) networks with one hidden layer. Such a 
network is characterized by a kernel K � IR +  IR. An RBF net of k nodes is of the form 
k 
f(go) -- E WiI ([2 -- �iltAi[go - �i1) - wo, (6) 
i=1 
where w0, wl,..., w are 
