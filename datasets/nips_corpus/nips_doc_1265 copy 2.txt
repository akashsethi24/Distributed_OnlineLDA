Representing Face Images for Emotion 
Classification 
Curtis Padgett 
Department of Computer Science 
University of California, San Diego 
La Jolla, CA 92034 
Garrison Cottrell 
Department of Computer Science 
University of California, San Diego 
La Jolla, CA 92034 
Abstract 
We compare the generalization performance of three distinct rep- 
resentation schemes for facial emotions using a single classification 
strategy (neural network). The face images presented to the clas- 
sifters are represented as: full face projections of the dataset onto 
their eigenvectors (eigenfaces); a similar projection constrained to 
eye and mouth areas (eigenfeatures); and finally a projection of 
the eye and mouth areas onto the eigenvectors obtained from 32x32 
random image patches from the dataset. The latter system achieves 
86% generalization on novel face images (individuals the networks 
were not trained on) drawn from a database in which human sub- 
jects consistently identify a single emotion for the face. 
I Introduction 
Some of the most successful research in machine perception of complex natural 
image objects (like faces), has relied heavily on reduction strategies that encode 
an object as a set of values that span the principal component sub-space of the 
object's images [Cottrell and Metcalfe, 1991, Pentland et al., 1994]. This approach 
has gained wide acceptance for its success in classification, for the efficiency in which 
the eigenvectors can be calculated, and because the technique permits an imple- 
mentation that is biologically plausible. The procedure followed in generating these 
face representations requires normalizing a large set of face views ( mug-shots ) and 
from these, identifying a statistically relevant sub-space. Typically the sub-space is 
located by finding either the eigenvectors of the faces [Pentland et al., 1994] or the 
weights of the connections in a neural network [Cottrell and Metcalfe, 1991]. 
In this work, we classify face images based on their emotional content and examine 
how various representational strategies impact the generalization results of a clas- 
sifter. Previous work using whole face representations for emotion classification by 
Representing Face Images for Emotion Classification 895 
Cottrell and Metcalfe [Cottrell and Metcalfe, 1991] was less encouraging than re- 
suits obtained for face recognition. We seek to determine if the problem in Cottrell 
and Metcalfe's work stems from bad data (i.e., the inability of the undergraduates 
to demonstrate emotion), or an inadequate representation (i.e. eigenfaces). 
Three distinct representations of faces are considered in this work- a whole face 
representation similar to that used in previous work on recognition, sex, and emo- 
tion [Cottrell and Metcalfe, 1991]; a more localized representation based on the eyes 
(eigeneyes and eigenmouths) and mouth [Pentland et al., 1994]; and a representa- 
tion of the eyes and mouth that makes use of basis vectors obtained by princi- 
pal components of random image blocks. By examining the generalization rate of 
the classifiers for these different face representations, we attempt to ascertain the 
sensitivity of the representation and its potential for broader use in other vision 
classification problems. 
2 Face Data 
The dataset used in Cottrell and Metcalfe's work on emotions consisted of the faces 
of undergraduates who were asked to pose for particular expressions. However, 
reigned emotions by untrained individuals exhibit significant differences from the 
prototypical face expression [Ekman and Friesen, 1977]. These differences often re- 
suit in disagreement between the observed emotion and the expression the actor 
is attempting to feign. A reigned smile for instance, differs around the eyes when 
compared with a natural smile. The quality of the displayed emotion is one of 
the reasons cited by Cottrell and Metcalfe for the poor recognition rates achieved 
by their classifier. 
To reduce this possibility, we made use of a validated facial emotion database (Pic- 
tures of Facial Affect) assembled by Ekman and Friesen [Ekman and Friesen, 1976]. 
Each of the face images in this set exhibits a substantial agreement between the 
labeled emotion and the observed response of human subjects. The actors used in 
this database were trained to reliably produce emotions using Facial Action Coding 
System [Ekman and Friesen, 1977] and their images were presented to undergrad- 
uates for testing. The agreement between the emotion the actor was required to 
express and the students' observations was at least 70% on all the images incor- 
porated in the database. We digitized a total of 97 images from 12 individuals (6 
male, 6 female). Each portrays one of 7 emotions- happy, sad, fear, anger, surprise, 
disgust or neutral. With the exception of the neutral faces, each image in the set is 
labeled with a response vector of the remaining six emotions indicating the fraction 
of total respondents classifying the image with a particular emotion. 
Each of the images was linearly stretched over the 8 bit greyscale range to re- 
duce lighting variations. Although care was taken in collecting the original images, 
natural variations in head size and the mouth's expression resulted in significant 
variation in the distance between the eyes (2.7 pixels) anal in the vertical distance 
from the eyes to the mouth (5.0 pixels). To achieve scale invariance, each image was 
scaled so that prominent facial features were located in the same image region. Eye 
and mouth templates were constructed from a number of images, and the most cor- 
related template was used to localize the respective feature. Similar techniques have 
been employed in previous work on faces [Brunelli and Poggio, 1993]. Examples of 
the normalized images and typical facial expressions can be found in Figure 1. 
896 C. Padgett and G. W. Cottrell 
Figure 1: The image regions from which the representations are derived. Image A 
is a typical normalized and cropped image used to generate the full face eigenvec- 
tors. Image B depicts the feature regions from which the feature eigenvectors are 
calculated. Image C indicates each of the block areas projected onto the random 
block eigenvectors. 
3 Representation 
From the normalized database, we develop three distinct representations that form 
independent pattern sets for a single classification scheme. The selected representa- 
tions differ in their scope (features or whole face) and in the nature of the sub-space 
(eigen- faces/features or eigenvectors of random image patches). The more familiar 
representational schemes (eigenfaces, eigenfeatures) are based on PCA of aligned 
features or faces. They hlve been shown to provide a reasonably compact represen- 
tation for recognition purposes but little is known about their suitability for other 
classification tasks. 
Random image patches are used to identify an alternative sub-space from which a 
set of localized face feature patterns are generated. This space is different in that 
the sub-space is more general, the variance captured by the leading eigenvectors is 
derived from patches drawn randomly over the set of face images. As we seek to 
develop generalizations across the rather small portion of image space containing 
faces or features, perturbations in this space will hopefully reflect more about class 
characteristics than individual distinctions. 
For each of the pattern sets, we normalized the resultant set of values obtained 
from their projections on the eigenvectors by their standard deviation to produce Z 
scores. The Z score obtained from each image constitutes a single input to the neural 
network classifier. The highest valued eigenvectors typically contain more average 
features so that presumably they would be more suitable for object classification. 
All the representations will make use of the top k principal components. 
The full-faced pattern has proved to be quite useful in identification and the 
same techniques using face features have also been valuable [Pentland et al., 1994, 
Cottrell and Metcalfe, 1991]. However representations useful for identification of 
individuals may not be suitable for emotion recognition. In determining the ap- 
propriate emotion, structural differences in faces need to be suppressed. One way 
to accomplish this is to eliminate portions of the face image where variation pro- 
vides little information with respect to emotion. Local changes in facial muscles 
around the eyes and mouth are generally associated with our perception of emo- 
tions [Ekman and Friesen, 1977]. The full face images presumably contain much 
information that is simply irrelevant to the task at hand which could impact the 
ability of the classifier to uncover the signal. 
Representing Face Images for Emotion Classification 897 
The feature based representations are derived from local windows around the eyes 
and mouth of the normalized whole face images (see Fig. lB). The eigenvectors of 
the feature sub-space are determined independently for each feature (left/right eye 
and mouth). A face pattern is generated by projecting the particular facial features 
on their respective eigenvectors. 
The random block pattern set is formed from image blocks extracted around the 
feature locations (see Fig. 1C). The areas around each eye are divided into two ver- 
tically overlapping blocks of size 32x32 and the mouth is sectioned into three. How- 
ever, instead of performing PCA on each individual block or all of them together, 
a more general PCA of random 32x32 blocks taken over the entire image was used 
to generate the eigenvectors. We used random blocks to reduce the uniqueness of a 
projection for a single individual and provide a more reasonable model of the early 
visual system. The final input pattern consists of the normalized projection of the 
seven extracted blocks for the image on the top n principal co
