On 
Neural 
Networks with Minimal 
Weights 
Vasken Bohossian 
Jehoshua Bruck 
California Institute of Technology 
Mail Code 136-93 
Pasadena, CA 91125 
E-maih {vincent, bruckparadise.caltech.edu 
Abstract 
Linear threshold elements are the basic building blocks of artificial 
neural networks. A linear threshold element computes a function 
that is a sign of a weighted sum of the input variables. The weights 
are arbitrary integers; actually, they can be very big integers-- 
exponential in the number of the input variables. However, in 
practice, it is difficult to implement big weights. In the present 
literature a distinction is made between the two extreme cases: 
linear threshold functions with polynomial-size weights as opposed 
to those with exponential-size weights. The main contribution of 
this paper is to fill up the gap by further refining that separation. 
Namely, we prove that the class of linear threshold functions with 
polynomial-size weights can be divided into subclasses according 
to the degree of the polynomial. In fact, we prove a more general 
result--that there exists a minimal weight linear threshold function 
for any arbitrary number of inputs and any weight size. To prove 
those results we have developed a novel technique for constructing 
linear threshold functions with minimal weights. 
1 Introduction 
Human brains are by far superior to computers for solving hard problems like combi- 
natorial optimization and image and speech recognition, although their basic build- 
ing blocks are several orders of magnitude slower. This observation has boosted 
interest in the field of artificial neural networks [Hop field 82], [Rumelhart 82]. The 
latter are built by interconnecting multiple artificial neurons (or linear threshold 
gates), whose behavior is inspired by that of biological neurons. Artificial neural 
networks have found promising applications in pattern recognition, learning and 
On Neural Networks with Minimal Weights 247 
other data processing tasks. However most of the research has been oriented to- 
wards the practical aspect of neural networks, simulating or building networks for 
particular tasks and then comparing their performance with that of more traditional 
methods for those particular tasks. To compare neural networks to other compu- 
tational models one needs to develop the theoretical settings in which to estimate 
their capabilities and limitations. 
1.1 Linear Threshold Gate 
The present paper focuses on the study of a single linear threshold gate (artificial 
neuron) with binary inputs and output as well as integer weights (synaptic coeffi- 
cients). Such a gate is mathematically described by a linear threshold function. 
Definition I (Linear Threshold Function) 
A linear threshold function of n variables is a Boolean function f 
-1, 1} that can be written as 
f 1 
f(M) = sgn(F(�)) = -1 
, for F(37) >_ 0 
, where F(37) = t-37 = y wixi 
, otherwise 
i=l 
for any 37 6 {-1, 1) and a fixed  6 Z. 
Although we could allow the weights wi to be real numbers, it is known [Muroga 71], 
[Raghavan 88] that for a, binary input neuron, one needs O(n log n) bits per weight, 
where n is the number of inputs. So in the rest of the paper, we will assume without 
loss of generality that all weights are integers. 
1.2 Motivation 
Many experimental results in the area of neural networks have indicated that the 
magnitudes of the coefficients in the linear threshold elements grow very fast with 
the size of the inputs and therefore limit the practical use of the network. One 
natural question to ask is the following. How limited is the computational power of 
the network if one limits oneself to threshold elements with only small growth in 
the size of the coefficients? To answer that question we have to define a measure of 
the magnitudes of the weights. Note that, given a function f, the weight vector  
is not unique (see Example I below). 
Definition 2 (Weight Space) 
Given a lineat' threshold function f we define W as the set of all weights that satisfy 
Definition 1, that is W 
Here follows a measure of the size of the weights. 
Definition 3 (Minimal Weight Size) 
We define the size of a weight vector as the sum of the absolute values of the weights. 
The minimal weight size of a linear threshold function is defined as: 
S[f] = min ( Iw, I) 
W  
i----1 
The particular vector that achieves the minimum is called a minimal weight vector. 
Naturally, S[f] is a function of n. 
248 V. BOHOSSIAN, J. BRUCK 
It has been shown [Hastad 94], [Myhill 611, [Shawe-Taylor 92], [Siu 91] that there 
exists a linear threshold function that can be implemented by a single threshold 
element with exponentially growing weights, S[f] ~ 2 ', but cannot be implemented 
by a threshold element with smaller: polynomialy growing weights, S[f]~ n a, d 
constant. In light of that result the above question was dealt with by defining a 
class within the set of linear threshold functions: the class of functions with small 
(i.e. polynomialy growing) weights [Siu 91]. Most of the recent research focuses on 
the power of circuits with small weights, relative to circuits with arbitrary weights 
[Goldmann 92], [Goldman 93]. Rather than dealing with circuits we are interested 
in studying a single threshold gate. The main contribution of the present paper is 
to further refine the division of small versus arbitrary weights. We separate the set 
of functions with small weights into classes indexed by d, the degree of polynomial 
growth and show that all of them are non-empty. In particular, we develop a 
technique for proving that a weight vector is minimal. We use that technique to 
construct a function of size S[f] = s for an arbitrary s. 
1.3 Approach 
The main difficulty in analyzing the size of the weights of a threshold element is due 
to the fact that a single linear threshold function can be implemented by different 
sets of weights as shown in the following exanple. 
Example 1 (A Threshold Function with Minimal Weights) 
Consider the following two sets of weights (weight vectors). 
t 1 = (1 24), FI() -' Xl +2x2+4xa 
//2 -- (2 4 8), F.2( ) -- 2Zl -[- 4x2 -]- 8z3 
They both implement the same threshold function 
f(�) = sgn(Fe(Z)) = sgn(2F1 (')) = sgn(F (3)) 
A closer look reveals that f(�) = sgn(xa), implying that none of the above weight 
vectors has minimal size. Indeed, the minimal one is a = (0 0 1) and S[f] = 1. 
It is in general difficult to determine if a given set of weights is minimal [Amaldi 93], 
[Willis 63]. Our technique consists of limiting the study to only a particular subset 
of linear threshold functions, a subset for which it is possible to prove that a given 
weight vector is minimal. That subset is loosely defined by the requirement that 
there exist input vectors for which .f() = .f(-). The existence of such a vector, 
called a root of f, puts a constraint on the weight vector used to implement f. The 
larger the set of roots - the larger the constraint on the set of weight vectors, which 
in turn helps determine the minimal one. A detailed description of the technique is 
given in Section 2. 
1.4 Organization 
Here follows a brief outline of the rest of the paper. Section 2 mathematically defines 
the setting of the problem as well as derives some basic results on the properties 
of functions that admit roots. Those results are used as building blocks for the 
proof of the main results in Section 3. It also introduces a construction method 
for functions with minimal weights. Section 3 presents the main result: for any 
weight size, s, and any number of inputs, n, there exists an n-input linear threshold 
function that requires weights of size S[f] = s. Section 4 presents some applications 
of the result of Section 3 and indicates future research directions. 
On Neural Networks with Minimal Weights 249 
2 Construction of Minimal Threshold Functions 
The present section defines the mathematical tools used to construct functions with 
minimal weights. 
2.1 Mathematical setting 
We are interested in constructing functions for which the minimal weight is easily 
determined. Finding the minimal weight involves a search, we are therefore inter- 
ested in finding functions with a constrained weight spaces. The following tools 
allows us to put constraints on W. 
Definition 4 (Root Space of a Boolean Function) 
A vector ff E {-1, 1 )'* such that f(v = f(-v is called a root of f. We define the 
root space, R, as the set of all roots of f. 
Definition 5 (Root Generator Matrix) 
For a given weight vector  E W and a root ff  R, the root generator matrix, 
G = (go), is a (n x k)-matrix, with entries in {-1,0, 1 ), whose rows ff are orthogonal 
to u7 and equal to 7 at all non-zero coordinates, namely, 
2. gij -- 0 or go = vj for all i and j. 
Example 2 (Root Generator Matrix) 
Suppose that we are given a linear threshold function specified by a weight 
vector  = (1,1,2,4,1,1,2,4). By inspection we determine one root ff = 
(1,1, 1, 1,-1,-1,-1,-1). Notice that Wl + w2 -w7 - 0 which can be written 
as tY' t = 0, where tY = (1, 1,0,0,0,0,-1,0) is a row of G. Set F= 7- 2tY. Since  
is equal to ff at all non-zero coordinates, F � {-1, 1} '. Also g.  = if. + tY' t = 0. 
We have generated a new root: F= (-1, -1, 1, 1,-1,-1, 1, -1). 
Lemma 6 (Orthogonality of G and W) 
For a given weight vector 7  W and a root ff  R 
fig r -' 6 
holds for any weight vector ff E W. 
Proof. For an arbitrary 3  W and an arbitrary row, g-, of G, let 
By definition offfi, 9   {-1,1)'* and 7 �  = 0. That implies f() = f(-O 
is a root of f. For any weight vector 3  W, sgn(3. ff) = sgn(-3. ff). Therefore 
3- 07- 2ffi) = 0 and finally, since if. 3 = 0 we get 3. g- = 0. [] 
Lemma 7 (Minimality) 
For a given weight vector   W and a root  E R if rank(G) = n - 1 (i.e. G 
has n - 1 independent rows) and Iwil = 1 for some i, then  is the minimal weight 
vector. 
Proof. From Lemma 6 any 
