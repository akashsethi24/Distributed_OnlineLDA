Mapping Classifier Systems 
Into Neural Networks 
Lawrence Davis 
BBN Laboratories 
BBN Systems and Technologies Corporation 
l0 Moulton Street 
Cambridge, MA 02238 
January 16, 1989 
Abstract
Classifier systems are machine learning systems incorporating a genetic al- 
gorithm as the learning mechanism. Although they respond to inputs that neural 
networks can respond to, their intemal structure, representation formalisms, and 
learning mechanisms differ markedly from those employed by neural network re- 
searchers in the same sorts of domains. As a result, one might conclude that these 
two types of machine learning formalisms are intrinsically different. This is one 
of two papers that, taken together, prove instead that classifier systems and neural 
networks are equivalent. In this paper, half of the equivalence is demonstrated 
through the description of a transformation procedure that will map classifier 
systems into neural networks that are isomorphic in behavior. Several alterations 
on the commonly-used paradigms employed by neural network researchers are 
required in order to make the transformation work. These alterations are noted 
and their appropriateness is discussed. The paper concludes with a discussion of 
the practical import of these results, and with comments on their extensibility. 
1 Introduction 
Classifier systems are machine learning systems that have been developed since the 
1970s by John Holland and, more recently, by other members of the genetic algorithm 
research community as well . Classifier systems are varieties of genetic algorithms 
-- algorithms for optimization and learning. Genetic algorithms employ techniques 
inspired by the process of biological evolution in order to evolve better and better 
1This paper has benefited from discussions with Wayne Mesard, Rich Sutton, Ron Williams, Stewart 
Wilson, Craig Shaefer, David Montana, Gii Syswerda and other members of BARGAIN, the Boston Area 
Research Group in Genetic Algorithms and Inductive Networks. 
50 Das 
individuals that are taken to be solutions to problems such as optimizing a function, 
traversing a maze, etc. (For an explanation of genetic algorithms, the reader is 
referred to [Goldberg 1989].) Classifier systems receive messages from an external 
source as inputs and organize themselves using a genetic algorithm so that they will 
learn to produce responses for internal use and for interaction with the external 
SOUrce. 
This paper is one of two papers exploring the question of the formal relationship 
between classifier systems and neural networks. As normally employed, the two sorts 
of algorithms are probably distinct, although a procedure for translating the operation 
of neural networks into isomorphic classifier systems is given in [Belew and Gherrity 
1988]. The technique Belew and Gherrity use does not include the conversion of the 
neural network learning procedure into the classifier system framework, and it appears 
that the technique will not support such a conversion. Thus, one might conjecture that 
the two sorts of machine learning systems employ learning techniques that cannot be 
reconciled, although if there were a subsumption relationship, Belew and Gherrity's 
result suggests that the set of classifier systems might be a superset of the set of 
neural networks. 
The reverse conclusion is suggested by consideration of the inputs that each sort 
of learning algorithm processes. When viewed as black boxes, both mechanisms 
for learning receive inputs, carry out self-modifying procedures, and produce outputs. 
The class of inputs that axe traditionally processed by classifier systems -- the class 
of bit strings of a fixed length -- is a subset of the class of inputs that have been 
traditionally processed by neural networks. Thus, it appears that classifier systems 
operate on a subset of the inputs that neural networks can process, when viewed as 
mechanisms that can modify their behavior. 
In fact, both these impressions are correct. One can translate classifier systems 
into neural networks, preserving their learning behavior, and one can translate neural 
networks into classifier systems, again preserving learning behavior. In order to do 
so, however, some specializations of each sort of algorithm must be made. This 
paper deals with the translation from classifier systems to neural networks and with 
those specializations of neural networks that are required in order for the translation 
to take place. The reverse translation uses quite different techniques, and is treated 
in [Davis 1989]. 
The following sections contain a description of classifier systems, a description of 
the transformation operator, discussions of the extensibility of the proof, comments 
on some issues raised in the course of the proof, and conclusions. 
2 Classifier Systems 
A classifier system operates in the context of an environment that sends messages to 
the system and provides it with reinforcement based on the behavior it displays. A 
classifier system has two components -- a message list and a population of rule-lilce 
entities called classifiers. Each message on the message list is composed of bits, and 
Mapping Classifier Systems Into Neural Networks 51 
each has a pointer to its source (messages may be generated by the environment or 
by a classifier.) Each classifier in the population of classifiers has three components: 
a match sty'rig made up of the characters 0,1, and # (for don't care); a message 
made up of the characters 0 and 1; and a strength. The top-level description of a 
classifier system is that it contains a population of production rules that attempt to 
match some condition on the message list (thus classifying some input) and post 
their message to the message list, thus potentially affecting the environment or other 
classifiers. Reinforcement from the environment is used by the classifier system to 
modify the strengths of its classifiers. Periodically, a genetic algorithm is invoked 
to create new classifiers, which replace certain members of the classifier set. (For 
an explanation of classifier systems, their potential as machine learning systems, and 
their formal properties, the reader is referred to [Holland et al 1986].) 
Let us specify these processing stages more precisely. A classifier system operates 
by cycling through a fixed list of procedures. In order, these procedures are: 
Message List Processing. 1. Clear the message list. 2. Post the environmental 
messages to the message list. 3. Post messages to the message list from classifiers 
in the post set of the previous cycle. 4. Implement environmental reinforcement by 
analyzing the messages on the message list and altering the strength of classifiers in 
the post set of the previous cycle. 
Form the Bid Set. 1. Determine which classifiers match a message in the 
message list. A classifier matches a message if each bit in its match field matches its 
corresponding message bit. A 0 matches a 0, a I matches a 1, and a # matches either 
bit. The set of all matching classifiers forms the current bid set. 2. Implement bid 
taxes by subtracting a portion of the strength of each classifier � in the bid set. Add 
the strength taken from � to the strength of the classifier or classifiers that posted 
messages matched by � in the prior step. 
Form the Post Set. 1. If the bid set is larger than the maximum post set size, 
choose classifiers stochastically to post from the bid set, weighting them in proportion 
to the magnitude of their bid taxes. The set of classifiers chosen is the post set. 
Reproduction Reproduction generally does not occur 'on every cycle. When it 
does occur, these steps are carried out: 1. Create n children from parents. Use 
crossover and/or mutation, choosing parents stochastically but favoring the strongest 
ones. (Crossover and mutation are two of the operators used in genetic algorithms.) 
2. Set the strength of each child to equal the average of the strength of that child's 
parents. (Note: this is one of many ways to set the strength of a new classifier. 
The transformation will work in analogous ways for each of them.) 3. Remove n 
members of the classifier population and add the n new children to the classifier 
population. 
3 Mapping Classifiers Into Classifier Networks 
The mapping operator that I shall describe maps each classifier into a classifier 
network. Each classifier network has links to environmental input units, links to 
52 Das 
other classifier networks, and match, post, and message units. The weights on the 
links leading to a match node and leaving a post node are related to the fields in 
the match and message lists in the classifier. An additional link is added to provide 
a bias term for the match node. (Note: it is assumed here that the environment 
posts at most one message per cycle. Modifications to the transformation operator to 
accommodate multiple environmental messages are described in the final comments 
of this paper.) 
Given a classifier system CS with n classifiers, each matching and sending mes- 
sages of length m, we can construct an isomorphic neural network composed of r 
classifier networks in the following way. For each classifier � in CS, we construct its 
corresponding classifier network, composed of r match nodes, I post node, and rn 
message nodes. One match node (the environmental match node) has links to inputs 
from the environment. Each of the other match nodes is linked to the message and 
post node of another classifier network. The reader is referred to Figure 2 for an 
example of such a transformation. 
Each match node in a classifier network has rn + 1 incoming links. The weights 
on the first m links are derived by applying the following transformation to the m 
elements of c's match field: 0 is associated with weight -1, 1 is associated with 
weight 1, and # is associated with weight 0. The weight. of the final link is set to 
rn 
