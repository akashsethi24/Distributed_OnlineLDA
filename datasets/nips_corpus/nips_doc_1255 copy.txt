On a Modification to the Mean Field EM 
Algorithm in Factorial Learning 
A. P. Dunmur D.M. Titterington 
Department of Statistics 
Maths Building 
University of Glasgow 
Glasgow G12 8QQ, UK 
alanstats.gla.ac.uk 
mike�stats.gla.ac.uk 
Abstract 
A modification is described to the use of mean field approxima- 
tions in the E step of EM algorithms for analysing data from latent 
structure models, as described by Ghahramani (1995), among oth- 
ers. The modification involves second-order Taylor approximations 
to expectations computed in the E step. The potential benefits of 
the method are illustrated using very simple latent profile models. 
I Introduction 
Ghahramani (1995) advocated the use of mean field methods as a means to avoid the 
heavy computation involved in the E step of the EM algorithm used for estimating 
parameters within a certain latent structure model, and Ghahramani & Jordan 
(1995) used the same ideas in a more complex situation. Dunrnur & Titterington 
(1996a) identified Ghahramani's model as a so-called latent profile model, they 
observed that Zhang (1992, 1993) had used mean field methods for a similar purpose, 
and they showed, in a simulation study based on very simple examples, that the 
mean field version of the EM algorithm often performed very respectably. By this 
it is meant that, when data were generated from the model under analysis, the 
estimators of the underlying parameters were efficient, judging by empirical results, 
especially in comparison with estimators obtained by employing the 'correct' EM 
algorithm: the examples therefore had to be simple enough that the correct EM 
algorithm is numerically feasible, although any success reported for the mean field 
432 A. P Dunmur and D. M. Titterington 
version is, one hopes, an indication that the method will also be adequate in more 
complex situations in which the correct EM algorithm is not implement able because 
of computational complexity. 
In spite of the above positive remarks, there were circumstances in which there was 
a perceptible, if not dramatic, lack of efficiency in the simple (naive) mean field 
estimators, and the objective of this contribution is to propose and investigate ways 
of refining the method so as to improve performance without detracting from the 
appealing, and frequently essential, simplicity of the approach. The procedure used 
here is based on a second order correction to the naive mean field well known in 
statistical physics and sometimes called the cavity or TAP method (Mezard, Parisi 
& Virasoro , 1987). It has been applied recently in cluster analysis (Hofmann & 
Buhmann, 1996). In Section 2 we introduce the structure of our model, Section 3 
explains the refined mean field approach, Section 4 provides numerical results, and 
Section 5 contains a statement of our conclusions. 
2 The Model 
The model under study is a latent profile model (Henry, 1983), which is a latent 
structure model involving continuous observables {xr : r - 1...p) and discrete 
latent variables {Yi: i - 1... d). The Yi are represented by indicator vectors such 
that for each i there is a single j such that Yij - I and yi - 0, for all k  j. The 
latent variables are connected to the observables by a set of weight matrices Wi 
in such a way that the distribution of the observations given the latent variables 
is a multivariate Gaussian with mean Yi WiYi and covariance matrix F. To ease 
the notation, the covariance matrix is taken to be the identity matrix, although 
extension is quite easy to the case where F is a diagonal matrix whose elements have 
to be estimated (Dunmur & Titterington, 1996a). Also to simplify the notation, 
the marginal distributions of the latent variables are taken to be uniform, so that 
the totality of unknown parameters is made up of the set of weight matrices, to be 
denoted by W -- (W, W2,..., Wd). 
3 Methodology 
In order to learn about the model we have available a dataset D = {x :/ = 1... N} 
of N independent, p dimensional realizations of the model, and we adopt the Max- 
imum Likelihood approach to the estimation of the weight matrices. As is typical 
of latent structure models, there is no explicit Maximum Likelihood estimate of the 
parameters of the model, but there is a version of the EM algorithm (Dempster, 
Laird & Rubin, 1977) that can be used to obtain the estimates numerically. The 
EM algorithm consists of a sequence of double steps, E steps and M steps. 
At stage m the E step, based on a current estimate W m- of the parameters, 
calculates 
w = w ), 
where, the expectation {- ) is over the latent variables y, and is conditional on D and 
W '-, and �c denotes the crucial part of the complete-data log-likelihood, given 
A Modification to Mean Field EM 433 
by 
The M step then maximizes Q with respect to W and gives the new parameter 
estimate Wm. 
For the simple model considered here, the M step gives 
-1 
where W - (W, W2,..., Wd) and yT _ (yT, yT,..., yT) and, for brevity, explicit 
mention of the conditioned quantities in the expectations (.) has been omitted. 
The above formula differs somewhat from that given by Ghahramani (1995). 
Hence we need to evaluate the sets of expectations (Yi) and ( YiY ) for each ex- 
ample in the dataset. (The superscript u is omitted, for clarity.) As pointed out 
in Ghahramani (1995), it is possible to evaluate these expectations directly by 
summing over all possible latent states. This has the disadvantage of becoming 
exponentially more expensive as the size of the latent space increases. 
The mean field approximation is well known in physics and can be used to reduce 
the computational complexity. At its simplest level, the mean field approximation 
replaces the joint expectations of the latent variables by the products of the indi- 
vidual expectations; this can be interpreted as bounding the likelihood from below 
(Saul, Jaakkola, Jordan, 1996). Here we consider a second order approximation, as 
outlined below. 
Since the latent variables are categorical, it is simple to sum over the state space 
of a single latent variable. Hence, following Parisi (1988), the expectations of the 
latent variables are given by 
= 
(1) 
where fj(ei) is the jth component of the softmax function; exp(eij)/Y.]e 
and the expectation (.) is taken over the remaining latent variables. The vector 
ei = {eij) contains the log probabilities (up to a constant) associated with each 
category of the latent variable for each example in the data set. For the simple 
model under study eij is given by 
(2) 
The expectation in (1) can be expanded in a Taylor series about the average, ( ei ), 
giving 0 2 
I fj(iei)) + O(Aea), (3) 
]el 
where Aeij -- eij -- (eij). The naive mean field approximation simply ignores all 
corrections. We can postulate that the second order fluctuations are taken care of 
434 A. P Dunmur and D. M. Titterington 
by a so called cavity field, (see, for instance, Mezard, Parisi & Virasoro, 1987, p.16), 
that is, 
(Yij ) = fj((ei) + hi), (4) 
where the vector of fields hi = {hik} has been introduced to take care of the 
correction terms. This equation may also be expanded in a Taylor series to give 
k 
Then, equating coefficients with (3) and after a little algebra, we get 
1 
k 
(5) 
where 5j is the Kronecker delta and, for the model under consideration, 
( AeijAei ) = (wit  Wm (( ymynT ) -- (ym } ( ynT )) wnTwi) (6) 
mni jk 
The naive mean field assumption may be used in (6), giving 
(( YiY ) - ( Yi ) ( Y ))t = 5ij ( Yi ) (5t - (Yit )). (7) 
Within the E step, for each realization in the data set, the mean fields (4), along 
with the cavity fields (5), can be evaluated by an iterative procedure which gives the 
individual expectations of the latent variables. The naive mean field approximation 
(7) is then used to evaluate the joint expectations ( yiY ). In the next section we 
report, for a simple model, the effect on parameter estimation of the use of cavity 
fields. 
4 Results 
Simulations were carried out using latent variable models (i) with 5 observables 
and 4 binary hidden variables and (ii) with 5 observables and 3 3-state hidden 
variables. The weight matrices were generated from zero mean Gaussian variables 
with standard deviation w. In order to make the M step trivial it was assumed that 
the matrices were known up to the scale parameter w, and this is the parameter 
estimated by the algorithm. A data set was generated using the known parameter 
and this was then estimated using straight EM, naive mean field (MF) and mean 
field with cavity fields (MFcav). 
Although datasets of sizes 100 and 500 were generated, only the results for N - 500 
are presented here since both scenarios showed the same qualitative behaviour. Also, 
the estimation algorithms were started from different initial positions; this too had 
no effect on the final estimates of the parameters. A representative selection of 
results follows; Table I shows the results for both the 5 x 4 x 2 model and the 
5 x 3 x 3 model. 
The results show that, when the true value, wtr, of the parameter was small, there 
is little difference among the three methods. This is due to the fact that at these 
A Modification to Mean Field EM 435 
Table 1: Table of results N=500, results averaged over 50 simulations for 5 ob- 
servables with 4 binary latent variables and for 5 observables with 3 3-state latent 
variables. The figures in brackets give the standard deviation of the estimates, west, 
in units according to the final decimal place. RMS is the root mean squared error 
of the estimate compared to the true value. 
5x4x2 5x3x3 
Method wtr Winit West RMS West RMS 
EM 0.1 0.05 0.09(1) 0.014 0.10(2) 0.024 
MF 0.1 0.05 0.09(1) 0.014 0.10(2) 0.023 
MFcv 0.1 0.05 0.09(1) 0.014 0.10(2) 0.023 
EM 0.5 0.1 0.49(2) 0.016 0.50(2) 0.019 
MF 0.5 0.1 0.47(2) 0.029 0.46(2) 0.038 
Mrcv 0.5 0.1 0.48(2) 0.026 0.47(2) 0.032 
EM 1.0 0.1 0.99(2) 0.016 1.00(2) 0.018 
MF 1.0 
