Computational Elements of the Adaptive 
Controller of the Human Arm 
Reza Shadmehr and Ferdinando A. Mussa-Ivaldi 
Dept. of Brain and Cognitive Sciences 
M. I. T., Cambridge, MA 02139 
Eraall: reza@ai.mit.edu, sandro@ai.mit.edu 
Abstract 
We consider the problem of how the CNS learns to control dynam- 
ics of a mechanical system. By using a paradigm where a subject's 
hand interacts with a virtual mechanical environment, we show 
that learning control is via composition of a model of the imposed 
dynamics. Some properties of the computational elements with 
which the CNS composes this model are inferred through the gen- 
eralization capabilities of the subject outside the training data. 
1 Introduction 
At about the age of three months, children become interested in tactile exploration 
of objects around them. They attempt to reach for an object, but often fail to 
properly control their arm and end up missing their target. In the ensuing weeks, 
they rapidly improve and soon they can not only reach accurately, they can also 
pick up the object and place it. Intriguingly, during this period of learning they 
tend to perform rapid, flailing-like movements of their arm, as if trying to excite 
the plant that they wish to control in order to build a model of its dynamics. 
From a control perspective, having a model of the arm's skeletal dynamics seems 
necessary because of the relatively low gain of the fast acting feedback system 
in the spinal neuro-muscular controllers (Crago et al. 1976), and the long delays in 
transmission of sensory information to the supra-spinal centers. Such a model could 
be used by the CNS to predict the muscular forces that need to be produced in order 
to move the arm along a desired trajectory. Yet, this model by itself is not sufficient 
1077 
1078 Shadmehr and Mussa-Ivaldi 
for performing a contact task because most objects which our hand interacts with 
change the arm's dynamics significantly. We are left with a situation in which we 
need to be able to quickly acquire a model of an object's dynamics so that we can 
incorporate it in the control system for the arm. How we learn to construct a model 
of a dynamical system and how our brains represent the composed model are the 
subjects of this research. 
2 Learning Dynamics of a Mechanical System 
To make the idea behind learning dynamics evident, consider the example of con- 
trolling a robotic arm. The arm may be seen as an inertially dominated mechanical 
admitance, accepting force as input and producing a change in state as its output: 
U(q) (F - C(q, 4)) (1) 
where q is the configuration of the robot, H is the inertia tensor, F is the input 
force from some controllable source (e.g., motors), and C is the coriolis/centripetal 
forces. In learning to control the arm, i.e., having it follow a certain state trajectory 
or reach a final state, we form a model which has as its input the desired change in 
the state of the arm and receive from its output a quantity representing the force 
that should be produced by the actuators. Therefore, what needs to be learned is 
a map from state and desired changes in state to force: 
3(q, 4, d) =/(q)d + (q, 4) (2) 
Combine the above model with a simple PD feedback system, 
and the dynamics of the system in Eq. (1) can now be written in terms of a new 
variable s = q - qa, i.e., the error in the trajectory. It is easy to see that if we have 
  H and   C, and if K and B are positive definite, then s will be a decreasing 
function of time, i.e., the system will be globally stable. 
Learning dynamics means forming the map in Eq. (2). The computational elements 
which we might use to do this may vary from simple memory cells that each have an 
address in the state space (e.g., Albus 1975, Raibert & Wimberly 1984, Miller et al. 
1987), to locally linear functions restricted to regions where we have data (Moore 
& Atkeson 1994), to sigmoids (Gomi & Kawato 1990) and radial basis functions 
which can broadly encode the state space (Botros & Atkeson 1991). Clearly, the 
choice that we make in our computational elements will affect how the learned map 
will generalize its behavior to regions of the state space outside of the training data. 
Furthermore, since the task is to learn dynamics of a mechanical system (as opposed 
to, for example, dynamics of a financial market), certain properties of mechanical 
systems can be used to guide us in our choice for the computational elements. For 
example, the map from states to forces for any mechanical system can be linearly 
parameterized in terms of its mass properties (Stotine and Li, 1991). In an inertially 
dominated system (like a multi-joint arm) these masses may be unknown, but the 
fact that the dynamics can be linearized in terms of the unknowns makes the task 
of learning control much simpler and orders of magnitude faster than using, for 
example, an unstructured memory based approach. 
Computational Elements of the Adaptive Controller of the Human Arm 1079 
/ 
/ 
O.lm[ 
.... ,.-,., : .... / '., :, ,...' , , , .f' .,. . .. ...:. '., 
I' 
B 1. see 
2.5 
'o ' :': }!  ':,,9>/':..,,...,5.'[.,/,.,.,-::,. -:...:.:.; ....   fi:....:>..,,.: 
I' 0 
m 2 4 6 8 10 
1. sec Time (sec) 
Figure 1: Dynamics of a real 2 DOF robot was learned so to produce a desired trajectory. 
A: Schematic of the robot. The desired trajectory is the quarter circle. Performance of a 
PD controller is shown by the gray line, as well as in B, where joint trajectories are drawn: 
the upper trace is the shoulder joint and the lower trace is the elbow joint. Desired joint 
trajectory is solid line, actual trajectory is the gray line. C: Performance when the PD 
controller is coupled with an adaptive model. D: Error in trajectory. Solid line is PD, 
Gray line is PD+adaptation. 
To illustrate this point, consider the task of learning to control a real robot arm. 
Starting with the assumption that the plant has 2 degrees of freedom with rota- 
tional joints, inertial dynamics of Eq. (2) can be written as a product of a known 
matrix-function of state-dependent geometric transformations Y, and an unknown 
(but constant) vector a, representing the masses, center of masses, and link lengths: 
1)(q,,a) -- Y(q,,a) . The matrix Y serves the function of referring the un- 
known masses to their center of rotation and is a geometric transformation which 
can be derived from our assumption regarding the structure of the robot. It is 
these geometric transformations that can guide us in choosing the computational 
elements for encoding the sensory data (q and ). 
We used this approach to learn to control a real robot. The adaptation law was 
derived from a Lyapunov criterion, as shown by Slotine and Li (1991): 
 _- _yr (q, , ) ( _ Oa(t) + q - qa(t)) 
The system converged to a very low trajectory tracking error within only three pe- 
riods of the movement (Fig. 1). This performance is achieved despite the fact that 
our model of dynamics ignores frictional forces, noise and delay in the sensors, and 
dynamics of the actuators. In contrast, using a sigmoid function as the basic corn- 
1080 Shadmehr and Mussa-Ivaldi 
putational element of the map and training via back-propagation led to comparable 
levels of performance in over 4000 repetitions of the training data (Shadmehr 1990). 
The difference in performance of these two approaches was strictly due to the choice 
of the computational elements with which the map of Eq. (2) was formed. 
Now consider the task of a child learning dynamics of his arm, or that of an adult 
picking up a hammer and pounding a nail. We can scarcely afford thousands of 
practice trials before we have built an adequate model of dynamics. Our proposal 
is that because dynamics of mechanical systems are distinctly structured, perhaps 
our brains also use computational elements that are particularly suited for learning 
dynamics of a motor task (as we did in learning to control the robot in Fig. 1). How 
to determine the structure of these elements is the subject of the following sections. 
3 A Virtual Mechanical Environment 
To understand how humans represent learned dynamics of a motor task, we designed 
a paradigm where subjects reached to a target while their hand interacted with a 
virtual mechanical environment. This environment was a force field produced by a 
manipulandum whose end-effector was grasped by the subject. The field of forces 
depended only on the velocity of the hand, e.g., F = B, as shown in Fig. 2A, 
and significantly changed the dynamics of the limb: When the robot's motors were 
turned off (null field condition), movements were smooth, straight line trajectories 
to the target (Fig. 2B). When coupled with the field however, the hand's trajectory 
was now significantly skewed from the straight line path (Fig. 2C). 
It has been suggested that in making a reaching movement, the brain formulates 
a kinematic plan describing a straight hand path along a smooth trajectory to the 
target (Morasso 1981). Initially we asked whether this plan was independent of the 
dynamics of the moving limb. If so, as the subject practiced in the environment, 
the hand path should converge to the straight line, smooth trajectory observed in 
the null field. Indeed, with practice, trajectories in the force field did converge to 
those in the null field. This was quantified by a measure of correlation which for all 
eight subjects increased monotonically with practice time. 
If the CNS adapted to the force field by composing a model of its dynamics, then 
removal of the field at the onset of movement (un-be-known to the subject) should 
lead to discrepancies between the actual field and the one predicted by the subject's 
model, resulting in distorted trajectories which we call after-effects. The expected 
dynamics of these after-effects can be predicted by a simple model of the upper 
arm (Shadmehr and Mussa-Ivaldi 1994). Sinc
