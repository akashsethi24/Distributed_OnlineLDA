388 Smith and Miller 
Bayesian Inference of Regular Grammar 
and Markov Source Models 
Kurt R. Smith and Michael I. Miller 
Biomedical Computer Laboratory 
and 
Electronic Signals and Systems Research Laboratory 
Washington University, St. Louis, MO 63130 
ABSTRACT 
In this paper we develop a Bayes criterion which includes the Rissanen 
complexity, for inferring regular grammar models. We develop two 
methods for regular grammar Bayesian infexemce. The fu'st method is 
based on treating the regular grammar as a 1-dimensional Markov 
source, and the second is based on the combinatoric characteristics of 
the regular grammar itseft. We apply the resulting Bayes criteria to a 
particular example in order to show the efficiency of each method. 
1 MOTIVATION 
We are interested in segmenting electron-microscope autoradiography (EMA) images by 
learning representational models for the textures found in the EMA image. In studying 
this problem, we have recognized that both structural and statistical features may be 
useful for characterizing textms. This has motivated us to study the source modeling 
problem for both structural sources and statistical stmrces. The statistical sources that 
we have examined are the class of one and two-dimenfional Markov sources (see [Smith 
1990] for a Bayesian treatment of Markov random field texture model inference), while 
the structural sources that we are primarily inte in here are the class of regular 
grammars, which are important due to the role that grammatical constraints may play in 
the development of structural features for texture representation. 
Bayesian Inference of Regular Grammar and Markov Source Models 389 
2 MARKOV SOURCE INFERENCE 
Our primary interest here is the development of a complete Bayesian framework for the 
process of inferring a regular grammar from a training sequence. However, we have 
shown previously that there exists a 1-D Markov source which generates the regular 
language defined via some regular grammar [Miller, 1988]. We can therefore develop a 
generalized Bayesian inference procedure over the class of 1-D Markov sources which 
enables us to learn the Markov source corresponding to the optimal regular grammar. 
We begin our analysis by developing the general structure for Bayesian source modeling. 
2.1 BAYESIAN APPROACH TO SOURCE MODELING 
We state the Bayesian approach to model learning: Given a set of source models 
{ , t,. ï¿½., tt- } and the observation x, choose the source model t which most accurately 
represents the unknown source that generated x. This decision is made by calculating 
Bayes risk over the possible models which produces a general decision criterion for the 
model learning problem: 
log P(xt) + log Pi . 
(2.1) 
Under the additional assumption that the apriori probabilities over the candidam models 
are equivalent, the decision criterion becomes 
max log (2.2) 
which is the quantity that we will use in measuring the accuracy of a model's 
representation. 
2.2 STOCHASTIC COMPLEXITY AND MODEL LEARNING 
It is well known that when given finite data, Bayesian procedures of this kind which do 
not have any prior on the models suffer from the fundamental limitation that they will 
predict models of greater and greater complexity. This has led others to introduce 
priors into the Bayes hypothesis testing procedure based on the complexity of the model 
being tested [Rissanen, 1986]. In particular, for the Markov case the complexity is 
directly proportional to the number of transition probabilities of the particular model 
being tested with the prior exponentially decreasing with the associated complexity. 
We now describe the inclusion of the complexity measure in greater detail. 
Following Rissanen, the basic idea is to uncover the model which assigns maximum 
probability to the observed data. while also being as simple as possible so as to require a 
small Kolmogorov description length. The complexity associated with a model having 
k real parameters and a likelihood with n independent samples, is the now well-known 
k2-/og n which allows us to express the generalization of the original Bayes lxocedure 
(2.2) as the quantity 
390 Smith and Miller 
max log P(x-I) - ? log n 
(2.3) 
Note well that  is the ksrdimensional parameter parameterizing model , which must 
be estimated from the observed data x-. An alternative view of (2.3) is discovered by 
viewing the second term as the prior in the Bayes model (2.1) where the prior is defined 
koi log n 
Psi= e'- (2.4) 
2.3 1-D MARKOV SOURCE MODELING 
Consider that x- is a 1-D n-length string of symbols which is generated by an unknown 
finite-state Markov source. In examining (2.3), we recognize that for 1-D Markov 
^ n-! 
sources logP(xtOi) may be written as log H P(S(x.)tS(xj.)) where S(x.) is a state 
j=l 
function which evaluates to a state in the Markov source state set S. Using this 
notation, the Bayes hypothesis test for 1-D Markov sources may be expressed as: 
n-1 
max log 
.... ' 
(2.5) 
For the general Markov source inference problem, we know only that the string x,, was 
generated by a 1-D Markov source, with the state set Ss and the transition probabilities 
P(Sg.t), k,le S. unknown. They must therefore be included in the inference procedure. 
To include the complexity term for this case, we note that the number of parameters to 
be estimated for model/ is simply the number of entries in the state-transition matrix 
Pa, i.e. ko = ISl 2. Therefore for 1-D Markov sources, the generalized Bayes hypothesis 
test including complexity may be stated as 
max 1  log P(S(x))lS(x.,)) - IS2log n, (2.6) 
{axs,..,su} n 2n 
where we have divided the entire quantity by n in order to express the criterion in terms 
of bits per symbol. Note that a candidate Markov source model Oi is initially specified 
by its order and corresponding state set Si. 
The procedure for inferring 1-D Markov source models can thus be stated as follows. 
Given a sequence x- from some unknown source, consider candidate Markov source 
models by computing the state function S(x.) (determined by the candidate model 
order) over the entire string x,, Enumerating the state transitions which occur in x. 
provides an estimate of the state-transition matrix Pa which is then used to compute 
(2.6). Now, the inferred Markov source becomes the one maximizing (2.6). 
Bayesian Inference of Regular Grammar and Markov Source Models 391 
3 REGULAR GRAMMAR INFERENCE 
Although the Bayes criterion developed for 1-D Markov sources (2.6) is a sufficient 
model learning criterion for the class of regular grammars, we will now show that by 
taking advantage of the apriori knowledge that the source is a regular grammar, the 
inference procedure can be made much more efficient. This apriori knowledge brings a 
special structure to the regular grammar inference problem in that not all allowable 
sets of Markov probabilities correspond to regular grammars. In fact, as shown in 
[Miller, 1988], corresponding to each regular grammar is a unique set of candidate 
probabilities, implying that the Bayesian solution which takes this into account will be 
far more efficient. We demonstrate that now. 
3.1 BAYESIAN CRITERION USING GRAMMAR COMBINATORICS 
Our approach is to use the combinatoric properties of the regular grammar in order to 
develop the optimal Bayes hypothesis test. We begin by defining the regular grammar. 
Definition: A regular grammar G is a quadruple {Vjv, Vr, Ss, R} where Vtv, Vr are finite 
sets of non-terminal symbols (or states) and terminal symbols respectively, Ss is the 
sentence start state, and R is a finite set of production rules consisting of the 
transformation of a non-terminal symbol to either a terminal followed by a non- 
terminal, or a terminal alone, i.e., 
Si--->WjSk or Si--->Wj, where Wf Vr, Sij, Vv . 
In the class of regular grammars that we consider, we define the depth of the language 
as the maximum number of terminal symbols which make up a nonterminal symbol. 
Corresponding to each regular grammar is an associated incidence matrix B with the i,k ts 
entry Bi equal to the number of times there is a production for some terminal j and 
non-terminals i,!: of the form Si }WR. Also associated with each grammar Gi is 
the set of all n-length strings produced by the grammar, deno.xl as the regular language 
l;,,(Gi). 
Now we make the quite reasonable assumption that no string in the language l,,(Gi) is 
more or less probable apriori than any other string in that language. This indicates that 
all n-length strings that can be generaw.,d by Gi are equiprobable with a probability 
dictated by the combinatorics of the language as 
P(x,,IGi) - 1 (3.1) 
where l ag.(G01 denotes the number of n-length sequences in the langmage which can be 
computed by considering the combinatorics of the language as follows: 
IL.(GOI = xa?, 
392 Smith and Miller 
with ,al corresponding to the largest eigenvalue of the state-transition matrix Bay 
This results from the combinatoric growth rate being determined by the sum of the 
entries in the n a power state-transition matrix B' 
o,, which grows as the largest 
eigenvalue ,a ofBo [Blahut, 1987]. We can now write (3.1) in these terms as 
P(x. IGi)= i, 
(3.2) 
which expresses the probability of the sequence x. in terms of the combinatorics of Gi. 
We now use this combinatoric interpretation of the probability to develop Bayes 
decision criterion over two candidate grammars. Assume that there exists a finite space 
of sequences X, all of which may be generated by one of the two possible grammars 
{Go, G }. Now by dividing this observation space X into two decision regions, X0 (for 
Go) and X (for G ), we can write Bayes risk R in terms of the observation probabilities 
P(xlGo), P(xlG ): 
R=  P(x, IGo)+  P(x,,ICO. (3.3) 
Xn X1 Xn XO 
This implementation of Bayes risk assumes that sequences from each grammar occur 
equiprobably apriori and that
