338 
The Connectivity Analysis of Simple Association 
How Many Connections Do You Need? 
Dan Hammerstrom * 
Oregon Graduate Center, Beaverton, OR 97006 
ABSTRACT 
The efficient realization, using current silicon technology, of Very Large Connection 
Networks (VLCN) with more than a billion connections requires that these networks exhibit 
a high degree of communication locMity. Real neural networks exhibi't significant locality, 
yet most connectionist/neurM network models have little. In this paper, the connectivity 
requirements of a simple associative network are analyzed using communication theory. 
Several techniques based on communication theory are presented that improve the robust- 
ness of the network in the face of sparse, local interconnect structures. Also discussed are 
some potential problems when information is distributed too widely. 
INTRODUCTION 
Connectionist/neural network researchers are learning to program networks that exhi- 
bit a broad range of cognitive behavior. Unfortunately, existing computer systems are lim- 
ited in their ability to emulate such networks efficiently. The cost of emulating a network, 
whether with special purpose, highly parallel, silicon-based architectures, or with traditional 
parallel architectures, is directly proportional to the number of connections in the network. 
This number tends to increase geometrically as the number of nodes increases. Even with 
large, massively parallel architectures, connections take time and silicon area. Many exist- 
ing neural network models scale poorly in learning time and connections, precluding large 
implementations. 
The connectivity 'costs of a network are directly related to its locality. A network 
exhibits locality of communication 1 if most of its processing elements connect to other physi- 
cally adjacent processing elements in any reasonable mapping of the elements onto a planar 
surface. There is much evidence that real neural networks exhibit locality 2 . In this paper, 
a technique is presented for analyzing the effects of locality on the process of association. 
These networks use a complex node similar to the higher-order learning units of Maxwell et 
al. 3 
NETWORK MODEL 
The network model used in this paper is now defined (see Figure 1). 
Definition 1: A recursive neural network, called a c-graph is 
F( V,E, C), where: 
� 
a graph structure, 
There is a set of CNs (network nodes), V, whose outputs can take a range of positive 
real values, vi, between 0 and 1. There are N nodes in the set. 
There is a set of codohs, E, that can take a range of positive real values, eii (for 
codon ] of node i), between 0 and 1. There are N� codons dedicated to each CN (the 
output of each codon is only used by its local CN), so there are a total of N� N codohs 
in the network. The fan-in or order of a codon is re. It is assumed that f� is the 
same for each codon, and N� is the same for each CN. 
*This work was supported in part by the Semiconductor Research Corporation contract no. 86-10-097, and 
jointly by the Office of Naval Research and Air Force Office of Scientific Research, ONR contract no. N00014 87 K 
0259. 
� American Institute of Physics 1988 
339 
f� i codon 
v 
Figure 1 - A CN 
� ciik 6 C is a set of connections of CNs to codons, l<i,k_<N, and i<_j<_N�, ciik can 
take two values {0,1} indicating the existence of a connection from CN k to codon j 
of CN i. [] 
Definition �: The value of CN i is 
v, = F 0+e// (1) 
The function, F, is a continuous non-linear, monotonic function, such as the sigmoid func- 
tion. [] 
Definition 3: Define a mapping, D(i,j,T)--*', where T is an input vector to F and ' is 
the f� element input vector of codon j of CN i. That is, ' has as its elements those ele- 
ments of x} of  where cii}=l , V k. [] 
The D function indicates the subset of  seen by codon j of CN i. Different input vec- 
tors may map to the same codon vectors, e.g., D(i,j,T)--* and D(i,j,z--., where TT. 
Definition 6: The codon values e�i are determined as follows. Let -rn) be input vector 
m of the M learned input vectors for CN i. For codon eii of CN i, let Tq be the set of f�- 
dimensional vectors such that Tii(m)ETii , and D(i,j,-m))-.Tq(m). That is, each vector, 
Tq(m) in Tq consists of those subvectors of -rn) that are in codon ij's receptive field. 
The variable I indexes the L(i,]) vectors of Tii. The number of distinct vectors in 
may be less than the total number of learned vectors (L(i,j)_<M). Though the -rn) are 
distinct, the subsets, Tii(rn), need not be, since there is a possible many to one mapping of 
the T vectors onto each vector 
Let X 1 be the subset of vectors where vi=l (CN i is supposed to output a 1), and X � be 
those vectors where vl--O , then define 
n.(/) = size_of{D(i,j,x-m))st vi=q} (2) 
for q=0,1, and t m that map to this I. That is, %�.(l) is the number of ' vectors that map 
340 
into 'q(!) where v�-0 and ni.(/) is the number of Y vectors that map into '/j(l), where vi---1. 
The compression of a codon for a vector'q(l) then is defined as 
= (s) 
(HCq(!)O when both n , n�=0.) The output of codon ij, q, is the maximum-likelyhd 
decoding 
= (4) 
ere HC indicates the likelyhd of vi=l when  vector Y that mps to g is input, nd g 
is that vector g) where min[d(g),] V l, D(i,j,)y, nd Y is the current input vec- 
tor.  other words, g is that vector (of the set of subset learned vectors that codon ij 
receives) that is closest (using distance measure d) to  (the subset input vector). n 
The output of  eodon is the most-likely output ccording to its inputs. For exm- 
pie, when there is no code compreion t  codon, ff=l, if the closest (in ter of some 
measure of vector distance, e.g. Hmming distance) subvector in the receptive field of the 
eodon belongs to  learned vector where the CN is to output  1. The codons described here 
re ve similar to those propped by Marr 4 and implement nearest-neighbor classification. 
It is umed that eodon function is determined stticlly prior to network operation, that 
is, the desired ctegofies hve lredy been learned. 
To measure performance, network epacity is used. 
Definition 5: The input noie, fl, is the verge d between n input vector nd the 
eldest (nimum d) learned vector, where d is  measure of the difference between two 
vectors - for bit vectors this cn be Hmming distance. The output nois, flo, is the verge 
distance between network output nd the learned output vector ssocited with the closest 
learned input vector. The information gain, G, is just 
Definition 6: The capacity of a network is the maximum number of learned vectors such 
that the information gain, Gi, is strictly positive (0). [] 
COMMUNICATION ANALOGY 
Consider a single connection network node, or CN. (The remainder of this paper will 
be restricted to a single CN.) Assume that the CN output value space is restricted to two 
values, 0 and 1. Therefore, the CN must decide whether the input it sees belongs to the 
class of 0 codes, those codes for which it remains off, or the class of 1 codes, those codes 
for which it becomes active. The inputs it sees in its receptive field constitute a subset of 
the input vectors (the D(...) function) to the network. It is also assumed that the CN is an 
ideal 1-NN (Nearest Neighbor) classifier or feature detector. That is, given a particular set 
of learned vectors, the CN will classify an arbitrary input according to the class of the 
nearest (using d h as a measure of distance) learned vector. This situation is equivalent to 
the case where a single CN has a single codon whose receptive field size is equivalent to that 
of the CN. 
Imagine a sender who wishes to send one bit of information over a noisy channel. The 
sender has a probabilistic encoder that choses a code word (learned vector) according to 
some probability distribution. The receiver knows this code set, though it has no knowledge 
of which bit is being sent. Noise is added to the code word during its transmission over the 
341 
channel, which is analogous to applying an input vector to a network's inputs, where the 
vector lies within some learned vector's region. The noise is represented by the distance 
(dh) between the input vector and the associated learned vector. 
The code word sent over the channel consists of those bits that are seen in the recep- 
tive field of the CN being modeled. In the associative mapping of input vectors to output 
vectors, each CN must respond with the appropriate output (0 or 1) for the associated 
learned output vector. Therefore, a CN is a decoder that estimates in which class the 
received code word belongs. This is a classic block encoding problem, where increasing the 
field size is equivalent to increasing code length. As the receptive field size increases, the 
performance of the decoder improves in the presence of noise. Using communication theory 
then, the trade-off between interconnection costs as they relate to field size and the func- 
tionality of a node as it relates to the correctness of its decision making process (output 
errors) can be characterized. 
As the receptive field size of a node increases, so does the redundancy of the input, 
though this is dependent on the particular codes being used for the learned vectors, since 
there are situations where increasing the field size provides no additional information. 
There is a point of diminishing returns, where each additional bit provides ever less reduc- 
tion in output error. Another factor is that interconnection costs increase exponentially 
with field size. The result of these two trends is a cost performance measure that has a sin- 
gle global maximum value. In other words, given a set of learned vectors and their proba- 
bilities, and a set of interconnection costs, a best receptive field size can be determined, 
beyond which, increasing connectivity brings diminishing return
