840 
LEARNING IN NETWORKS OF 
NONDETERMINISTIC ADAPTIVE LOGIC ELEMENTS 
Richard C. Windecker* 
AT&T Bell Laboratories, Middletown, NJ 07748 
ABSTRACT 
This paper presents a model of nondeterministic adaptive automata that are 
constructed from simpler nondeterministic adaptive information processing 
elements. The first half of the paper describes the model. The second half discusses 
some of its significant adaptive properties using computer simulation examples. 
Chief among these properties is that network aggregates of the model elements can 
adapt appropriately when a single reinforcement channel provides the same positive 
or negative reinforcement signal to all adaptive elements of the network at the same 
time. This holds for multiple-input, multiple-output, multiple-layered, 
combinational and sequential networks. It also holds when some network elements 
are hidden in that their outputs are not directly seen by the external 
environment. 
INTRODUCTION 
There are two primary motivations for studying models of adaptive automata 
constructed from simple parts. First, they let us learn things about real biological 
systems whose properties are difficult to study directly: We form a hypothesis 
about such systems, embody it in a model, and then see if the model has reasonable 
learning and behavioral properties. In the present work, the hypothesis being tested 
is: that much of an animal's behavior as determined by its nervous system is 
intrinsically nondeterministic; that learning consists of incremental changes in the 
probabilities governing the animal's behavior; and that this is a consequence of the 
animal's nervous system consisting of an aggregate of information processing 
elements some of which are individually nondeterministic and adaptive. The second 
motivation for studying models of this type is to find ways of building machines 
that can learn to do (artificially) intelligent and practical things. This approach has 
the potential of complementing the currently more developed approach of 
programming intelligence into machines. 
We do not assert that there is necessarily a one-to-one correspondence 
between real physiological neurons and the postulated model information processing 
elements. Thus, the model may be loosely termed a neural network model, but is 
more accurately described as a model of adaptive automata constructed from simple 
adaptive parts. 
The main ideas in this paper were conceived and initially developed while the 
author was at the University of Chiang Mai, Thailand (1972-73). The ideas were 
developed further and put in a form consistent with existing switching and 
automata theory during the next four years. For two of those years, the author 
was at the University of Guelph, Ontario, supported of National Research 
Council of Canada Grant #A6983. 
@ American Institute of Physics 1988 
841 
It almost certainly has to be a property of any acceptable model of animal 
learning that a single reinforcement channel providing reinforcement to all the 
adaptive elements in a network (or subnetwork) can effectively cause that network 
to adapt appropriately. Otherwise, methods of providing separate, specific 
reinforcement to all adaptive elements in the network must be postulated. Clearly, 
the environment reinforces an animal as a whole and the same reinforcement 
mechanism can cause the animal to adapt to many types of situation. Thus, the 
reinforcement system is non-specific to particular adaptive elements and particular 
behaviors. The model presented here has this property. 
The model described here is a close cousin to the family of models recently 
described by Barto and coworkers 1-4 The most significant difference are: 1) In 
the present model, we define the timing discipline for networks of elements more 
explicitly and completely. This particular timing discipline makes the present 
model consistent with a nondeterministic extension of switching and automata 
theory previously described 5. 2) In the present model, the reinforcement algorithm 
that adjusts the weights is kept very simple. With this algorithm, positive and 
negative reinforcement have symmetric and opposite effects on the weights. This 
ensures that the logical signals are symmetric opposites of each other. (Even small 
differences in the reinforcement algorithm can make both subtle as well as profound 
differences in the behavior of the model.) We also allow, null, or zero, 
reinforcement. 
As in the family of models described by Barto, networks constructed within 
the present model can get stuck at a suboptimal behavior during learning and 
therefore not arrive at the optimal adapted state. The complexity of the Barto 
reinforcement algorithm is designed partly to overcome this tendency. In the 
present work, we emphasize the use of training strategies when we wish to ensure 
that the network arrives at an optimal state. (In nature, it seems likely that getting 
stuck at suboptimal behavior is common.) In all networks studied so far, it has 
been easy to find strategies that prevent the network from getting stuck. 
The chief contributions of the present work are: 1) The establishment of a 
close connection between these types of models and ordinary, nonadaptive, 
switching and automata theory 5. This makes the wealth of knowledge in this area, 
especially network synthesis and analysis methods, readily applicable to the study 
of adaptive networks. 2) The experimental demonstration that sequential 
(recurrent) nondeterministic adaptive networks can adapt appropriately. Such 
networks can learn to produce outputs that depend on the recent sequence of past 
inputs, not just the current inputs. 3) The demonstration that the use of training 
strategies can not only prevent a network from getting stuck, but may also result in 
more rapid learning. Thus, such strategies may be able to compensate, or even 
more than compensate, for reduced complexity in the model itself. References 
2-4 and 6 provide a comprehensive background and guide to the 
literature on both deterministic and nondeterministic adaptive automata including 
those constructed from simple parts and those not. 
THE MODEL ADAPTIVE ELEMENT 
The model adaptive element postulated in this work is a nondeterministic, 
adaptive generalization of threshold logic 7. Thus, we call these elements 
Nondeterministic Adaptive Threshold-logic gates (NATs). The output chosen by a 
NAT at any given time is not a function of its inputs. Rather, it is chosen by a 
stochastic process according to certain probabilities. It is these probabilities that 
are a function of the inputs. 
A NAT is like an ordinary logic gate in that it accepts logical inputs that are 
two-valued and produces a logical output that is two-valued. We let these values be 
842 
+1 and -1. A NAT also has a timing input channel and a reinforcement input 
channel. The NAT operates on a three-part cycle: 1) Logical input signals are 
changed and remain constant. 2) A timing signal is received and the NAT selects a 
new output based on the inputs at that moment. The new output remains 
constant. 3) A reinforcement signal is received and the weights are incremented 
according to certain rules. 
Let N be the number of logical input channels, let xi represent the i th input 
signal, and let z be the output. The NAT has within it N+I weights, 
Wo, w, ..., wv. The weights are confined to integer values. For a given set of 
inputs, the gate calculates the quantity W: 
W = Wo + WlX + w2x2 + wsx3 + .... + WNXN = Wo + - (1) 
Then the probability that output z = q-1 is chosen is: 
w 
P(z = q- 1) = I  --- 
VO' - � 
�  '/v' 
I  
dx = f e- (2) 
where  = x/v/-er. (An equivalent formulation is to let the NAT generate a 
random number, w, according to the normal distribution with mean zero and 
variance tr 2. Then if W >-w, the gate selects the output z = +1. If 
W -w, the gate se]ects output z= -1. If W= -w, the gate selects output 
-1 or + 1 with equal probability.) 
Reinforcement signals, R, may have one of three values: q-l, --1, and 0 
representing positive, negative, and no reinforcement, respectively. If +1 
reinforcement is received, each weight is incremented by one in the direction that 
makes the current output, z, more likely to occur in the future when the same 
inputs are applied; if-1 reinforcement is received, each weight is incremented in 
the direction that makes the current output less likely; if 0 reinforcement is 
received, the weights are not changed. These rules may be summarized: AWo = zR 
and AWl = xizR for i > 0. 
NATs operate in discrete time because if the NAT can choose output +1 or 
-1, depending on a stochastic process, it has to be told when to select a new 
output. It cannot run freely, or it could be constantly changing output. Nor can 
it change output only when its inputs change because it may need to select a new 
output even when they do not change. 
The normal distribution is used for heuristic reasons. If a real neuron (or an 
aggregate of neurons) uses a stochastic process to produce nondeterministic 
behavior, it is likely that process can be described by the normal distribution. In 
any case, the exact relationship between P(z ---- +1) and W is not critical. What is 
important is that P(z = +1) be monotonically increasing in W, go to 0 and 1 
asymptotically as W goes to -oe and + oe, respectively, and equal 0.5 at W -- 0. 
The parameter tr is adjustable. We use 10 in the computer simulation 
experiments described below. Experimentally, values near 10 work reasonably well 
for networks of NATs having few inputs. Note that as tr goes to zero, the behavior 
of a NAT approximates that of an ordinary deterministic adapt, ire threshold logic 
gate with the difference that the output for the case W = 0 is not arbitrary: The 
NAT will select output + 1 or --1 with equal probability. 
Note that for all values of W, the probabilities are gre
