Just One View: 
Invariances in Inferotemporal Cell Tuning 
Maximilian Riesenhuber Tomaso Poggio 
Center for Biological and Computational Learning and 
Department of Brain and Cognitive Sciences 
Massachusetts Institute of Technology, E25-201 
Cambridge, MA 02139 
{max,tp}@ai.mit.edu 
Abstract 
In macaque inferotemporal cortex (IT), neurons have been found to re- 
spond selectively to complex shapes while showing broad tuning (in- 
variance) with respect to stimulus transformations such as translation 
and scale changes and a limited tuning to rotation in depth. Training 
monkeys with novel, paperclip-like objects, Logothetis et al. 9 could in- 
vestigate whether these invariance properties are due to experience with 
exhaustively many transformed instances of an object or if there are mech- 
anisms that allow the cells to show response invariance also to previously 
unseen instances of that object. They found object-selective cells in an- 
terior IT which exhibited limited invariance to various transformations 
after training with single object views. While previous models accounted 
for the tuning of the cells for rotations in depth and for their selectiv- 
ity to a specific object relative to a population of distractor objects, 4,  
the model described here attempts to explain in a biologically plausible 
way the additional properties of translation and size invariance. Using 
the same stimuli as in the experiment, we find that model IT neurons 
exhibit invariance properties which closely parallel those of real neurons. 
Simulations show that the model is capable of unsupervised learning of 
view-tuned neurons. 
We thank Peter Dayan, Marcus Dill, Shimon Edelman, Nikos Logothetis, Jonathan Mumick and 
Randy O'Reilly for useful discussions and comments. 
216 M. Riesenhuber and T. Poggio 
1 Introduction 
Neurons in macaque inferotemporal cortex (IT) have been shown to respond to views of 
complex objects, 8 such as faces or body parts, even when the retinal image undergoes size 
changes over several octaves, is translated by several degrees of visual angle z or rotated in 
depth by a certain amount 9 (see [13] for a review). 
These findings have prompted researchers to investigate the physiological mechanisms 
underlying these tuning properties. The original model TM that led to the physiological 
experiments of Logothetis et al.  explains the behavioral view invariance for rotation in 
depth through the learning and memory of a few example views, each represented by a 
neuron tuned to that view. Invariant recognition for translation and scale transformations 
have been explained either as a result of object-specific learning 4 or as a result of a 
normalization procedure (shifter) that is applied to any image and hence requires only 
one object-view for recognition.  
A problem with previous experiments has been that they did not illuminate the mechanism 
underlying invariance since they employed objects (e.g., faces) with which the monkey was 
quite familiar, having seen them numerous times under various transformations. Recent 
experiments by Logothetis et al.  addressed this question by training monkeys to recog- 
nize novel objects (paperclips and amoeba-like objects) with which the monkey had no 
previous visual experience. After training, responses of IT cells to transformed versions of 
the training stimuli and to distractors of the same type were collected. Since the views the 
monkeys were exposed to during training were tightly controlled, the paradigm allowed to 
estimate the degree of invariance that can be extracted from just one object view. 
In particular, Logothetis et al.  tested the cells' responses to rotations in depth, translation 
and size changes. Defining invariance as yielding a higher response to test views than 
to distractor objects, they report ' 0 an average rotation invariance over 30% translation 
invariance over +2 �, and size invariance of up to +1 octave around the training view. 
These results establish that there are cells showing some degree of invariance even after 
training with just one object view, thereby arguing against a completely learning-dependent 
mechanisms that requires visual experience with each transformed instance that is to be 
recognized. On the other hand, invariance is far from perfect but rather centered around the 
object views seen during training. 
2 The Model 
Studies of the visual areas in the ventral stream of the macaque visual system 8 show a 
tendency for cells higher up in the pathway (from V1 over V2 and V4 to anterior and 
posterior IT) to respond to increasingly complex objects and to show increasing invariance 
to transformations such as translations, size changes or rotation in depth. TM 
We tried to construct a model that explains the receptive field properties found in the 
experiment based on a simple feedforward model. Figure 1 shows a cartoon of the model: 
A retinal input pattern leads to excitation of a set of VI cells, in the figure abstracted 
as having derivative-of-Gaussian receptive field profiles. These VI cells are tuned to 
simple features and have relatively small receptive fields. While they could be cells from a 
variety of areas, e.g., V1 or V2 (cf. Discussion), for simplicity, we label them as VI cells 
(see figure). Different cells differ in preferred feature, e.g., orientation, preferred spatial 
frequency (scale), and receptive field location. VI cells of the same type (i.e., having 
the same preferred stimulus, but of different preferred scale and receptive field location) 
feed into the same neuron in an intermediate layer. These intermediate neurons could be 
complex cells in V1 or V2 or V4 or even posterior IT: we label them as V4 cells, in the 
Just One View: Invariances in Inferotemporal Cell Tuning 217 
same spirit in which we labeled the neurons feeding into them as V 1 units. Thus, a V4 
cell receives inputs from V 1 cells over a large area and different spatial scales ([8] reports 
an average receptive field size in V4 of 4.4 � of visual angle, as opposed to about 1 � in V 1; 
for spatial frequency tuning, [3] report an average FWHM of 2.2 octaves, compared to 1.4 
(foveally) to 1.8 octaves (parafoveally) in V15). These V4 cells in turn feed into a layer 
of IT neurons, whose invariance properties are to be compared with the experimentally 
observed ones. 
IT 
Retina 
Figure 1: Cartoon of the model. See text for explanation. 
A crucial element of the model is the mechanism an intermediate neuron uses to pool the 
activities of its afferents. From the computational point of view, the intermediate neurons 
should be robust feature detectors, i.e., measure the presence of specific features without 
being confused by clutter and context in the receptive field. More detailed considerations 
(Riesenhuber and Poggio, in preparation) show that this cannot be achieved with a response 
function that just summates over all the afferents (cf. Results). Instead, intermediate 
neurons in our model perform a max operation (akin to a Winner-Take-All) over all 
their afferents, i.e., the response of an intermediate neuron is determined by its most strongly 
excited afferent. This hypothesis appears to be compatible with recent data, 15 that show 
that when two stimuli (gratings of different contrast and orientation) are brought into the 
receptive field of a V4 cell, the cell's response tends to be close to the stronger of the two 
individual responses (instead of e.g., the sum as in a linear model). 
218 M. Riesenhuber and T. Poggio 
Thus, the response function oi of an intermediate neuron i to stimulation with an image v 
is 
with ,Ai the set of afferents to neuron i, c(j) the receptive field center of afferent j, v(j) the 
(square-normalized) image patch centered at c(j) that corresponds in size to the receptive 
field, j (also square-normalized) of afferent j and . the dot product operation. 
Studies have shown that V4 neurons respond to features of intermediate complexity 
such as gratings, corners and crosses. 8 In V4 the receptive fields are comparatively large 
(4.4 c of visual angle on average 8), while the preferred stimuli are usually much smallertl 
Interestingly, cells respond independently of the location of the stimulus within the receptive 
field. Moreover, average V4 receptive field size is comparable to the range of translation 
invariance of IT cells (_< +2 �) observed in the experiment. 9 For afferent receptive fields 
j, we chose features similar to the ones found for V4 cells in the visual system: 8 bars 
(modeled as second derivatives of Gaussians) in two orientations, and corners of four 
different orientations and two different degrees of obtuseness. This yielded a total of 10 
intermediate neurons. This set of features was chosen to give a compact and biologically 
plausible representation. Each intermediate cell received input from cells with the same 
type of preferred stimulus densely covering the visual field of 256 x 256 pixels (which thus 
would correspond to about 4.4 c of visual angle, the average receptive field size in V48), 
with receptive field sizes of afferent cells ranging from 7 to 19 pixels in steps of 2 pixels. 
The features used in this paper represent the first set of features tried, optimizing feature 
shapes might further improve the model's performance. 
The response tj of top layer neuron j with connecting weights w i to the intermediate layer 
was set to be a Gaussian, centered on w j, 
tj -  exp 2o. 2 (2) 
where o is the excitation of the intermediate layer and cr the variance of the Gaussian, which 
was chosen based on the distribution of responses (for section 3.1) or learned (for section 
3.2). 
The stimulus images were views of 21 randomly generated paperclips of the type used in 
the physiology experimentfi Distractors were 60 other paperclip images generated by the 
same method. Training size was 128 x 128
