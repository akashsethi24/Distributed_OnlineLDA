Finite-dimensional approximation of 
Gaussian processes 
Giancarlo Ferrari Trecate 
Dipartimento di Informatica e Sistemistica, Universitk di Pavia, 
Via Ferrata 1, 27100 Pavia, Italy 
f errari�conpro. unipv. it 
Christopher K. I. Williams 
Department of Artificial Intelligence, University of Edinburgh, 
5 Forrest Hill, Edinburgh EH1 2QL, 
ckiw�dai. ed. ac. uk. 
Manfred Opper 
Neural Computing Research Group 
Division of Electronic Engineering and Computer Science 
Aston University, Birmingham, B4 7ET, UK 
m. opper�aston. ac. uk 
Abstract 
Gaussian process (GP) prediction suffers from O(n 3) scaling with the 
data set size n. By using a finite-dimensional basis to approximate the 
GP predictor, the computational complexity can be reduced. We de- 
rive optimal finite-dimensional predictors under a number of assump- 
tions, and show the superiority of these predictors over the Projected 
Bayes Regression method (which is asymptotically optimal). We also 
show how to calculate the minimal model size for a given n. The 
calculations are backed up by numerical experiments. 
I Introduction 
Over the last decade there has been a growing interest in the Bayesian approach to 
regression problems, using both neural networks and Gaussian process (GP) prediction, 
that is regression performed in function spaces when using a Gaussian random process 
as a prior. 
The computational complexity of the GP predictor scales as O(n3), where n is the size 
Finite-Dimensional Approximation of Gaussian Processes 219 
of the dataset 1. This suggests using a finite-dimensional approximating function space, 
which we will assume has dimension m < n. The use of the finite-dimensional model is 
motivated by the need for regression algorithms computationally cheaper than the GP 
one. Moreover, GP regression may be used for the identification of dynamical systems 
(De Nicolao and Ferrari Trecate, 1998), the next step being a model-based controller 
design. In many cases it is easier to accomplish this second task if the model is low 
dimensional. 
Use of a finite-dimensional model leads naturally to the question as to which basis is 
optimal. Zhu et al. (1997) show that, in the asymptotic r6gime, one should use the 
first m eigenfunctions of the covariance function describing the Gaussian process. We 
call this method Projected Bayes Regression (PBR). 
The main results of the paper are: 
1. Although PBR is asymptotically optimal, for finite data we derive a predictor 
h�(x) with computational complexity O(n2m) which outperforms PBR, and 
obtain an upper bound on the generalization error of h�(x). 
2. In practice we need to know how large to make m. We show that this depends 
on n and provide a means of calculating the minimal m. We also provide 
empirical results to back up the theoretical calculation. 
2 Problem statement 
Consider the problem of estimating an unknown function f(x) : ll a -> ll, from the 
noisy observations 
ti = f(xi) + ei, i= 1,...,n 
where i are i.i.d. zero-mean Gaussian random variables with variance a 2 and the 
samples xi are drawn independently at random from a distribution p(x). The prior 
probability measure over the function f(.) is assumed to be Gaussian with zero mean 
and autocovariance function C(,2). Moreover we suppose that f(.), Xi, el, are 
mutually independent. Given the data set Z)n = {2, t-}, where 2 = [xl,...,xn] and 
 = [t,..., tn]', it is well known that the posterior probability P(fIZ)n) is Gaussian 
and the GP prediction can be computed via explicit formula (e.g. Whittle, 1963) 
f(x) - m[fl/)n](x ) - [C(x, xl) ... C(x, xn)] H-I, {H}i j '-C(x,,xj) + a250 
where H is a n x n matrix and 5ij is the Kronecker delta. 
In this work we are interested in approximating f in a suitable m-dimensional space 
that we are going to define. Consider the Mercer-Hilbert expansion of C(1, 2) 
jf C(,2)Ti(2)p(2)d2 = /ii(1), JfR Ti()Tj()p()d=Sij (1) 
d d 
i----1 
where the eigenvalues hi are ordered in a decreasing way. 
Then, in (Zhu et al., 1997) is shown that, at least asymptotically, the optimal model 
belongs to A4 = Span {i, i = 1,..., m}. This motivates the choice of this space even 
when dealing with a finite amount of data. 
Now we introduce the finite-dimensional approximator which we call Projected Bayes 
Regression. 
10(n) arises from the inversion of a n x n matrix. 
220 G. Ferrar-Trecate, C. K. I. Williams and M. Opper 
Definition ! 
A- (A-l+/(I)'(I)), (A)ij'--Ai6ij and 
k(x)-' , -' 
m(x) 
The PBR approximator is b(x) = k' (x)w, where w-' fA-'f, f-' l/a 2, 
I 91 (xl) ''' (/9ra (Xl) 1 
tfl (Xn)... m(Xn) 
The name PBR comes from the fact that b(x) is the GP predictor when using the 
mis-specified prior 
m 
](x) = ywiTi(x), w  N(0, A) (2) 
i=1 
whose autocovariance function is the projection of C(1, 2) on 2M. From the compu- 
tational point of view, is interesting to note that the calculation of PBR scales with 
the data as O(m2n), assuming that n >> m (this is the cost of computing the matrix 
product A - (I)'). 
Throughout the paper the following measures of performance will be extensively used. 
Definition 2 Let s(x) be a predictor that uses only information from T)n. Then its 
2-error and generalization error are respectively defined as 
[(t* - s(x*))2], 
An estimator s�(x) belonging to a class 7/ is said 2-optimal or simply optimal if, re- 
spectively, Eso(n, 2) _< Es(n,) or E,9o(n) < Es9(n), for all the s(x)  7/ and the data 
sets 2. 
Note that 2-optimality means optimality for each fixed vector 2 of data points. Obvi- 
ously, if s�(x) is 2-optimal it is also simply optimal. These definitions are motivated by 
the fact that for Gaussian process priors over functions and a predictor s that depends 
linearly on , the computation of Es(n, 2) can be carried out with finite-dimensional 
matrix calculations (see Lemma 4 below), although obtaining Esa(n ) is more difficult, 
as the average over 2 is usually analytically intractable. 
3 Optimal finite-dimensional models 
We start considering two classes of linear approximators, namely 
7/-' {g(x)- k'(x)LlL mxn} and 7/2'- {h(x)- k'(x)F'lF mxm}, where 
the matrices L and F are possibly dependent on the xi samples. We point out that 
7/2 C 7/ and that the PBR predictor b(x)  7-/2. Our goal is the characterization of 
the optimal predictors in 7-/ and 7/2. Before stating the main result, two preliminary 
lemmas are given. The first one is proved in (Pilz, 1991) while the second follows from 
a straightforward calculation. 
Lemma 3 Let A E II *x, B E ,x, A > O. Then it holds that 
inf Tr[(ZAZ' - ZB-B'Z')] = Tr[-B'A-B] 
ZERx 
and the minimum is achieved for the matrix Z* = B' A -. 
Lemma 4 Let g(x) E 7/. Then it holds that 
E9(n'2) = Z Ai + a 2 + q(L), 
i----1 
q(L)-' Tr [LHL' 
- 2L(I)A]. 
Finite-Dimensional Approximation of Gaussian Processes 221 
Proof. In view of the 
[C(x*,x) ... C(x*,x,)]' , it holds 
[(t* - = 
�-error definition, setting r(x*) 
(3) 
Note that Ex. [k(x*)k'(x*)] : Ira, Ex. [r(x*)k'(x*)] - A, and, from the Mercer- 
Hilbert expansion (1), E. [C(x*,x*)] +o 
= i= ,Xi. Then, taking the mean of (3) w.r.t. 
x*, the result follows. D 
Theorem 5 
given by F = F � = A'I)I)('I)' H'I)) -, Vn _> m, are 2-optimal. Moreover 
Ego(n, ) = y,,Xi + 62- Tr [A'H-A] 
i=1 
Eho(n,.) = y. Xi + 6 2- Tr [A�((I)'H)-i(I)'A] 
i=l 
The predictors g�(x) e -1 given by L = L � = A,I)' H - and h�(x) 6 742 
(4) 
Proof. We start considering the g�(x) case. In view of Lemma 4 we need only to 
minimize q(L) w.r.t. to the matrix L. By applying Lemma 3 with B = (I)A, A = H > 0, 
Z = L, one obtains 
argnnq(L)-'L � = A'H - minq(L)= -Tr [A'H-A] (5) 
L 
so proving the first result. For the second case, we apply Lemma 4 with L = F' and 
then perform the minimization of q(F'), w.r.t. the matrix F. This can be done as 
before noting that ' H -1 > 0 only when n >_ m. [] 
Note that. the only difference between g�(x) and the GP predictor derives from the 
approximation of the functions C(x, xk) with -.im=l ,igi(X)gi(Xk). Moreover the com- 
plexity of g�(x) is O(n 3) the same of f(x). On the other hand h�(x) scales as O(n2m), 
so having a computational cost intermediate between the GP predictor and PBR. Intu- 
itively, the PBR method is inferior to h � as it does not take into account the � locations 
in setting up its prior. We can also show that the PBR predictor b(x) and h�(x) are 
asymptotically equivalent. 
/,From (4) is clear that the explicit evaluations of Ea9o(n ) and Ego(n) are in 
general very hard problems, because the mean w.r.t. the xi samples that 
enters in the (I) and H matrices. In the remainder of this section we 
will derive an upper bound on Ego(n). Consider the class of approximators 
743--' {U(X)--' ff (x)D'I/-[D 6 ]mxm,(m)i j : diSij }. Because of the inclusions 743 C 
742 C 74, if u�(x) is the -optimal predictor in 743, then Eo(n) _< Ego(n) _< Eo(n). 
Due the diagonal structure of the matrix D, an upper bound to Euo(n) may be expli- 
citly computed, as stated in the next Theorem. 
Theorem 6 
The approximator u�(x) 6 743 given by 
( O )ij -- ( O� )ij --  )),.. (ij , 
(6) 
222 G. Ferrari-Trecate, C. K. I. Williams and M. Opper 
is -optimal. Moreover an upper-bound on its generalization error is given by 
Eo _< Ai+a2--n yqkAk, qk=c 
i=1 
c, = (n- 1)A, +/C(x,x)T(x)p(x)dx + a . 
(7) 
Proof. In order to find the -optimal approximator in J/a, we start applying the 
Lemma 4 with L = Drb'. Then we need to minimize 
ra m 
q(Dq)') = y d (q)'H))ii- 2 y di ((I)'(I)A) . (8) 
i=1 i=1 
w.r.t. di so obtaining (6). To bound Eo (n), we first compute the generalization error 
of a generic approximation u(x) that is E = E [q(D')] + i Ai + a2. After 
verifying that 
r [('A)ii] = Ain, E [('H)ii] =nci, 
we obtain from (8), assuming the di constant, 
-{-oo m m 
Eu 9 = y.ki +o '2 +nZdci- 2nydi.i. 
i=1 i=1 i=1 
Minimizing E w.r.t. di, and recalling' that u�(x) is
