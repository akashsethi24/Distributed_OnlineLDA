Representation and Induction of Finite 
State Machines using Time-Delay Neural 
Networks 
Daniel S. Clouse 
Computer Science & Engineering Dept. 
University of California, San Diego 
La Jolla, CA 92093-0114 
dclouse@ucsd.edu 
C. Lee Giles 
NEC Research Institute 
4 Independence Way 
Princeton, NJ 08540 
giles@research.nj .nec.com 
Bill G. Horne 
NEC Research Institute 
4 Independence Way 
Princeton, NJ 08540 
horne@ research. nj. nec. corn 
Garrison W. Cottrell 
Computer Science & Engineering Dept. 
University of California, San Diego 
La Jolla, CA 92093-0114 
gcottrell@ucsd.edu 
Abstract 
This work investigates the representational and inductive capabili- 
ties of time-delay neural networks (TDNNs) in general, and of two 
subclasses of TDNN, those with delays only on the inputs (IDNN), 
and those which include delays on hidden units (HDNN). Both ar- 
chitectures are capable of representing the same class of languages, 
the definite memory machine (DMM) languages, but the delays on 
the hidden units in the HDNN helps it outperform the IDNN on 
problems composed of repeated features over short time windows. 
I Introduction 
In this paper we consider the representational and inductive capabilities of time- 
delay neural networks (TDNN) [Waibel et al., 1989] [Lang et al., 1990], also known 
as NNFIR [Wan, 1993]. A TDNN is a feed-forward network in which the set of 
inputs to any node i may include the output from previous layers not only in the 
current time step t, but from d earlier time steps as well. The activation function 
404 D. S. Clouse, C. L. Giles, B. G. Home and G. W. Cottrell 
for node i at time t in such a network is given by equation 1: 
i--1 d 
yj wijk) (1) 
j-1 k=O 
where y is the activation of node i at time t, wijk is the connection strength from 
node j to node i at delay k, and h is the squashing function. 
TDNNs have been used in speech recognition [Waibel et al., 1989], and time series 
prediction [Wan, 1993]. In this paper we concentrate on the language induction 
problem. A training set of variable-length strings taken from a discrete alphabet 
{0, 1} is generated. Each string is labeled as to whether it is in some language L 
or not. The network must learn to discriminate strings which are in the language 
from those which are not, not only for the training set strings, but for strings the 
network has never seen before. The language induction problem provides a simple, 
familiar domain in which to gain insight into the capabilities of different network 
architectures. 
Specifically, in this paper, we will look at the representational and inductive capa- 
bilities of the general class of TDNNs versus a subclass of TDNNs, the input-delay 
neural networks (IDNNs). An IDNN is a TDNN in which delays are limited to 
the network inputs. In section 2, we will show that the classes of functions repre- 
sentable by general TDNNs and IDNNs are equivalent. In section 3, we will show 
that the class of languages representable by the TDNNs, are the definite memory 
machine (DMM) languages. In section 4, we will demonstrate the inductive ca- 
pability of the TDNNs in a simulation in which a large DMM is learned using a 
small percentage of the possible, short training examples. In section 5, a second set 
of simulations will show the difference between representational and inductive bias, 
and will demonstrate the utility of internal delays in a TDNN network. 
2 TDNNs and IDNNs Are Functionally Equivalent 
Since every IDNN is also a TDNN, the set of functions computable by any TDNN 
includes all those computable by the IDNNs. [Wan, 1993] also shows that the IDNNs 
can compute any function computable by the TDNNs making these two classes of 
network architectures functionally equivalent. For completeness, here we include a 
description of how to construct from a TDNN, an equivalent IDNN. 
Figure la shows a TDNN with a single input u at the current time (ut), and at 
four earlier time steps (ut-1 ... ut-4). The inputs to node R consist of the outputs 
of nodes P and Q at the current time step along with one or two previous time 
steps. At time t, node P computes fp(ut,...ut_4), a function of the current input 
and four delays. At time t- 1, node P computes fp(ut_,...ut_5). This serves as 
one of the delayed inputs to node R. This value could also be computed by sliding 
node P over one step in the input tap-delay line along with its incoming weights as 
shown in figure lb. Using this construction, all the internal delays can be removed, 
and replaced by copies of the original nodes P and Q, along with their incoming 
weights. This method can be applied recursively to remove any internal delay in any 
TDNN network. Thus, for any function computable by a TDNN, we can construct 
an IDNN which computes the same function. 
3 TDNNs Can Represent the DMM Languages 
In this section, we show that the set of languages which are representable by some 
TDNN are exactly those languages representable by the definite memory machines 
Representation and Induction of Finite State Machines using TDNNs 405 
a) General TDNN 
fp(u t ..... ut_ 4 ) 
fp(U t. 1 ..... Ut. 5 ) 
fp(U t. 2 ..... Ut_ 6 ) 
ut. 1 -.. ut. 6 
b) Equivalent IDNN 
Figure 1: Constructing an IDNN equivalent to a given TDNN 
(DMMs). According to Kohavi (1978) a DMM of order d is a finite state machine 
(FSM) whose present state can always be determined uniquely from the knowledge 
of the most recent d inputs. We equivalently define a DMM of order d as an FSM 
whose accepting/rejecting behavior is a function of only the most recent d inputs. 
To fit TDNNs and IDNNs into the language induction framework, we consider only 
networks with a single 0/1 input. Since any boolean function can be represented 
by a feed-forward network with enough hidden units [Horne and Hush, 1994], an 
IDNN exists which can perform the mapping from d most recent inputs to any 
accepting/rejecting behavior. Therefore, any DMM language can be represented 
by some IDNN. Since every IDNN computes a function of its most recent d inputs, 
by the definition of DMM, there is no boolean output IDNN which represents a 
non-DMM language. Therefore, the IDNNs represent exactly the DMM languages. 
Since the TDNN and IDNN classes are functionally equivalent, TDNNs implement 
exactly the DMM languages as well. 
The shift register behavior of the input tap-delay line in an IDNN completely de- 
termines the state transition behavior of any machine represented by the network. 
This state transition behavior is fixed by the architecture. For example, figure 2a 
shows the state transition diagram for any machine representable by an IDNN with 
two input delays. The mapping from the current state to accept or reject is all 
that can be changed with training. Clouse et al. (1994) describes the conditions 
under which such a mapping results in a minimal FSM. All mappings used in the 
subsequent simulations are minimal FSM mappings. 
4 Simulation 1: Large DMM 
To demonstrate the close relationship between TDNNs and DMMs, here we present 
the results of a simulation in which we trained an IDNN to reproduce the behavior 
of a DMM of order 11. The mapping function for the DMM is given in equation 2. 
Figure 2b shows the minimal 2048 state transition diagram required to represent 
the DMM. The symbol  in equation 2 represents the if-and-only-if function. The 
overbar notation, k, represents the negation of u, the input at time k. Yk is the 
network output at time k. Y > 0.5 is interpreted as accept the string seen so far. 
Yk _< 0.5 means reject. 
Yk = Uk-10 -* (kk-lk-2 q- k-2Ztk-3 q- Itk-lltk-2) (2) 
To create training and test sets, we randomly split in two the set of all 4094 
406 D. $. Clouse, C. L. Giles, B. G. Horne and G. W. Cottrell 
1 
a) DMM of order 3 b) DMM of order 11 
Figure 2: Transition diagrams for two DMMs. 
0.4' 
0.2 
1'0 2'0 3'0 
Percent of total samples (4094) used in training 
Figure 3: Generalization error on 2048 state DMM. 
strings of length 11 or less. We will report results using various percentages of 
possible strings for the training set. The IDNN had 10 input tap-delays, and seven 
hidden units. All tap-delays were cleared to 0 before introduction of a new input 
string. Weights were trained using online back propagation with learning rate 0.25, 
and momentum 0.25. To speed up the algorithm, weights were updated only if the 
absolute error on an example was greater than 0.2. Training was stopped when 
weight updates were required for no examples in the training set. This generally 
required 200 epochs or fewer, though there were trials which required almost 4000 
epochs. 
Each point in figure 3 represents the mean classification error on the test set across 
20 trials. Error bars indicate one standard deviation on each side of the mean. 
Each trial consists of a different randomly-chosen training set. The graph plots 
error at various training set sizes. Note that with training sets as small as 12 
percent of possible strings the network generalizes perfectly to the remaining 88 
percent. This kind of performance is possible because of the close match between 
the representational bias of the IDNN and this specific problem. 
5 Simulation 2: Inductive biases of IDNNs and HDNNs 
In section 2, we showed that the IDNNs and general TDNNs can represent the same 
class of functions. It does not follow that these two architectures are equally capable 
of learning the same functions. In this section, we show that the inductive biases are 
Representation and Induction of Finite State Machines using TDNNs 407 
indeed different. We will present our intuitions about the kinds of problems each 
architecture is well suited to learning, then back up our intuitions with supporting 
simulations. 
In the following simulations, we compare two specific networks. The network repre- 
senting the general TDNNs includes delays on hidden layer outputs. We'll refer to 
this as the hidden 
