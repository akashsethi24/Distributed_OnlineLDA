Monte Carlo Matrix Inversion and 
Reinforcement Learning 
Andrew Barto and Michael Duff 
Computer Science Department 
University of Massachusetts 
Amherst, MA 01003 
Abstract 
We describe the relationship between certain reinforcement learn- 
ing (RL)methods based on dynamic programming (DP)and a class 
of unorthodox Monte Carlo methods for solving systems of linear 
equations proposed in the 1950's. These methods recast the solu- 
tion of the linear system as the expected vlue of a statistic suitably 
defined over sample paths of a Markov chain. The significance of 
our observations lies in arguments (Curriss, 1954) that these Monte 
Carlo methods scale better with respect to state-space size than do 
standard, iterative techniques for solving systems of linear equa- 
tions. This analysis also establishes convergence rate estimates. 
Because methods used in RL systems for approximating the evalu- 
ation function of a fixed control policy also approximate solutions 
to systems of linear equations, the connection to these Monte Carlo 
methods establishes that algorithms very similar to TD algorithms 
(Sutton, 1988) are asymptotically more efficient in a precise sense 
than other methods for ewluating policies. Further, all DP-based 
RL methods have some of the properties of these Monte Carlo al- 
gorithms, which suggests that although RL is often perceived to 
be slow, for sufficiently large problems, it may in fact be more ef- 
ficient than other known classes of methods capable of producing 
the same results. 
687 
688 Barto and Duff 
Introduction 
Consider a system whose dynamics are described by a finite state Markov chain with 
transition matrix P, and suppose that at each time step, in addition to making a 
transition from state zt - i to zt+ - j with probability pi, the system produces 
a randomly determined reward, rt+, whose expected value is R/. The evaluation 
function, Ix, maps states to their expected, infinite-horizon discounted returns: 
t----0 
It is well known that V uniquely satifies a linear system of equations describing 
local consistency: 
or 
(1) 
The problem of computing or estimating V is interesting and important in its 
own right, but perhaps more significantly, it arises as a (rather computationally- 
burdensome) step in certain techniques for solving Markov Decision Problems. In 
each iteration of Policy-Iteration (Howard, 1960), for example, one must determine 
the evaluation function associated with some fixed control policy, a policy that 
improves with each iteration. 
Methods for solving (1) include standard iterative techniques and their variants-- 
successive approximation (Jacobi or Gauss-Seidel versions), successive over- 
relaxation, etc. They also include some of the algorithms used in reinforcement 
learning (RL) systems, such as the family of TD algorithms (Sutton, 1988). Here 
we describe the relationship between the latter methods and a class of unorthodox 
Monte Carlo methods for solving systems of linear equations proposed in the 1950's. 
These methods recast the solution of the linear system as the expected value of a 
statistic suitably defined over sample paths of a Markov chain. 
The significance of our observations lies in arguments (Curriss, 1954) that these 
Monte Carlo methods scale better with respect to state-space size than do stan- 
dard, iterative techniques for solving systems of linear equations. This analysis also 
establishes convergence rate estimates. Applying this analysis to particular mem- 
bers of the family of TD algorithms (Sutton, 1988) provides insight into the scaling 
properties of the TD family as a whole and the reasons that TD methods can be 
effective for problems with very large state sets, such as in the backgammon player 
of Tesauro (Tesauro, 1992). 
Further, all DP-based RL methods have some of the properties of these Monte 
Carlo algorithms, which suggests that although RL is often slow, for large problems 
(Markov Decision Problems with large numbers of states) it is in fact far more prac- 
tical than other known methods capable of producing the same results. First, like 
many RL methods, the Monte Carlo algorithms do not require explicit knowledge 
of the transition matrix, P. Second, unlike standard methods for solving systems 
of linear equations, the Monte Carlo algorithms can approximate the solution for 
some variables without expending the computational effort required to approximate 
Monte Carlo Matrix Inversion and Reinforcement Learning 689 
the solution for all of the variables. In this respect, they are similar to DP-based 
RL algorithms that approximate solutions to Markovian decision processes through 
repeated trials of simulated or actual control, thus tending to focus computation 
onto regions of the state space that are likely to be relevant in actual control (Barto 
at. al., 1991). 
This paper begins with a condensed summary of Monte Carlo algorithms for solv- 
ing systems of linear equations. We show that for the problem of determining an 
evaluation function, they reduce to simple, practical implementations. Next, we 
recall arguments (Curriss, 1954) regarding the scaling properties of Monte Carlo 
methods compared to iterative methods. Finally, we conclude with a discussion of 
the implications of the Monte Carlo technique for certain algorithms useful in RL 
systems. 
2 
Monte Carlo Methods for Solving Systems of Linear 
Equations 
The Monte Carlo approach may be motivated by considering the statistical evalua- 
tion of a simple sum, k ak. If {p} denotes a set of values for a probability mass 
function that is arbitrary (save for the requirement that a  0 imply p  0), then 
(-) p, which may be interpreted as the expected value of a random 
variable Z defined by Pr {Z = -} = p. 
P 
om equation (1) and the Neumann series representation of the inverse it is is clear 
that 
V: (I- 7P)-R = R +TPR +7PR +... 
whose i * component is 
� ..+� +... (2) 
i...i 
and it is this series that we wish to evMuate by statisticM means. 
A technique originated by Ulam and yon-Neumann (Forsythe & Leibler, 1950) uti- 
zes an arbitrarily defined Markov chn with transition matrk  and state set 
{1, 2, ..., n} (V is assumed to have n components). The chain begins in state i and 
is aowed to make k transitions, where k is drawn from a geometric distribution 
with parameter p,,p; i.e., Pr{k state transitions} 
= p,,p(1 - p,,p). The Markov 
chain, governed by P and the geometricay-distributed stopping criterion, defines 
a mass function assigning probabty to every trajectory of every length starting in 
state i, 0 = i0 = i  : i  .-.  t: it, and to each such trajectory there 
corresponds a unique term in the sum (2). 
For the case of value estimation, Z is defined by 
- H=x 
690 Barto and Duff 
which for/5 = p and P, tee= ' becomes 
The sample average of sampled values of Z is guaranteed to converge (as the number 
of samples grows lrge) to state i's expected, infinite-horizon discounted return. 
In Wasow's method (Wasow, 1952), the truncated Neumann series 
E 
i ixia i...i 
is expressed as  plus the expected value of the sum of N random variables 
Z, Z, ..., ZN, the intention being that 
E(Z) = 7   PiqPqi  'Pi_i. 
i...i 
Let trajectories of length N be generated by he Markov chain governed by P. A 
given term 7kPiiPixi a .. 'pi_i is associated with a trajectories i  i  i  
� ..  i  i+  ...  iN whose first k + 1 states are i,i,...,i. The measure 
of this set of trajectories is just iiixia '' 'Pi_xi. Thus, the random variables Zn, 
k: 1, N are defined by 
PiixPixia ' ' ' Pi_xi 
If P: P, then the estimate becomes an average of sample truncated, discounted 
returns: i =  + , +  + ' + 7N. 
The Ulam/von Neumann approach may be reconciled with that of Wasow by pro- 
cessing a given trajectory a posterJori, converting it into a set of terminated paths 
consistent with any choice of stopping-state transition probabties. For example, 
for a stopping state transition probabty of 1 - , a path of length k has proba- 
bmty Each prefix of the observed path z(0)  z(1)  (2) .-. can 
be weighted by the probabty of a path of corresponding length, resulting in an 
estimate, V, that is the sampled, discounted return: 
V =  7 R(n). 
=0 
3 Complexity 
In (Curtiss, 1954) Curtiss establishes a theoretical comparison of the complexity 
(number of multiplications) required by the Ulam/von Neumann method and a 
stationary linear iterative process for computing a single component of the solution 
to a system of linear equations. Curtiss develops an analytic formula for bounds 
on the conditional mean and variance of the Monte-Carlo sample estimate, V, and 
mean and variance of a sample path's time to absorption, then appeals to the 
Monte Carlo Matrix Inversion and Reinforcement Learning 691 
n 
1000 
9oo 
8oo 
7oo 
6oo 
5oo 
4oo 
3oo 
2oo 
lOO 
0 
0 100 200 300 400 500 600 700 800 900 1000 
Figure 1: Break-even size of state space versus accuracy. 
Central Limit Theorem to establish a 95%-confidence interval for the complexity of 
his method to reduce the initial error by a given factor,/. x 
For the case of value-estimation, Curtiss' formula for the Monte-Carlo complexity 
may be written as 
1 (1 + --) (3) 
This is compared to the complexity of the iterative method, which for the value- 
estimation problem takes the form of the classical dynamic programming recursion, 
V (n+l) -- R  7PV('): 
WORKiterati. e --- 
log____{) n2 + 
1 + log '7 
The iterative method's complexity has the form ar,  + r,, with a > 1, while the 
Monte-Carlo complexity is independent of n--it is most sensitive to the amount of 
error reduction desired, signified by . Thus, given a fixed amount of computation, 
for large enough r,, the Monte-Carlo method is likely (with 95% confidence level) to 
produce better estimates. The theoretical break-even points are plotted in Figure
