I II I I Ill II I I I 
Unification of Information Maximization 
and Minimization 
Ryotaro Kamimura 
Information Science Laboratory 
Tokai University 
1117 Kitakaname Hiratsuka Kanagawa 259-12, Japan 
E-mail: ryo@cc.u-tokai.ac.jp 
Abstract 
In the present paper, we propose a method to unify information 
maximization and minimization in hidden units. The information 
maximization and minimization are performed on two different lev- 
els: collective and individual level. Thus, two kinds of information: 
collective and individual information are defined. By maximizing 
collective information and by minimizing individual information, 
simple networks can be generated in terms of the number of con- 
nections and the number of hidden units. Obtained networks are 
expected to give better generalization and improved interpretation 
of internal representations. This method was applied to the infer- 
ence of the maximum onset principle of an artificial language. In 
this problem, it was shown that the individual information min- 
imization is not contradictory to the collective information max- 
imization. In addition, experimental results confirmed improved 
generalization performance, because over-training can significantly 
be suppressed. 
I Introduction 
There have been many attempts to interpret neural networks from the information 
theoretical point of view [2], [4], [5]. Applied to the supervised learning, informa- 
tion has been maximized and minimized, depending on problems. In these methods, 
information is defined by the outputs of hidden units. Thus, the methods aim to 
control hidden unit activity patterns in an optimal manner. Information maximiza- 
tion methods have been used to interpret explicitly internal representations and 
simultaneously to reduce the number of necessary hidden units [5]. On the other 
hand, information minimization methods have been especially used to improve gen- 
eralization performance [2], [4] and to speed up learning. Thus, if it is possible to 
Unification of lnformation Maximization and Minimization 509 
maximize and minimize information simultaneously, information theoretic methods 
are expected to be applied to a wide range of problems. 
In this paper, we unify the above mentioned two methods, namely, information 
maximization and minimization methods, into one framework to improve general- 
ization performance and to interpret explicitly internal representations. However, it 
is apparently impossible to maximize and minimize simultaneously the information 
defined by the hidden unit activity. Our goal is to maximize and to minimize infor- 
mation on two different levels, namely, collective and individual levels. This means 
that information can be maximized in collective ways and information is minimized 
for individual input-hidden connections. The seeming contradictory proposition of 
the simultaneous information maximization and minimization can be overcome by 
assuming the existence of the two levels for the information control. 
Information is supposed to be controlled by an information controller located outside 
neural networks and used exclusively to control information. By assuming the 
information controller, we can clearly see how information appropriately defined can 
be maximized or minimized. In addition, the actual implementation of information 
methods is much easier by introducing a concept of the information controller. 
2 Concept of Information 
In this section, we explain a concept of information in a general framework of an in- 
formation theory. Let Y take on a finite number of possible values y, y2,..., YM with 
probabilities p(y), p(y2), ..., P(YM), respectively. Then, initial uncertainty H(Y) of 
a random variable Y is defined by 
M 
H(Y) = - E P(YJ) logp(yj). 
where 
I(Y l - yp(yj l a,)log p(y 
j=l 
Now, consider conditional uncertainty after the observation of another random vari- 
able X, taking possible values a:, a:, ..., a:$ with probabilities p(a:), p(a:), ..., p(:cM), 
respectively. Conditional uncertainty H(Y I X) can be defined as 
$ M 
H(YIX)- - ZP(X) EP(Yj I a:) l�gp(a:j lye). (2) 
=1 j=l 
We can eily verify that conditional uncertainty is always less than or equal to 
initial uncertainty. Information is usually defined  the decrede of this uncertainty 
I(Y I X ) = H(Y)- H(Y I X ) 
M S M 
= - p(yj)logp(yj)+ p() p(yj I,)1ogp(yj I) 
j=l s=l j=l 
= P(x)p(Yj ])log p(yj ) 
 j P(YJ) 
= 
510 R. Karnirnura 
I m,)logp(y)+ P(Yi I m,)l�gp(Yi I m,), 
(4) 
which is referred to as conditional information. Especially, when prior uncertainty 
is maximum, that is, a prior probability is equi-probable (l/M), then inforrnation 
is 
$ M 
I(Y I x) = logM + yp(z,)p(yj I z,)logp(y1 Ix,) (5) 
s=l j=l 
where log M is maximum uncertainty concering A. 
3 Formulation of Information Controller 
In this section, we apply a concept of information to actual network architectures 
and define collective information and individual information. The notation in the 
above section is changed into ordinary notation used in the neural network. 
3.1 Unification by Information Controller 
Two kinds of information, collective information and individual information, are 
controlled by using an information controller. The information controller is devised 
to interpret the mechanism of the information maximization and minimization more 
explicitly. As shown in Figure 1, the information controller is composed of two 
subcomponents, that is, an individual information minimizer and collective infor- 
mation maximizer. A collective information maximizer is used to increase collective 
information as much as possible. An individual information minimizer is used to 
decrease individual information. By this minimization, the majority of connections 
are pushed toward zero. Eventually, all the hidden units tend to be intermediately 
activated. Thus, when the collective information maximizer and individual infor- 
mation maximizer are simultaneously applied, a hidden unit activity pattern is a 
pattern of the maximum information in which only one hidden unit is on, while 
all the other hidden units are off. However, multiple strongly negative connections 
to produce a maximum information state, are replaced by extremely weak input- 
hidden connections. Strongly negative connections are inhibited by the individual 
information minimization. This means that by the information controller, informa- 
tion can be maximized and at the same time one of the most important properties 
of the information minimization, namely, weight decay or weight elimination, can 
approximately be realized. Consequently, the information controller can generate 
much simplified networks in terms of hidden units and in terms of input-hidden 
connections. 
3.2 Collective Information Maximlzer 
A neural network to be controlled is composed of input, hidden and output units 
with bias, as shown in Figure 1. The jth hidden unit receives a net input from 
input units and at the same time fi'om a collective information maximizer: 
L 
k=O 
where x i is an information maximizer from the jth collective information maximizer 
to the jth hidden unit, � is the number of input units, wik is a connection from 
the kth input unit to the jth hidden unit and . is the kth element of the sth input 
Unification of lnforrnation Maximization and Minimization 511 
Bias  Bias- Itidden 
\ ,--, Connections Bias-Output 
\ ,X ',, / 
Xv'  \ / Bias ] 
Inpu  W i Hidden 
u..  X'J o u.. xW. 
 * ' - 'Ouut 
I  Wi' 
  , utput. 
k / -i X x. 
,  ; J 
: 
  Information 
; . 
,' Maximizers 
Individual 
Information 
Minimizer 
Collective 
Information 
Maximizer 
Information Controller 
Figure 1: A network architecture, realizing the information con- 
troller. 
pattern. The jth hidden unit produces an activity or an output by a sigmoidal 
activation function: 
1 
1 q- exp(-uj)' 
(7) 
The collective information maximizer is used to maximize the information contained 
in hidden units. For this purpose, we should define collective information. Now, 
suppose that in the previous formulation in information, a symbol X and Y repre- 
sent a set of input patterns and hidden units respectively. Then, let us approximate 
a probability p(yj ] :r,,) by a normalized output pj of the jth hidden unit computed 
by 
; (8) 
Em=l Vrn 
where the summation is over all the hidden units. Then, it is reasonable to suppose 
that at an initial stage all the hidden units are activated randomly or uniformly 
and all the input patterns are also randomly given to networks. Thus, a probability 
p(yj) of the activation of hidden units at the initial stage is equi-probable, that is, 
1/M. A probability p(a:) of input patterns is also supposed to be equi-probable, 
namely, 1/$. Thus, information in the equation (3) is rewritten as 
I(YIX)  
M $ M 
1 1 1 
- E  log +  E EP; logp; 
j=l s=lj=l 
512 R. Kamimura 
Input Unit (�) Hidden Unit (1)) 
k J 
pj Fires 
Fire 
1-p jk  Do$ not fire 
Figure 2: An interpretation of an input-hidden connection for 
defining the individual information. 
$ M 
= logM+EEpjlogpj (9) 
*=lj=l 
where logM is maximum uncertainty. This information is considered to be the 
information acquired in a course of learning. Information maximizers are updated to 
increase collective information. For obtaining update rules, we should differentiate 
the information function with respect to information maximizers 
A - fi$ OI(Y I X) 
) 
= ogj- 
=1 m=l 
where fi is a parameter. 
3.3 Individual Information Minimization 
For representing individual information by a concept of information discussed in 
the previous section, we consider an output P. ik from the jth hidden unit only with 
a connection from the kth input unit to the jth output unit: 
p./:: f(wj:) (11) 
which is supposed to be a probability of the firing of the jth hidden unit, given the 
firing of the kth input unit, as shown in Figure 2. Since this probability is 
