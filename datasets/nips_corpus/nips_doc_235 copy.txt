542 Kassebaum, Tenorio and Schaefers 
The Cocktail Party Problem: 
Speech/Data Signal Separation Comparison 
between Backpropagation and SONN 
John Kassebaum 
jakec.ecn.purdue.edu 
Manoel Fernando Tenorio 
tenorioee.ecn.purdue.edu 
Chrlstoph Schaefers 
Parallel Distributed Structures Laboratory 
School of Electrical Engineering 
Purdue University 
W. Lafayette, IN. 47907 
ABSTRACT 
This work introduces a new method called Self Organizing Neural 
Network (SONN) algorithm and compares its performance with Back 
Propagation in a signal separation application. The problem is to 
separate two signals; a modem data signal and a male speech signal, 
added and transmitted through a 4 khz channel. The signals are sam- 
pled at 8 khz, and using supervised learning, an attempt is made to 
reconstruct them. The SONN is an algorithm that constructs its own 
network topology during training, which is shown to be much smaller 
than the BP network, faster to trained, and free from the trial-and- 
error network design that characterize BP. 
1. INTRODUCTION 
The research in Neural Networks has witnessed major changes in algorithm design 
focus, motivated by the limitations perceived in the algorithms available at the 
time. With the extensive work performed in that last few years using multilayered 
networks, it was soon discovered that these networks present limitations in tasks 
The Cocktail Party Problem: 543 
that: {a) are difficult to determine problem complexity a priori, and thus design 
network of the correct size, {b) training not only takes prohibitively long times, 
but requires a large number of samples as well as fine parameter adjustment, 
without guarantee of convergence, (c} such networks do not handle the system 
identification task efficiently for systems whose time varying structure changes 
radically, and, {d) the trained network is little more than a black box of weights 
and connections, revealing little about the problem structure; being hard to find 
the justification for the algorithm weight choice, or an explanation for the output 
decisions based on an input vector. We believe that this need is sparking the 
emergence of a third generation of algorithms to address such questions. 
2. THE SELF 
ALGORITHM 
ORGANIZING 
NEURA 1, 
NETWORK 
2.1 SELF ORGANIZING NETWORK FAMILY 
A family of Self Organizing Structure (SOS) Algorithms can be readily designed 
with our present knowledge, and can be used as a tool to research the motivating 
questions. Each individual algorithm in this family might have different charac- 
teristics, which are summarized in the following list: 
- A search strategy for the structure of the final model 
- A rule of connectivity 
- A performance criteria 
- A transfer function set with appropriate training rule 
As we will show here, by varying each one of these components, a different 
behavior of the algorithm can be imposed. 
Self organizing structure algorithms are not new. These algorithms have been 
present in the statistical literature since the mid 70's in a very different context. 
As far as we know, the first one to propose such an algorithm was Ivahnenko 
[1971] which was followed by a host of variations on that original proposal 
[Duffy&Franklin, 1975; Ikeda, et al., 1976; Tomura&Kondo, 1980; Farlow,1989]. 
Ivahnenko's subfamily of algorithms (GMI)H - Group Method of Data Handling) 
can be characterized in our classification by the same four-tuple criterion: (1) gra- 
dient descent local search, (2) creation of regular feedforward layers with elements 
pairwisely connected, (3) least-mean-squares estimation, and (4) a single element 
set comprised of a 2 order bivariate function. 
Here we want to present our subfamily (SON - Self Organizing Networks) of the 
SOS algorithm family, characterized differently by: (1) global optimization search, 
(2) arbitrary connectivity based on an arbitrary number of neuron inputs, (3) 
Structure Estimation Criteria (SEC) (a variation of Rissanen's [1983]. Minimum 
Description Length Criteria, extended to the hierarchical case), and, (4) for train- 
ing speed, activation functions are restricted to be linear on the parameters and 
the output functions need to be invertible, no other restriction is imposed in kind 
or number. The particular algorithm presented here is called the Self Organizing 
544 Kassebaum, Tenorio and Schaefers 
Neural Network (SONN) [Tenorio&Lee, 1988,1989; Tenorio 1990 a,b]. It was com- 
posed of: (1) a graph synthesis procedure based on Simulated Annealing [Kirkpa- 
trick et al., 1983]; (2) two input neurons that are arbitrarily connected; (3) the 
Structure Estimation Criteria; and, (4) a set of all polynomials that are special 
cases of 2nd order bivariates and inclusive, followed or not by sigmoid functions. 
The SONN algorithm performs a search in the model space by the construction 
of hypersurfaces. A network of nodes, each node representing a hypersurface, is 
organized to be an approximate model of the real system. Below, the components 
of SONN are discussed. 
2.2 THE ALGORITHM STRUCTURE 
The mechanisms behind the algorithm works as follows. First, create a set of ter- 
minals which are the output of the nodes available for connection to other nodes. 
This set is initialized with the output of the input nodes; in other words, the input 
variables themselves. From this set, with uniform probability, select a subset (2 in 
our case) of terminals, and used them as inputs to the new node. To construct the 
new node, select all the function of the set of prototype functions (activation fol- 
lowed by output function), and evaluate the SEC using the terminals as inputs. 
Selecting the best function, test for the acceptance of that node according to the 
Simulated Annealing move acceptance criterion. If the new node is accepted, place 
its output in the set of terminals and iterate until the optimum model is found. 
The details of the algorithm can be found in [Tenorio&Lee, 1989]. 
2.2.1 The Prototype Functions 
Consider the Mahalanobis distance: 
yi = 
(1) 
This distance can be rewritten as a second order function, whose parameters are 
the indirect representation of the covariance matrix X and the mean vector . 
This function is linear in the parameters, which makes it easy to perform trainLug, 
and it is the function with the smallest degree of non lineaxity; only simpler is the 
linear case. Interestingly enough, this is the same prototype function used in the 
GIV[DH algorithm to form the Ivahnenko polynomial for apparently completely 
different reasons. In the SONN, this function is taken to be 2-input and all its pos- 
sible variations (32) by setting parameters to zero are included in the set of 
activation functions. This set combined with the output function (the identify or 
sigmoid), for the set of prototype functions, used by the algorithm in the node 
construction. 
2.2.2 Evaluation of the Model Based on the MDL Criterion 
The selection rule of the neuron transfer function was based on a modification of 
the M. inlmal Description Length (MDL) information criterion. In [Rissanen, 1978], 
the principle of minimal description for statistical estimation was developed. The 
reason for the choice of such a criterion is that, in general the accuracy of the 
model can increase at the expense of simplicity in the number of parameters. The 
The Cocktail Party Problem: 545 
increase of complexity might also be accompanied by the overfitting of the model. 
To overcome this problem, the MDL provides a trade-off between the accuracy 
and the complexity of the model by including the structure estimation term of the 
final model. The final model (with the minimal MDL) is optimum in the sense of 
being a consistent estimate of the number of parameters while achieving the 
minimum error [Rissanen, 1980]. Given a sequence of observations 
X IX2...X N from the random variable X, the dominant term of the MDL in 
[Rissanen, 1978] is: 
MDL = --log f(x ]8)q-0.5 k log N (2) 
where f(x Io)is the estimated probability density function of the model, k is the 
number of parameters, and N is the number of observations. The first term is 
actually the negative of the maximum likelihood (ML) with respect to the 
estimated parameter. The second term describes the structure of the models and it 
is used as a penalty for the complexity of the model. 
3. EXAMPLE- THE COCKTAIL PARTY PROBLEM 
The Cocktail Party Problem is the name given to the phenomenon that people 
can understand and track speech in a noisy environment, even when the noise is 
being made by other speakers. A simpler version of this problem is presented here: 
a 4 khz channel is excited with male speech and modem data additively at the 
same time. The task presented to the network is to separate both signals. 
To compare the accuracy of the signal separation between the SONN and the 
Back Propagation algorithms a normalized RMSE is used as a performance 
index: 
normalized RMSE - 
RMSE 
StandardDerision 
(2) 
3.1. EXPERIMENTS WITH BACK PROPAGATION 
In order to design a filter using Back Propagation for this task, several architec- 
tures were considered. Since the input and output to the problem are time series, 
and such architectures are static, modifications to the original paradigm is 
required to deal with the time dimension. Several proposals have been made in 
this respect: tapped delay filters, recurrent architectures, low pass filter transfer 
functions, modified discriminant functions, and self excitatory connections (see 
[Wah, Tenorio, Merha, and Fortes, 90] ). The best result for this task was 
achieved by two tapped delay lines in the input layer, one for the input signal, the 
other for the output signal. The network was trained to recognize the speech sig- 
nal from the mixed signal. The mixed signal had a speech to modem data energy 
ratio of 4:1, or 2.5 dB. 
The network was designed to be a feedforward with 42 inputs (21 delayed versions 
of the i
