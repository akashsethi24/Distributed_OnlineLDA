ALCOVE: A Connectionist Model of 
Human Category Learning 
John K. Kruschke 
Department of Psychology and Cognitive Science Program 
Indiana University, Bloomington IN 47405-4201 USA 
e-mail: kruschke@ucs.indiana.edu 
Abstract 
ALCOVE is a connectionist model of human category learning that fits a 
broad spectrum of human learning data. Its architecture is based on well- 
established psychological theory, and is related to networks using radial 
basis functions. From the perspective of cognitive psychology, ALCOVE can 
be construed as a combination of exemplar-based representation and error- 
driven learning. From the perspective of connectionism, it can be seen as 
incorporating constraints into back-propagation networks appropriate for 
modelling human learning. 
1 INTRODUCTION 
ALCOVE is intended to accurately model human, perhaps non-optimal, performance 
in category learning. While it is a feed-forward network that learns by gradient 
descent on error, it is unlike standard back propagation (Rumelhart, Hinton &; 
Williams, 1986) in its architecture, its behavior, and its goals. Unlike the standard 
back-propagation network, which was motivated by generalizing neuron-like per- 
ceptrons, the architecture of ALCOVE was motivated by a molar-level psychological 
theory, Nosofsky's (1986) generalized context model (6CM). The psychologically 
constrained architecture results in behavior that captures the detailed course of hu- 
man category learning in many situations where standard back propagation fares 
less well. And, unlike most applications of standard back propagation, the goal of 
ALCOVE is not to discover new (hidden-layer) representations after lengthy training, 
but rather to model the course of learning itself (Kruschke, 1990c), by determining 
which dimensions of the given representation are most relevant to the task, and how 
strongly to associate exemplars with categories. 
649 
650 ruscle 
� � 
Category nodes. 
Learned association weights. 
Exemplar nodes. 
Learned attention trengths. 
Stimulus dimension nodes. 
Figure 1: The architecture of ALCOVE (Attention Learning COvEring map). Exem- 
plar nodes show their activation profile when r - q - i in Eqn. 1. 
2 THE MODEL 
Like the GCM, ALCOVE assumes that input patterns can be represented as points in a 
multi-dimensional psychological space, as determined by multi-dimensional scaling 
algorithms (e.g., Shepard, 1962). Each input node encodes a single psychological 
dimension, with the activation of the node indicating the value of the stimulus on 
that dimension. Figure 1 shows the architecture of ALCOVE, illustrating the case of 
just two input dimensions. 
Each input node is gated by a dimensional attention strength ci. The a.ttention 
strength on a dimension reflects the relevance of that dimension for the particular 
categorization task at hand, and the model learns to allocate more attention to 
relevant dimensions and less to irrelevant dimensions. 
Each hidden node corresponds to a position in the multi-dimensional stimulus space, 
with one hidden node placed at the position of every training exemplar. Each hidden 
node is activated according to the psychological similarity of the stimulus to the 
exemplar represented by the hidden node. The similarity function comes from the 
CCM and the work of Shepard (1962; 1987): Let the position of the jtn hidden 
node be denoted as (hjl , hi2,...), and let the activation of the jtn hidden node be 
_nia Then 
denoted as .j . 
( (. 
_nict _ . ihji ai 
uj exp - c a i - i'lr (1) 
where c is a positive constant called the specitcity of the node, where the sum is 
taken over all input dimensions, and where r and q are constants determining the 
similarity metric and similarity gradient, respectively. For separable psychological 
ALCOVE: A Connectionist Model of Human Category Learning 651 
(h) 
Figure 2: (a) Increasing attention on the horizontal axis and decreasing attention on 
the vertical axis causes exemplars of the two categories (denoted by dots and +'s) to 
have greater between-category dissimilarity and greater within-category similarity. 
(After Nosofsky, 1986, Fig. 2.) (b) ALCOVE cannot differentially attend to diagonal 
axes. 
dimensions, the city-block metric (r = 1) is used, while integral dimensions might 
call for a Euclidean metric (r = 2). An exponential similarity gradient (q = 1) is 
used here (Shepard, 1987; this volume), but a Gaussian similarity gradient (q = 2) 
can sometimes be appropriate. 
The dimensional attention strengths adjust themselves so that exemplars from dif- 
ferent categories become less similar, and exemplars within categories become more 
similar. Consider a simple case of four stimuli that form the corners of a square in 
input space, as in Figure 2(a). The two left stimuli are mapped to one category 
(indicated by dots) and the two right stimuli are mapped to another category (indi- 
cated by +'s). ALCOVE learns to increase the attention strength on the horizontal 
axis, and to decrease the attention strength on the vertical axis. On the other hand, 
ALCOVE cannot stretch or shrink diagonally, as suggested in Figure 2(b). This con- 
straint is an accurate reflection of human performance, in that categories separated 
by a diagonal boundary tend to take longer to learn than categories separated by a 
boundary orthogonal to one dimension. 
Each hidden node is connected to output nodes that correspond to response cate- 
gories. The connection from the jtn hidden node to the ktn category node has a 
connection weight denoted wkj , called the association weight between the exemplar 
and the category. The output (category) nodes are activated by the linear rule used 
in the CCM and the network models of Gluck and Bower (1988a,b): 
hid 
aUt = E wkjaj ' 
hid 
J 
(2) 
In ALCOVE, unlike the GCM, the association weights are learned and can take on any 
real value, including negative values. Category activations are mapped to response 
probabilities using the same choice rule as was used in the GCM and network models. 
Thus, 
Pr(K) = exp( a t) / E exp(c)aUt) (3) 
/ out 
k 
652 Kruschke 
where 6 is a real-valued scaling constant. In other words, the probability of classify- 
ing the given stimulus into category K is determined by the magnitude of category 
K's activation relative to the sum of all category activations. 
The dimensional attention strengths, ai, and the association weights, wkj , are 
learned by gradient descent on sum-squared error, as used in standard back prop- 
agation (Rumelhart et al., 1986) and in the network models of Gluck and Bower 
(1988a,b). Details can be found in Kruschke (1990a,b). In fitting ALCOVE to human 
learning data, there are four free parameters: the fixed specificity c in Equation 1; 
the probability mapping constant 6 in Equation 3; the association weight learning 
rate; and, the attention strength learning rate. 
In summary, ALCOVE extends Nosofsky's (1986) GCM by having a learning mecha- 
nism and by allowing any positive or negative values for association weights, and 
it extends Gluck and Bower's (1988a,b) network models by including explict atten- 
tion strengths and by using continuous input dimensions. It is a combination of 
exemplar-based category representations with error-driven learning, as alluded to 
by Estes et al. (1989; see also Hurwitz, 1990). ALCOVE can also be construed as a 
form of (non-)radial basis function network, if r = q = 2 in Equation 1. In the form 
described here, the hidden nodes are placed at positions where training exemplars 
occur, but another option, described by Kruschke (1990a,b), is to scatter hidden 
nodes over the input space to form a covering map. Both these methods work 
well in fitting human data in some situations, but the exemplar-based approach 
has advantages (Kruschke, 1990a,b). ALCOVE can also be compared to a standard 
back-propagation network that has adaptive attentional multipliers on its input 
nodes (cf. Mozer and Smolensky, 1989), but with fixed input-to-hidden weights 
(Kruschke 1990b, p.33). Such a network behaves similarly to a covering-map ver- 
sion of ALCOVE. Moreover, such back-prop networks are susceptible to catastrophic 
retroactive interference (Ratcliff, 1990; McCloskey &; Cohen, 1989), unlike ALCOVE. 
3 APPLICATIONS 
Several applications of ALCOVE to modelling human performance are detailed else- 
where (Kruschke, 1990a,b); a few will be summarized here. 
3.1 RELATIVE DIFFICULTY OF CATEGORY STRUCTURES 
The classic work of Shepard, Hovland and Jenkins (1961) explored the relative 
difficulty of learning different category structures. As a simplified example, the 
linearly separable categories in Figure 2(a) are easier to learn than the exclusive-or 
problem (which would have the top-left and bottom-right exemplars mapped to 
one category, and the top-right and bottom-left mapped to the other). Shepard et 
al. carefully considered several candidate explanations for the varying difficulties, 
and concluded that some form of attentional learning was necessary to account for 
their results. That is, people seemed to be able to determine which dimensions 
were relevant or irrelevant, and they allocated attention to dimensions accordingly. 
Category structures with fewer relevant dimensions were easier to learn. ALCOVE 
has just the sort of attentional learning mechanism called for, and can match the 
relative difficulties observed by Shepard et al. 
ALCOVE: A Connectionist Model of Human Category Learning 653 
3.2 BASE-RATE NEGLECT 
A recent series of experiments (Gluck &; Bower, 1988b; Estes et al., 1989; Shanks, 
1990; Nosofsky et al., 1991) investigated category learning when the assignment of 
exemplars to categories was probabilistic and the base rates of the categories were 
unequal. In these experiments, there were two categories (one rare and the other 
common) and four binary-valued stimulus dimensions. The stilnulus values were 
denoted sl and 
