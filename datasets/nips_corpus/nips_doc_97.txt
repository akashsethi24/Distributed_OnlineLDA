116 
THE BOLTZMANN PERCEPTRON NETWORK: 
A MULTI-LAYERED FEED-FORWARD NETWORK 
EQUIVALENT TO THE BOLTZMANN MACHINE 
Eyal Yair and Allen Gersho 
Center for Information Processing Research 
Departmere of Electrical & Computer Engineering 
University of California, Santa Barbara, CA 93106 
ABSTRACT 
The concept of the stochastic Boltzmann machine (BM) is attractive for 
decision making and pattern classification purposes since the probability of 
attaining the network states is a function of the network energy. Hence, the 
probability of attaining particular energy minima may be associated with the 
probabilities of making certain decisions (or classifications). However, 
because of ill stochastic nature, the complexity of the BM is fairly high and 
therefore such networks are not very likely to be used in practice. In this 
paper we suggest a way to alleviate this drawback by converting the sto- 
elmstic BM into a deterministic network which we call the Boltzmann Per- 
ceplxon Network (BPN). The BPN is functionally equivalent to the BM but 
has a feed-forward structure and low complexity. No annealing is required. 
The conditions under which such a conversion is feasible are given. A 
learning algorithm for the BPN based on the conjugate gradient method is 
also provided which is somewhat akin to the backpropagation algorithm. 
INTRODUCTION 
In decision-making applications, it is desirable to have a network which computes the pro- 
babilities of deciding upon each of M possible propositions for any given input pattern. In 
principle, the Boltzmann machine (BM) (Hinton, Sejnowsld and Ackley, 1984) can provide 
such a capability. The network is composed of a set of binary units connected through sym- 
metric connection links. The units are randomly and asynchronously changing their values 
in {0,1} according to a stochastic Iransition rule. The transition rule used by Hinton et. al. 
defines the probability of a unit to be in the 'on' state as the logistic function of the energy 
change resulting by changing the value of that unit. The BM can be described by an 
ergodic Markov chain in which the thermal equilibrium probability of attaining each state 
obeys the Boltzmann distribution which is a function of only the energy. By associating the 
set of possible propositions with subsell of network states, the probability of deciding upon 
each of these propositions can be measured by the probability of a,alning the correspond- 
ing set of states. This probability is also affected by the temperature. As the temperature 
This wod was supported by the W�izmann Foundation for scientific research, by the 
University of California MICRO program, and by Bell Communications Research, Inc. 
The Boltzmann Perceptron Network 
increases, the Boltzmann probability distribution become more uniform and thus the deci- 
sion made is 'vague'. The lower the temperature the greater is the probability of a_n__aining 
states with lower energy thereby leading to more 'distinctive' decisions. 
This approach, while very attractive in principle, has two major drawbacks which make the 
complexity of the computations become non-feasible for nontrivial problems. The first is 
the need for thermal equilibrium in order to obtain the Boltzmann distribution. To make 
distinctive decisions a low temperature is required. This implies slower convergence 
towards thermal equilibrium. Generally, the method used to reach thermal equilibrium is 
simulated annealing (SA) (Kirkpatrick et. al., 1983) in which the temperature starts from a 
high value and is gradually reduced to the desired final value. In order to avoid 'freezing' 
of the network, the cooling schedule should be fairly slow. SA is thus time consuming and 
computationally expensive. The second drawback is due to the stochastic nature of the com- 
putation. Since the network state is a random vector, the desired probabilities have to be 
estimated by accumulating statistics of the network behavior for only a finite period of 
time. Hence, a trade-off between speed and accuracy is unavoidable. 
In this paper, we propose a mechanism to alleviate the above computational drawbacks by 
converting the stochastic BM into a functionally equivalent deterministic network, which 
we call the Boltzmann Perceptron Network (BPN). The BPN circumvents the need for a 
Monte Carlo type of computation and instead evaluates the desired probabilities using a 
multilayer perceptron-like network. The very time consuming learning pn:rss for the BM 
is similarly replaced by a deterministic learning scheme, somewhat akin to the backpropa- 
gation algorithm, which is computationally affordable. The similarity between the learning 
algorithm of a BM having a layered structure and that of a two-layer perceptron has been 
recently pointed out by Hopfield (1987). In this paper we further elaborate on such an 
equivalence between the BM and the new perceptton-like network, and give the conditions 
under which the conversion of the stochastic BM into the deterministic BPN is possible. 
Unlike the original BM, the BPN is virtually always in thermal equilibrium and thus SA is 
no longer required. Nevertheless, the temperature still plays the same role and thus varying 
it may be beneficial to control the 'sofmess' of the decisions made by the BPN. Using the 
BPN as a soft classifier is described in details in (Yair and Gersho, 1989). 
THE BOLTZMANN PERCEPTRON NETWORK 
Suppose we have a network of K units connected through symmetric connection links with 
no self-feedback, so that the connection matrix r is symmetric and zero-diagonal. Let us 
categorize the units into three different types: input, output and hidden units. The input 
pattern will be supplied to the network by clamping the input units, denoted by 
x_= (x,..,xi ,..,xt) r, with this pattern. x_ is a real-valued vector in R . The output of the net- 
work will be observed on the output units E=(y,..,y,,..,yt) r, which is a binary vector. 
The remaining units, denoted v=(v,..,vi,..,vj) r, are the hidden units, which are also 
binary-valued. The hidden and output units are asynchronously and randomly changing 
their binary values in {0,1 } according to inputs they receive from other units. 
The state of the network will be denoted by the vector u which is partitioned as follows: 
u r =(x_r,vr,r). The energy associated with state u is denoted by Eu and is given by: 
-E,, = �urF_u + ur$ (1) 
where J is a vector of bias values, partitioned to comply with the partition of _u as follows: 
b r = (_rr,_c_r,sr). 
118 Yair and Gersho 
The transition from one state to another is performed by selecting each time one unit, say 
unit k, at random and determine its output value according to the following stochastic rule: 
set the output of the unit to 1 with probability p,, and to 0 with a probability 1-p,. The 
parameter p, is determined locally by the k-th unit as a function of the energy change 
in the following fashion: 
pk=g(AF.k) ; g(x) =a 1 (2) 
1 +e-Ix 
AE, = {E,,(unit k is off) - E,, (unit k is on) }, and I= 1/r is a control parameter. T is called 
the temperature and g (-) is the logistic function. With this transition rule the thermal equili- 
brium probability P,, of attaining a state _u obeys the Boltzmann distribution: 
p, 1 -  E. 
= e (3) 
z 
where Z,, called the partition function, is a normalization factor (independent of v_ and y.) 
such that the sum of Pu over all the 2 s+u possible states will sum to unity. 
In order to use the network in a dterministic fashion rather than accumulate statistics while 
observing its random behavior, we should be able to explicitly compute the probability of 
This 
auaining a certain vector ig on the output units while x is clamped on the input units. 
probability, denoted by Py ,,, can be expressed as: 
(4) 
where B is the set of all binary vectors of length J, and v ,y Ix denotes a state 1t in which 
a specific input vector x is clamped. The explicit evaluation of the desired probabilities 
therefore involves the computation of the partition function for which the number of opera- 
tions grows exponentially with the number of units. That is, the complexity is O(2J+s). 
Obviously, this is computationally unacceptable. Nevertheless, we shall see that under a 
certain restriction on the connection malTix F the explicit computation of the desired proba- 
bilities becomes possible with a complexity of 0 (JM) which is computationally feasible. 
Let us assume that for each input pattern we have M possible propositions which are asso- 
ciated with the M output vectors: lu = {It ,..J_ ,..J }, where I_,,, is the m-th column of the 
M,,M identity matrix. Any state of the network having output vector i[=I_,,, (for any m) 
will be denoted by v ,m Ix and will be called a feasible state. All other state vectors v ,y Ix 
for x;l_,,, will be considered as intermediate steps between two feasible states. This 
redefinition of the network states is equivalent to redefining the states of the underlying 
Markov model of the network and thus conserves the equilibrium Boltzmann distribution. 
The probability of proposition m for a given input x, denoted by P,, ,,, will be taken as the 
probability of obtaining output vector x=l_,,, given that the output is one of the feasible 
values. That is, 
P,, = Pr {X = I_,,, I x, lelu } (5) 
which can be computed from (4) by restricting the state space to the 2sM feasible state 
vectors and by setting =. The partition function, conditioned on restricting : to lie in 
the set of feasible outputs, lu, is denoted by Z and is given by: 
The Boltzmann Perceptron Network 119 
Let us now partition the connection matrix V to comply with the partition of the state vec- 
tor and rewrite the energy for the feasible state v,m ix as: 
-E,,,,, =_vT(Rx_+Q +�D2Y_+.g.)+ I_r(Wx+�D I: +S)+ x_T(AD_x+f) . (7) 
Since x is clamped on the input units, the last term in the energy expression serves onl
