Learning curves for Gaussian processes 
Peter Sollich* 
Department of Physics, University of Edinburgh 
Edinburgh EH9 3JZ, U.K. Email: P. Sollich�ed. ac. uk 
Abstract 
I consider the problem of calculating learning curves (i.e., average 
generalization performance) of Gaussian processes used for regres- 
sion. A simple expression for the generalization error in terms of 
the eigenvalue decomposition of the covariance function is derived, 
and used as the starting point for several approximation schemes. 
I identify where these become exact, and compare with existing 
bounds on learning curves; the new approximations, which can 
be used for any input space dimension, generally get substantially 
closer to the truth. 
1 INTRODUCTION: GAUSSIAN PROCESSES 
Within the neural networks community, there has in the last few years been a 
good deal of excitement about the use of Gaussian processes as an alternative to 
feedforward networks [1]. The advantages of Gaussian processes are that prior 
assumptions about the problem to be learned are encoded in a very transparent 
way, and that inference at least in the case of regression that I will consider--is 
relatively straightforward. One crucial question for applications is then how 'fast' 
Gaussian processes learn, i.e., how many training examples are needed to achieve a 
certain level of generalization performance. The typical (as opposed to worst case) 
behaviour is captured in the learning curve, which gives the average generalization 
error e as a function of the number of training examples n. Several workers have 
derived bounds on e(n) [2, 3, 4] or studied its large n asymptotics. As I will illustrate 
below, however, the existing bounds are often far from tight; and asymptotic results 
will not necessarily apply for realistic sample sizes n. My main aim in this paper 
is therefore to derive approximations to e(n) which get closer to the true learning 
curves than existing bounds, and apply both for small and large n. 
In its simplest form, the regression problem that I am considering is this: We are 
trying to learn a function 9' which maps inputs x (real-valued vectors) to (real- 
valued scalar) outputs 9*(x). We are given a set of training data D, consisting of n 
Present address: Department of Mathematics, King's College London, Strand, London 
WC2R 2LS, U.K. Email peter. sollichkcl.ac .uk 
Learning Curves for Gaussian Processes 345 
input-output pairs (xl, yl); the training outputs yt may differ from the 'clean' target 
outputs 0* (xt) due to corruption by noise. Given a test input x, we are then asked 
to come up with a prediction O(x) for the corresponding output, expressed either in 
the simple form of a mean prediction (x) plus error bars, or more comprehensively 
in terms of a 'predictive distribution' P(O(x)Ix, D). In a Bayesian setting, we do this 
by specifying a prior P(O) over our hypothesis functions, and a likelihood P(DIO ) 
with which each 0 could have generated the training data; from this we deduce the 
posterior distribution P(O[D) cr P(D[O)P(O). In the case of feedforward networks, 
where the hypothesis functions 0 are parameterized by a set of network weights, the 
predictive distribution then needs to be extracted by integration over this posterior, 
either by computationally intensive Monte Carlo techniques or by approximations 
which lead to analytically tractable integrals. For a Gaussian process, on the other 
hand, obtaining the predictive distribution is trivial (see below); one reason for 
this is that the prior P(O) is defined directly over input-output functions 0. How 
is this done? Any 0 is uniquely determined by its output values O(x) for all x 
from the input domain, and for a Gaussian process, these are simply assumed to 
have a joint Gaussian distribution (hence the name). This distribution can be 
specified by the mean values (O(x)l o (which I assume to be zero in the following, 
as is commonly done), and the covariances (O(x)O(x')) o = C(x,x'); C(x,x') is 
called the covariance function of the Gaussian process. It encodes in an easily 
interpretable way prior assumptions about the function to be learned. Smoothness, 
for example, is controlled by the behaviour of C(x, x') for x' -+ x: The Ornstein- 
Uhlenbeck (OU) covariance function C(x, x') cr exp(-Ix-x']/l) produces very rough 
(non-differentiable) functions, while functions sampled from the squared exponential 
(SE) prior with C(x,x') cr exp(-]x - x'12/(212)) are infinitely differentiable. The 
'length scale' parameter l, on the other hand, corresponds directly to the distance in 
input space over which we expect our function to vary significantly. More complex 
properties can also be encoded; by replacing I with different length scales for each 
input component, for example, relevant (small l) and irrelevant (large l) inputs can 
be distinguished. 
How does inference with Gaussian processes work? I only give a brief summary here 
and refer to existing reviews on the subject (see e.g. [5, 1]) for details. It is simplest 
to assume that outputs y are generated from the 'clean' values of a hypothesis 
function O(x) by adding Gaussian noise of x-independent variance a 2. The joint 
distribution of a set of training outputs {yt} and the function values O(x) is then 
also Gaussian, with covariances given by 
(YlYm) -- C(Xl,Xm) q- t72(lm --' (K)/m, (ylO(X)) -- C(Xl,X ) ---- (k(x))/; 
here I have defined an n x n matrix K and x-dependent n-component vectors k(x). 
The posterior distribution P(OID ) is then obtained by simply conditioning on the 
{Yl}. It is again Gaussian and has mean and variance 
(O(x))ol D = (x) = k(x)TK-y (1) 
((O(x) -t(x))2)olr ) = C(x,x) - k(x)X'K-k(x) (2) 
Eqs. (1,2) solve the inference problem for Gaussian process: They provide us directly 
with the predictive distribution P(O(x)]x,D). The posterior variance, eq. (2), in 
fact also gives us the expected generalization error at x. Why? If the teacher 
is 0', the squared deviation between our mean prediction and the teacher output 
is  ((x) - 0* (x))2; averaging this over the posterior distribution of teachers P(O* 
just gives (2). The underlying assumption is that our assumed Gaussian process 
1One can also one measure the generalization by the squared deviation between the 
prediction O(x) and the noisy teacher output; this simply adds a term a 2 to eq. (3). 
346 P $ollich 
prior is the true one from which teachers are actually generated (and that we are 
using the correct noise model). Otherwise, a more complicated expression for the 
expected generalization error results; in line with most other work on the subject, I 
only consider the 'correct prior' case in the following. Averaging the generalization 
error at x over the distribution of inputs gives then 
e(D) = (C(x,x) - k(x)TK-k(x))x 
(3) 
This form of the generalization error (which is well known [2, 3, 4, 5]) still depends 
on the training inputs (the fact that the training outputs have dropped out already 
is a signature of the fact that Gaussian processes are linear predictors, compare (1)). 
Averaging over data sets yields the quantity we are after, 
e = (e(D))r). (4) 
This average expected generalization error (I will drop the 'average expected' in the 
following) only depends on the number of training examples n; the function e(n) 
is called the learning curve. Its exact calculation is difficult because of the joint 
average in eqs. (3,4) over the training inputs xl and the test input x. 
2 LEARNING CURVES 
As a starting point for an approximate calculation of e(n), I first derive a repre- 
sentation of the generalization error in terms of the eigenvalue decomposition of 
the covariance function. Mercer's theorem (see e.g. [6]) tells us that the covariance 
function can be decomposed into its eigenvalues/ki and eigenfunctions �i (x): 
C(x,x') = (5) 
i=l 
This is simply the analogue of the eigenvalue decomposition of a finite symmetric 
matrix; the eigenfunctions can be taken to be normalized such that (c)i(x)c)j (x))x - 
50. Now write the data-dependent generalization error (3) as e(D) = (C(x, x)) - 
tr (k(x)k(x)T)z K - and perform the x-average in the second term: 
((k(x)k(x)T)l,)z = ' AAjc)(x,) (c)(x)c)j(x)) c)j(x,) = ' ,*i(Xl)*i(Xm) 
ij i 
This suggests introducing the diagonal matrix (A)ij -/kidij and the 'design matrix' 
()ti = Oi(x), so that (k(x)k(x)T)x = (I)A2(I)T. One then also has (C(x,x)) = 
tr A, and the matrix K is expressed as K = a2I q- A T, I being the identity 
matrix. Collecting these results, we have 
e(D) =trA - tr (a2I + AT)-A2 T 
This can be simplified using the Woodbury formula for matrix inverses (see e.g. [7]), 
which applied to our case gives (a2I+ A T) - = a -2 [I - (a2I+ AT) -  AT]; 
after a few lines of algebra, one then obtains the final result 
e=(e(D)}D, �(D)=trcr2A(cr2I+AT)-=tr(A-+cr-2T) -1 (6) 
This exact representation of the generalization error is one of the main results of this 
paper. Its advantages are that the average over the test input x has already been 
carried out, and that the remainin dependence on the training data is contained 
entirely in the matrix T. It also includes as a special case the well-known result 
for linear regression (see e.g. [8]); A - and T can be interpreted as suitably 
generalized versions of the weight decay (matrix) and input correlation matrix. 
Starting from (6), one can now derive approximate expressions for the learning 
Learning Curves for Gaussian Processes 347 
curve e(n). The most naive approach is to entirely neglect the fluctuations in (I)T(I) 
over different data sets and replace it by its average, which is simply/(I,TI,)ij ) r> = 
Yt Ic)i (xt)qSj (xt)}  = nSij. This leads to the Naive approximation 
eN(n) = tr(A - + cr-2nI) - (7) 
which is not, in general, very good. It does however become exact in the large noise 
limit a 2 - oo at constant n/a2: The fluctuations of the elements of the matrix 
a
