Analytical study of the interplay between 
architecture and predictability 
Avner Priel, Ido Kanter, David A. Kessler 
Minerva Center and Department of Physics, Bar Ilan University, 
Ramat-Gan 52900, Israel. 
e-mail: priel@mail.cc.biu.ac.il (web-page: http://faculty. biu.ac.il/priel) 
Abstract 
We study model feed forward networks as time series predictors 
in the stationary limit. The focus is on complex, yet non-chaotic, 
behavior. The main question we address is whether the asymptotic 
behavior is governed by the architecture, regardless the details of 
the weights. We find hierarchies among classes of architectures 
with respect to the attractor dimension of the long term sequence 
they are capable of generating; larger number of hidden units can 
generate higher dimensional attractors. In the case of a perceptron, 
we develop the stationary solution for general weights, and show 
that the flow is typically one dimensional. The relaxation time 
from an arbitrary initial condition to the stationary solution is 
found to scale linearly with the size of the network. In multilayer 
networks, the number of hidden units gives bounds on the number 
and dimension of the possible attractors. We conclude that long 
term prediction (in the non-chaotic regime) with such models is 
governed 'by attractor dynamics related to the architecture. 
Neural networks provide an important tool as model free estimators for the solution 
of problems when the real model is unknown, or weakly known. In the last decade 
there has been a growing interest in the application of such tools in the area of time 
series prediction (see Weigand and Gershenfeld, 1994). In this paper we analyse a 
typical class of architectures used in this field, i.e. a feed forward network governed 
by the following dynamic rule: 
S +1 --- Sou t ; S +1 -- S_ 1 j = 2,...,N (1) 
where Sour is the network's output at time step t and S are the inputs at that time; 
N is the size of the delayed input vector. The rational behind using time delayed 
vectors as inputs is the theory of state space reconstruction of a dynamic system 
316 A. Priel, I. Kanter and D. A. Kessler 
using delay coordinates (Takens 1981, Sauer Yorke and Casdagli 1991). This the- 
ory address the problem of reproducing a set of states associated with the dynamic 
system using vectors obtained from the measured time series, and is widely used for 
time series analysis. A similar architecture incorporating time delays is the TDNN 
- time-delay neural network with a recurrent loop (Waibel et. al. 1989). This type 
of networks is known to be appropriate for learning temporal sequences, e.g. speech 
signal. In the context of time series, it is mostly used for short term predictions. Our 
analysis focuses on the various long-time properties of the sequence generated by a 
given architecture and the interplay between them. The aim of such an investiga- 
tion is the understanding and characterization of the long term sequences generated 
by such architectures, and the time scale to reach this asymptotic behavior. Such 
knowledge is necessary to define adequate measures for the transition between a 
locally dependent prediction and the long term behavior. Though some work has 
been done on characterization of a dynamic system from its time series using neu- 
ral networks, not much analytical results that connect architecture and long-time 
prediction are available (see M. Mozer in Weigand and Gershenfeld, 1994). Nev- 
ertheless, practical considerations for choosing the architecture were investigated 
extensively (Weigand and Gershenfeld, 1994 and references therein). It has been 
shown that such networks are capable of generating chaotic like sequences. While 
it is possible to reconstruct approximately the phase space of chaotic attractors (at 
least in low dimension), it is clear that prediction of chaotic sequences is limited 
by the very nature of such systems, namely the divergence of the distance between 
nearby trajectories. Therefore one can only speak about short time predictions with 
respect to such systems. Our focus is the ability to generate complex sequences, 
and the relation between architecture and the dimension of such sequences. 
1 Perceptron 
We begin with a study of the simplest feed forward network, the perceptron. We 
analyse a perceptron whose output Sour at time step t is given by: 
Sour = tanh  (Wj + Wo)$J (2) 
where  is a gain parameter, N is the input size. The bias term ,W0, plays the same 
role as the common 'external field' used in the literature, while preserving the same 
qualitative asymptotic solution. In a previous work (Eisenstein et. al., 1995) it was 
found that the stationary state (of a similar architecture but with a sign activation 
function instead of the tanh, equivalently  -+ x) is influenced primarily by one 
of the larger Fourier components in the power spectrum of the weights vector W 
of the perceptron. This observation motivates the following representation of the 
vector W. Let us start with the case of a vector that consists of a single biased 
Fourier component of the form: 
Wj -- acos(2'Kj/N) j -- 1,...,N ; Wo = b 
(3) 
where a, b are constants and K is a positive integer. This case is generalized later on, 
however for clarity we treat first the simple case. Note that the vector W can always 
be represented as a Fourier decomposition of its values. The stationary solution for 
the sequence ($1) produced by the output of the perceptron, when inserting this 
choice of the weights into equation (2), can be shown to be of the form: 
S ! = tanh [A() cos(27rK1/N) + B()] 
(4) 
There are two non-zero solutions possible for the variables (A, B): 
The Interplay between Architecture and Predictability 317 
A = �3Nay.p= D(p)(A/2)2p-(p!) -2 
B = fiNby.p__ D(p)B2p-((2p)!) - 
; B=O 
; A=O 
(5) 
where D(p) = 22P(22p --1)B2p and B2p are the Bernoulli numbers. Analysis 
of equations (5) reveals the following behavior as a function of the parameter 3. 
Each of the variables is the amplitude of an attractor. The attractor represented 
by (A  0, B = 0) is a limit cycle while the attractor represented by (B  0, A = 0) 
is a fixed point of the dynamics. The onset of each of the attractors A(B) is at 
Jcl --- 2(aN) - (3c2 = (bN) -1) respectively. One can identify three regimes: (1) 
/ < /cl,c2 - the stable solution is S t = 0. (2) min(3c, 3c2) < 3 < max(3cl, 3c2) - 
the system flows for all initial conditions into the attractor whose tic is smaller. (3) 
3 > fic,c2 - depending on the initial condition of the input vector, the system flows 
into one of the attractors, namely, the stationary state is either a fixed point or a 
periodic flow. 3cl is known as a Hopf bifurcation point. Naturally, the attractor 
whose 3c is smaller has a larger basin of attraction, hence it is more probable to 
attract the flow (in the third regime). 
1.0 I , o , o o oooo,,,,,, 
;.--'2 
/ ..-/ 
0.0i, / 
..- 
-1.0 -0.5 0.0 0.5 
S I 
.0 
Figure 1: Embedding of a se- 
quence generated by a percep- 
tron whose weights follow eq. 3 
(6). Periodic sequence (outer 
curve) N = 128, k = 17, b = 0.3, 
3 = 1/40 and quasi periodic (in- 
ner) k = 17, b = 0.123, 3 = 
1/45 respectively. 
Next we discuss the more general case where the weights of eq. (3) includes an 
arbitrary phase shift of the form: 
Wj = acos(2rKj/N - rb) � e (-1,1) (6) 
The leading term of the stationary solution in the limit N >> I is of the form: 
S t = tanh [A(3) cos(2r(K - c))l/N) + B(3)] (7) 
where the higher harmonic corrections are of O(1/K). A note should be made here 
that the phase shift in the weights is manifested as a frequency shift in the solution. 
In addition, the attractor associated with A  0 is now a quasi-periodic flow in the 
generic case when & is irrational. The onset value of the fixed point (c2) is the same 
as before, however the onset of the quasi-periodic orbit is 3c - sin(,re))2(aN) -1 
The variables A, B follow similar equations to (5): 
A = /Jz, a ,re) Yp=zO(p)(A/2) 2-z(p])-2; B=O 
X (x:) 
B = 3 by.p= D(p)B2p-((2p)!) - ; A = 0 
(8) 
The three regimes discussed above appear in this case as well. Figure 1 shows the 
attractor associated with (A  0, B - 0) for the two cases where the series generated 
by the output is embedded as a sequence of two dimensional vectors (S t+l, St). 
318 A. Priel, I. Kanter and D. A. Kessler 
The general weights can be written as a combination of their Fourier components 
with different K's and b's: 
m 
Wj - E aicos(2rKij/N- rci) ci e (-1,1) (9) 
i=1 
When the different K's are not integer divisors of each other, the general solution 
is similar to that described above: 
Sl=tanh[Ai(/3) cos(2r(Ki-ci)I/N)+B(/3)] (10) 
i----1 
where ra is the number of relevant Fourier components. As above, the variables 
Ai , B are coupled via self consistent equations. Nevertheless, the generic stationary 
flow is one of the possible attractors, depending on/3 and the initial condition; i.e. 
(Aq  O, Ai = 0 Vi  q ,B = 0) or (B  O, Ai = 0). By now we can conclude that 
the generic flow for the perceptton is one of three: a fixed point, periodic cycle or 
quasi-periodic flow. The first two have a zero dimension while the last describes a 
one dimensional flow. we stress that more complex flows are possible even in our 
solution (eq. 10), however they require special relation between the frequencies and 
a very high value of/3, typically more than an order of magnitude greater than 
bifurcation value. 
2 Relaxation time 
At this stage the reader might wonder about the relation between the asymptotic 
results presented above and the ability of such a model to predict. In fact, the 
practical use of feed forwaxd networks in time series prediction is divided into two 
phases. In the first phase, the network is trained in an open loop using a given time 
series. In the second phase, the network operates in a closed loop and the sequence it 
