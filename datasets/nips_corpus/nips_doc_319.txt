A Recurrent Neural Network for Word Identification 
from Continuous Phoneme Strings 
Robert B. Allen 
Bellcore 
Morristown, NJ 07962-1910 
Candace A. Kamm 
Bellcore 
Morristown, NJ 07962-1910 
Abstract 
A neural network architecture was designed for locating word boundaries and 
identifying words from phoneme sequences. This architecture was tested in 
three sets of studies. First, a highly redundant corpus with a restricted 
vocabulary was generated and the network was trained with a limited number of 
phonemic variations for the words in the corpus. Tests of network performance 
on a transfer set yielded a very low error rate. In a second study, a network was 
trained to identify words from expert transcriptions of speech. On a transfer 
test, error rate for correct simultaneous identification of words and word 
boundaries was 18%. The third study used the output of a phoneme classifier as 
the input to the word and word boundary identification network. The error rate 
on a transfer test set was 49% for this task. Overall, these studies provide a first 
step at identifying words in connected discourse with a neural network. 
1 INTRODUCTION 
During the past several years, researchers have explored the use of neural networks for 
classifying spectro-temporal speech patterns into phonemes or other sub-word units (e.g., 
Harrison & Fallside, 1989; Kamm & Singhal, 1990; Waibel et al., 1989). Less effort has 
focussed on the use of neural nets for identifying words from the phoneme sequences that 
these spectrum-to-phoneme classifiers might produce. Several recent papers, however, 
have combined the output of neural network phoneme recognizers with other techniques, 
including dynamic time warping (DTW) and hidden Markov models (HMM) (e.g., 
Miyatake, et al., 1990; Morgan & Bourlard, 1990). 
Simple recurrent neural networks (Allen, 1990; Elman, 1990; Jordan, 1986) have been 
shown to be able to recognize simple sequences of features and have been applied to 
linguistic tasks such as resolution of pronoun reference (Allen, 1990). We consider 
whether they can be applied to the recognition of words from phoneme sequences. This 
paper presents the resuks of three sets of experiments using recurrent neural networks to 
locate word boundaries and to identify words from phoneme sequences. The three 
experiments differ primarily in the degree of similarity between the input phoneme 
sequences and the input information that would typically be generated by a spectrum-to- 
phoneme classifier. 
2 NETWORK ARCHITECTURE 
The network architecture is shown in Figure 1. Sentence-length phoneme sequences are 
stepped past the network one phoneme at a time. The input to the network on a given 
206 
A Recurrent Neural Network 207 
time step within a sequence consists of three 46-element vectors (corresponding to 46 
phoneme classes) that identify the phoneme and the two subsequent phonemes. The 
activation of state unit Si on the step at time t is a weighted sum of the activation of its 
corresponding hidden unit (H) and the state unit's activation on the previous time step, 
where  is the weighting factor for the hidden unit activation and ].t is the state memory 
weighting factor: Si,t=Hi,t_l+Si,t_l . Ill this research =1.0 and g=0.5. The output of 
the network consists of one unit for each word in the lexicon and an additional unit 
whose activation indicates the presence of a word boundary. 
Weights from the hidden units to the word units were updated based on error observed 
only at phoneme positions that corresponded to the end of a word (Allen, 1988). The end 
of the phoneme sequence was padded with codes representing silence. State unit 
activations were reset to zero at the end of each sentence. The network was trained using 
a momentum factor of a,--0.9 and an average learning rate of 1=0.05. The learning rate 
was adjusted for each output unit proportionally to the relative frequency of occurrence 
of the word corresponding to that unit. 
Output 
units 
123 ... 
word units 
word- 
boundary 
n unit 
Hidden 
units 
input input input 
t t+ 1 t+2 
2 ' 
Input 
units * ' 
, ; 
138 input units 
(46 phoneme classes x 3 time steps) 
State 
units 
Figure 1' Recurrent Network for Word Identification 
3 EXPERIMENT 1: DICTIONARY TRANSCRIPTIONS 
3.1 PROCEDURE 
A corpus was constructed from a vocabulary of 72 words. The words appeared in a 
variety of training contexts across sentences and the sentences were constrained to a very 
small set of syntactic constructions. The vocabulary set included a subset of rhyming 
words. Transcriptions of each word were obtained from Webster's Seventh Collegiate 
Dictionary and from an American-English orthographic-phonetic dictionary (Shoup, 
1973). These transcriptions describe words in isolation only and do not reflect 
coarticulations that occur when the sentences are spoken. For this vocabulary, 26 of the 
words had one pronunciation, 19 had two variations, and the remaining 27 had from 3 to 
17 variations. The corpus consisted of 18504 sentences of about 7 words each. 6000 of 
these sentences were randomly selected and reserved as transfer sentences. 
208 Allen and Kamm 
The input to the network was a sequence of 46-element vectors designed to emulate the 
activations that might be obtained from a neural network phoneme classifier with 46 
output classes (Kamm and Singhal, 1990). Since a phoneme classifier is likely to 
generate a set of phoneme candidates for each position in a sequence, we modified the 
activations in each input vector to mimic this expected situation. Confusion data were 
obtained from a neural network classifier trained to map spectro-temporal input to an 
output activation vector for the 46 phoneme classes. In this study, input activations for 
phonemes that accounted for fewer than 5% of the confusions with the correct phoneme 
remained set to 0.0, while input activations for phonemes accounting for higher 
proportions of the total confusions with the correct phoneme were set to twice those 
proportions, with an upper limit of 1.0. This resulted in relatively high activation levels 
for one to three elements and activation of 0.0 for the others. Overall, the network had 
138 (46x3) input units, 80 hidden units, 80 state units, and 73 output units (one for each 
word and one boundary detection unit). The network was trained for 50000 sentence 
sequences chosen randomly (with replacement) from the training corpus. Each sequence 
was prepared with randomly selected transcriptions from the available dialectal 
variations. 
3.2 RESULTS 
In all the experiments discussed in this paper, performance of the network was analyzed 
using a sequential decision strategy. First, word boundaries were hypothesized at all 
locations where the activation of the boundary unit exceeded a predefined threshold (0.5). 
Then, the activations of the word units were scanned and the unit with the highest 
activation was selected as the identified word. By comparing the locations of true word 
boundaries with hypothesized boundaries, a false alarm rate (i.e., the number of spurious 
boundaries divided by the number of non-boundaries) was computed. Word error rate 
was then computed by dividing the number of incorrect words at correctly-detected word 
boundaries by the total number of words in the transfer set. This word error rate includes 
both deletions (i.e., missed boundaries) and word substitutions (i.e., incorrectly-identified 
words at correct boundaries). Total error rate is obtained by summing the word error rate 
and the false alarm rate. 
On the 6000 sentence test set, the network correctly located 99.3% of the boundaries, 
with a word error rate of 1.7% and a false alarm rate of 0.3% Overall, this yielded a total 
error rate of 2.0%. To further test the robustness of this procedure to noisy input, three 
networks were trained with the same procedures as above except that the input phoneme 
sequences were distorted. In the first network, there was a 30% chance that input 
phonemes would be duplicated. In a second network, there was a 30% chance that input 
phonemes would be deleted. In the third network, there was a 70% chance that an input 
phoneme would be substituted with another closely-related phoneme. Total error rates 
were 11.7% for the insertion network, 20.9% for the deletion network, and 10.0% for the 
substitution network. 
Even with these fairly severe distortions, the network is moderately successful at the 
boundary detection/word identification task. Experiment 2 was designed to study 
network performance on a more diverse and realistic training set. 
A Recurrent Neural Network 209 
4 EXPERIMENT 2: EXPERT TRANSCRIPTIONS OF SPEECH 
4.1 PROCEDURE 
To provide a richer and more natural sample of transcriptions of continuous speech, 
training sequences were derived from phonetic transcriptions of 207 sentences from the 
DARPA Acoustic-Phonetic (AP) speech corpus (TIMIT) (Lamel, et al., 1986). The 
training set consisted of 4-5 utterances of each of 50 sentences, spoken by different 
talkers. One other utterance of each sentence (spoken by different talkers) was used for 
transfer tests. This corpus contained 277 unique words. For training, the transcripts were 
also segmented into words. When a word boundary was spanned by a single phoneme 
because of coarticulation (for example, the transcription /haer/ for the phrase had 
your), the coarticulated phoneme (in this example,//) was arbitrarily assigned to only 
the first word of the phrase. These transcriptions differ from those used in Experiment 1 
primarily in the amount of phonemic variation observed at word boundaries, and so 
provide a more difficult boundary detection task for the network. As in Experiment 1, 
the input to the network on any time step was a set of three 46-element vectors. The 
original input vectors (obtained from the phonetic transcriptions) were modified based on 
the phoneme confusion data descr
