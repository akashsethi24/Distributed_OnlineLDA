EMPATH: 
Face, Emotion, and Gender 
Recognition Using Holons 
Garrison W. Cottrell 
Computer Science and Engineering Dept. 
Institute for Neural Computation 
University of California San Diego 
La Jolla, CA 92093 
Janet Metcalfe 
Department of Psychology 
Dartmouth College 
Hanover, NH 03755 
Abstract 
The dimensionality of a set of 160 face images of i0 male and 10 
female subjects is reduced from 4096 to 40 via an autoencoder 
network. The extracted features do not correspond to the features used 
in previous face recognition systems (Kanade, 1973), such as ratios of 
distances between facial elements. Rather, they are whole-face 
features we call holons. The holons are given to 1 and 2 layer back 
propagation networks that are trained to classify the input features for 
identity, feigned emotional state and gender. The automatically 
extracted holons provide a sufficient basis for all of the gender 
discriminations, 99% of the identity discriminations and several of the 
emotion discriminations among the training set. Network and human 
judgements of the emotions are compared, and it is found that the 
networks tend to confuse more distant emotions than humans do. 
I Introduction and motivation 
We describe further research on the use of dimensionality-reduction networks for face 
recognition first described in (Cottrell & Fleming, 1990; Fleming & Cottre!l, 1990). 
There, we demonstrated that an unsupervised autoencoding network was able to extract 
features from faces sufficient for identity discrimination. Here we extend that work to 
show that a network so trained can also recognize feigned emotional states. 
564 
EMPATH: Face, Emotion, and Gender Recognition Using Holons 565 
Cottrell, Munro & Zipset (1987) showed that a back propagation network could be used 
for image compression. The network is trained to simply reproduce its input, and so can 
be seen as a non-linear version of Kohonen's (1977) auto-associator. However it must 
do this through a narrow channel of hidden units, so it must extract regularities from the 
input vectors during leaming. Empirical analysis of the trained network showed that the 
hidden units span the principal subspace of the image vectors, with some noise on the 
first principal component due to network nonlinearity (Cottrell & Munro, 1988). 
Although the network uses error-correction learning, no teacher other than the input is 
provided, so the learning can be regarded as unsupervised. We suggested that this 
network could be used for automatic feature extraction in a pattern recognition system. 
This is the approach taken here. 
The model is shown in Figure 1. The image compression network extracts the features, 
and then the hidden unit reprentafion so developed is given as input to one and two 
layer classification networks which yield identity, gender, and emotion as output. In 
previous work, we showed that the features developed by the model are holistic rather 
than discrete features, that they can combine to form faces that the model has never been 
trained on, and that they form a redintegrative memory, able to complete noisy or partial 
inputs (Kohonen et al., 1977). We have dubbed them hoions. 
2 Materials 
The images that comprised the input to the network were selected from full face pictures 
taken of 10 females and 10 males. All subjects were introductory psychology students at 
the University of California, San Diego who received partial course credit for 
participating. Following the procedure outlined by Gaiton 0878), images were aligned 
along the axes of the eyes and mouth. These images were captured by a frame grabber, 
and reduced to 64x64 pixels by averaging. To prevent the use of first order statistics for 
discrimination, the images were normalized to have equal brightness and variance. The 
gray levels were linearly scaled to the range [0,8]. Part of the training set and its 
reproduction by the autoencoder are shown in Figure 2. 
output  a s gender names 
hiddens 
input inputs (hiddens from auto-associator) 
Figure 1: 
Auto-associator Face information extractor 
a b 
The face recognition model. (a) An image compression network is trained 
first to compress the 4096 inputs into 40 hidden units. (b) The hidden units 
from the first network are used as inputs to various recognition networks. 
566 Cottrell and Metcalfe 
Each column corresponds to one of 8 different feigned emotional expressions. Russell 
(1980) has shown that subjects' judgements of adjectives describing human emotions can 
be represented in a two-dimensional emotion space (Figure 3). The horizontal 
dimension can be characterized as plea.sure/displeasure; the vertical dimension as high 
arousal/low arousal. Russell and his colleagues have shown using multidimensional 
scaling techniques (Russell, 1980, 1983; Russell & Bullock, 1986) that most common 
human emotions fall on a circle within this space. We chose adjectives from this circle to 
be the emotions that we asked our subjects to feign. The adjectives for each class are 
those in the numbered circles in Figure 3. If the subject did not respond well to one of 
the adjectives, others from the circled region were given as encouragement to form the 
proper facial expression. We labeled these classes with one adjective from each region: 
astonished, happy, pleased, relaxed, sleepy, bored, miserable and angry. The adjectives 
were presented in randomized order to offset possible carry over effects. 
We found that subjects were enthusiastically expressive with certain of these emotions, 
such as astonished and delighted. However, despite claims of negative feelings when 
cued with adjectives such as miserable, bored and sleepy, the subjects did not overtly 
express these negative emotions very clearly. 
3 Procedure 
The whole image is input at once to our network, so the input layer is 64x64. We used 
40 hidden units, and a 64x64 output layer, with a sigmoidal activation function with 
range [-1,1]. Due to the extreme difference in fan-in at the hidden and output layers 
(4096 vs. 40), differential learning rates were used at the two layers. Use of a single 
learning rate led to most of the hidden units becoming pinned at full off or on regardless 
Figure 2: Three subjects and their reproductions by the compression network. 
EMPATH: Face, Emotion, and Gender Recognition Using Holons 567 
of the input. We used a learning rate of .25 at the output layer during the first epoch, in 
order to quickly learn the bias, or palette, then a rate of. 1 was used for the remaining 
49 epochs, where an epoch corresponds to the presentation of all 160 images. The 
hidden layer used a constant learning rate of .0001. The initial weight span was .1 (+/- 
.05). We used no momentum or weight decay. The average squared error per unit at the 
end was .0017. This corresponds to about 12 gray levels per pixel. Sample 
reproductions of trained images are shown in Figure 2. 
The 40-element vectors produced by the hidden units of the compression network are 
then given as input to a single layer network that has a Iocalist unit for every name and a 
unit for gender. A two-layer network with 20 hidden units is used for identifying which 
of the 8 emotion adjectives that were given to the subjects pertains to each image. The 
network is trained to produce .5 for the wrong answer, and 1 for the correct answer. The 
emotion network is trained for 1000 epochs, which reduces the total sum squared error to 
22. To investigate how performance changed with further training, we trained this 
network for 2000 more epochs. 9 other networks were trained using the features from 
the same compression network for 1000 epochs from different initial random weights for 
comparison to human subjects on the same task. 
4 Results 
The criteria for correctness was that the output unit with the maximum activation must 
correspond to the correct answer. The network learned to discriminate 99% of the 
training set for identity. One image of one woman was taken for another. Sex 
discrimination was perfect. It was found that performance was better on these tasks 
without a hidden layer. The emotion classification network performed better with a 
AtARMED � 
AFRAID � 
, s .' 
DSIRESSE 
FRUSTRAIE 
SED 11 E) 
D 
Mullidimennionnl scaling solution for 28 affect words. 
Figure 3: 
The two-dimensional emotion space extracted via multi-dimensional scaling 
from similarity ratings. Data from Russell (1980). 
568 Cottrell and Metcalfe 
Table 1: Percentage hits on each emotion (Generalization in parentheses) 
Emotion/Epochs 1000 2000 3000 
astonished 100 (40) 100 (60) 100 (40) 
delighted 55 (20) 75 (40) 90 (40) 
pleased 80 (100) 90 (40) 80 (40) 
relaxed 80 (40) 35 (0) 45 (20) 
sleepy 20 (0) 70 (0) 80 (0) 
bored 5 (0) 85 (0) 85 (0) 
miserable 65 (0) 75 (0) 80 (20) 
angry 85 (40) 85 (0) 70 (0) 
hidden layer. However, the observation during data acquisition that negative emotions 
are poorly portrayed was confirmed by the network's early performance (See Table 1). 
Initially, the network was much better at detecting positive states than negative ones. 
Later training improved some categories at the expense of others, suggesting that the 
network did not have enough capacity to perform all the discriminations. Generalization 
tests were performed on a small set of 5 subjects (40 faces in all), with the results shown 
in parentheses in Table 1. Generalization gets worse with more training, suggesting the 
network is becoming overtmined. The network is best at generalization on the positive 
emotions. This also suggests that the negative emotions are not easily discriminable in 
our data set. The generalization results, while not spectacular, should be considered in 
the light of the fact that the training set only contained 20 subjects, and it should be noted 
that the compression network was not trained on the images in this test set. 
Internal representation 
We investigated the repre
