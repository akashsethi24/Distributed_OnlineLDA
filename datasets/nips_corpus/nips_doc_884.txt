On-line Learning of Dichotomies 
N. Barkai 
Racah Institute of Physics 
The Hebrew University 
Jerusalem, Israel 91904 
naamaf iz .huj i. ac. il 
H. S. Seung 
AT&T Bell Laboratories 
Murray Hill, NJ 07974 
seungphysics.att.com 
H. Sompolinsky 
Racah Institute of Physics 
The Hebrew University 
Jerusalem, Israel 91904 
and AT&T Bell Laboratories 
haimf iz. huj i. ac. il 
Abstract 
The performance of on-line algorithms for learning dichotomies is studied. In on-line learn- 
ing, the number of examples P is equivalent to the learning time, since each example is 
presented only once. The learning curve, or generalization error as a function of P, depends 
on the schedule at which the learning rate is lowered. For a target that is a perceptton rule, 
the learning curve of the perceptton algorithm can decrease as fast as p-i, if the sched- 
ule is optimized. If the target is not realizable by a perceptton, the perceptton algorithm 
does not generally converge to the solution with lowest generalization error. For the case 
of unrealizability due to a simple output noise, we propose a new on-line algorithm for a 
perceptron yielding a learning curve that can approach the optimal generalization error as 
fast as p-l/2. We then generalize the perceptron algorithm to any class of thresholded 
smooth functions learning a target from that class. For well-behaved input distributions, 
if this algorithm converges to the optimal solution, its learning curve can decrease as fast 
1 Introduction 
Much work on the theory of learning from examples has focused on batch learning, in which the learner is 
given all examples simultaneously, or is allowed to cycle through them repeatedly. In many situations, it is 
more natural to consider on-line learning paradigms, in which at each time step a new example is chosen. 
The examples are never recycled, and the learner is not allowed to simply store them (see e.g, Heskes, 
1991; Hansen, 1993; Radons, 1993). Stochastic approximation theory (Kushner, 1978) provides a framework 
for understanding of the local convergence properties of on-line learning of smooth functions. This paper 
addresses the problem of on-line learning of dichotomies, for which no similarly complete theory yet exists. 
304 N. Barkai, H. S. Seung, H. Sompolinsky 
We begin with on-line learning of perceptron rules. Since its introduction in the early 60's, the perceptron 
algorithm has been used as a simple model of learning a binary classification rule. The algorithm has been 
proven to converge in finite time and to yield a half plane separating any set of linearly separable examples. 
The perceptron algorithm, however, is not efficien in the sense of distribution-free PAC learning (Valiant, 
1984), for one can construct input distributions that require an arbitrarily long convergence time. In a recent 
paper (Baum, 1990) Baum proved that the perceptton algorithm applied in an on-line mode, converges as 
p-US when learning a half space under a uniform input distribution, where P is the number of presented 
examples drawn at random. For on-line learning P is also the number of time steps. Baum also generalized 
his result to any non-malicious distribution. Kabashima has found the same power law for learning a 
two-layer parity machine with non-overlapping inputs, using an on-line least action algorithm (Kabashima, 
1994). 
If efficiency is measured only by the number of examples used (disregarding time), these particular on-line 
algorithms are much worse than batch algorithms. Any batch algorithm which is able to correctly classify a 
given set of P examples will converge as P- (Vapnik, 1982; Amari, 1992; Seung, 1992). In this paper, we 
construct on-line algorithms that can actually achieve the same power law as batch algorithms, demonstrating 
that the results of Banm and Kabashima do not reflect a fundamental limitation of on-line learning. 
In Section 3, we study on-line algorithms for perceptron learning of a target rule that is not realizable by 
a perceptron. Here it is nontrivial to construct an algorithm that even converges to the optimal one, let 
alone to optimize the rate of convergence. For the special case of a target rule that is a perceptron corrupted 
by output noise this can be done. In Section 4, our results are generalized to dichotomies generated by 
thresholding smooth functions. In Section 5 we summarize the results. 
2 On-line learning of a perceptton rule 
We consider a half space rule generated by a normalized teacher perceptron Wo 6 R N, Wo � Wo = 1 such 
that any vector S 6 R N is given a label ao(S) = sgn(Wo � S). We study the case of a Gaussian input 
distribution centered at zero with a unit variance in each direction in space: 
N 
1 _S./. 
= II .-- (1) 
Averages over this input distribution will be written with angie brackets 0. A student perceptron W is 
trained by an on-line perceptton algorithm. At each time step, an input S  R t is drawn at random, 
according to distribution Eq. (1) and the student's output a(S) = sgn(W � S) is calculated. The student is 
then updated according to the perceptron rule: 
W' = W + 4S;W)0(S)S (2) 
and is then normalized so that W. W = 1 at all times. The factor e(S; W) denotes the error of the student 
perceptron on the input S: e = 1 if a(S)a0(S) = 1, and 0 otherwise. The learning rate 1 is the magnitude 
of change of the weights at each time step. It is scaled by N to ensure that the change in the overlap 
R = W. W0 is of order 1IN. Thus, a change of O(1) occurs only after presentation of P = O(N) examples. 
The performance of the student is measured by the generalization error, defined as the probability of dis- 
agreement between the student and the teacher on an arbitrary input % = (e(S; W)). In the present case, 
% is 
cos- R 
 =  (3) 
Although for simplicity we analyze below the performance of the perceptron rule (2) only for large N, 
our results apply to finite N as well. Multiplying Eq. (2) by W0 after incorporation of the normalization 
operation and averaging with respect to the input distribution (1), yields the following differential equation 
for R(r) where c = P/N, 
dR 1 - R 2 q2 Rcos -1 R 
da -   2r (4) 
On-line Learning of Dichotomies 305 
Here terms of order  have been neglected. 
The evolution of the overlap R, and thus of the generalization error, depends on the schedule at which the 
learning rate 7 decreases. We consider two cases, a constant 7 and a time-dependent 7. 
Constant learning rate: When 7 is held fixed, Eq. (4) has a stable fixed point at R < 1, and hence 
converges to an 7-dependent nonzero value e(7). For 7 << 1, 1 - R(7) or 7 2 and % or x/T- R is therefore 
proportional to 7, 
oo (7) = 7/2v5-; � (5) 
The convergence to this value is exponential in a, % (a) - coo(7) ~ exp(-7a/v/). 
Time-dependent learning rate: Convergence to % = 0 can be achieved if 7 decreases slowly enough with 
a. We study the limiting behaviour of the system for 7 which is decreasing with time as 7 = (70v-) 
z > 1. In this case the rate is reduced too fast before a sufficient number of examples have been seen. This 
results in R which does not converge to 1 but instead to a smaller value that depends on its initial value. 
z < 1. The system follows the change in 7 adiabatically. Hence, to first order in 
Thus, % converges to zero with an asymptotic rate %(a) ~ a -z. 
z = 1. The behaviour of the system depends on the prefactor 
70 - 1 a 70 > 1 
70 1 (6) 
A 
~ 70<1 
where A depends on the initial condition. Thus the optimal asymptotic change of 7 is 2vr/a, in which case 
the error will behave asymptotically as %(a) ~ 1.27/a. This is not far from the batch asymptotic (Seung, 
1992) (a) ~ 0.628/a. We have confirmed these results by numerical simulation of the algorithm Eq. (2). 
Figure I presents the results of the optimal learning schedule, i.e., 7 = 2vr/a. The numerical results are 
in excellent agreement with the prediction (a) = 1.27/a for the asymptotic behavior. Finally, we note 
that our analysis of the time-dependent case is similar to that of Kabashima and Shinomoto for a different 
on-line learning problem (Kabashima, 1993). 
3 On-line learning of a perceptron with output noise 
In the case discussed above, the task can be fully realized by a perceptron, i.e., there is a perceptron W 
such that eg = 0. In more realistic situations a perceptron will only provide an approximation of the target 
function, so that the minimal value of e9 is greater than zero. These cases are called unrealizable tasks. A 
drawback of the above on-line algorithm is that, for a general unrealizable task, it does not converge to 
the optimal perceptron, i.e., it does not approach the minimum of %. To illustrate this fact we consider a 
perceptron rule corrupted by output noise. The label of an input S is a0(S), where a0(S) = sgn(W0 � S) 
with probability 1 - p, and - sgn(W0 � S) with probability p. We assume 0 <_ p <_ 1/2. For reasons which 
will become clear later, the input distribution is taken as a Gaussian centered at U 
N 
p(s) = -L-e (7) 
.=  
In this case % is given by 
v'l - R z q '/ ' (8) 
where q0 = U � W0 denotes the overlap betwn the center of the distribution d the tether perceptton, 
d q = U � W is the overlap between the center of the distribution and W. The integrs in Eq. (8) are 
306 N. Barkai, H. S. Seung, H. Sompolinsky 
with respect to a Gaussian measure Dy = exp(-y2/2)/vf and H(x) = foo Dy. Note that the optimal 
perceptron is the teacher W = W0 i.e., R = 1, q = q0, which yields the minimal error erain ---- p. 
First, we consider training with the normalized perceptron rule (2). In this case, we obtain differential 
equations for two variables: R and q. Solving these equations we find that in general, W converges to a 
vector with a direction which is in the plane of W0 and U and is does not point in the direction of W0 even 
in the limit of q -- 0. Here we present the result for the limit of q --
