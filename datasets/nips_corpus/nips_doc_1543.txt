The Bias-Variance Tradeoff and the Randomized 
GACV 
Grace Wahba, Xiwu Lin and Fangyu Gao 
Dept of Statistics 
Univ of Wisconsin 
1210 W Dayton Street 
Madison, W! 53706 
wahba, xiwu, fgaostat .wisc. edu 
Dong Xiang 
SAS Institute, Inc. 
SAS Campus Drive 
Cary, NC 27513 
sasdxxunx. sas. com 
Ronald Klein, MD and Barbara Klein, MD 
Dept of Ophthalmalogy 
610 North Walnut Street 
Madison, WI 53706 
kleinr, kleinbepi. ophth.wisc. edu 
Abstract 
We propose a new in-sample cross validation based method (randomized 
GACV) for choosing smoothing or bandwidth parameters that govern the 
bias-variance or fit-complexity tradeoff in 'soft' classification. Soft clas- 
sification refers to a learning procedure which estimates the probability 
that an example with a given attribute vector is in class 1 vs class 0. The 
target for optimizing the the tradeoff is the Kullback-Liebler distance 
between the estimated probability distribution and the 'true' probabil- 
ity distribution, representing knowledge of an infinite population. The 
method uses a randomized estimate of the trace of a Hessian and mimics 
cross validation at the cost of a single relearning with perturbed outcome 
data. 
1 INTRODUCTION 
We propose and test a new in-sample cross-validation based method for optimizing the bias- 
variance tradeoff in 'soft classification' (Wahba et al 1994), called ranGACV (randomized 
Generalized Approximate Cross Validation). Summarizing from Wahba et al(1994) we are 
given a training set consisting of r examples, where for each example we have a vector 
t C 7- of attribute values, and an outcome /, which is either 0 or 1. Based on the training 
data it is desired to estimate the probability p of the outcome 1 for any new examples in the 
The Bias- Variance Tradeoff and the Randomized GACV 621 
future. In 'soft' classification the estimate/3(t) ofp(t) is of particular interest, and might be 
used by a physician to tell patients how they might modify their risk p by changing (some 
component of) t, for example, cholesterol as a risk factor for heart attack. Penalized like- 
lihood estimates are obtained for p by assuming that the logit f(t), t E T, which satisfies 
p(t) = e/(t)/(1 + e/(t)) is in some space 7-/of functions. Technically 7-/is a reproducing 
kernel Hilbert space, but you don't need to know what that is to read on. Let the training 
set be {yi,ti,i: 1,... ,n}. Letting fi = f(ti), the negative log likelihood 12{yi,ti, fi} of 
the observations, given f is 
C{yi, ti,fi} -- Z[--Yifi + b(fi)], 
i=1 
(1) 
where b(f) = lo9(1 + el). The penalized likelihood estimate of the function f is the 
solution to: Find f E 7-/to minimize Ix (f)' 
i=1 
(2) 
where Jx(f) is a quadratic penalty functional depending on parameter(s) A = (A1, ..., Aq) 
which govern the so called bias-variance tradeoff. Equivalently the components of A con- 
trol the tradeoff between the complexity of f and the fit to the training data. In this paper we 
sketch the derivation of the ranGACV method for choosing A, and present some prelim- 
inary but favorable simulation results, demonstrating its efficacy. This method is designed 
for use with penalized likelihood estimates, but it is clear that it can be used with a variety 
of other methods which contain bias-variance parameters to be chosen, and for which mini- 
mizing the Kullback-Liebler (KL) distance is the target. In the work of which this is a part, 
we are concerned with A having multiple components. Thus, it will be highly convenient 
to have an in-sample method for selecting A, if one that is accurate and computationally 
convenient can be lbund. 
Let px be the the estimate and p be the 'true' but unknown probability function and let 
pi = p(ti),pxi = px(ti). For in-sample tuning, our criteria for a good choice of A is 
i n (1--pi) 
the KL distance KL(p,px) =  Ei=l[PilOg pi q- (1 -- pi)log (i--px/)]' We may replace 
KL(p, px) by the comparative KL distance (CKL), which differs from KL by a quantity 
which does not depend on A. Letting fxi = fx(ti), the CKL is given by 
CKL(p, px) --- CKL(A) = i [-Pifi + b(fi)]. 
n 
i---1 
(3) 
CKL(A) depends on the unknown p, and it is desired is to have a good estimate or proxy 
for it, which can then be minimized with respect to A. 
It is known (Wong 1992) that no exact unbiased estimate of CKL(A) exists in this case, so 
that only approximate methods are possible. A number of authors have tackled this prob- 
lem, including Utans and M9ody(1993), Liu(1993), Gu(1992). The iterative UBR method 
of Gu(1992) is included in GRKPACK (Wang 1997), which implements general smooth- 
ing spline ANOVA penalized likelihood estimates with multiple smoothing parameters. It 
has been successfully used in a number of practical problems, see, for example, Wahba 
et al (1994,1995). The present work represents an approach in the spirit of GRKPACK 
but which employs several approximations, and may be used with any data set, no matter 
how large, provided that an algorithm for solving the penalized likelihood equations, either 
exactly or approximately, can be implemented. 
622 G. Wahba et al. 
2 THE GACV ESTIMATE 
In the general penalized likelihood problem the minimizer f), (.) of (2) has a representation 
M n 
f),(t) =  d3(t) +  ciQx(ti,t) (4) 
u=l i=1 
where the gS, span the null space of J),, Q), (s, t) is a reproducing kernel (positive definite 
function) for the penalized part of 7-/, and c = (c,- �., cn)  satisfies M linear conditions, 
so that there are (at most) n free parameters in f),. Typically the unpenalized functions 
gSvare low degree polynomials. Examples of Q(ti, ') include radial basis functions and 
various kinds of splines; minor modifications include sigmoidal basis functions, tree basis 
functions and so on. See, for example Wahba(1990,1995), Girosi, Jones and Poggio(1995). 
If f),(.) is of the form (4) then J),(f),) is a quadratic form in c. Substituting (4) into (2) 
results in Ix a convex functional in c and d, and c and d are obtained numerically via a 
Newton Raphson iteration, subject to the conditions on c. For large n, the second sum on 
K 
the right of (4) may be replaced by 5.k= ciQx(ti, t), where the ti are chosen via one 
of several principled methods. 
To obtain the GACV we begin with the ordinary leaving-out-one cross validation function 
CV(A) for the CKL: 
cv(x) = 1 + 
n 
i---1 
where fx I-i] the solution to the variational problem of (2) with the ith data point left out 
and fx[? is the value of A -i] at ti. Although fx(') is computed by solving for c and d 
the GAGV is derived in terms of the values (f,..., fn)  of f at the ti. Where there is 
no confusion between functions f(.) and vectors (f,..., fn)' of values of f at t,. �., tn, 
we let f: (f,...,, fn)'. For any f(.) of the form (4), J),(f) also has a representation as 
a non-negative definite quadratic form in (f, � � �, f)'. Letting E), be twice the matrix of 
this quadratic form we can rewrite (2) as 
I),(f, y) = Z[-yifi + b(fi)] + f'Exf. (6) 
i=1 
Let W = W(f) be the n x n diagonal matrix with aii -= pi(1 - Pi) in the iith position. 
Using the fact that aii is the second derivative of b(fi), we have that H = [W + E),] - 
is the inverse Hessian of the variational problem (6). In Xiang and Wahba (1996), several 
Taylor series approximations, along with a generalization of the leaving-out-one lemma 
(see Wahba 1990) are applied to (5) to obtain an approximate cross validation function 
AGV(A), which is a second order approximation to CV(/). Letting hii be the iith entry 
of H, the result is 
 1 hiiYi(Yi-Pxi) 
CV(A)  ACV(A) - i [-yifxi + b(f),i)] + [1 - hiio'ii] (7) 
Tt 1 Tt i=1 
Then the GACV is obtained from the ACV by replacing hii by  i= hii - 
and replacing 1 - ha by �tr[I - (W/2HW/2)], giving 
n tr(H) Y'in=l Yi(Yi -- PXi) 
GACV(A) = 1 Z[_yifi + b(fi)] + (8) 
I i1 Tt 1'[I- (W1/2HW1/2)] ' 
where W is evaluated at fx. Numerical results based on an exact calculation of (8) appear 
in Xiang and Wahba (1996). The exact calculation is limited to small n however. 
The Bias-Variance Tradeoff and the Randomized GACV 623 
3 THE RANDOMIZED GACV ESTIMATE 
Given any 'black box' which, given A, and a training set {Yi, ti} produces f,x (') as the min- 
imizer of (2), and thence f,x = (f,x, � � ', f,x,)', we can produce randomized estimates of 
trH and tr[I - W1/2HW 1/2] without having any explicit calculations of these matrices. 
This is done by running the 'black box' on perturbed data {yi + 5i, ti}. For the Yi Gaus- 
sian, randomized trace estimates of the Hessian of the variational problem (the 'influence 
matrix') have been studied extensively and shown to be essentially as good as exact calcu- 
lations for large n, see for example Girard(1998). Randomized trace estimates are based 
on the fact that if A is any square matrix and 5 is a zero mean random n-vector with inde- 
pendent components with variance a, then E5'A5 = -atrA. See Gong et a1(1998) and 
references cited there for experimental results with multiple regularization parameters. Re- 
turning to the 0-1 data case, it is easy to see that the minimizer f,x (') of I,x is continuous in 
y, not withstanding the fact that in our training set the Yi take on only values 0 or 1. Letting 
f = (fl,', fan)' be the minimizer of (6) given y = (yl,', Yn)', and f:+6 be the 
minimizer given data y+5 = (yl +51, � �., yn+Sn)' (the ti remain fixed), Xiang and Wahba 
(1997) show, again using Taylor series expansions, that f+6 _ f ,, [W(f,) + E),] -5. 
This suggests that -,/a 5' (f,+6 _ f,) provides an estimate of tr[W (f) + Ex] - . However, 
if we take the solution f, to the nonlinear system for the original data y as the initial value 
for a Newton-Raphson calculation of f,+6 things become even simpler. Applying a one 
step Newton-Raphson iteration gives 
02Ix  OIx 
f+*' : f -[Of, Of(f,y + 5)]- --]-w,x, Y + 5). 
(9) 
Since -f(f,y + 5) : -5 + -f (f,,y) : -5, and [ox_(f,y
