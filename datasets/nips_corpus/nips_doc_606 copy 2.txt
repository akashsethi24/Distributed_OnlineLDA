Nets with Unreliable Hidden Nodes 
Error-Correcting Codes 
Learn 
Stephen Judd 
Siemens Corporate Research 
755 College Road East 
Princeton NJ 08540 
judd@learning.siemens. corn 
Paul W. Munro 
Department of Information Science 
University of Pittsburgh 
Pittsburgh, PA 15260 
munro@lis.pitt. edu 
ABSTRACT 
In a multi-layered neural network, any one of the hidden layers can be 
viewed as computing a distributed representation of the input. Several 
encoder experiments have shown that when the representation space is 
small it can be fully used. But computing with such a representation 
requires completely dependable nodes. In the case where the hidden 
nodes are noisy and unreliable, we find that error correcting schemes 
emerge simply by using noisy units during training; random errors in- 
jected during backpropagation result in spreading representations apart. 
Average and minimum distances increase with misfire probability, as 
predicted by coding-theoretic considerations. Furthermore, the effect of 
this noise is to protect the machine against permanent node failure, 
thereby potentially extending the useful lifetime of the machine. 
1 INTRODUCTION 
The encoder task described by Ackley, Hinton, and Sejnowski (1985) for the Boltzmann 
machine, and by Rumelhart, Hinton, and Williams (1986) for feed-forward networks, has 
been used as one of several standard benchmarks in the neural network literature. 
Cottrell, Munro, and Zipset (1987) demonstrated the potential of such autoencoding archi- 
tectures to 1ossy compression of image data. In the encoder architecture, the weights con- 
necting the input layer to the hidden layer play the role of an encoding mechanism, and 
the hidden-output weights are analogous to a decoding device. In the terminology of 
Shannon and Weaver (1949), the hidden layer corresponds to the communication channel. 
By analogy, channel noise corresponds to a fault (misfiring) in the hidden layer. Previous 
89 
90 Judd and Munro 
encoder studies have shown that the representations in the hidden layer correspond to opti- 
really efficient (i.e., fully compressed) codes, which suggests that introducing noise in 
the form of random interference with hidden unit function may lead to the development of 
codes more robust to noise of the kind that prevailed during learning. Many of these 
ideas also appear in Chiueh and Goodman (1987) and S&luin and Clay (1990). 
We have tested this conjecture empirically, and analyzed the resulting solutions, using a 
standard gradient-descent procedure (backpropagation). Although there are alternative tech- 
niques to encourage fault tolerance through construction of specialized error functions 
(eg., Chauvin, 1989) or direct attacks (eg., Neff, Schneider, and Young, 1990), we have 
used a minimalist approach that simply introduces intermittent node misfirings during 
mining that mimic the errors anticipated during normal performance. 
In traditional approaches to developing error-correcting codes (eg., Hamming, 1980), each 
symbol from a source alphabet is mapped to a codeword (a sequence of symbols from a 
code alphabet); the distance between codewords is directly related to the code's robustness. 
2 METHODOLOGY 
Computer simulations were performed using strictly layered feed forward networks. The 
nodes of one of the hidden layers randomly misfke during training; in most experiments, 
this channel layer was the sole hidden layer. Each input node corresponds to a transmit- 
ted symbol, output nodes to received symbols, channel representations to codewords; 
other layers are introduced as needed to enable nonlinear encoding and/or decoding. After 
training, the networks were ,analyzed under various conditions, in terms of performance 
and coding-theoretic measures, such as Hamming distance between codewords. 
The response, r, of each unit in the channel layer is computed by passing the weighted 
sum, x, through the hyperbolic tangent (a sigmoid that ranges from -1 to + 1). The re- 
sponses of those units randomly designated to misfire ,are then multiplied by -1 as this is 
most comparable with concepts from coding theory for binary channels. I The misfire op- 
eration influences the course of learning in two ways, since the erroneous information is 
both passed on to units further downstream in the net, and used as the presynaptic factor 
in the synaptic modification rule. Note that the derivative factor in the backpropagation 
procedure is unaffected for units using the hyperbolic tangent, since dr/dx = (l+r)(1-r )/2. 
These misfh'ings were randomly assigned according to various kinds of probability distri- 
butions: independent identically distributed (i.i.d), k-of-n, correlated across hidden units, 
and correlated over the input distribution. The hidden unit representations required to han- 
dle uncorrelated noise roughly correspond to Hamming spheres 2 , and can be decoded by a 
 Other possible misfire modes include setting the node's activity to zero (or some other 
constant) or randomizing it. The most appropriate mode depends on v,-u'ious factors, in- 
cluding the situation to be simulated and the type of analysis to be performed. For exam- 
ple, simulating neuronal death in a biological situation may warrant a different failure 
mode than simulating failure of an electronic component. 
2 Consider an n-bit block code, where each codeword lies on the vertex of an n-cube. The 
Hamming sphere of radius k is the neighborhood of vertices that differ from the codeword 
by a number of bits less than or equal to k. 
Nets with Unreliable Hidden Nodes Learn Error-Correcting Codes 91 
single layer of weights; thus the entire network consists of just three sets of units: 
source-channel-sink. However, correlated noise generally necessitates additional layers. 
All the experiments described below use the encoder task described by Ackley, Hinton, 
and Sejnowki (1986); that is, the input pattern consists of just one unit active and the 
others inactive. The task is to activate only the corresponding unit in the output layer. 
By comparison with coding theory, the input units are thus analogous to symbols to be 
encoded, and the hidden unit representations are analogous to the codewords. 
3 RESULT S 
3.1. PERFORMANCE 
The fkst experiment supports the claim of S6quin and Clay (1990) that training with 
faults improves network robustness. Four 8-30-8 encoders were trained with fault proba- 
bility p = 0, 0.05, 0.1, and 0.3 respectively. After training, each network was tested with 
fault probabilities varying from 0.05 to 1.0. The results show enhanced performance for 
networks trained with a higher rate of hidden unit misfiring. Figure 1 shows four perfor- 
mance curves (one for each training fault probability), each as a function of test fault 
probability. 
Interesting convergence properties were also observed; as the training fault probabilty, p, 
was varied from 0 to 0.4, networks converge reliably faster for low nonzero values 
(0.05<p<0.15) than they do at p=0. 
0.8' 
0.6 
0.4 
0.2 
training fault probability 
[] p=O.00 
& p=O.05 
 p=O. 10 
1 
-- p=0.30 
0.0 
0.0 0.2 0.4 0.6 0.8 1.0 
Test fault probability 
Figure 1. Performance for various training conditions. Four 8-30-8 encoders were 
trained with different probabilities for hidden unit misfiring. Each data point is an 
average over 1000 random stimuli with random hidden unit faults. Outputs are 
scored correct if the most active output node corresponds to the active input node. 
92 Judd and Munro 
3.2. DISTANCE 
3.2.1 Distances increase with fault probability 
Distances were measured between all pairs of hidden unit representations. Several net- 
works trained with different fault probabilities and various numbers of hidden units were 
examined. As expected, both the minimum distances and average distances increase with 
the training fault probability until it approaches 0.5 per node (see Figure 2). For proba- 
bilities above 0.25, the minimum distances fall within the theoretical bounds for a 30 bit 
code of a 16 symbol alphabet given by Gilbert and Elias (see Blahut, 1987). 
14 
12 
6 
4 
Elias Bound 
Gilbert Bound 
0.0 0.1 0.2 0.3 0.4 
training fault probability 
Figure 2. Distance increases with fault probability. Average and minimum L 1 
distances are plotted for 16-30-16 networks trained with fault probabilities 
ranging from 0.0 to 0.4. Each data point represents an average over 100 
networks trained using different weight initializations. 
3.2.2. Input probabilities affect distance 
The probability distribution over the inputs influences the relative distances of the repre- 
sentations at the hidden unit level. To illustrate this, a 4-10-4 encoder was trained using 
various probabilities for one of the four inputs (denoted P*), distributing the remaining 
probabilty uniformly among the other three. The average distance between the representa- 
tion of P* and the others increases with its probability, while the average distance among 
the other three decreases as shown in the upper part of Figure 3. The more frequent pat- 
terns are generally expected to claim a larger region of representation space. 
Nets with Unreliable Hidden Nodes Learn Error-Correcting Codes 93 
6 
5 
4 
3 
5 
4 
3 
0.0 0.1 0.2 0.3 0.4 0.5 
Prob(P*) 
Figure 3. Non-uniform input distribution. 4-10-4 encoders were trained using failure 
probabilities of 0 (squares), 0.1 (circles), and 0.2 (triangles). The input distribution was 
skewed by varying the probability of one of the four items (denoted P*) in the training set 
from 0.05 to 0.5, keeping the other probabilities uniform. Average L 1 distances are 
shown from the manipulated pattern to the other three (open symbols) and among the 
equiprobables (filled symbols) as well. In the upper figure, failure is independent of the 
input, while in the lower figure, failure is induced only when P* is presented. 
94 Judd and Munro 
The dashed line in Figure 3 indicates a unif
