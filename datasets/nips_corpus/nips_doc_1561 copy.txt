Convergence of The Wake-Sleep Algorithm 
Shiro Ikeda 
PRESTO, JST 
Wako, Saitama, 351-0198, Japan 
shiro@brain.riken.go.jp 
Shun-ichi Amari 
RIKEN Brain Science Institute 
Wako, Saitama, 351-0198, Japan 
amari@brain.riken.go.jp 
Hiroyuki Nakahara 
RIKEN Brain Science Institute 
hirobrain.riken.go.jp 
Abstract 
The W-S (Wake-Sleep) algorithm is a simple learning rule for the models 
with hidden variables. It is shown that this algorithm can be applied to 
a factor analysis model which is a linear version of the Helmholtz ma- 
chine. But even for a factor analysis model, the general convergence is 
not proved theoretically. In this article, we describe the geometrical un- 
derstanding of the W-S algorithm in contrast with the EM (Expectation- 
Maximization) algorithm and the era algorithm. As the result, we prove 
the convergence of the W-S algorithm for the factor analysis model. We 
also show the condition for the convergence in general models. 
1 INTRODUCTION 
The W-S algorithm[5] is a simple Hebbian learning algorithm. Neal and Dayan applied the 
W-S algorithm to a factor analysis model[7]. This model can be seen as a linear version of 
the Helmholtz machine[3]. As it is mentioned in[7], the convergence of the W-S algorithm 
has not been proved theoretically even for this simple model. 
From the similarity of the W-S and the EM algorithms and also from empirical results, the 
W-S algorithm seems to work for a factor analysis model. But there is an essential differ- 
ence between the W-S and the EM algorithms. In this article, we show the era algorithm[2], 
which is the information geometrical version of the EM algorithm, and describe the essen- 
tial difference. From the result, we show that we cannot rely on the similarity for the reason 
of the W-S algorithm to work. However, even with this difference, the W-S algorithm works 
on the factor analysis model and we can prove it theoretically. We show the proof and also 
show the condition of the W-S algorithm to work in general models. 
240 S. Ikeda, S. Amari and H. Nakahara 
2 FACTOR ANALYSIS MODEL AND THE W-S ALGORITHM 
A factor analysis model with a single factor is defined as the following generative model, 
Generative model x =/ + yg + e, 
where x = (xt,..., Xn) T is a n dimensional real-valued visible inputs, y -- 
./V'(0, 1) is the single invisible factor, g is a vector of factor loadings,/ is the 
overall means vector which is set to be zero in this article, and e  ./V'(0, E) is the 
noise with a diagonal covariance matrix, E = diag(a). In a Helmholtz machine, 
this generative model is accompanied by a recognition model which is defined as, 
Recognition model y = rTx + 6, 
where r is the vector of recognition weights and 6  Af(0, s 2) is the noise. 
When data xt,... , xN is given, we want to estimate the MLE(Maximum Likelihood Es- 
timator) of g and E. The W-S algorithm can be applied[7] for learning of this model. 
Wake-phase: From the training set {xs} choose a number of x randomly and for each 
data, generate y according to the recognition model y = rtrx + 5, 6  ./V'(O, st2). 
Update g and .U as follows using these x's and y's, where c is a small positive 
number and/3 is slightly less than 1. 
gt+l -- gt + a(x -- gtY)Y (1) 
o '2 - /3a, t + (1 -/3)(xi - gi,tY) 2 (2) 
i,t+l --  
where denotes the averaging over the chosen data. 
Sleep-phase: According to the updated generarive model x = Ygt+ q- e,y  
./V'(0, 1), e  ./V'(0, diag(o-t2+)), generate a number of x and y. And update r 
and s 2 as, 
rt+ = rt + a(y - r[x)x (3) 
st2+ : /3st 2 + (1 -/3)(y - rtrx) 2. (4) 
By iterating these phases, they try to find the MLE as the converged point. 
For the following discussion, let us define two probability densities p and q, where p is the 
density of the generative model, and q is that of the recognition model. 
Let 0 = (g, .U), and the generative model gives the density function of x and y as, 
( 1 ( ) 
p(y,x;O) = exp -(y xr)A Y -- b(O) 
_ ( 1+- I--- ) I(E 2 1) log2-) 
--a _ ,8(0) =  log + (n + , 
while the recognition model gives the distribution of y conditional to x as the following, 
q(y[x; rl) . ./V(r r x, s2), 
where, r/--- (r, s2). From the data x,... , xN, we define, 
1/: 
C --  3s3s T, q(x)  ./V'(O, C). 
s----1 
With this q(x), we define q(y, x; rl) as, 
q(y x;rl)=q(x)q(y[x'rl)=exp (-(y xr)B ( Y )-- 
1(11 ) 
B= -i -r s2C -l + rr fi' 
(5) 
(6) 
1 (log s 2 
,�(n--  q- loglCI q- (n q- 1) log2-). 
Convergence of the Wake-Sleep Algorithm 241 
3 
THE EM AND THE em ALGORITHMS FOR A FACTOR 
ANALYSIS MODEL 
It is mentioned that the W-S algorithm is similar to the EM algorithm[4]([5][7]). But there 
is an essential difference between them. In this section, first, we show the EM algorithm. 
We also describe the era algorithm[2] which gives us the information geometrical under- 
standing of the EM algorithm. With these results, we will show the difference between 
W-S and the EM algorithms in the next section. 
The EM algorithm consists of the following two steps. 
E-step: 
Define Q(O, Or) as, 
N 
Q(O, Ot) :  Z Ep(yl*;�) [logp(y, acs;O)] 
sl 
M-step: Update 0 as, 
Ot+ = argmax Q(O, Or), 
o 
T --1 --1 
(l+t t t)Ct t 
gt+l : T --1 --1 T -1 ' 
gt Xt C't gt + l + gt Xt gt 
Xt+ = diag (C - gt+l 
T --1 
+ _r%-t  ' 
1 Yt t gt / 
(7) 
Ep [-] denotes taking the average with the probability distribution p. The iteration of these 
two steps converges to give the MLE. 
The EM algorithm only uses the generative model, but the era algorithm[2] also uses the 
recognition model. The era algorithm consists of the e and ra steps which are defined as the 
e and ra projections[l] between the two manifolds M and D. The manifolds are defined 
as follows. 
Model manifold M: M d�=f {p(y, ac; O)lO = (g, diag(a) ), g 
Data manifold D: D deal {q(y,x;r)Ir = (r, s2),r  Rn,0 < s < cx}, q(x)include the 
matrix C which is defined by the data, and this is called the data manifold. 
Figure 1: Information geometrical understanding of the era algorithm 
Figure 1 schematically shows the era algorithm. It consists of two steps, e and ra steps. On 
each step, parameters of recognition and generative models are updated respectively. 
242 S. Ikeda, S. Amari and H. Nakahara 
e-step: 
Update r/as the e projection ofp(y, a:; Or) on D. 
rh+l = argmin KL(q(rl),p(Ot) ) 
-1 
_ 't gt st2+ 1 -'- T --1 
T -1  ' 
rt+l 1 + gt 't gt 1 + gt t gt 
where KL(q(rl), p(O)) is the Kullback-Leibler divergence defined as, 
KL(q(rl),p(O)) = Eq(y,;,v)logp(y,m;O)] 
(8) 
(9) 
ra-step: Update 0 as the ra projection ofq(y, a:; r/t) on M. 
Ot+ = argmin KL(q(rh+),p(O)) (10) 
0 
Urt+ 
t+ :diag(U-gt+lrtr+u). (11) 
gt+ = st2+  + rtUrt+  , 
By substituting (9) for rt+t and st2+t in (11), it is easily proved that (11) is equivalent to 
(7), and the era and EM algorithms are equivalent. 
4 
THE DIFFERENCE BETWEEN THE W-S AND THE EM 
ALGORITHMS 
The wake-phase corresponds to a gradient flow of the M-step[7] in the stochastic sense. 
But the sleep-phase is not a gradient flow of the E-step. In order to see these clear, we show 
the detail of the W-S phases in this section. 
First, we show the averages of (1), (2), (3, and (4,, 
t+ = rt - (1- ) (rt -diag(C- 2(Crt)a[ + (s + r[cr,)a,aD) 
rt+, =rt-,(Zt+, +gt+,gt+,) rt- l +g[+,Z[gt+, (14) 
 th -C divrg i rwritt a X(q(n,p(O)), 
1 n+l 
K(q(n,p(O)) = t(-'n) 2 + (0) - (, 
the derivatives of this K-L divergence with respect to 0: (g, ) arc, 
ff (Cr) (16, 
KL(q(.),p(O)) = X -2 (X-diag(C- 2Cr  + (s 2 + rrCr)v)).(17) 
With these results, we can rewrite the wake-phase as, 
o 
+ =  -( - V)r7(q(v),p(o,)) 9 
Convergence of the Wake-Sleep Algorithm 243 
Since  is a positive definite matrix, the wake-phase is a gradient flow of m-step which is 
defined as (10). 
On the other hand, KL(p(O), q(rl)) is, 
KL(p(O),q(rl)) = 
 n 
t(.4 B) - + �(v - �(o). 
The derivatives of this K-L divergence respect to r and s 2 are, 
0 2 ( X-g ) (20) 
orKL(p(O),q(rl)) = 5_(. U + ggT) r- 1 + gTE-g 
0 1 
O(s2)KL(p(O),q(rl)) = (s2) 2 (s2-((1-gTr)2+rT,Ur)). (21) 
Therefore, the sleep-phase can bc rewritten as, 
rt+ = rt - st 2 KL(p(Ot+),q(rh)) (22) 
o 
st2+ = st 2 - (1 -/3)(st2)20(st2) KL(p(Ot+),q(rlt)). (23) 
These are also a gradient flow, but because of the asymmctricity of K-L divergence, (22), 
(23) arc different from the on-linc version of the m-step. This is the essential difference 
between the EM and W-S algorithms. Therefore, wc cannot prove the convergence of the 
W-S algorithm based on the similarity of these two algorithms[7]. 
n t 
KLfi(O)KL(q(nlP 'O,,  )M ) 
D 
Figure 2: The Wake-Sleep algorithm 
5 CONVERGENCE PROPERTY 
We want to prove the convergence property of the W-S algorithm. If we can find a Lyapnov 
function for the W-S algorithm, the convergence is guaranteed[7]. But we couldn't find it. 
Instead of finding a Lyapnov function, we take the continuous time, and see the behavior 
of the parameters and K-L divergence, K L(q(r/t ), p( Ot ) ). 
KL(q(rl), p(O)) is a function of g, r, ' and s 2. The derivatives with respect to g and w 
are given in (16) and (17). The derivatives with respect to r and s 2 are, 
-rKL(q(rl),P(O)) 
O(s 
E-lg ) (24) 
1 + gT-g 
= 2(l+gT-g)C (r- 
1 
--KL(q(rl),p(O)) = 1 + gT-g s 2' (25) 
244 S. Ikeda, S. Amari and H. Nakahara 
On the other hand, we set the flows of g, r,  and s 2 to follow the updating due to the W-S 
algorithm, that is, 
d 
d 
d 
dt 
dt 
Crt ) 
= -a'(st 2 + rtTCrt) gt- %2 + rtTCrt (26) 
= --o'(wt +grOt T) rt- 1+ t2.,t Ot 
gr- (27) 
- -/ (-ig(C-.rL +(s +c)) (s 
= -fi' (st 2 ((1 r 2 
- -gtrt) + rtrwtrt)) (29) 
With theses results, dK L( q( rh ), p( Ot ) ) /dt is, 
dKL(q(rtt),p(Ot)) OKLda OKLdr 
= + 
dt Og dt Or dt 
OKL d w OKL d(s 2) 
+ --- + -- (30) 
OE dt 0(s 2) dt 
First 3 terms in the right side of (30) are apparently non-positive. Only the 4th one is not 
clear. 
OKL d(s2) -/3' (st 2 ((1 T 2 rtTwtrt) ( T- 1) 
O(s 2) d
