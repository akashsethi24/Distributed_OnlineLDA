186 
AN APPLICATION OF THE PRINCIPLE OF 
MAXIMUM INFORMATION PRESERVATION 
TO LINEAR SYSTEMS 
Ralph Linsker 
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598 
ABSTRACT 
This paper addresses the problem of determining the weights for a 
set of linear filters (model cells) so as to maximize the 
ensemble-averaged information that the cells' output values jointly 
convey about their input values, given the statistical properties of 
the ensemble of input vectors. The quantity that is maximized is the 
Shannon information rate, or equivalently the average mutual 
information between input and output. Several models for the role 
of processing noise are analyzed, and the biological motivation for 
considering them is described. For simple models in which nearby 
input signal values (in space or time) are correlated, the cells 
resulting from this optimization process include center-surround 
cells and cells sensitive to temporal variations in input signal. 
INTRODUCTION 
I have previously proposed [Linsker, 1987, 1988] a principle of maximum 
information preservation, also called the infomax principle, that may account for 
certain aspects of the organization of a layered perceptual network. The principle 
applies to a layer L of cells (which may be the input layer or an intermediate layer 
of the network) that provides input to a next layer M. The mapping of the input 
signal vector L onto an output signal vector M, f:L  M, is characterized by a 
conditional probability density function (pdf) P(MIL). The set S of allowed 
mappings f is specified. The input pdf Pt(L) is also given. (In the cases considered 
here, there is no feedback from M to L.) The infomax principle states that a 
mapping f should be chosen for which the Shannon information rate [Shannon, 
1949] 
-- f f e(ul )log[P(MIL)/Pa4(M)] 
(1) 
is a maximum (over all f in the set S). Here PM(M) = fdLP.(L)P(MI L) is the pdf 
of the output signal vector M. R is identical to the average mutual information 
between L and M. 
Maximum Information Preservation to Linear Systems 187 
To understand better how the infomax principle may be applied to biological systems 
and complex synthetic networks, it is useful to solve the infomax optimization 
problem explicitly for simpler systems whose properties are nonetheless biologically 
motivated. This paper therefore deals with the practical computation of infomax 
solutions for cases in which the mappings f are constrained to be linear. 
INFOMAX SOLUTIONS FOR A SET OF LINEAR FILTERS 
We consider the case of linear model neurons with multivariate Gaussian input 
and additive Gaussian noise. There are N input (L) cells and N  output (M) cells. 
The input column vector L--(/-a, L2, ..., LN) r is randomly selected from an 
N-dimensional Gaussian distribution having mean zero. That is, 
PL(L ) = (2r)-N/2(Det QL)-I/2 exp[- (1/2)LT(QL)-IL] 
(2) 
where Q- is the covariance matrix of the input activities, Q =. fdL P.(L)LiL i . 
(Superscript T denotes the matrix transpose.) 
To specify the set S of allowed mappings f:L  M, we define a processing model 
that includes a description of (i) how noise enters during processing, (ii) the 
independent variables over which we are to maximize R, and (iii) any constraints 
on their values. Figure 1 shows several such models. We shall analyze the simplest, 
then explain the motivation for the more complex models and analyze them in turn. 
Model A -- Additive noise of constant variance 
In Model A of Fig. 1 the output signal value of the nth M cell is: 
M n = Y, iCniLi + v n. 
(3) 
The noise components Pn are independently and identically distributed (i.i.d.) 
random variables drawn from a Gaussian distribution having a mean of zero and 
variance B. 
Each mapping f.'L  M is characterized by the values of the {Cni} and the noise 
parameter B. The elements of the covariance matrix of the output activities are 
(using Eqn. 3) 
M f 
Qnm(f)  dM PM(M) MnM m = Bnm + YidCniQijCmj, (4) 
where lnm --' 1 if n - rn and 0 otherwise. 
Evaluating Eqn. 1 for this processing model gives the information rate: 
R(f) = (1/2) lnDet W(f) 
(5) 
where Wnm  Q/B. (R is the difference of two entropy terms. See [Shannon, 
1949], p.57, for the entropy of a Gaussian distribution.) 
188 Linsker 
If the components Ci of the C matrix are allowed to be arbitrarily large, then the 
information rate can be made arbitrarily large, and the effects of noise become 
arbitrarily small. One way to limit C is to impose a resource constraint on each 
M cell. An example of such a constraint is YiCi = 1 for all n. One can then attempt 
directly, using numerical methods, to maximize Eqn. 5 over all allowed C for given 
B. However, when some additional conditions (below) are satisfied, further 
analytical progress can be made. 
Suppose the N L-cells are uniformly spaced along the line interval [0,1 ] with periodic 
boundary conditions, so that cell N is next to cell 1. [The analysis can be extended 
to a two- (or higher-) dimensional array in a straightforward manner.] Suppose also 
that (for given N) the covariance Q of the input values at cells i and j is a function 
QL(su) only of the displacement s u from i to j. (We deal with the periodicity by 
defining sab - b- a- �a,N and choosing the integer �b such that 
-N/2 < sb < N/2.) Then QL is a Toeplitz matrix, and its eigenvalues {hk} are the 
components of the discrete Fourier transform (F.T.) of Qt(s): 
X k = ZsQL(s)exp( -2rv/--I ks/N), (-N/2) < k < N/2. 
(6) 
We now impose two more conditions: (1) /V  = N. This simplifies the resulting 
expressions, but is otherwise inessential, as we shall discuss. (2) We constrain each 
M cell to have the same arrangement of C-values relative to the M cell's position. 
That is, Cni is to be a function C(Sn) only of the displacement Sni from n to i. This 
constraint substantially reduces the computational demands. We would not expect 
Li (B,C) 
Li ?.i (D) 
'! 'i Cni M n 
Figure 
Four processing models (A)-(D): Each diagram shows a single 
M cell (indexed by n) having output activity M n. Inputs {Li} may 
be common to many M cells. All noise contributions (dotted 
lines) are uncorrelated with one another and with {Li}. GC = 
gain control (see text). 
Maximum Information Preservation to Linear Systems 189 
it to hold in general in a biologically realistic model -- since different M cells should 
be allowed to develop different arrangements of weights -- although even then it 
could be used as an Ansatz to provide a lower bound on R. The section, 
Temporally-correlated input patterns, deals with a situation in which it is 
biologically plausible to impose this constraint. 
Under these conditions, (QnUm) is also a Toeplitz matrix. Its eigenvalues are the 
components of the F.T. of QSS(Snm). For N  = N these eigenvalues are (B + hzk) , 
where zk = I 2 and c  XsC(s) exp( -2rv- ks/N) is the F.T. of C(s). [This 
expression for the eigenvalues is obtained by rewriting Eqn. 4 as: 
QSS(Snm) = Bn-m,O + Xi,iC(Sni)QL(sii)C(Sm) , and taking the F.T. of both sides.] 
Therefore 
R = (1/2)Y k ln[1 + Xzk/B]. 
(7) 
We want to maximize R subject to XsC(s)2 = 1, which is equivalent to Xz--N. 
Using the Lagrange multiplier method, we maximize A -= R +/(Xz -N) over all 
nonnegative {zk} . Solving for OA/Oz = 0 and requiring z, > 0 for all k gives the 
solution: 
max[(-1/2/) - (B/X), 0], 
(8) 
where (given B)/ is chosen such that Xz = N. 
Note that while the optimal {z} are uniquely determined, the phases of the {ck} are 
completely arbitrary [except that since the {C(s)} are real, we must have c* = c_ 
for all k]. The {C(s)} values are therefore not uniquely determined. Fig. 2a shows 
two of the solutions for.an example in which QL(s) = exp[ - (S/So) 2] with s o = 6, 
N--N  --64, and B- 1. Both solutions have z0,_+  .... ,-+6=5.417, 5.409, 5.378, 
5.306, 5.134, 4.689, 3.376, and all other z -- 0. Setting all c phases to zero yields 
the solid curve; a particular random choice of phases yields the dotted c[ii've. We 
shall later see that imposing locality conditions on the {C(s)} (e.g., penalizing 
nonzero C(s) for large Is I ) can remove the phase ambiguity. 
Our solution (Eqn. 8) can be described in terms of a so-called water-filling 
analogy: If one plots B/Xk versus k, then z is the depth of water at k when one 
pours into the vessel defined by the B/X curve a total quantity of water that 
corresponds to Yz = N and brings the water level to (-1/2/). 
Let us contrast this problem with two other problems to which the water-filling 
analogy has been applied in the information-theory literature. In our notation, they 
are: 
1. 
Given a transfer function {C(s)} and the noise variance B, how should a given 
total input signal power YX be apportioned among the various wavenumbers 
k so as to maximize the information rate R [Gallager, 1968]? Our problem is 
complementary to this: we fix the input signal properties and seek an optimal 
transfer function subject to constraints. 
190 Linsker 
Rate-distortion (R-D) calculation [Berger, 1971 ]: Given a distortion measure 
(that defines a distance between the actual input signal and an estimate of it 
that can be reconstructed from the channel's output), and the input power 
spectrum {Xk}, what choice of {Zk} minimizes the average distortion for given 
information rate (or minimizes the required rate for given distortion)? In the 
R-D problem there is a process of reconstruction, and a given measure for 
assessing the goodness of reconstruction. In contrast, in our network there 
is no reconstruction of the input signal, and no criterion of the goodness of 
such a hypothetical reconstruction is provided. 
Note also that infomax optimization is not the same as computing which channel 
(that is, which mapping f:L  M) selected from an allowed set has the maximum 
information-theoretic capacity. In that problem, one is free to 
