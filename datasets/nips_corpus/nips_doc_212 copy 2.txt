622 Atlas, Cole, Connor, EI-Sharkawi, Marks, Muthusamy and Barnard 
Performance Comparisons Between 
Backpropagation Networks and Classification 
on Three Real-World Applications 
Trees 
Les Atlas 
Dept. of EE, FT-10 
University of Washington 
Seattle, Washington 98195 
Ronald Cole 
Dept. of CS&E 
Oregon Graduate Institute 
Beaverton, Oregon 97006 
Jerome Connor, Mohamed EI-Sharkawi, and Robert J. Marks II 
University of Washington 
Yeshwant Muthusamy 
Oregon Graduate Institute 
Etienne Barnard 
Carnegie-Mellon University 
ABSTRACT 
Multi-layer percepttons and trained classification trees are two very 
different techniques which have recently become popular. Given 
enough data and time, both methods are capable of performing arbi- 
trary non-linear classification. We first consider the important 
differences between multi-layer percepttons and classification trees 
and conclude that there is not enough theoretical basis for the clear- 
cut superiority of one technique over the other. For this reason, we 
performed a number of empirical tests on three real-world problems 
in power system load forecasting, power system security prediction, 
and speaker-independent vowel identification. In all cases, even for 
piecewise-linear trees, the multi-layer perceptron performed as well 
as or better than the trained classification trees. 
Performance Comparisons 623 
1 INTRODUCTION 
In this paper we compare regression and classification systems. A regression system 
can generate an output Y for an input X, where both X and Y are continuous and, 
perhaps, multi-dimensional. A classification system can generate an output class, C, 
for an input X, where X is continuous and multi-dimensional and C is a member of a 
finite alphabet. 
The statistical technique of Classification And Regression Trees (CART) was 
developed during the years 1973 (Meisel and Michalpoulos) through 1984 (Breiman et 
a/). As we show in the next section, CART, like the multi-layer perceptron (MLP), 
can be trained to solve the exclusive-OR problem. Furthermore, the solution it pro- 
vides is extremely easy to interpret. Moreover, both CART and MLPs are able to pro- 
vide arbitrary piecewise linear decision boundaries. Although there have been no 
links made between CART and biological neural networks, the possible applications 
and paradigms used for MLP and CART are very similar. 
The authors of this paper represent diverse interests in problems which have the com- 
monality of being both important and potentially well-suited for trainable classifiers. 
The load forecasting problem, which is partially a regression problem, uses past load 
trends to predict the critical needs of future power generation. The power security 
problem uses the classifier as an interpolator of previously known states of the system. 
The vowel recognition problem is representative of the difficulties in automatic 
speech recognition due to variability across speakers and phonetic context. 
In each problem area, large amounts of real data were used for training and disjoint 
data sets were used for testing. We were careful to ensure that the experimental con- 
ditions were identical for the MLP and CART. We concentrated only on performance 
as measured in error on the test set and did not do any formal studies of training or 
testing time. (CART was, in general, quite a bit faster.) 
In all cases, even with various sizes of training sets, the multi-layer perceptron per- 
formed as well as or better than the trained classification trees. We also believe that 
integration of many of CART's well-designed attributes into MLP architectures could 
only improve the already promising performance of MLP's. 
2 BACKGROUND 
2.1 Multi-Layer Perceptrons 
The name artificial neural networks has in some communities become almost 
synonymous with MLP's trained by back-propagation. Our power studies made use of 
this standard algorithm (Rumelhart et al, 1986) and our vowel studies made use of a 
conjugate gradient version (Barnard and Casasent, 1989) of back-propagation. In all 
cases the training data consisted of ordered pairs {(X,Y)} for regression, or {(X,C)} 
for classification. The input to the network is X and the output is, after training, 
hopefully very close to Y or C. 
When MLP's are used for regression, the output, Y, can take on real values between 0 
and 1. This normalized scale was used as the prediction value in the power forecast- 
ing problem. For MLP classifiers the output is formed by taking the (0,1) range of the 
output neurons and either thresholding or finding a peak. For example, in the vowel 
624 Atlas, Cole, Connor, EI-Sharkawi, Marks, Muthusamy and Barnard 
study we chose the maximum of the 12 output neurons to indicate the vowel class. 
2.2 Classification and Regression Trees (CART) 
CART has already proven to be useful in diverse applications such as radar signal 
classification, medical diagnosis, and mass spectra classification (Breiman et al, 1984). 
Given a set of training examples {(X,C)}, a binary tree is constructed by sequentially 
partitioning the p-dimensional input space, which may consist of quantitative and/or 
qualitative data, into p-dimensional polygons. The trained classification tree divides 
the domain of the data into non-overlapping regions, each of which is assigned a class 
label C. For regression, the estimated function is piecewise constant over these re- 
gions. 
The first split of the data space is made to obtain the best global separation of the 
classes. The next step in CART is to consider the partitioned training examples as two 
completely unrelated sets--those examples on the left of the selected hyper-plane, and 
those on the fight. CART then proceeds as in the first step, treating each subset of the 
training examples independently. A question which had long plagued the use of such 
sequential schemes was: when should the splitting stop? CART implements a novel, 
and very clever approach; splits continue until every training example is separated 
from every other, then a pruning criterion is used to sequentially remove less impor- 
tant splits. 
2.3 Relative Expectations of MLP and CART 
The non-linearly separable exclusive-OR problem is an example of a problem which 
both MLP and CART can solve with zero error. The left side of Figure 1 shows a 
trained MLP solution to this problem and the fight side shows the very simple trained 
CART solution. For the MLP the values along the arrows represent trained multipli- 
cative weights and the values in the circles represent trained scalar offset values. For 
the CART figure, y and n represent yes or no answers to the trained thresholds and the 
values in the circles represent the output Y. It is interesting that CART did not train 
correctly for equal numbers of the four different input cases and that one extra exam- 
ple of one of the input cases was sufficient to break the symmetry and allow CART to 
train correctly. (Note the similarity to the well-known requirement of random and 
different initial weights for training the MLP). 
Figure 1: 
(X < 0.$) 
The MTP and CART solutions to the exclusive-OR problem. 
Performance Comparisons 625 
CART trains on the exclusive-OR very easily since a piecewise-linear partition in the 
input space is a perfect solution. In general, the MLP will construct classification re- 
gions with smooth boundaries, whereas CART will construct regions with sharp 
comers (each region being, as described previously, an intersection of half planes). 
We would thus expect MLP to have an advantage when classification boundaries tend 
to be smooth and CART to have an advantage when they are sharper. 
Other important differences between MLP and CART include: 
For an MLP the number of hidden units can be selected to avoid overfitting or 
underfitting the dam. CART fits the complexity by using an automatic pruning tech- 
nique to adjust the size of the tree. The selection of the number of hidden units or the 
tree size was implemented in our experiments by using data from a second training set 
(independent of the firs0. 
An MLP becomes a classifier through an ad hoc application of thresholds or peak- 
picking to the output value(s). Great care has gone into the CART splitting rules 
while the usual MLP approach is rather arbitrary. 
A trained MLP represents an approximate solution to an optimization problem. The 
solution may depend on initial choice of weights and on the optimization technique 
used. For complex MLP's many of the units are independenfiy and simultaneously 
adjusting their weights to best minimize output error. 
MLP is a distributed topology where a single point in the input space can have an 
effect across all units or analogously, one weight, acting alone, will have minimal 
affect on the outputs. CART is very different in that each split value can be mapped 
onto one segment in the input space. The behavior of CART makes it much more 
useful for data interpretation. A trained tree may be useful for understanding the 
structure of the data. The usefulness of MLP's for data interpretation is much less 
clear. 
The above points, when taken in combination, do not make a clear case for either 
MLP or CART to be superior for the best performance as a trained classifier. We thus 
believe that the empirical studies of the next sections, with their consistent perfor- 
mance trends, will indicate which of the comparative aspects are the most significant. 
3 LOAD FORECASTING 
3.1 The Problem 
The ability to predict electric power system loads from an hour to several days in the 
future can help a utility operator to efficiently schedule and utilize power generation. 
This ability to forecast loads can also provide information which can be used to stra- 
tegically trade energy with other generating systems. In order for these forecasts to be 
useful to an operator, they must be accurate and computationally efficient. 
3.2 Methods 
Hourly temperature a
