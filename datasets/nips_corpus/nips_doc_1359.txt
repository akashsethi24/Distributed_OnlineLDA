Recurrent Neural Networks Can Learn to 
Implement Symbol-Sensitive Counting 
IMul Rodriguez 
Department of Cognitive Science 
University of California, San Diego 
La Jolla, CA. 92093 
prodrigu @ cogsci.ucsd.edu 
Janet Wiles 
School of Information Technology and 
Department of Psychology 
University of Queensland 
Brisbane, Queensland 4072 Australia 
janetw@it.uq.edu.au 
Abstract 
Recently researchers have derived formal complexity analysis of analog 
computation in the setting of discrete-time dynamical systems. As an 
empirical consWast, training recurrent neural networks (RNNs) produces 
seff-organizeA systems that are realizations of analog mechanisms. Pre- 
vious work showed that a RNN can learn to process a simple context-free 
language (CFL) by counting. Herein, we extend that work to show that a 
RNN can learn a harder CFL, a simple palindrome, by organizing its re- 
sources into a symbol-sensitive counting solution, and we provide a dy- 
namical systems analysis which demonstrates how the network can not 
only count, but also copy and store counting information. 
1 INTRODUCTION 
Several researchers have recently derived results in analog computation theory in the set- 
ting of discrete-time dynamical systems(Siegelmann, 1994; Maass & Opren, 1997; Moore, 
1996; Casey, 1996). For example, a dynamical recognizer (DR) is a discrete-time continu- 
ous dynamical system with a given initial starting point and a finite set of Boolean output 
decision functions(Pollack, 1991; Moore, 1996; see also Siegelmann, 1993). The dynami- 
cal system is composed ofa space,R n , an alphabet A, a set of functions (1 per element of A) 
that each maps n _+ n and an accepting region H, in n. With enough precision and 
appropriate diffeaemtial equations, DRs can use real-valued variables to encode contents of 
a stack or counter (for details see Siegelmann, 1994; Moore, 1996). 
As an empirical contrast, training recurrent neural networks (RNNs) produces self- 
organized implementations of analog mechanisms. In previous work we showed that an 
RNN can learn to process a simple context-free language, a nb n, by organizing its resources 
into a counter which is similar to hand-coded dynamical recognizers but also exhibits some 
88 P. Rodriguez and J. Wiles 
novelties (Wiles & Elman, 1995). In particuiar, similar to hand-coded counters, the network 
developed proportional contracting and expanding rates and precision matters - but unex- 
pectedly the network distributed the contraction/expansion axis among hidden units, devel- 
oped a saddle point to transition between the first haft and second half of a string, and used 
oscillating dynamics as a way to visit regions of the phase space around the fixed points. 
In this work we show that an RNN can implement a solution for a harder CFL, a simple 
palindrome language(bed below), which requires a symbol-sensitive counting solu- 
tion. We provide a dynamical systems analysis which demonstrates how the network can 
not only count, but also copy and store counting information implicitly in space around a 
2 TRAINING an RNN TO PROCESS CFLs 
We use a discrete-time RNN that has 1 hidden layer with recmxet connections, and 1 output 
layer without recurrent connections so that the accepg regions are determined by the out- 
put units. The RNN processes output in Tune(n), where n is the length of the input, and it 
can recogni?e languages that are a proper subset of context-sensitive languages and a proper 
superset of regular languages(Moore, 1996). Consequently, the RNN we investigate can in 
principle embody the computational power needed to process self-recursion. 
Furthermore, many connectionlst models of language processing have used a prediction 
task(e.g. Elman, 1990). Hence, we trained an RNN to be a real-time transducer version 
of a dynamical recognizer that predicts the next input in a sequence. Although the network 
does not explicitly accept or reject strings, if our network makes all the right predictions 
possible then performing the prediction task subsumes the accept task, and in principle one 
could simply rejea;t unmatched predictions. We used a threxhhold criterion of .5 such that if 
an ouput node has a value greatex than .5 then the network is considered to be making that 
prediction. If the network makes all the right predictions possible for some input string, 
then it is correctly proceasing that string. Although a finite dimensional RNN cannot pro- 
cess CFLs robustly with a margin for error (e.g.Casey, 1996;Maass and Orponen, 1997), we 
will show that it can acquire the right kind of trajectory to proce the language in a way 
that genemliz_ to longer strings. 
2.1 A SIMPLE PALINDROME LANGUAGE 
A palindrome language (mirror language) consists of a set of strings, S, such that each 
string, s 6S, s = ww r, is a concatenation of a substring, w, and its reverse, w r. The rele- 
vant aspect of this language is that a mechanism cannot use a simple counter to process the 
string but must use the functional equivalent of a stack that enables it to match the symbols 
in second haft of the string with the first haft. 
We investigated a palindrome language that uses only two symbols for w, two other sym- 
bols for w r, such that the second haft of the string is fully predictable once the change in 
symbols occurs. The language we used is a simple version restricted such that one sym- 
bol is always present and precedes the other, for example: w = a m, w r = B m A e.g. 
aaaabbbBBBAAAA, (where n > 0, m >= 0). Note that the embedded subsequence 
bmB m is just the simple-CFL used in W'des & Elman (1995) as mentioned above, hence, 
one can reasonably expect that a solution to this task has an embedded counter for the sub- 
sequence b...B. 
2.2 LINEAR SYSTEM COUNTERS 
A basic counter in analog computation theory uses real-valued precision (e.g. Siegelman 
1994; Moore 1996). For example, a 1-dimensional up/down counter for two symbols { a, b } 
RNNs Can Learn Symbol-Sensitive Counting 89 
is the system f(z) = .Sz + .Sa, f(z) = 2z - .Sb where z is the state variable, a is the input 
variable to count up(push), and b is the variable to count down(pop). A sequence of input 
aaabbb has state values(sg at 0): .5,.75,875, .75,.5,0. 
Similarly, for our transducer version one can develop piecewise linear system equations in 
which counting takes place along different dimensions so that different predictions can be 
made at appropriate time steps I . The linear system serves as a hypothesis before running any 
simulations to understand the implementation issues for an RNN. For example, using the 
function f(z) = z for z  [0, 1], 0 for z < 0, 1 for z > 1, then for the simple palindrome 
task one can explicitly encode a mechanism to copy and store the count for a across the 
b...B subsequences. If we assign dimension-1 to a, dimension-2 to b, dimension-3 to A, 
dimension-4 to B, and dimension-5 to store the a value, we can build a system so that for 
a sequence aaabbBBAAA we get state variables values: initial, (0,0,0,0,0), (.5,0,0,0,0), 
(.75,0,0,0,0), (.875,0,0,0,0), (0,.5,0,0,.875), (0,.75,0,0,.875), (0,0,0,.5,.875), (0,0,0,0,.875), 
(0,0,.75,0,0), (0,0,.5,0,0), (0,0,0,0,0). The matrix equations for such a system could be: 
0 .5 0 0 0 0 .5 0 -5 
Xt = f( 0 2 0 2 * Xt-l + 0 0 --1 --5 *It) 
2 0 2 0 0 -5 0 -1 
1 0 0 0 1 -5 0 -5 0 
where t is time, Xt is the 5-dimensional state vector,/t is the 4-dimensional input vector 
using 1-hot encoding ofa = [1, O,O,O];& = [O,I,O,O];A = [O, O, I, O], B = [0,0,0,1]. 
The simple trick is to use the input weights to turn on or off the counting. For eample, 
the dimension-5 state variable is mined off when input is a or A, but then turned on when 
b is input, at which time it copies the last a value and holds on to it. It is then easy to add 
Boolean output decision functions that keep predictions linearly separable. 
However, other solutions are possible. Rather than store the a count one could keep count- 
ing up in dimension-1 for b input and then cancel it by counting down for B input. The 
questions that arise are: Can an RNN implement a solution that generalizes? What kind of 
store and copy mechanism does an RNN discover? 
2.3 TRAINING DATA & RESULTS 
The training set consists of 68 possible strings of total length <_ 25, which means a maxi- 
m}m of n + rn = 12, or 12 symbols in the first haft, 12 symbols in the second half, and 
1 end symbol 2. Th complete training set has more short strings so that the network does 
not disregard the transitions at the end of the string or at the end of the b...B subsequence. 
The network consists of 5 input, 5 hiddeAl, 5 output onits, with a bias node. The hidden and 
recurrent units are updated in the same time step as the input is presented. The recurrent 
layer activations are input on the next time step. The weight updates are performed using 
back-propagation thru time training with error injected at each time step backward for 24 
time steps for each input. 
We found that about haft our simulations learn to make lxedions for transitions, and most 
will have few genemlizons on longer strings not seen in the training set. However, no 
network learned the complete training set perfectly. The best network was trained for 250K 
sweeps (1 per character) with a learning parameter of .001, and 136K more sweeps with 
.0001, for a total of about 51K strings. The network made 28 total prediction errors on 28 
IThese can be expanded relatively easily to include more symbols, different symbol representa- 
tions, harder palindwme sequences, or different kind of decision planes. 
2We removed training strings to -- anb,for n > 1; it tums out that the network interpolates on 
the B-to-A transition for these. Also, we added an end symbol to help reset the system to a consistent 
starting value. 
90 P. Rodriguez and J. Wiles 
different strings in the test set of 68 possible strings seen in training. All of thes
