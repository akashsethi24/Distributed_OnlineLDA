Foraging in an Uncertain Environment Using 
Predictive Hebbian Learning 
P. Read Montague ; Peter Dayan, and Terrence J. Sejnowski 
Computational Neurobiology Lab, The Salk Institute, 
10010 N. Torrey Pines Rd, 
La Jolla, CA, 92037, USA 
readbohr. bcm. tmc. edu 
Abstract 
Survival is enhanced by an ability to predict the availability of food, 
the likelihood of predators, and the presence of mates. We present a 
concrete model that uses diffuse neurotransmitter systems to implement 
a predictive version of a Hebb learning rule embedded in a neural ar- 
chitecture based on anatomical and physiological studies on bees. The 
model captured the strategies seen in the behavior of bees and a number of 
other animals when foraging in an uncertain environment. The predictive 
model suggests a unified way in which neuromodulatory influences can 
be used to bias actions and control synaptic plasticity. 
Successful predictions enhance adaptive behavior by allowing organisms to prepare for fu- 
ture actions, rewards, or punishments. Moreover, it is possible to improve upon behavioral 
choices if the consequences of executing different actions can be reliably predicted. Al- 
though classical and instrumental conditioning results from the psychological literature [ 1] 
demonstrate that the vertebrate brain is capable of reliable prediction, how these predictions 
are computed in brains is not yet known. 
The brains of vertebrates and invertebrates possess small nuclei which project axons 
throughout large expanses of target tissue and deliver various neurotransmitters such as 
dopamine, norepinephrine, and acetylcholine [4]. The activity in these systems may report 
on reinforcing stimuli in the world or may reflect an expectation of future reward [5, 6, 7, 8]. 
*Division of Neumscience, Baylor College of Medicine, Houston, TX 77030 
598 
Foraging in an Uncertain Environment Using Predictive Hebbian Learning 599 
A particularly striking example is that of the honeybee. Honeybees can be conditioned to 
a sensory stimulus such as a color, visual pattern, or an odorant when the sensory stimulus 
is paired with application of sucrose to the antennae or proboscis. An identified neuron, 
VUMmxl, projects widely throughout the entire bee brain, becomes active in response 
to sucrose, and its firing can substitute for the unconditioned odor stimulus in classical 
conditioning experiments [8]. Similar diffusely projecting neurons in the bee brain may 
substitute for reward when paired with a visual stimulus. 
In this paper, we suggest a role for diffuse neurotransmitter systems in learning and behavior 
that is analogous to the function we previously postulated for them in developmental self- 
organization[3, 2]. Specifically, we: (i) identify a neural substrate/architecture which is 
known to exist in both vertebrates and invertebrates and which delivers information to 
widespread regions of the brain; (ii) describe an algorithm that is both mathematically 
sound and biologically feasible; and (iii) show that a version of this local algorithm, in the 
context of the neural architecture, reproduces the foraging and decision behavior observed 
in bumble bees and a number of other animals. 
Our premise is that the predictive relationships between sensory stimuli and rewards are 
constructed through these diffuse systems and are used to shape both ongoing behavior and 
reward-dependent synaptic plasticity. We illustrate this using a simple example from the 
ethological literature for which constraints are available at a number of different levels. 
A Foraging Problem 
Real and colleagues [9, 10] performed a series of experiments on bumble bees foraging on 
artificial flowers whose colors, blue and yellow, predicted of the delivery of nectar. They 
examined how bees respond to the mean and variability of this reward delivery in a foraging 
version of a stochastic two-armed bandit problem [ 11 ]. All the blue flowers contained 
of nectar, � of the yellow flowers contained 6gt, and the remaining - of the yellow flowers 
contained no nectar at all. In practice, 85% of the bees' visits were to the constant yield 
blue flowers despite the equivalent mean return from the more variable yellow flowers. 
When the contingencies for reward were reversed, the bees switched their preference for 
flower color within 1 to 3 visits to flowers. They further demonstrated that the bees could be 
induced to visit the variable and constant flowers with equal frequency if the mean reward 
from the variable flower type was made sufficiently high. 
This experimental finding shows that bumble bees, like honeybees, can learn to associate 
color with reward. Further, color and odor learning in honeybees has approximately the 
same time course as the shift in preference described above for the bumble bees [ 12]. It also 
indicates that under the conditions of a foraging task, bees prefer less variable rewards and 
compute the reward availability in the short term. This is a behavioral strategy utilized by 
a variety of animals under similar conditions for reward [9, 10, 13] suggesting a common 
set of constraints in the underlying neural substrate. 
The Model 
Fig. 1 shows a diagram of the model architecture, which is based on the considerations 
above about diffuse systems. Sensory input drives the units 'B' and 'Y' representing blue 
and yellow flowers. These neurons (outputs x and xt y respectively at time t) project 
600 Montague, Dayan, and Sejnowski 
N ;ctar Sensory input 
Action selection 
inhibition 
Motor 
systems 
Figure 1: Neural architecture showing how predictions about future expected rein- 
forcement can be made in the brain using a diffuse neurotransmitter system [3, 2]. In 
the context of bee foraging [9], sensory input drives the units B and Y representing blue and 
yellow flowers. These units project to a reinforcement neuron P through a set of variable 
weights (filled circles w B and w �) and to an action selection system. Unit $ provides input 
to P, and fires while the bee sips the nectar. R projects its output rt through a fixed weight 
to P. The variable weights onto P implement predictions about future reward rt (see text) 
and P's output is sensitive to temporal changes in its input. The output projections of P, 6t 
(lines with arrows), influence learning and also the selection of actions such as steering in 
flight and landing, as in equation 5 (see text). Modulated lateral inhibition (dark circle) in 
the action selection layer symbolizes this. Before encountering a flower and its nectar, the 
output of P will reflect the temporal difference only between the sensory inputs B and Y. 
During an encounter with a flower and nectar, the prediction error 6t is determined by the 
output of B or Y and R, and learning occurs at connections w e and w �. These strengths 
are modified according to the correlation between presynaptic activity and the prediction 
error 6t produced by neuron P as in equation 3 (see text). Learning is restricted to visits to 
flowers [14]. 
through excitatory connection weights both to a diffusely projecting neuron P (weights 
w e and w �) and to other processing stages which control the selection of actions such as 
steering in flight and landing. P receives additional input rt through unchangeable weights. 
In the absence of nectar (rt 0), the net input to P becomes B  '� � 
- Vt  wt'gt -- wtx t --wtx t � 
The first assumption in the construction of this model is that learning (adjustment of 
weights) is contingent upon approaching and landing on a flower. This assumption is 
supported specifically by data from learning in the honeybee: color learning for flowers is 
restricted to the final few seconds prior to landing on the flower and experiencing the nectar 
[14]. 
This fact suggests a simple model in which the strengths of variable connections wt are 
adjusted according to a presynaptic correlational rule: 
(1) 
where oc is the learning rate [ 15]. There are two problems with this formulation: (i) learning 
would only occur about contingencies in the presence of a reinforcing stimulus (r t 5h 0); 
Foraging in an Uncertain Environment Using Predictive Hebbian Learning 601 
A 
B 
1.0 
0.8 
0.4 
0.2 
100.0 
. 80.0 
 60.0 
40.0 
;> 20.0 
0.0 0.0 
0.0 5.0 10.0 
Nectar volume (gl) 
0 5 0 5 20 25 30 
Trial 
Figure 2: Simulations of bee foraging behavior using predictive Hebbian learning. A) 
Reinforcement neuron output as a function of nectar volume for a fixed concentration of 
nectar[9, 10]. B) Proportion of visits to blue flowers. Each trial represents approximately 
40 flower visits averaged over 5 real bees and exactly 40 flower visits for a single model 
bee. Trials 1 - 15 for the real and model bees had blue flowers as the constant type, the 
remaining trials had yellow flowers as constant. At the beginning of each trial, w � and w e 
were set to 0.5 consistent with evidence that information from past foraging bouts is not 
used[ 14]. The real bees were more variable than the model bees - sources of stochasticity 
such as the two-dimensional feeding ground were not represented. The real bees also had a 
slight preference for blue flowers [21]. Note the slower drop for 2 = 0.1 when the flowers 
are switched. 
and (ii) there is no provision for allowing a sensory event to predict the future delivery of 
reinforcement. The latter problem makes equation 1 inconsistent with a substantial volume 
of data on classical and instrumental conditioning [16]. Adding a postsynaptic factor to 
equation 1 does not alter these conclusions [ 17]. 
This inadequacy suggests that another form of learning rule and a model in which P has a 
direct input from ft. Assume that the firing rate of P is sensitive only to changes in its input 
over time and habituates to constant or slowly varying input, like magnocellular ganglion 
cells in the retina [18]. Under this assumption, the output of P, 6t, reflects a temporal 
deri
