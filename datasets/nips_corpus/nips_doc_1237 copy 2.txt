NeuroScale: Novel Topographic Feature 
Extraction using RBF Networks 
David Lowe 
D. Loweraston. ac.uk 
Michael E. Tipping 
E. T�pp�ngast on. ac. uk 
Neural Computing Research Group 
Aston University, Aston Triangle, Birmingam B4 7ET,UK 
http://www. ncrg. aston. ac. uk/ 
Abstract 
Dimension-reducing feature extraction neural network techniques 
which also preserve neighbourhood relationships in data have tra- 
ditionally been the exclusive domain of Kohonen self organising 
maps. Recently, we introduced a novel dimension-reducing feature 
extraction process, which is also topographic, based upon a Radial 
Basis Function architecture. It has been observed that the gener- 
alisation performance of the system is broadly insensitive to model 
order complexity and other smoothing factors such as the kernel 
widths, contrary to intuition derived from supervised neural net- 
work models. In this paper we provide an effective demonstration 
of this property and give a theoretical justification for the apparent 
'self-regularising' behaviour of the 'NEUROSCALE' architecture. 
1 
'NeuroScale': A Feed-forward Neural Network 
Topographic Transformation 
Recently an important class of topographic neural network based feature extraction 
approaches, which can be related to the traditional statistical methods of Sammon 
Mappings (Sammon, 1969) and Multidimensional Scaling (Kruskal, 1964), have 
been introduced (Mao and Jain, 1995; Lowe, 1993; Webb, 1995; Lowe and Tipping, 
1996). These novel alternatives to Kohonen-like approaches for topographic feature 
extraction possess several interesting properties. For instance, the NEUROSCALE 
architecture has the empirically observed property that the generalisation perfor- 
544 D. Lowe and M. E. Tipping 
mance does not seem to depend critically on model order complexity, contrary to 
intuition based upon knowledge of its supervised counterparts. This paper presents 
evidence for their 'self-regularising' behaviour and provides an explanation in terms 
of the curvature of the trained models. 
We now provide a brief introduction to the NEUROSCALE philosophy of nonlinear 
topographic feature extraction. Further details may be found in (Lowe, 1993; Lowe 
and Tipping, 1996). We seek a dimension-reducing, topographic transformation of 
data for the purposes of visualisation and analysis. By 'topographic', we imply that 
the geometric structure of the data be optimally preserved in the transformation, 
and the embodiment of this constraint is that the inter-point distances in the feature 
space should correspond as closely as possible to those distances in the data space. 
The implementation of this principle by a neural network is very simple. A Radial 
Basis Function (RBF) neural network is utilised to predict the coordinates of the 
data point in the transformed feature space. The locations of the feature points are 
indirectly determined by adjusting the weights of the network. The transformation 
is determined by optimising the network parameters in order to minimise a suitable 
error measure that embodies the topographic principle. 
The specific details of this alternative approach are as follows. Given an m- 
dimensional input space of N data points Xq, an n-dimensional feature space of 
points yq is generated such that the relative positions of the feature space points 
minimise the error, or 'STRESS', term: 
N 
Z -'  y (dp - dqp)2, (1) 
P q>P 
where the dqp are the inter-point Euclidean distances in the data space: dqp - 
v/(Xq -- Xp)W(Xq -- Xp), and the dqp are the corresponding distances in the feature 
space: dqp = V/(yq - yp)T(yq _ yp). 
The points y are generated by the RBF, given the data points as input. That is, 
yq -- f(Xq; W), where f is the nonlinear transformation effected by the RBF with 
parameters (weights and any kernel smoothing factors) W. The distances in the 
feature space may thus be given by dqp =ll f(Xq) - f(xp) II and so more explicitly 
by 
dqp = 
2 
II) - I[)] , (2) 
where b 0 are the basis functions,/z are the centres of those functions, which are 
fixed, and wt are the weights from the basis functions to the output. 
The topographic nature of the transformation is imposed by the STRESS term which 
attempts to match the inter-point Euclidean distances in the feature space with 
those in the input space. This mapping is relatively supervised because there is no 
specific target for each yq; only a relative measure of target separation between each 
yq, yp pair is provided. In this form it does not take account of any additional in- 
formation (for example, class labels) that might be associated with the data points, 
but is determined strictly by their spatial distribution. However, the approach may 
be extended to incorporate the use of extra 'subjective' information which may be 
NeuroScale: Novel Topographic Feature Extraction using RBF Networks 545 
used to influence the transformation and permits the extraction of 'enhanced', more 
informative, feature spaces (Lowe and Tipping, 1996). 
Combining equations (1) and (2) and differentiating with respect to the weights in 
the network allows the partial derivatives of the STRESS OE/Owtk to be derived 
for each pattern pair. These may be accumulated over the entire pattern set and 
the weights adjusted by an iterative procedure to minimise the STRESS term E. 
Note that the objective function for the RBF is no longer quadratic, and so a 
standard analytic matrix-inversion method for fixing the final layer weights cannot 
be employed. 
We refer to this overall procedure as 'NEuROSCALE'. Although any universal ap- 
proximator may be exploited within NEUROSCALE, using a Radial Basis Function 
network allows more theoretical analysis of the resulting behaviour, despite the fact 
that we have lost the usual linearity advantages of the RBF because of the STRESS 
measure. A schematic of the NEUROSCALE model is given in figure 1, and illustrates 
the rSle of the RBF in transforming the data space to the feature space. 
RBF 
Figure 1: The NEUROSCALE architecture. 
2 Generalisation 
In a supervised learning context, generalisation performance deteriorates for over- 
complex networks as 'overfitting' occurs. By contrast, it is an interesting empirical 
observation that the generalisation performance of NEUROSCALE, and related mod- 
els, is largely insensitive to excessive model complexity. This applies both to the 
number of centres used in the RBF and in the kernel smoothing factors which them- 
selves may be viewed as regularising hyperparameters in a feed-forward supervised 
situation. 
This insensitivity may be illustrated by Figure 2, which shows the training and 
test set performances on the IRIS data (for 5-45 basis functions trained and tested 
on 45 separate samples). To within acceptable deviations, the training and test set 
546 D. Lowe and M. E. Tipping 
STRESS values are approximately constant. This behaviour is counter-intuitive when 
compared with research on feed forward networks trained according to supervised 
approaches. We have observed this general trend on a variety of diverse real world 
problems, and it is not peculiar to the IRIS data. 
X 10 -3 Training and Test Errors 
4.5 
3.5 
3 
5 10 15 20 25 30 35 40 45 
Number o! Basis Functions 
Figure 2: Training and test errors for NEuROSCALE Radial Basis Functions with 
various numbers of basis functions. Training errors are on the left, test errors are 
on the right. 
There are two fundamental causes of this observed behaviour. Firstly, we may 
derive significant insight into the necessary form of the functional transformation 
independent of the data. Secondly, given this prior functional knowledge, there 
is an appropriate regularising component implicitly incorporated in the training 
algorithm outlined in the previous section. 
2.1 Smoothness and Topographic Transformations 
For a supervised problem, in the absence of any explicit prior information, the 
smoothness of the network function must be determined by the data, typically neces- 
sitating the setting of regularising hyperparameters to counter overfitting behaviour. 
In the case of the distance-preserving transformation effected by NEUROSCALE, an 
understanding of the necessary smoothness may be deduced a priori. 
Consider a point Xq in input space and a nearby test point xp -- Xq q- Epq, where 
Epq is an arbitrary displacement vector. Optimum generalisation demands that the 
distance between the corresponding image points yq and yp should thus be 
Considering the Taylor expansions around the point yq we find 
Ily Yq II 
-- -' (epqgql) q- O(e4), 
--Ep'rq (gqlgq51 Epqq-O(E4), 
/=1 
-- pWqGqEpq 'q- 0(4), 
(3) 
NeuroScale: Novel Topographic Feature Extraction using RBF Networks 547 
where the matrix C_.q -- -.=lgqtgqTt and gqt is the gradient vector 
(Oyt(q)/Ox,... ,Oyt(q)/Ox,) T evaluated at x - Xq. For structure preservation 
the corresponding distances in input and output spaces need to be retained for all 
values of apq: II Yp -- Yq II 2-- aTa, and so (q -- I with the requirement that second- 
and higher-order terms must vanish. In particular note that measures of curvature 
proportional to (02 yt (q)/Ox) 2 should vanish. In general, for dimension reduction, 
we cannot ensure that exact structure preservation is obtained since the rank of Gq 
is necessarily less than n and hence can never equate to the identity matrix. How- 
ever, when minimising STRESS we are locally attempting to minimise the residual 
III -- Cq [I, which is achieved when all the vectors apq of interest lie within the range 
of (q. 
2.2 The Training Mechanism 
An important feature of this class of topographic transformations is that the STRESS 
measure is invariant under arbitrary rotations and transformations of the output 
configuration. The algorithm outlined previously tends towards those configurations 
that generally reduce the sum-of-squared weight values (Tipping, 1996). This is 
achieved 
