Learning to Make Coherent Predictions in 
Domains with Discontinuities 
Suzanna Becker and Geoffrey E. Hinton 
Department of Computer Science, University of Toronto 
Toronto, Ontario, Canada M5S 1A4 
Abstract 
We have previously described an unsupervised learning procedure that 
discovers spatially coherent properties of the world by maximizing the in- 
formation that parameters extracted from different parts of the sensory 
input convey about some common underlying cause. When given random 
dot stereograms of curved surfaces, this procedure learns to extract sur- 
face depth because that is the property that is coherent across space. It 
also learns how to interpolate the depth at one location from the depths 
at nearby locations (Becker and Hinton. 1992). In this paper, we pro- 
pose two new models which handle surfaces with discontinuities. The first 
model attempts to detect cases of discontinuities and reject them. The 
second model develops a mixture of expert interpolators. It learns to de- 
tect the locations of discontinuities and to invoke specialized, asymmetric 
interpolators that do not cross the discontinuities. 
I Introduction 
Standard backpropagation is implausible as a model of perceptual learning because 
it requires an external teacher to specify the desired output of the network. We 
have shown (Becker and Hinton, 1992) how the external teacher can be replaced 
by internally derived teaching signals. These signals are generated by using the 
assumption that different parts of the perceptual input have common causes in 
the external world. Small modules that Look at separate but related parts of the 
perceptual input discover these common causes by striving to produce outputs that 
agree with each other (see Figure 1 a). The modules may look at different modalities 
(e.g. vision and touch), or the same modality at different times (e.g. the consecutive 
2-D views of a rotating 3-D object), or even spatially adjacent parts of the same 
image. In previous work, we showed that when our learning procedure is applied 
372 
Learning to Make Coherent Predictions in Domains with Discontinuities 373 
to adjacent patches of 2-dimensional images, it allows a neural network that has no 
prior knowledge of the third dimension to discover depth in random dot stereograms 
of curved surfaces. A more general version of the method allows the network to 
discover the best way of interpolating the depth at one location from the depths 
at nearby locations. We first summarize this earlier work, and then introduce 
two new models which allow coherent predictions to be made in the presence of 
discontinuities. 
a) b) 
max I 
patch A patch B 
Figure 1: a) Two modules that receive input from corresponding parts of stereo 
images. The first module receives input from stereo patch A, consisting of a hori- 
zontal strip from the left image (striped) and a corresponding strip from the right 
image (hatched). The second module receives input from an adjacent stereo patch 
B. The modules try to make their outputs, d, and do, convey as much informa- 
tion as possible about some underlying signal (i.e., the depth) which is common to 
both patches. b) The architecture of the interpolating network, consisting of multiple 
copies of modules like those in a) plus a layer of interpolating units. The network 
tries to maximize the information that the locally extracted parameter dc and the 
contextually predicted parameter cc convey about some common underlying signal. 
We actually used 10 modules and the central 6 modules tried to maximize agreement 
between their outputs and contextually predicted values. We used weight averaging 
to constrain the interpolating function to be identical for all modules. 
2 Learning spattally coherent features in images 
The simplest way to get the outputs of two modules to agree is to use the squared 
difference between the outputs as a cost function, and to adjust the weights in each 
module so as to minimize this cost. Unfortunately, this usually causes each module 
to produce the same constant output that is unaffected by the input to the module 
and therefore conveys no information about it. What we want is for the outputs 
of two modules to agree closely (i.e. to have a small expected squared difference) 
relative to how much they both vary as the input is varied. When this happens, the 
two modules must be responding to something that is common to their two inputs. 
In the special case when the outputs, d,, do, of the two modules are scalars, a good 
374 Becker and Hinton 
measure of agreement is: 
v(da + d0) (1) 
I = 0.5 log V(d. - 
where V is the variance over the training cases. If d, and do are both versions 
of the same underlying Gaussian signal that have been corrupted by independent 
Gaussian noise, it can be shown that I is the mutual information between the 
underlying signal and the average of d, and do. By maximizing I we force the two 
modules to extract as pure a version as possible of the underlying common signal. 
2.1 The basic stereo net 
We have shown how this principle can be applied to a multi-layer network that learns 
to extract depth from random dot stereograms (Becker and Hinton, 1992). Each 
network module received input from a patch of a left image and a corresponding 
patch of a right image, as shown in Figure 1 a). Adjacent modules received input 
from adjacent stereo image patches, and learned to extract depth by trying to 
maximize agreement between their outputs. The real-valued depth (relative to the 
plane of fixation) of each patch of the surface gives rise to a disparity between 
features in the left and right images; since that disparity is the only property that 
is coherent across each stereo image, the output units of modules were able to learn 
to accurately detect relative depth. 
2.2 The interpolating net 
The basic stereo net uses a very simple model of coherence in which an underlying 
parameter at one location is assumed to be approximately equal to the parameter at 
a neighbouring location. This model is fine for the depth of fronto-parallel surfaces 
but it is far from the best model of slanted or curved surfaces. Fortunately, we can 
use a far more general model of coherence in which the parameter at one location 
is assumed to be an unknown linear function of the parameters at nearby locations. 
The particular linear function that is appropriate can be learned by the network. 
We used a network of the type shown in Figtire i b). The depth computed locally 
by a module, do, was compared with the depth predicted by a linear combination 
of the outputs of nearby modules, and the network tried to maximize the agreement 
between dc and 
The contextual prediction, dc, was produced by computing a weighted sum of 
the outputs of two adjacent modules on either side. The interpolating weights 
used in this sum, and all other weights in the network, were adjusted so as to 
maximize agreement between locally computed and contextually predicted depths. 
To speed the learning, we first trained the lower layers of the network as be- 
fore, so that agreement was maximized between neighbouring locally computed 
outputs. This made it easier to learn good interpolating weights. When the 
network was trained on stereograms of cubic surfaces, it learned interpolating 
weights of-0.147,0.675,0.656,-0.131 (Becker and Hinton, 1992). Given noise 
free estimates of local depth, the optimal linear interpolator for a cubic surface 
is -0.167, 0.667, 0.667, -0.167. 
Learning to Make Coherent Predictions in Domains with Discontinuities 375 
3 Throwing out discontinuities 
If the surface is continuous, the depth at one patch can be accurately predicted from 
the depths of two patches on either side. If, however, the training data contains cases 
in which there are depth discontinuities (see figure 2) the interpolator will also try 
to model these cases and this will contribute considerable noise to the interpolating 
weights and to the depth estimates. One way of reducing this noise is to treat the 
discontinuity cases as outliers and to throw them out. Rather than making a hard 
decision about whether a case is an outlier, we make a soft decision by using a 
mixture model. For each training case, the network compares the locally extracted 
depth, do, with the depth predicted from the nearby context, {c. It assumes that 
d - 0 is drawn from a zero-mean Gaussian if it is a continuity case and from a 
uniform distribution if it is a discontinuity case. It can then estimate the probability 
of a continuitv case: 
Spline 
curve 
Left 
image 
Right 
image 
Figure 2: Top: A curved surface strip with a discontinuity created by fitting 2 
cubic splines through randomly chosen control points, 25 pixels apart, separated by 
a depth discontinuity. Feature points are randomly scattered on each spline with an 
average of 0.22 features per pixel. Bottom: A stereo pair of intensity images 
of the surface strip formed by taking two different projections of the feature points, 
filtering them through a gaussian, and sampling the filtered projections at evenly 
spaced sample points. The sample values in corresponding patches of the two images 
are used as the inputs to a module. The depth of the surface for a particular zmage 
region is directly related to the disparity between corresponding features in the left 
and right patch. Disparity ranges continuously from -1 to +1 image pixels. Each 
stereo image was 120 pixels wide and divided into 10 receptive fields I0 pixels wide 
and separated by 2 pixel gaps, as input for the networks shown in figure i. The 
receptive field of an interpolating unit spanned 58 image pixels, and discontinuities 
were randomly located a minimum of dO pixels apart, so only rarely would more 
than one discontinuity lie within an interpolator's receptive field. 
376 Becker and Hinton 
pco. t(ac - do): N(ac - do, 0, co.,(ac - do)) (2) 
where N is a gaussian
