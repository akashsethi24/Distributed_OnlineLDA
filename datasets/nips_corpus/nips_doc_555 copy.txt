Unsupervised learning 
of distributions on binary vectors 
using two layer networks 
Yoav reund ' 
Computer and Information Sciences 
University of California Santa Cruz 
Santa Cruz, CA 95064 
David Haussler 
Computer and Information Sciences 
University of California Santa Cruz 
Santa Cruz, CA 95064 
Abstract 
We study a particular type of Boltzmann machine with a bipartite graph structure called a harm 
nium. Our interest is in using such a machine to model a probability distribution on binary input 
vectors. We analyze the class of probability distributions that can be modeled by such machines, 
showing that for each n _ I this class includes arbitrarily good apprximation to any distribution 
on the set of all n-vectors of binary inputs. We then present two learning algorithms for these 
machines.. The first learning algorithm is the standard gradient ascent heuristic for computing 
maximum likelihood estimates for the parameters (i.e. weights and thresholds) of the model. Here 
we give a closed form for this gradient that is significantly easier to compute than the corresponding 
gradient for the general Boltzmann machine. The second learning algorithm is a greedy method 
that creates the hidden units and computes their weights one at a time. This method is a variant 
of the standard method for projection pursuit density estimation. We give experimental results for 
these learning methods on synthetic data and natural data from the domain of handwritten digits. 
I Introduction 
Let us suppose that each example in our input data is a binary vector ' = {x,...,x,]  {+1} , and that 
each such example is generated independently at random according some unknown distribution on {+1} '. 
This situation arises, for instance, when each example consists of (possibly noisy) measurements of n different 
binary attributes of a randomly selected object. In such a situation, uusupervised learning can be usefully 
defined as using the input data to find a good model of the unknown distribution on {+l) ' and thereby 
learning the structure in the data. 
The process of learning an unknown distribution from examples is usually called denszty estzraation or 
parameter estimation in statistics, depending on the nature of the class of distributions used as models. 
Connectionist models of this type include Bayes networks [14], mixture models [3,13], and Markov random 
fields [14,8]. Network models based on the notion of energy minimization such as Hopfield nets [9] and 
Boltzmann machines [1] can also be used as models of probability distributions. 
* yoavcis. ucsc.ed u 
912 
Unsupervised learning of distributions on binary vectors using 2-1ayer networks 913 
The models defined by Hopfield networks are a special case of the more general Markov random field models 
in which the local interaction are restricted to symmetric pairwise interactions between components of 
the input. Boltzmann machines also use only pairwise interactions, but in addition they include hidden 
units, which correspond to unobserved variables. These unobserved vsriables intersct with the observed 
variables represented by components of the input vector. The overall distribution on the set of possible 
input vectors is defined as the marginal distribution induced on the components of the input vector by the 
Markov random field over all variables, both observed snd hidden. While the Hopfield network is relatively 
well understood, it is limited in the types of distributions that it can model. On the other hand, Boltzmann 
machines are universal in the sense that they are powerful enough to model any distribution (to any degree 
of approximation), but the mathematical analysis of their capabilities is often intractable. Moreover, the 
standard learning algorithm for the Boltzmann machine, a gradient ascent heuristic to compute the maximum 
likelihood estimates for the weights and thresholds, requires repeated stochastic approximation, which results 
in unacceptably slow lesrning. t In this work we attempt to narrow the gap between Hopfield networks and 
Boltzmann machines by finding a model that will be powerful enough to be universal,  yet simple enough 
to be analyzable and computationally efficient. a We have found such a model in a minor variant of the 
special type of Boltzmann machine defined by Smolensky in his fiarmon theory [18][Ch.8]. This special type 
of Boltzmann machine is defined by a network with a simple bipartite graph structure, which he called a 
harmonium. 
The harmonium consists of two types of units: input units, each of which holds one component of the input 
vector, and hidden units, representing hidden variables. There is a weighted connection between each input 
unit and each hidden unit, and no connections between input units or between hidden units (see Figure (1)). 
The presence of th hidden units induces dependencies, or correlations, between the variables modeled by 
input units. To illustrate the kind of model that results, consider the distribution of people that visit a 
specific coffee shop on Sunday. Let each of the n input variables represent the presence (+1) or absence (-1) 
of a particular person that Sunday. These random variables are clearly not independent, e.g. if Fred's wife 
and daughter are there, it is more likely that Fred is there, if you see three members of the golf club, you 
expect to see other members of the golf club, if Bill is there you are unlikely to see Brenda there, etc. This 
situation can be modeled by a harmonium model in which each hidden variable represents the presence or 
absence of a social group. The weights connecting a hidden unit and an ipput unit measure the tendency of 
the corresponding person to be associated with the corresponding group. In this coffee shop situation, several 
social groups may be present at one time, exerting a combined influence on the distribution of customers. 
This can be modeled easily with the harmonium, but is difficult to model using Bayes networks or mixture 
models. 4 
2 The Model 
Let us begin by formalizing the harmonium model. To model a distribution on {+1} we will use n input 
units and some number m > 0 of hidden units. These units are connected in a bipartite graph as illustrated 
in Figure (1). 
The random variables represented by the input units each take values in {+ 1, -1 }, while the hidden variables, 
represented by the hidden units, take values in {0, 1). The state of the machine is defined by the values 
of these random variables. Define 7 = (:at,...,x,)  {+1} to be the state of the input units, and  = 
(h,..., h,)  {0, 1} m to be the state of the hidden units. 
The connection weights between the input units and the ith laidden unit are denoted s by (')  R n and the 
bias of the ith hidden unit is denoted by 0 (i)  R. The parameter vector 0 = {(W(),0 O)) .... , (('), 0('))} 
XOne possible solution to this is the mema-field approxination [15], discussed further in section 2 below. 
2In [4] we show that itny distribution over {4-1} cltn be itpproximltted to within itny desired ccurcy by it 
ha.rmonium model using 2 hidden units. 
See also other work relating Bityes nets mad Boltzmitnn m.hines [12,7]. 
Noisy-OR gitres hve been introduced in the fritmework of Bayes Networks to allow for such combinittions. 
However, using this in networks with hidden units h not been studied, to the best of our knowledge. 
Sln [16][Ch.6], binitry connection weights aze used. Here we use rel-vlued weights. 
914 Freund and Haussler 
Hidden Units 
Input Units 
h ha 
-3 
Figure 1: The bipartite graph of the harmonium 
defines the entire network, and thus also the probability model induced by the network. For a given 4, the 
energy of a state configuration of hidden and input units is defined to be 
lO) = - 
i--1 
and the probability of a configuration is 
Pr(i,fD) = � e-E('il*) where 
Summing over g, it is easy to show that in the general case the probability distribution over possible state 
vectors on the input units is given by 
This product form is particular to the harmonium structure, and does not hold for general Boltzmann 
machines. Product form distribution models have been used for density estimation in Projection Pursuit 
[10,6,5]. We shall look further into this relationship in section 5. 
3 Discussion of the model 
The right hand side of Equation (2) has a simple intuitive interpretation. The ith factor in the product 
corresponds to the hidden variable h, and is an increasing function of the dot product between  and the 
weight vector of the ith hidden unit. Hence an input vector  will tend to have large probability when it is 
in the direction of one of the weight vectors -'(0 (i.e. when F (i) �  is large), and small probability otherwise. 
This is the way that the hidden variables can be seen to exert their influence; each corrponds to a 
preferred or prototypicM direction in space. 
The next to the lt formula in Equation (2) shows that the harmonium model can be written  a mixture 
of 2 m distributions of the form 
Z() exp (C (') � + O('))h, , 
Unsupervised learning of distributions on binary vectors using 2-1ayer networks 915 
where   {0, 1} m and Z() is the appropriate normalization fad:tot. It is easily verified that each of these 
distributions is in ft & product of n Bernoulli distributions on {+1, -1}, one for ech input viable z. 
Hence the hmonium model c be interpred  � kind of mixture model. Howev, the number of 
components in the mixture reprnd by a hmonium is exponential in the number of hidden uni. 
It is internting to compe the cl of harmonium models to the standard cl of models defined by a 
mixture of products of Bernoulli distributions. The sme biprtite graph dcribed in Figure (1) cn be 
used to define � standard mixture model. Aign ech of the m hidden units � weight vector ) d  
probability pi such that i Pi = 1. To generate n example, c
