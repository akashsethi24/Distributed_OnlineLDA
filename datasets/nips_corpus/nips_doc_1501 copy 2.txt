Exploratory Data Analysis Using Radial Basis 
Function Latent Variable Models 
Alan D. Marrs and Andrew R. Webb 
DERA 
St Andrews Road, Malvern 
Worcestershire U.K. WR14 3PS 
{marrs, webb} @s ina 1. dera. ov. uk 
(British Crown Copyright 1998 
Abstract 
Two developments of nonlinear latent variable models based on radial 
basis functions are discussed: in the first, the use of priors or constraints 
on allowable models is considered as a means of preserving data structure 
in low-dimensional representations for visualisation purposes. Also, a 
resampling approach is introduced which makes more effective use of 
the latent samples in evaluating the likelihood. 
1 INTRODUCTION 
Radial basis functions (RBF) have been extensively used for problems in discrimination 
and regression. Here we consider their application for obtaining low-dimensional repre- 
sentations of high-dimensional data as part of the exploratory data analysis process. There 
has been a great deal of research over the years into linear and nonlinear techniques for 
dimensionality reduction. The technique most commonly used is principal components 
analysis (PCA) and there have been several nonlinear generalisations, each taking a partic- 
ular definition of PCA and generalising it to the nonlinear situation. 
One approach is to find surfaces of closest fit (as a generalisation of the PCA definition 
due to the work of Pearson (1901) for finding lines and planes of closest fit). This has 
been explored by Hastie and $tuetzle (1989), Tibshirani (1992) (and further by LeBlanc 
and Tibshirani, 1994) and various authors using a neural network approach (for example, 
Kramer, 1991). Another approach is one of variance maximisation subject to constraints 
on the transformation (Hotelling, 1933). This has been investigated by Webb (1996), using 
a transformation modelled as an RBF network, and in a supervised context in Webb (1998). 
An alternative strategy also using RBFs, based on metric multidimensional scaling, is de- 
scribed by Webb (1995) and Lowe and Tipping (1996). Here, an optimisation criterion, 
530 A. D. Marrs and A. R. Webb 
termed stress, is defined in the transformed space and the weights in an RBF model deter- 
mined by minimising the stress. 
The above methods use a radial basis function to model a transformation from the high- 
dimensional data space to a low-dimensional representation space. A complementary ap- 
proach is provided by Bishop et al (1998) in which the structure of the data is modelled as 
a function of hidden or latent variables. Termed generative topographic mapping (GTM), 
the model may be regarded as a nonlinear generalisation of factor analysis in which the 
mapping from latent space to data space is characterised by an RBF. 
Such generafive models are relevant to a wide range of applications including radar target 
modelling, speech recognition and handwritten character recognition. 
However, one of the problems with GTM that limits its practical use for visualising data on 
manifolds in high dimensional space arises from distortions in the structure that it imposes. 
This is acknowledged in Bishop et aI (1997) where 'magnification factors' are introduced 
to correct for the GTM's deficiency as a means of data visualisation. 
This paper considers two developments: constraints on the permissible models and resam- 
pling of the latent space. Section 2 presents the background to latent variable models; 
Model constraints are discussed in Section 3. Section 4 describes a re-sampling approach 
to estimation of the posterior pdf on the latent samples. An illustration is provided in Sec- 
tion 5. 
2 BACKGROUND 
Briefly, we shall re-state the basic GTM model, retaining the notation of Bishop et al 
(1998). Let {.ti, i = 1,..., N}, ti  1 I represent measurements on the data space vari- 
ables; :r  1 ' represent the latent variables. ' 
Let t be normally-distributed with mean//(a:; W) and covariance matrix/3-11;//(a:; W) 
is a nonlinear transformation that depends on a set of parameters W. Specifically, we shall 
assume a basis function model 
M 
i----1 
where the vectors wi  l:t D 
1,..., M} is a set of basis functions defined on the latent space. 
The data distribution may be written 
p(tlW,/3) = f p(tla:; 
where, under the assumptions of normality, 
p(tlm;W,/3)- ,'] exp{-lY(:r;W)-tl 
are to be determined through optimisation and {bi, i = 
(1) 
Approximating the integral by a finite sum (assuming the functions p(a:) and//(a:) do not 
vary too greatly compared with the sample spacing), we have 
K 
p(tIW,/3) = W,/3) (2) 
i=1 
which may be regarded as a function of the parameters W and/3 that characterise 
Exploratory Data Analysis Using Radial Basis Function Latent Variable Models 531 
Given the data set {ti, i = 1,..., N}, the log likelihood is given by 
L(W,) 
N 
E ln[p(tlW, )] 
=1 
which may be maximised using a standard EM approach (Bishop et al, 1998). 
In this case, we have 
as the re-estimate of the mixture component weights, pj, at the (m + 1) step, where 
(3) 
t. = P?)P(tla:; w(m)'t(m)) (4) 
Eip?)p(t,l=,; w(m),(')) 
and (.)(m) denotes values at the ruth step. Note that Bishop et al (1998) do not re-estimate 
pj; all values are taken to be equal. 
The number of pj terms to be re-estimated is K, the number of terms used to approximate 
the integral (1). We might expect that the density is smoothly varying and governed by a 
much fewer number of parameters (not dependent on K). 
The re-estimation equation for the D x M matrix W = [wxl... IWM] is 
W (m+) = TTRT'I'['I'TG'I']- (5) 
where G is the K x K diagonal matrix with 
N 
and T r = [txl... Itv], r = [4(=)1... 14(=K)]. The term  is re-estimated as 
1/O(m) = V(vv) Ef= 
Once we have determined parameters of the transformation, we may invert the model by 
asking for the distribution of a: given a measurement ti. That is, we require 
p(tile)p(e) 
p(x[t) = f p(tile)p(e)cke (6) 
For example, we may plot the position of the peak of the distribution p([ti) for each data 
sample 
3 APPLYING A CONSTRAINT 
One way to retain structure is to impose a condition that ensures that a unit step in the 
latent space corresponds to a unit step in the data space (more or less). For a single latent 
variable, z, we may impose the constraints that 
which may be written, in terms of W as 
jlTWTw51 = I 
532 A. D. Marrs and A. R. Webb 
where j = o' 
The derivative of the data space variable with respect to the latent variable has unit mag- 
nitude. The derivative is of course a function of :r and imposing such a condition at each 
sample point in latent space would not be possible owing to the smoothness of the RBF 
model. However, we may average over the latent space, 
where(.) denotes average over the latent space. 
In general, for L latent variables we may impose a constraint JTwTwJ -- IL leading 
to the penaity term 
Tr {A(jTwTwJ- IL)} 
where J is an M x L matrix with jth column Oqb/Oxy and A is a symmetric matrix of 
Lagrange multipliers. This is very similar to regularisation terms. It is a condition on 
the norm of W; it incorporates the Jacobian matrix J and a symmetric L x L matrix of 
Lagrange multipliers, A. The re-estimation solution for W may be written 
W = TTRTq'(q'TGq' + jAjT) - (7) 
with A chosen so that the constraint ,I T W T WJ = I is satisfied. 
We may aiso use the derivatives of the transformation to define a distortion measure or 
magnification factor, 
M(:r; W) = IIJTWTWJ -- 112 
which is a function of the latent variables and the model parameters. A value of zero shows 
that there is no distortion . 
An alternative to the constraint approach above is to introduce a prior on the allowable 
transformations using the magnification factor; for example, 
P(W)  exp(-AM(a:;W)) (8) 
where A is a regularisation parameter. This leads to a modification to the M-step re- 
estimation equation for W, providing a maximum a posteriori estimate. Equation (8) pro- 
vides a naturai generalisation of PCA since for the speciai case of a linear transformation 
(bi = xi, M = L), the solution for W is the PCA space as A - . 
4 RESAMPLING THE LATENT SPACE 
Having obtained a mapping from latent space to data space using the above constraint, we 
seek a better estimate to the posterior pdf of the latent samples. Current versions of GTM 
require the latent samples to be uniformly distributed in the latent space which leads to 
distortions when the data of interest are projected into the latent space for visualisation. 
Since the responsibility matrix R can be used to determine a weight for each of the latent 
samples it is possible to update these samples using a resampling scheme. 
We propose to use a resampling scheme based upon adaptive kernel density estimation. The 
basic procedure places a Gaussian kernel on each latent sample. This results in a Gaussian 
Note that this differs from the measure in the paper by Bishop et al, where a ratio-of-areas 
criterion is used, a factor which is unity for zero distortion, but may also be unity for some distortions. 
Exploratory Data Analysis Using Radial Basis Function Latent Variable Models 533 
mixture representation of the pdf of the latent samples p(a:[t), 
K 
p(:rlt ) = EpiN(li, zi), 
i----1 
(9) 
where each mixture component is weighted according to the latent sample weight Pi. Ini- 
tially, the Eli's are all equal, taking their value from the standard formula of Silverman 
(1986), 
Eli = hz'V, (10) 
where matrix V is an estimate of the covariance ofp(a:)and, 
(11) 
h = [{4/(L + 
If the kernels are centered exactly on the latent samples, this model artificially inflates the 
variance of the latent samples. Following West (1993) we perform kernel shrinkage by 
making the la i take the values 
(12) 
I i = /1 - hï¿½a:i + (1 - V/- hl') 
where  is the mean of the latent samples. This ensures that there is no artificial inflation 
of the variance. 
To reduce the redundancy in our initially large n
