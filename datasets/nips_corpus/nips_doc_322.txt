A Comparative Study of the Practical 
Characteristics of Neural Network and 
Conventional Pattern Classifiers 
Kenney Ng 
BBN Systems and Technologies 
Cambridge, MA 02138 
Abstract 
Richard P. Lippmann 
Lincoln Laboratory, MIT 
Lexington, MA 02173-9108 
Seven different pattern classifiers were implemented on a serial computer 
and compared using artificial and speech recognition tasks. Two neural 
network (radial basis function and high order polynomial GMDH network) 
and five conventional classifiers (Gaussian mixture, linear tree, K nearest 
neighbor, KD-tree, and condensed K nearest neighbor) were evaluated. 
Classifiers were chosen to be representative of different approaches to pat- 
tern classification and to complement and extend those evaluated in a 
previous study (Lee and Lippmann, 1989). This and the previous study 
both demonstrate that classification error rates can be equivalent across 
different classifiers when they are powerful enough to form minimum er- 
ror decision regions, when they are properly tuned, and when sufficient 
training data is available. Practical characteristics such as training time, 
classification time, and memory requirements, however, can differ by or- 
ders of magnitude. These results suggest that the selection of a classifier 
for a particular task should be guided not so much by small differences in 
error rate, but by practical considerations concerning memory usage, com- 
putational resources, ease of implementation, and restrictions on training 
and classification times. 
1 INTRODUCTION 
Few studies have compared practical characteristics of adaptive pattern classifiers 
using real data. There has frequently been an over-emphasis on back-propagation 
classifiers and artificial problems and a focus on classification error rate as the main 
performance measure. No study has compared the practical trade-offs in training 
time, classification time, memory requirements, and complexity provided by the 
970 
Practical Characteristics of Neural Network and Conventional Pattern Classifiers 971 
many alternative classifiers that have been developed (e.g. see Lippmann 1989). 
The purpose of this study was to better understand and explore practical character- 
istics of classifiers not included in a previous study (Lee and Lippmann, 1989; Lee 
1989). Seven different neural network and conventional pattern classifiers were eval- 
uated. These included radial basis function (RBF), high order polynomial GMDH 
network, Gaussian mixture, linear decision tree, K nearest neighbor (KNN), KD 
tree, and condensed K nearest neighbor (GKNN) classifiers. All classifiers were 
implemented on a serial computer (Sun 3-110 Workstation with FPA) and tested 
using a digit recognition task (7 digits, 22 cepstral inputs, 16 talkers, 70 training 
and 112 testing patterns per talker), a vowel recognition task (10 vowels, 2 formant 
frequency inputs, 67 talkers, 338 training and 333 testing patterns), and two ar- 
tificial tasks with two input dimensions that require either a single convex or two 
disjoint decision regions. Tasks are as in (Lee and Lippmann, 1989) and details of 
experiments are described in (Ng, 1990). 
2 TUNING EXPERIMENTS 
Internal parameters or weights of classifiers were determined using training data. 
Global free parameters that provided low error rates were found experimentally 
using cross-validation and the training data or by using test data. Global parameters 
included an overall basis function width scale factor for the RBF classifier, order 
of nodal polynomials for the GMDH network, and number of nearest neighbors for 
the KNN, KD tree, and CKNN classifiers. 
Experiments were also performed to match the complexity of each classifier to that 
of the training data. Many classifiers exhibit a characteristic divergence between 
training and testing error rates as a function of their complexity. Poor performance 
results when a classifier is too simple to model the complexity of training data 
and also when it is too complex and over-fits the training data. Cross-validation 
and statistical techniques were used to determine the correct size of the linear tree 
and GMDH classifiers where training and test set error rates diverged substantially. 
An information theoretic measure (Predicted Square Error) was used to limit the 
complexity of the GMDH classifier. This classifier was allowed to grow by adding 
layers and widening layers to find the number of layers and the layer width which 
minimized predicted square error. Nodes in the linear tree were pruned using 10- 
fold cross-validation and a simple statistical test to determine the minimum size tree 
that provides good performance. Training and test set error rates did not diverge 
for the R, BF and Gaussian mixture classifiers. Test set performance was thus used 
to determine the number of Gaussian centers for these classifiers. 
A new multi-scale radial basis function classifier was developed. It has multiple 
radial basis functions centered on each basis function center with widths that vary 
over 1 1/2 orders of magnitude. Multi-scale RBF classifiers provided error rates 
that were similar to those of more conventional RBF classifiers but eliminated the 
need to search for a good value of the global basis function width scale factor. 
The CKNN classifier used in this study was also new. It was developed to reduce 
memory requirements and dependency on training data order. In the more conven- 
tional CKNN classifier, training patterns are presented sequentially and classified 
using a KNN rule. Patterns are stored as exemplars only if they are classified in- 
972 Ng and Lippmann 
correctly. In the new CKNN classifier, this conventional CKNN training procedure 
is repeated N times with different orderings of the training patterns. All exemplar 
patterns stored using any ordering are combined into a new reduced set of training 
patterns which is further pruned by using it as training data for a final pass of 
conventional CKNN training. This approach typically required less memory than 
a KNN or a conventional CKNN classifier. Other experiments described in (Chang 
and Lippmann, 1990) demonstrate how genetic search algorithms can further reduce 
KNN classifier memory requirements. 
A) GAUSSIAN MIXTURE 
B) POLYNOMIAL GMDH NETWORK 
4000 
3000 
20O0 
1000 
500 
40O0 
3000 
2000 
1000 
500 
o 14oo 
500 1000 
F1 (Hz) 
Figure 1: Decision Regions Created by (A) RBF and (B) GMDH Classifiers for the 
Vowel Problem. 
3 DECISION REGIONS 
Classifiers differ not only in their structure and training but also in how decision 
regions are formed. Decision regions formed by the RBF classifier for the vowel 
problem are shown in Figure 1A. Boundaries are smooth spline-like curves that 
can form arbitrarily complex regions. This improves generalization for many real 
problems where data for different classes form one or more roughly ellipsoidal clus- 
ters. Decision regions for the high-order polynomial (GMDH) network classifier are 
shown in Figure lB. Decision region boundaries are smooth and well behaved only 
in regions of the input space that are densely sampled by the training data. Decision 
boundaries are erratic in regions where there is little training data due to the high 
polynomial order of the discriminant functions formed by the GMDH classifier. As 
a result, the GMDH classifier generalizes poorly in regions with little training data. 
Decision boundaries for the linear tree classifier are hyperplanes. This classifier may 
also generalize poorly if data is in ellipsoidal clusters. 
4 ERROR RATES 
Figure 2 shows the classification (test set) error rates for all classifiers on the bulls- 
eye, disjoint, vowel, and digit problems. The solid line in each plot represents the 
Practical Characteristics of Neural Network and Conventional Pattern Classifiers 973 
lO lO 
8 
6 
n.- 
h.- 2 
g 0 
-- 25 
i, 
o 20 
o 15 
lO 
5 
BULLSEYE 
DIGIT 50% 
I I I I I I I 
8 
6 
4 
2 
o 
30 
25 
20 
DISJOINT 
i I I I I i i 
VOWEL 
15- 
10- 
5 
I I I I I I I 
0 
Figure 2: Test Data Error Rates for All Classifiers and All Problems. 
mean test set error rate across all the classifiers for that problem. The shaded re- 
gions represent one binomial standard deviation, rr, above and below. The binomial 
standard deviation was calculated as rr = X/�(1 - �)/N, where � is the estimated 
average problem test set error rate and N is the number of test patterns for each 
problem. The shaded region gives a rough measure of the range of expected sta- 
tistical fluctuation if the error rates for different classifiers are identical. A more 
detailed statistical analysis of the test set error rates for classifiers was performed 
using McNemar's significance test. At a significance level of a = 0.01, the error 
rates of the different classifiers on the bullseye, disjoint, and vowel problems do not 
differ significantly from each other. 
Performance on the more difficult digit problem, however, did differ significantly 
across classifiers. This problem has very little training data (10 training patterns 
per class) and high dimensional inputs (an input dimension of 22). Some classifiers, 
including the IBF and Gaussian mixture classifiers, were able to achieve very low 
error rates on this problem and generalize well even in this high dimensional space 
with little training data. Other classifiers, including the multi-scale RBF, KD- 
tree, and GKNN classifiers, provided intermediate error rates. The GMDH network 
classifier and the linear tree classifier provided high error rates. 
The linear tree classifier performed poorly on the digit problem because there is 
974 Ng and Lippmann 
not enough training data to sample the input space densely enough for the training 
algorithm to form decision boundaries that can generalize well. The poor perfor- 
mance of the GMDH network classifier is due, in part, to the inability of the GMDH 
network classifier to extrapolat
