A Realizable Learning Task which 
Exhibits Overfitting 
Siegfried BSs 
Laboratory for Information Representation, RIKEN, 
Hirosawa 2-1, Wako-shi, Saitama, 351-01, Japan 
email: boes@zoo.riken.go.jp 
Abstract 
In this paper we examine a perceptron learning task. The task is 
realizable since it is provided by another perceptron with identi- 
cal architecture. Both perceptrons have nonlinear sigmoid output 
functions. The gain of the output function determines the level of 
nonlinearity of the learning task. It is observed that a high level 
of nonlinearity leads to overfitting. We give an explanation for this 
rather surprising observation and develop a method to avoid the 
overfitting. This method has two possible interpretations, one is 
learning with noise, the other cross-validated early stopping. 
I Learning Rules from Examples 
The property which makes feedforward neural nets interesting for many practical 
applications is their ability to approximate functions, which are given only by ex- 
amples. Feed-forward networks with at least one hidden layer of nonlinear units 
are able to approximate each continuous function on a N-dimensional hypercube 
arbitrarily well. While the existence of neural function approximators is already 
established, there is still a lack of knowledge about their practical realizations. Also 
major problems, which complicate a good realization, like overfitting, need a better 
understanding. 
In this work we study overfitting in a one-layer perceptron model. The model 
allows a good theoretical description while it exhibits already a qualitatively similar 
behavior as the multilayer perceptron. 
A one-layer perceptron has N input units and one output unit. Between input 
and output it has one layer of adjustable weights Wi, (i -- 1,..., N). The output z 
is a possibly nonlinear function of the weighted sum of inputs xi, i.e. 
N 
z=g(h), with h= v .= 
A Realizable Learning Task Which Exhibits Overfitting 219 
The quality of the function approximation is measured by the difference between 
the correct output z, and the net's output z averaged over all possible inputs. In 
the supervised learning scheme one trains the network using a set of examples x_ u 
(/ -- 1,...,P), for which the correct output is known. It is the learning task to 
minimize a certain cost function, which measures the difference between the correct 
output z, u and the net's output zU averaged over all examples. 
Using the mean squared error as a suitable measure for the difference between 
the outputs, we can define the training error ET and the generalization error EG 
as 
P 
I 1 
:= - , Ec := < - (2) 
The development of both errors as a function of the number P of trained examples 
is given by the learning curves. Training is conventionally done by gradient descend. 
For theoretical purposes it is very useful to study learning tasks, which are pro- 
vided by a second network, the so-called teacher network. This concept allows a 
more transparent definition of the difficulty of the learning task. Also the monitor- 
ing of the training process becomes clearer, since it is always possible to compare 
the student network and the teacher network directly. 
Suitable quantities for such a comparison are, in the perceptron case, the following 
order parameters, 
r :: iiWi---   Wi* Wi, q := IIWII = -.(Wi) 2 . (3) 
i=1 i=1 
Both have a very transparent interpretation, r is the normalized overlap between 
the weight vectors of teacher and student, and q is the norm of the student's weight 
vector. These order parameters can also be used in multilayer learning, but their 
number increases with the number of all possible permutations between the hidden 
units of teacher and student. 
2 The Learning Task 
Here we concentrate on the case in which a student perceptton has to learn a 
mapping provided by another perceptton. We choose identical networks for teacher 
and student. Both have the same sigmoid output function, i.e. g,(h) = g(h) = 
tanh(?h). Identical network architectures of teacher and student are realizable tasks. 
In principle the student is able to learn the task provided by the teacher exactly. 
UnreaIizabIe tasks can not be learnt exactly, there remains always a finite error. 
If we use uniformally distributed random inputs x_ and weights W, the weighted 
sum h in (1) can be assumed as Gaussian distributed. Then we can express the 
generalization error (2) by the order parameters (3), 
1 {tanh [TZ1]- tanh [q(rz1 q- V -- r 2 z2)] 
with the Gaussian measure 
Dz := oo Vr2_  exp - (5) 
From equation (4) we can see how the student learns the gain 7 of the teachers 
output function. It adjusts the norm q of its weights. The gain 7 plays an important 
role since it allows to tune the function tanh(7h) between a linear function (7 << 1) 
and a highly nonlinear function (7 >> 1). Now we want to determine the learning 
curves of this task. 
220 S. BOS 
3 Emergence of Overfitting 
3.1 Explicit Expression for the Weights 
Below the storage capacity of the perceptton, i.e. a = 1, the minimum of the training 
error ET is zero. A zero training error implies that every example has been learnt 
exactly, thus 
(6) 
The weights with minimal norm that fulfill this condition are given by the Pseu- 
doinverse (see Hertz et al. 1991), 
/,,= 1 i=1 
Note, that the weights are completely independent of the output function g(h) - 
g,(h). They are the same as in the simplest realizable case, linear perceptron learns 
linear perceptron. 
3.2 Statistical Mechanics 
The calculation of the order parameters can be done by a method from statistical 
mechanics which applies the commonly used replica method. For details about the 
replica approach see Hertz et al. (1991). The solution of the continuous perceptron 
problem can be found in BSs et al. (1993). Since the results of the statistical me- 
chanics calculations are exact only in the thermodynamic limit, i.e. N -+ oo, the 
variable c is the more natural measure. It is defined as the fraction of the number 
of patterns P over the system size N, i.e. c :- PIN. In the thermodynamic limit 
N and P are infinite, but c is still finite. Normally, reasonable system sizes, such 
as N > 100, are already well described by this theory. 
Usually one concentrates on the zero temperature limit, because this implies that 
the training error ET accepts its absolute minimum for every number of presented 
examples P. The corresponding order parameters for the case, linear perceptron 
learns linear student, are 
q = v vfS, r = vfS. (8) 
The zero temperature limit can also be called exhaustive training, since the student 
net is trained until the absolute minimum of ET is reached. 
For small c and high gains ?, i.e levels of nonlinearity, exhaustive training leads 
to overfitting. That means the generalization error EG(a) is not, as it should, 
monotonously decreasing with a. It is one reason for overfitting, that the training 
follows too strongly the examples. The critical gain ?c, which determines whether 
the generalization error EG(c 0 is increasing or decreasing function for small values 
of a, can be determined by a linear approximation. For small a, both order param- 
eters (3) are small, and the student's tanh-function in (4) can be approximated by 
a linear function. This simplifies the equation (4) to the following expression, 
Ea(e) = Ea(0) -  [ 2H(?) - 7], 
with H(?):=/Dz tanh(?z)z. (9) 
Since the function H(?) has an upper bound, i.e. V//r, the critical gain is reached 
if ?c - 2H(%). The numerical solution gives % - 1.3371. If ? is higher, the slope 
of Ea(a) is positive for small a. In the following considerations we will use always 
the gain ? - 5 as an example, since this is an intermediate level of nonlinearity. 
A Realizable Learning Task Which Exhibits Overfitting 221 
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
0.0 
I I I I 
lOO.O 
10.0 ..... 
2.0 .......... 
1.0 ..... 
0.2 0.4 0.6 0.8 1.0 
P/N 
Figure 1: Learning curves E(a) for the problem, tanh-perceptron learns tanh- 
perceptron, for different values of the gain ?. Even in this realizable case, exhaustive 
training can lead to overfitting, if the gain ? is high enough. 
3.3 How to Understand the Emergence of Overfitting 
Here the evaluation of the generalization error in dependence of the order parameters 
r and q is helpful. Fig. 2 shows the function EG(r, q) for r between 0 and 1 and q 
between 0 and 1.27. 
The exhaustive training in realizable cases follows always the line q(r) - ?r 
independent of the actual output function. That means, training is guided only by 
the training error and not by the generalization error. If the gain ? is higher than 
%, the line EG = Ea(0, 0) starts with a lower slope than q(r) = ?r, which results 
in overfitting. 
4 How to Avoid Overfitting 
From Fig. 2 we can guess already that q increases too fast compared to r. Maybe the 
ratio between q and r is better during the training process. So we have to develop 
a description for the training process first. 
4.1 Training Process 
We found already that the order parameters for finite temperatures (T > 0) of 
the statistical mechanics approach are a good description of the training process 
in an unrealizable learning task (BSs 1995). So we use the finite temperature order 
parameters also in this task. These are, again taken from the task 'linear perceptton 
learns linear perceptton', 
V/( ) (1 +a)a-2a r(a,a)= /() a 2 -a (10) 
q(a,a) = ? a 2 -a ' (l+a)a-2a' 
with the temperature dependent variable 
a:= 1+[/3(Q-q)] -x (11) 
222 S. BOS 
q 
6.0 
5.0 
4.0 
3.0 
2.0 
1.o 
: I : I 
...local min.. ..... 
.: abs. mifi. 
; local mih. - ..... :: 
I 
I 
I 
I 
I 
I , 
: 
� . I I 
� I ' I 
0.0 
0.0 0.2 0.4 0.6 0.8 1.0 
Figure 2: Contour plot of Ea(r,q) defined by (4), the generalization error as a 
function of the two order parameters. Starting from the minimum EG -- 0 at (r, q) -- 
(1, 5) the contour lines for EG -- 0.1, 0.2, ..., 0.8 are given (dotted lin
