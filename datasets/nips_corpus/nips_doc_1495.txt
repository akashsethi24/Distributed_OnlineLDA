Restructuring Sparse High Dimensional Data for 
Effective Retrieval 
Charles Lee Isbell, Jr. 
AT&T Labs 
180 Park Avenue Room A255 
Florham Park, NJ 07932-0971 
Paul Viola 
Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
Abstract 
The task in text retrieval is to find the subset of a collection of documents relevant 
to a user's information request, usually expressed as a set of words. Classically, 
documents and queries are represented as vectors of word counts. In its simplest 
form, relevance is defined to be the dot product between a document and a query 
vector-a measure of the number of common terms. A central difficulty in text 
retrieval is that the presence or absence of a word is not sufficient to determine 
relevance to a query. Linear dimensionality reduction has been proposed as a tech- 
nique for extracting underlying structure from the document collection. In some 
domains (such as vision) dimensionality reduction reduces computational com- 
plexity. In text retrieval it is more often used to improve retrieval performance. 
We propose an alternative and novel technique that produces sparse represen- 
tations constructed from sets of highly-related words. Documents and queries 
are represented by their distance to these sets, and relevance is measured by the 
number of common clusters. This technique significantly improves retrieval per- 
formance, is efficient to compute and shares properties with the optimal linear 
projection operator and the independent components of documents. 
1 Introduction 
The task in text retrieval is to find the subset of a collection of documents relevant to a user's infor- 
mation request, usually expressed as a set of words. Naturally, we would like to apply techniques 
from natural language understanding to this problem. Unfortunately, the sheer size of the data to be 
represented makes this difficult. We wish to process tens or hundreds of thousands of documents, 
each of which may contain hundreds of thousands of different words. It is clear that any useful 
approach must be time and space efficient. 
Following (Salton, 1971), we adopt a modified Vector Space Model (VSM) for document represen- 
tation. A document is a vector where each dimension is a count of occurrences for a different word I . 
In practice, suffixes are removed and counts are re-weighted by some function of their natural frequency 
Restructuring Sparse High Dimensional Data for Effective Retrieval 481 
Afdca national football 
Mandela South league college 
Figure 1: A Model of Word Generation. Independent topics give rise to specific words words 
according an unknown probability distribution (Line thickness indicates the likelihood of generating 
a word). 
A collection of documents is a matrix, D, where each column is a document vector di. Queries are 
similarly represented. 
We propose a topic based model for the generation of words in documents. Each document is 
generated by the interaction of a set of independent hidden random variables called topics. When a 
topic is active it causes words to appear in documents. Some words are very likely to be generated 
by a topic and others less so. Different topics may give rise to some of the same words. The final set 
of observed words results from a linear combination of topics. See Figure 1 for an example. 
In this view of word generation, individual words are only weak indicators of underlying topics. 
Our task is to discover from data those collections of words that best predict the (unknown) under- 
lying topics. The assumption that words are neither independent of one another or conditionally 
independent of topics motivates our belief that this is possible. 
Our approach is to construct a set of linear operators which extract the independent topic structure of 
documents. We have explored different algorithms for discovering these operators include indepen- 
dent components analysis (Bell and Sejnowski, 1995). The inferred topics are then used to represent 
and compare documents. 
Below we describe our approach and contrast it with Latent Semantic Indexing (LSI), a technique 
that also attempts to linearly transform the documents from word space into one more appropriate 
for comparison (Hull, 1994; Deerwester et al., 1990). We show that the LSI transformation has very 
different properties than the optimal linear transformation. We characterize some of these properties 
and derive an unsupervised method that searches for them. Finally, we present experiments demon- 
strating the robustness of this method and describe several computational and space advantages. 
2 The Vector Space Model and Latent Semantic Indexing 
The similarity between two documents using the VSM model is their inner product, diTdj. Queries 
are just short documents, so the relevance of documents to a query, q, is Drq. There are several 
advantages to this approach beyond its mathematical simplicity. Above all, it is efficient to compute 
and store the word counts. While the word-document matrix has a very large number of potential 
entries, most documents do not contain very many of the possible words, so it is sparsely populated. 
Thus, algorithms for manipulating the matrix only require space and time proportional to the average 
number of different words that appear in a document, a number likely to be much smaller than the 
full dimensionality of the document matrix (in practice, non-zero elements represent about 2% of 
the total number of elements). Nevertheless, VSM makes an important tradeoff by sacrificing a great 
deal of document structure, losing context that may disambiguate meaning. 
Any text retrieval system must overcome the fundamental difficulty that the presence or absence 
of a word is insufficient to determine relevance. This is due to two intrinsic problems of natural 
(Frakes and Baeza-Yates, 1992). We incorporate these methods; however, such details are unimportant for this 
discussion. 
482 C. L. Isbell and P. 14ola 
language: synonyrny and polysemy. Synonymy refers to the fact that a single underlying concept 
can be represented by many different words (e.g. car and automobile refer to the same class 
of objects). Polysemy refers to the fact that a single word can refer to more than one underlying 
concept (e.g. apple is both a fruit and a computer company). Sy'nonymy results in false negatives 
and polysemy results in false positives. 
Latent semantic indexing is one proposal for addressing this problem. LSI constructs a smaller 
document matrix that retains only the most important information from the original, by using the 
Singular Value Decomposition (SVD). Briefly, the SVD of a matrix D is: USV T where U and V 
contain orthogonal vectors and $ is diagonal (see (Golub and Loan, 1993) for further properties and 
algorithms). Note that the co-occurrence matrix, DD T, can be written as $,.2sr; U contains the 
eigenvectors of the co-occurrence matrix while the diagonal elements of $ (referred to as singular 
values) contain the square roots of their corresponding eigenvalues. The eigenvectors with the largest 
eigenvalues capture the axes of largest variation in the data. 
In LSI, each document is projected into a lower dimensional space/) = [1 0[D where k and/_k 
which contain only the largest k singular values and the corresponding eigenvectors, respectively. 
The resulting document matrix is of smaller size but still provably represents the most variation in the 
original matrix. Thus, LSI represents documents as linear combinations of orthogonal features. It is 
hoped that these features represent meaningful underlying topics present in the collection. Queries 
are also projected into this space, so the relevance of documents to a query is DrO-2Orq. 
This type of dimensionality reduction is very similar to principal components analysis (PCA), which 
has been used in other domains, including visual object recognition (Turk and Pentland, 1991). In 
practice, there is some evidence to suggest that LSI can improve retrieval performance; however, it 
is often the case that LSI improves text retrieval performance by only a small amount or not at all 
(see (Hull, 1994) and (Deerwester et al., 1990) for a discussion). 
3 Do Optimal Projections for Retrieval Exist? 
Hypotheses abound for the success of LSI, including: i) LSI removes noise from the document 
set; ii) LSI finds words that are synonyms; iii) LSI finds clusters of documents. Whatever it does, 
LSI operates without knowledge of the queries that will be presented to the system. We could 
T T 
instead attempt a supervised approach, searching for a matrix P such that D PP q results in large 
values for documents in D that are known to be relevant for a particular query, q. The choice for 
the structure of P embodies assumptions about the structure of D and q and what it means for 
documents and queries to be related. 
For example, imagine that we are given a collection of documents, D, and queries, Q. For each query 
we are told which documents are relevant. We can use this information to construct an optimal P 
such that: DTppTQ  R, where Rij equals 1 if document i is relevant to query j, and 0 otherwise. 
We find P in two steps. First we find an X minimizing [IDrXQ - lIF, where [1 ï¿½ [IF denotes 
the Frobenius norm of a matrix 2. Second, we find P by decomposing X into ppr. Unfortunately, 
this may not be simple. The matrix ppr has properties that are not necessarily shared by X. In 
T 
particular, while PP is symmetric, there is no guarantee that X will be (in our experiments X is 
far from symmetric). We can however take SVD of X = UxSxVf, using matrix Ux to project the 
documents and V to project the queries. 
We can now compare LSI's projection axes, U with the optimal U computed as above. One measure 
of comparison is the distribution of documents as projected onto these axes. Figure 2a shows the 
distribution of Medli
