652 
Scaling Properties of Coarse-Coded Symbol Memories 
Ronald Rosenfeld 
David S. Touretzky 
Computer Science Department 
Carnegie Mellon University 
Pittsburgh, Pennsylvania 15213 
Abstract
Coarse-coded symbol memories have appeared in several neural network 
symbol processing models. In order to determine how these models would scale, one 
must first have some understanding of the mathematics of coarse-coded representa- 
tions. We define the general structure of coarse-coded symbol memories and derive 
mathematical relationships among their essential parameters: memort size, slmbol-set 
size and capacitor. The computed capacity of one of the schemes agrees well with actual 
measurements of the coarse-coded working memory of DCPS, Touretzky and Hinton's 
distributed connectionist production system. 
1 Introduction 
A dstributed representation is a memory scheme in which each entity (concept, symbol) 
is represented by a pattern of activity over many units [3]. If each unit participates 
in the representation of many entities, it is said to be coarsell tuned, and the memory 
itself is called a coarse-coded memorl. 
Coarse-coded memories have been used for storing symbols in several neural network 
symbol processing models, such as Touretzky and Hinton's distributed connectionist 
production system DCPS [8,9], Touretzky's distributed implementation of linked list 
structures on a Boltzmann machine, BoltzCONS [10], and St. John and McClelland's 
PDP model of case role defaults [6]. In all of these models, memory capacity was mea- 
sured empirically and parameters were adjusted by trial and error to obtain the desired 
behavior. We are now able to give a mathematical foundation to these experiments by 
analyzing the relationships among the fundamental memory parameters. 
There are several paradigms for coarse-coded memories. In a feature-based repre- 
sentation, each unit stands for some semantic feature. Binary units can code features 
with binary values, whereas more complicated units or groups of units are required to 
code more complicated features, such as multi-valued properties or numerical values 
from a continuous scale. The units that form the representation of a concept define 
an intersection of features that constitutes that concept. Similarity between concepts 
composed of binary features can be measured by the Hamming distance between their 
representations. In a neural network implementation, relationships between concepts 
are implemented via connections among the units forming their representations. Certain 
types of generalization phenomena thereby emerge automatically. 
A different paradigm is used when representing points in a multidimensional contin- 
uous space [2,3]. Each unit encodes values in some subset of the space. Typically the 
American Institute of Physics 1988 
653 
subsets are hypercubes or hyperspheres, but they may be more coarsely tuned along 
some dimensions than others [1]. The point to be represented is in the subspace formed 
by the intersection of all active units. As more units are turned on, the accuracy of the 
representation improves. The density and degree of overlap of the units' receptive fie]ds 
determines the system's resolution [7]. 
Yet another paradigm for coarse-coded memories, and the one we will deal with 
exclusive]y, does not involve features. Each concept, or symbol, is represented by an 
arbitrary subset of the units, called its pattern. Unlike in feature-based representations, 
the units in the pattern bear no relationship to the meaning of the symbol represented. A 
symbol is stored in memory by turning on all the units in its pattern. A symbol is deemed 
present if all the units in its pattern are active.  The receptive field of each unit is defined 
as the set of all symbols in whose pattern it participates. We call such memories coarse- 
coded symbol memories (CCSMs). We use the term symbol instead of concept to 
emphasize that the internal structure of the entity to be represented is not involved in 
its representation. In CCSMs, a short Hamming distance between two symbols does 
not imply semantic similarity, and is in general an undesirable phenomenon. 
The efficiency with which CCSMs handle sparse memories is the major reason they 
have been used in many connectionist systems, and hence the major reason for studying 
them here. The unit-sharing strategy that gives rise to efficient encoding in CCSMs 
is also the source of their major weakness. Symbols share units with other symbols. 
As more symbols are stored, more and more of the units are turned on. At some 
point, some symbol may be deemed present in memory because all of its units are 
turned on, even though it was not explicitly stored: a ghost is born. Ghosts are 
an unwanted phenomenon arising out of the overlap among the representations of the 
various symbols. The emergence of ghosts marks the limits of the system's capacity: 
the number of symbols it can store simultaneously and reliably. 
2 Definitions and Fundamental Parameters 
A coarse coded symbol memory in its most general form consists of.' 
set of N binary state units. 
An alphabet of c symbols to be represented. Symbols in this context are atomic 
entities: they have no constituent structure. 
A memory scheme, which is a function that maps each symbol to a subset of 
the units - its pattern. The receptive field of a unit is defined as the set of 
all symbols to whose pattern it belongs (see Figure 1). The exact nature of the 
This criterion can be generalized by introducing a visibility threshold: a fraction of 
the pattern that should be on in order for a symbol to be considered present. Our analy- 
sis deals only with a visibility criterion of 100%, but can be generalized to accommodate 
noise. 
654 
Figure 1: A memory scheme (N = 6, c -- 8) defined in terms of units Ui and symbols 
Sj. The columns are the symbols' patterns. The rows are the units' receptive fields. 
memory scheme mapping determines the properties of the memory, and is the 
central target of our investigation. 
As symbols are stored, the memory fills up and ghosts eventually appear. It is not 
possible to detect a ghost simply by inspecting the contents of memory, since there is 
no general way of distinguishing a symbol that was stored from one that emerged out of 
overlaps with other symbols. (It is sometimes possible, however, to conclude that there 
are no ghosts.) Furthermore, a symbol that emerged as a ghost at one time may not be 
a ghost at a later time if it was subsequently stored into memory. Thus the definition 
of a ghost depends not only on the state of the memory but also on its history. 
Some memory schemes guarantee that no ghost will emerge as long as the number of 
symbols stored does not exceed some specified limit. In other schemes, the emergence 
of ghosts is an ever-present possibility, but its probability can be kept arbitrarily low 
by adjusting other parameters. We analyze systems of both types. First, two more bits 
of notation need to be introduced: 
Pghost: Probability of a ghost. The probability that at least one ghost will appear 
after some number of symbols have been stored. 
k: Capacity. The maximum number of symbols that can be stored simultaneously 
before the probability of a ghost exceeds a specified threshold. If the threshold is 
0, we say that the capacity is guaranteed. 
A localist representation, where every symbol is represented by a single unit and 
every unit is dedicated to the representation of a single symbol, can now be viewed as 
a special case of coarse-coded memory, where k -- N -- c and pghost -- 0. Localist 
representations are well suited for memories that are not sparse. In these cases, coarse- 
coded memories are at a disadvantage. In designing coarse-coded symbol memories we 
are interested in cases where k  N  c. The permissible probability for a ghost in 
these systems should be low enough so that its impact can be ignored. 
655 
3 Analysis of Four Memory Schemes 
3.1 Bounded Overlap (guaranteed capacity) 
If we want to construct the memory scheme with the largest possible c (given N and 
k) while guaranteeing Pghost - 0, the problem can be stated formally as: 
Given a set of size N, find the largest collection of subsets of it such that no 
union of k such subsets subsumes any other subset in the collection. 
This is a well known problem in Coding Theory, in slight disguise. Unfortunately, 
no complete analytical solution is known. We therefore simplify our task and consider 
only systems in which all symbols are represented by the same number of units (i.e. all 
patterns are of the same size). In mathematica] terms, we restrict ourselves to constant 
weight codes. The problem then becomes: 
Given a set of size N, find the largest collection of subsets of size exactlt 
L such that no union of k such subsets subsumes any other subset in the 
collection. 
There are no known complete analytical solutions for the size of the largest collection 
of patterns even when the patterns are of a fixed size. Nor is any efficient procedure 
for constructing such a collection known. We therefore simplify the problem further. 
We now restrict our consideration to patterns whose pairwise overlap is bounded by a 
given number. For a given pattern size L and desired capacity k, we require that no 
two patterns overlap in more than rn units, where: 
Memory schemes that obey this constraint are guaranteed a capacity of at least k 
symbols, since any k symbols taken together can overlap at most L- 1 units in the 
pattern of any other symbol - one unit short of making it a ghost. Based on this 
constraint, our mathematical problem now becomes: 
Given a set of size N, find the largest collection of subsets of size exactly L 
such that the intersection of any two such subsets is of size _ rn (where rn 
is given by equation 1.) 
Coding theory has yet to produce a complete solution to this problem
