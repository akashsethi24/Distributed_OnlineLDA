Data Visualization and Feature Selection: 
New Algorithms for Nongaussian Data 
Howard Hua Yang and John Moody 
Oregon Graduate Institute of Science and Technology 
20000 NW, Walker Rd., Beaverton, OR97006, USA 
hyang@ece.ogi.edu, moody@cse.ogi.edu, FAX:503 7481406 
Abstract 
Data visualization and feature selection methods are proposed 
based on the joint mutual information and ICA. The visualization 
methods can find many good 2-D projections for high dimensional 
data interpretation, which cannot be easily found by the other ex- 
isting methods. The new variable selection method is found to be 
better in eliminating redundancy in the inputs than other methods 
based on simple mutual information. The efficacy of the methods 
is illustrated on a radar signal analysis problem to find 2-D viewing 
coordinates for data visualization and to select inputs for a neural 
network classifier. 
Keywords: feature selection, joint mutual information, ICA, vi- 
sualiz ation, classification. 
1 INTRODUCTION 
Visualization of input data and feature selection are intimately related. A good 
feature selection algorithm can identify meaningful coordinate projections for low 
dimensional data visualization. Conversely, a good visualization technique can sug- 
gest meaningful features to include in a model. 
Input variable selection is the most important step in the model selection process. 
Given a target variable, a set of input variables can be selected as explanatory 
variables by some prior knowledge. However, many irrelevant input variables cannot 
be ruled out by the prior knowledge. Too many input variables irrelevant to the 
target variable will not only severely complicate the model selection/estimation 
process but also damage the performance of the final model. 
Selecting input variables after model specification is a model-dependent approach[6]. 
However, these methods can be very slow if the model space is large. To reduce the 
computational burden in the estimation and selection processes, we need model- 
independent approaches to select input variables before model specification. One 
such approach is 5-Test [7]. Other approaches are based on the mutual information 
(MI) [2, 3, 4] which is very effective in evaluating the relevance of each input variable, 
but it fails to eliminate redundant variables. 
In this paper, we focus on the model-independent approach for input variable selec- 
688 H. H. Yang and d.. Moody 
tion based on joint mutual information (JMI). The increment from MI to JMI is the 
conditional MI. Although the conditional MI was used in [4] to show the monotonic 
property of the MI, it was not used for input selection. 
Data visualization is very important for human to understand the structural re- 
lations among variables in a system. It is also a critical step to eliminate some 
unrealistic models. We give two methods for data visualization. One is based on 
the JMI and another is based on Independent Gomponent Analysis (IGA). Both 
methods perform better than some existing methods such as the methods based on 
PGA and canonical correlation analysis (GGA) for nongaussian data. 
2 Joint mutual information for input/feature selection 
Let Y be a target variable and Xi's are inputs. The relevance of a single input is 
measured by the MI 
Y) = 
where K(pllq) is the Kullback-Leibler divergence of two probability functions p and 
q defined by K (p(x)llq(x)) = - p(x) log q). 
The relevance of a set of inputs is defined by the joint mutual information 
I (Xg, � � �, Xk; Y) - g (p(zg, � � �, zk, y)IlP(zi, � � �, zn)p(y) ). 
Given two selected inputs zj and z,, the conditional MI is defined by 
I(Xi ; SlXj,X,) -  p(xj, x,)K (p(xi, ylx, x,)llp(xilx, x,)p(ylx, x,) ). 
j,33k 
Similarly define I(X;Y]Xj,...,X,) conditioned on more than two variables. 
The conditional MI is always non-negative since it is a weighted average of the 
Kullback-Leibler divergence. It has the following property 
.l'(X1,',Xn-l,Xn;V)- .l'(X1,...,Xn_l;V) : .l'(Xn;VlX1,',Xn-1 )  O. 
Therefore, I(Xx,...,X,_,X,;Y) >_ I(Xx,...,X,_x;Y), i.e., adding the variable 
X, will always increase the mutual information. The information gained by adding 
a variable is measured by the conditional MI. 
When X, and Y are conditionally independent given Xx, � �., X_ x, the conditional 
MI between X and Y is 
=0, 
so X, provides no extra information about Y when X,..., X,_ are known. In 
particular, when X, is a function of X,..-, X,_x, the equality (1) holds. This is 
the reason why the joint MI can be used to eliminate redundant inputs. 
The conditional MI is useful when the input variables cannot be distinguished by 
the mutual information I(X;Y). For example, assume I(Xt;Y) = I(X2;Y) = 
I(Xa;Y), and the problem is to select (zx,z2),(zx,za) or (z2,za). Since 
s) - r(x,x3; s) = slx) - r(x3; YIX), 
we should choose (zi,z2) rather than (zi,za) if I(X2;Y]Xi) > I(Xa;YIXi). Oth- 
erwise, we should choose (zl, za). All possible comparisons are represented by a 
binary tree in Figure 1. 
To estimate I(X,...,X&;Y), we need to estimate the joint probability 
p(zi,...,z,, y). This suffers from the curse of dimensionality when k is large. 
Data Visualization and Feature Selection 689 
Sometimes, we may not be able to estimate high dimensional MI due to the sample 
shortage. Further work is needed to estimate high dimensional joint MI based on 
parametric and non-parametric density estimations, when the sample size is not 
large enough. 
In some real world problems such as mining large data bases and radar pulse classi- 
fication, the sample size is large. Since the parametric densities for the underlying 
distributions are unknown, it is better to use non-parametric methods such as his- 
tograms to estimate the joint probability and the joint MI to avoid the risk of 
specifying a wrong or too complicated model for the true density function. 
I(X2;YIXI)>-- 3(X ;YIXI;YIXI) 
(xl, x2) 
I(X I,YIX2)>=IOC3;YIX2) I;YIX2)<I(X3;YIX2) 
(xl ,x2) (x2,x3) 
(xl,x3) 
l(X 1 ;YIX3)>=I(X2;Y[X3/YIX3)<IOC2;Y[X3) 
(xl,x3) (x2,x3) 
Figure 1: Input selection based on the conditional MI. 
In this paper, we use the joint mutual information I(Xi,Xj;Y) instead of the 
mutual information I(Xi; Y) to select inputs for a neural network classifier. Another 
application is to select two inputs most relevant to the target variable for data 
visualization. 
3 Data visualization methods 
We present supervised data visualization methods based on joint MI and discuss 
unsupervised methods based on ICA. 
The most natural way to visualize high-dimensional input patterns is to dis- 
play them using two of the existing coordinates, where each coordinate corre- 
sponds to one input variable. Those inputs which are most relevant to the tar- 
get variable corresponds the best coordinates for data visualization. Let (i*, j*) = 
arg max(i,j)I(Xi , Xj; Y). Then, the coordinate axes (xi., xj.) should be used for 
visualizing the input patterns since the corresponding inputs achieve the maximum 
joint MI. To find the maximum I(Xi., Xj. ]Y), we need to evaluate every joint MI 
I(Xi,Xj;Y) for i < j. The number of evaluations is O(n2). 
Noticing that I(Xi,Xj;Y) = I(Xi;Y) + I(Xj;YIXi), we can first maximize the 
MI I(Xi; Y), then maximize the conditional MI. This algorithm is suboptimal, but 
only requires n- 1 evaluations of the joint MIs. Sometimes, this is equivalent to 
exhaustive search. One such example is given in next section. 
Some existing methods to visualize high-dimensional patterns are based on dimen- 
sionality reduction methods such as PCA and CCA to find the new coordinates to 
display the data. The new coordinates found by PCA and CCA are orthogonal in 
Euclidean space and the space with Mahalanobis inner product, respectively. How- 
ever, these two methods are not suitable for visualizing nongaussian data because 
the projections on the PCA or CCA coordinates are not statistically independent 
for nongaussian vectors. Since the JMI method is model-independent, it is better 
for analyzing nongaussian data. 
690 H. H. Yang and J. Moody 
Both CCA and maximum joint MI are supervised methods while the PCA method 
is unsupervised. An alternative to these methods is ICA for visualizing clusters [5]. 
The ICA is a technique to transform a set of variables into a new set of variables, 
so that statistical dependency among the transformed variables is minimized. The 
version of ICA that we use here is based on the algorithms in [1, 8]. It discovers 
a non-orthogonal basis that minimizes mutual information between projections on 
basis vectors. We shall compare these methods in a real world application. 
4 Application to Signal Visualization and Classification 
4.1 Joint mutual information and visualization of radar pulse patterns 
Our goal is to design a classifier for radar pulse recognition. Each radar pulse 
pattern is a 15-dimensional vector. We first compute the joint MIs, then use them 
to select inputs for the visualization and classification of radar pulse patterns. 
A set of radar pulse patterns is denoted by D = {(a: i, yi) : i = 1,.-., N} which 
consists of patterns in three different classes. Here, each a: i E -R 5 and each yi  
{1,2,3}. 
1 
0.4 
0. 
0 
' 't:::' , . 
0 
10 15 I 2 3 4 5 6 7 8 9 10 11 12 13 14 15 
bundle numbel 
(b) 
Figure 2: (a) MI vs conditional MI for the radar pulse data; maximizing the MI then 
the conditional MI with O(n) evaluations gives I(Xi, Xj ;Y) - 1.201 bits. (b) The 
joint MI for the radar pulse data; maximizing the joint MI gives I(X., Xj.;Y) = 
1.201 bits with O(n 2) evaluations of the joint MI. (i,j) = (i', j') in this case. 
Let i = arg maxiI(Xi;Y ) and j = arg maxjviI(Xj;YlXil). From Figure 2(a), 
we obtain (i,j) = (2,9) and I(Xi,Xj;Y) = I(X,;Y) + I(Xj;YIXi) = 
1.201 bits. If the number of total inputs is n, then the number of evaluations for 
computing the mutual information �(Xi; Y) and the conditional mut
