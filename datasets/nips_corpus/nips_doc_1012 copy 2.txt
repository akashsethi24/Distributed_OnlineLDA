Worst-case Loss Bounds 
for Single Neurons 
David P. Helmbold 
Department of Computer Science 
University of California, Santa Cruz 
Santa Cruz, CA 95064 
USA 
Jyrki Kivinen 
Department of Computer Science 
P.O. Box 26 (Teollisuuskatu 23) 
FIN-00014 University of Helsinki 
Finland 
Manfred K. Warmuth 
Department of Computer Science 
University of California, Santa Cruz 
Santa Cruz, CA 95064 
USA 
Abstract 
We analyze and compare the well-known Gradient Descent algo- 
rithm and a new algorithm, called the Exponentiated Gradient 
algorithm, for training a single neuron with an arbitrary transfer 
function. Both algorithms are easily generalized to larger neural 
networks, and the generalization of Gradient Descent is the stan- 
dard back-propagation algorithm. In this paper we prove worst- 
case loss bounds for both algorithms in the single neuron case. 
Since local minima make it difficult to prove worst-case bounds 
for gradient-based algorithms, we must use a loss function that 
prevents the formation of spurious local minima. We define such 
a matching loss function for any strictly increasing differentiable 
transfer function and prove worst-case loss bound for any such 
transfer function and its corresponding matching loss. For exam- 
ple, the matching loss for the identity function is the square loss 
and the matching loss for the logistic sigmoid is the entropic loss. 
The different structure of the bounds for the two algorithms indi- 
cates that the new algorithm out-performs Gradient Descent when 
the inputs contain a large number of irrelevant components. 
310 D.P. HELMBOLD, J. KIVINEN, M. K. WARMUTH 
1 INTRODUCTION 
The basic element of a neural network, a neuron, takes in a number of real-valued 
input variables and produces a real-valued output. The input-output mapping of 
a neuron is defined by a weight vector w  R v, where N is the number of input 
variables, and a transfer function 4. When presented with input given by a vector 
x 6 R v, the neuron produces the output ) = O(w. x). Thus, the weight vector 
regulates the influence of each input variable on the output, and the transfer function 
can produce nonlinearities into the input-output mapping. In particular, when the 
transfer function is the commonly used logistic function, 4(p) = 1/(1 + e-P), the 
outputs are bounded between 0 and 1. On the other hand, if the outputs should 
be unbounded, it is often convenient to use the identity function as the transfer 
function, in which case the neuron simply computes a linear mapping. In this 
paper we consider a large class of transfer functions that includes both the logistic 
function and the identity function, but not discontinuous (e.g. step) functions. 
The goal of learning is to come up with a weight vector w that produces a 
desirable input-output mapping. This is achieved by considering a sequence 
$ = ((x,y),...,(xt,yt)) of ecamples, where for t = 1,...,� the value yt  R 
is the desired output for the input vector xt, possibly distorted by noise or other 
errors. We call xt the tth instance and yt the tth outcome. In what is often called 
batch learning, all � examples are given at once and are available during the whole 
training session. As noise and other problems often make it impossible to find a 
weight vector w that would satisfy b(w. xt) = yt for all t, one instead introduces a 
loss function L, such as the square loss given by L(y,)) = (y- ))2/2, and finds a 
weight vector w that minimizes the empirical loss (or training error) 
Loss(w, $): ] L(yt, (w. xt)) (1) 
t=l 
With the square loss and identity transfer function ;b(p) = p, this is the well-known 
linear regression problem. When ;b is the logistic function and L is the entropic loss 
given by L(y,)) = yln(y/)) + (1 - y)ln((1 - y)/(1 - ))), this can be seen as a 
special case of logistic regression. (With the entropic loss, we assume 0 < yt, )t < 1 
for all t, and use the convention 0 In 0 = 0 In(0/0) = 0.) 
In this paper we use an on-line prediction (or life-long learning) approach to the 
learning problem. It is well known that on-line performance is closely related to 
batch learning performance (Littlestone, 1989; Kivinen and Warmuth, 1994). 
Instead of receiving all the examples at once, the training algorithm begins with 
some fixed start vector w, and produces a sequence Wl,. �., wt+ of weight vectors. 
The new weight vector wt+ is obtained by applying a simple update rule to the 
previous weight vector wt and the single example (xt, yt). In the on-line prediction 
model, the algorithm uses its tth weight vector, or hypothesis, to make the prediction 
)t = ;b(wt 'xt). The training algorithm is then charged a loss L(yt, .Or) for this tth 
trial. The performance of a training algorithm A that produces the weight vectors 
wt on an example sequence $ is measured by its total (cumulative) loss 
Loss(A, $) =  L(yt, qS(wt. xt)) (2) 
t=l 
Our main results are bounds on the cumulative losses for two on-line prediction 
algorithms. One of these is the standard Gradient Descent (GD) algorithm. The 
other one, which we call EG + is also based on the gradient but uses it in a different 
, 
Worst-case Loss Bounds for Single Neurons 311 
manner than GD. The bounds are derived in a worst-case setting: we make no as- 
sumptions about how the instances are distributed or the relationship between each 
instance xt and its corresponding outcome yt. Obviously, some assumptions are 
needed in order to obtain meaningful bounds. The approach we take is to compare 
the total losses, Loss(GD, S) and Loss(EG+,S), to the least achievable empirical 
loss, infw Loss(w, $). If the least achievable empirical loss is high, the dependence 
between the instances and outcomes in $ cannot be tracked by any neuron using 
the transfer function, so it is reasonable that the losses of the algorithms are also 
high. More interestingly, if some weight vector achieves a low empirical loss, we 
also require that the losses of the algorithms are low. Hence, although the algo- 
rithms always predict based on an initial segment of the example sequence, they 
must perform almost as well as the best fixed weight vector for the whole sequence. 
The choice of loss function is crucial for the results that we prove. In particular, 
since we are using gradient-based algorithms, the empirical loss should not have spu- 
rious local minima. This can be achieved for any differentiable increasing transfer 
function q5 by using the loss function L defined by 
,-() 
3-() 
(3) 
For y < ) the value L(y, )) is the area in the z x qS(z) plane below the function 
65(z), above the line 6(z) = y, and to the left of the line z = qS-()). We call L the 
matching loss function for transfer function qS, and will show that for any example 
sequence $, if L = L then the mapping from w to Loss(w, $) is convex. For 
example, if the transfer function is the logistic function, the matching loss function 
is the entropic loss, and if the transfer function is the identity function, the matching 
loss function is the square loss. Note that using the logistic activation function with 
the square loss can lead to a very large number of local minima (Auer et al., 1996). 
Even in the batch setting there are reasons to use the entropic loss with the logistic 
transfer function (see, for example, Solla et al., 1988). 
How much our bounds on the losses of the two algorithms exceed the least empirical 
loss depends on the maximum slope of the transfer function we use. More impor- 
tantly, they depend on various norms of the instances and the vector w for which 
the least empirical loss is achieved. As one might expect, neither of the algorithms 
is uniformly better than the other. Interestingly, the new EG + algorithm is better 
when most of the input variables are irrelevant, i.e., when some weight vector w 
with wi = 0 for most indices i has a low empirical loss. On the other hand, the 
GD algorithm is better when the weight vectors with low empirical loss have many 
nonzero components, but the instances contain many zero components. 
The bounds we derive concern only single neurons, and one often combines a number 
of neurons into a multilayer feedforward neural network. In particular, applying 
the Gradient Descent algorithm in the multilayer setting gives the famous back 
propagation algorithm. Also the EG + algorithm, being gradient-based, can easily 
be generalized for multilayer feedforward networks. Although it seems unlikely 
that our loss bounds will generalize to multilayer networks, we believe that the 
intuition gained from the single neuron case will provide useful insight into the 
relative performance of the two algorithms in the multilayer case. Furthermore, the 
EG + algorithm is less sensitive to large numbers of irrelevant attributes. Thus it 
might be possible to avoid multilayer networks by introducing many new inputs, 
each of which is a non-linear function of the original inputs. Multilayer networks 
remain an interesting area for future study. 
Our work follows the path opened by Littlestone (1988) with his work on learning 
312 D.P. HELMBOLD, J. KIVINEN, M. K. WARMUTH 
thresholded neurons with sparse weight vectors. More immediately, this paper is 
preceded by results on linear neurons using the identity transfer function (Cesa- 
Bianchi et al., 1996; Kivinen and Warmuth, 1994). 
2 THE ALGORITHMS 
This section describes how the Gradient Descent training algorithm and the new 
Exponentiated Gradient training algorithm update the neuron's weight vector. 
For the remainder of this paper, we assume that the transfer function b is increasing 
and differentiable, and Z is a constant such that ;b'(p) < Z holds for all p 6 R. For 
the loss function L defined by (3) we have 
� x)) 
Owi -- (b(w. x) - y)xi (4) 
Treating L(y, b(w.x)) for fixed x and y as a function of w, we see that the Hessian 
H of the function is given by Hi1 = q(w.x)xixj. Then vT
