767 
SELF-ORGANIZATION OF ASSOCIATIVE DATABASE 
AND ITS APPLICATIONS 
Hisashi Suzuki and Suguru Arimoto 
Osaka University, Toyonaka, Osaka 560, Japan 
ABSTRACT 
An efficient method of self-organizing associative databases is proposed together with 
applications to robot eyesight systems� The proposed databases cn associate ny input 
with some output. In the first half prt of discussion, n algorithm of self-organization is 
proposed. From an aspect of hardware, it produces a new style of neural network. In the 
latter half part, an pplicability to handwritten letter recognition and that to an autonomous 
mobile robot system are demonstrated. 
INTRODUCTION 
Let  mpping f: X --, Y be given. Here, X is a finite or infinite set, and Y is another 
finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly 
from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some 
estimate f: X --, Y of f to make small, the estimation error in some measure. 
Usually we say that: the faster the decrease of estimation error with increase of the num- 
ber of samples, the better the learning machine. However, such expression on performance 
is incomplete. Since, it lacks consideration on the candidates of f of f assumed prelimi- 
narily. Then, how should we find out good learning machines? To clarify this conception, 
let us discuss for a while on some types of learning machines. And, let us advance the 
understanding of the self-organization of associative database. 
� Parameter Type 
An ordinary type of learning machine assumes an equation relatin x's and y's with 
parameters being indefi^nite,namely, a structure of f. It is equivalent to define implicitly a 
set 1  of candidates of f. (F is some subset of mappings from X to Y.) And, it computes 
values of the parameters based on the observed samples. We call such type a parameter 
type. 
For a learning machine defined well, if 1   f, f pproaches f as the number of samples 
increases. In the alternative case, however, some estimation error remains eternally. Thus, 
a problem of designing  learning machine returns to find out a proper structure of f in this 
sense. 
On the other hand, the assumed structure of f is demanded to be as compact as possible 
to achieve a fast learning. In other words, the number of parameters should be small. Since, 
if the parameters are few, some f can be uniquely determined even though the observed 
samples are few. However, this demand of being proper contradicts to that of being compact. 
Consequently, in the parameter type, the better the compactness of the assumed structure 
that is proper, the better the learning machine. This is the most elementary conception 
when we design learning ma&ines. 
� Universality and Ordinary Neural Networks 
Now suppose that a sufficient knowledge on f is given though j itself is unknown. In 
this case, it is comparatively easy to find out proper and compact structures of j. In the 
alternative case, however, it is sometimes difficult. A possible solution is to give up the 
compactness and assume an almighty structure that can cover various f's. A combination 
of some orthogonal bases of the infinite dimension is such a structure. Neural networks '2 
are its approximations obtained by truncating finitely the dimension for implementation� 
@ American Institute of Physics 1988 
768 
A min topic in designing neural networks is to establish such desirable structures of f. 
This work includes developing practical procedures that compute values of coefiScients from 
the observed samples. Such discussions are fiourishing since 1980 while many efiScient meth- 
ods have been proposed. Recently, even hardware units computing coefiScients in pardlid 
for speed-up are sold, e.g., ANZA, Mark III, Odyssey and 
Nevertheless, in neural networks, there always exists a danger of some error remaining 
eternally in estimating f. Precisely speaking, suppose that a combination of the bases of a 
finite number can define a structure of f essentially. In other words, suppose that P /, or 
f is located near 1 '. In such case, the estimation error is none or negligible. However, if f 
is distant from i', the estimation error never becomes negligible. Indeed, many researches 
report that the following situation appears when f is too complex. Once the estimation 
error converges to some value (> 0) as the number of samples increases, it decreases hardly 
even though the dimension is heighten. This property sometimes is a considerable defect of 
neural networks. 
� Recursive Type 
The recursive type is founded on another methodology of learning that should be as 
follows. At the initial stage of no sample, the set F0 (instead of notation ) of candidates 
of f' equals to the set of all mappings from X to Y. After observing the first sample 
(x,,) � X x Y, & is reduced to  so that f(x) =  for any f � . Aaer observing 
th eona sample (xa, ya) � X x Y, g is further reduced to Pa so that f() = yx and 
f(xa) = ya for any f � P. Thus, the candidate set P becom gruy smM1  observaaon 
of samples proceeds. The f after observing i-staples, which we write fi, is one of the most 
hkehhood timation of f selected in . Hence, contrily to the pameter type, the 
recursive type guantees surely that f approhes to f  the number of samples increase. 
The recursive type, if observes a sample (xi, Yi), rewrit vMues A-()'s to A()'s for 
some 's correlated to the staple. Hence, this type h an architecture composed of a rule 
for rewriting and a free memory spce. Such architecture for natury a nd of dtabase 
that builds up magement syste of data in a self-orgizing way. However, this datable 
differs kom ordiny on in the following sense. It does not only record the samples Mready 
observed, but computes some estimation of f(x} for any x � X. We cM1 such datable  
socitive datable. 
The first subject in constructing sociative datables is how we estabhsh the rule for 
rewriting. For this purpose, we Mapt a measure cMled the dissimilarity. Here,  dissilarity 
means a mapping d: X x X  {reals > 0} such that for any (x, ) � X x X, d(x, ) > 0 
whenever f(x) � (). However, it is not necessily aea,a with  single formula. It is 
definable with, for example, a collection of rules written in forms of if ... then .... 
The dissilarity d defines a structure of f locy in X x Y. Hence, even though 
the knowledge on f is imperfect, we can reflect it on d in some heuristic way. Hence, 
contrarily to neurM networks, it is possible to celerate the speed of leaning by establishing 
d wall. Especiy, we c eily find out simple d's for those f's which process anMocy 
information like a human. (See the pplications in this paper.) And, for such f's, the 
rursive type shows strongly its effectiveness. 
We denote a sequence of observed sampl by (x, y), (xa, ya), '. One of the simplest 
constructions of associative databases fi after observing i-staples (i = 1, 2,.-.) is as follows� 
Algorithm 1. At the initiM stage, let S0 be the empty set. For every i = 
1,2,..., let A-(x) for any x � X equM some y* such that (x*,y*) � Si- and 
(1) 
Furthermore, d (xi, y) to S_ to produce S, i.e., S = S_ u {(xi, yi)}. 
769 
Another version improved to economize the memory is as follows. 
Algorithm 2. At the initial stage, let So be composed of an arbitrary element 
in X x Y. For every i = 1, 2,-.-, let fi-l(a:) for any a: � X equal some ,* such 
that (x*, ?,,*) � Si-1 and 
d(x,x'): rain 
Furthermore, if fi-l(a:i) :/: ' then let S = Si-1, or dad (a:i,q) to Si-1 to 
produce Si, i.e., Si: Si-10 {(a:i, 'i)}. 
In either construction, fl approaches to f as i increases. However, the computation time 
grows proportionally to the size of Si. The second subject in constructing associative 
databases is what addressing rule we should employ to economize the computation time. In 
the subsequent chapters, a construction of associative database for this purpose is proposed. 
It manages data in a form of binary tree. 
SELF-ORGANIZATION OF ASSOCIATIVE DATABASE 
Given a sample sequence (xl, ?,q), (x, ?,,a),..., the algorithm for constructing associative 
database is as follows. 
Algorithm 3.' 
Step l(Initialization): Let (a:[rootl,,[root]) = (aq,q). Here, a:[-] and ,[-] are 
warlables assigned for respective nodes to memorize data. Furthermore, let t = 1. 
Step 2: Increase t by 1, and put a:t in. After reset a pointer n to the root, repeat 
the following until n axrives at some terminal node, i.e., le. 
Notations  and 2 mean the descendant nodes of ,. If d(a:,,a:[a]) < 
d(,,tl), let n: a. Otherwise, let n: 
Step 3: Display g,[n] as the related information. Next, put g, in. If ,[n] = q, bak 
to step 2. Otherwise, first establish new descendant nodes a and h. Secondly, 
let 
: (2) 
(x[h],y[iz]) = (:rt,yt). (3) 
Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time 
and also can be continued. 
Now, suppose that gate elements, namely, artificial synapses that play the role of branch- 
ing by d are prepared. Then, we obtain a new style of neural network with gate elements 
being randomly connected by this algorithm. 
LETTER HECOGNITION 
Recently, the vertical slitting method for recognizing typographic English letters 3, the 
elastic matching method for recognizing handwritten discrete English letters 4, the global 
training and fuzzy logic search method for recognizing Chinese characters written in square 
style , etc. axe published. The self-organization of associative database realizes the recogni- 
tion of handwritten continuous English letters. 
770 
Fig. 1. Source document. 
Wi ndow --' B�undary 
Fig. 2. Windowing. 
0 1000 2000 3000 4000 0 1000 
Nuner of samples 
2000 3000 4000 
Number of samples 
Fig. 3. An experiment result. 
An image scanner takes a document image (Fig. 1). The letter recognizer uses a par- 
allelogram window
