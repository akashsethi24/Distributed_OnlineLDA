Deriving Receptive Fields Using An 
Optimal Encoding Criterion 
Ralph Linsker 
IBM T. J. Watson Research Center 
P.O. Box 218, Yorktown Heights, NY 10598 
Abstract 
An information-theoretic optimization principle ('infomax') has 
previously been used for unsupervised learning of statistical reg- 
ularities in an input ensemble. The principle states that the input- 
output mapping implemented by a processing stage should be cho- 
sen so as to maximize the average mutual information between 
input and output patterns, subject to constraints and in the pres- 
ence of processing noise. In the present work I show how infomax, 
when applied to a class of nonlinear input-output mappings, can 
under certain conditions generate optimal filters that have addi- 
tional useful properties: (1) Output activity (for each input pat- 
tern) tends to be concentrated among a relatively small number 
of nodes. (2) The filters are sensitive to higher-order statistical 
structure (beyond pairwise correlations). If the input features are 
localized, the filters' receptive fields tend to be localized as well. 
(3) Multiresolution sets of filters with subsampling at low spatial 
frequencies - related to pyramid coding and wavelet representations 
- emerge as favored solutions for certain types of input ensembles. 
I INTRODUCTION 
In unsupervised network learning, the development of the connection weights is 
influenced by statistical properties of the ensemble of input vectors, rather than by 
the degree of mismatch between the network's output and some 'desired' output. 
An implicit goal of such learning is that the network should transform the input 
so that salient features present in the input are represented at the output in a 
953 
954 Linsker 
more useful form. This is often done by reducing the input dimensionaltry in a way 
that preserves the high-variance components of the input (e.g., principal component 
analysis, Kohonen feature maps). 
The principle of maximum information preservation ('infomax') is an unsupervised 
learning strategy that states (Linsker 1988): From a set of allowed input-output 
mappings (e.g., parametrized by the connection weights), choose a mapping that 
maximizes the (ensemble-averaged) Shannon information that the output vector 
conveys about the input vector, in the presence of noise. Such a mapping maximizes 
the ensemble-averaged mutual information (MI) between input and output. 
This paper (a) summarize earlier results on infomax solutions for linear networks, 
(b) identifies some limitations of these solutions (ways in which very different filter 
sets are equally optimal from the infomax standpoint), and (c) shows how, by adding 
a small nonlinearity to the network, one can remove these limitations and at the 
same time improve the utility of the output representations. We show that infomax, 
acting on the modified network, tends to favor sparsely coded representations and 
(depending on the input ensemble) sets of filters that span multiple resolution scales 
(related to wavelets and 'pyramid coding'). 
2 INFOMAX IN LINEAR NETWORKS 
For definiteness and brevity, we consider a linear network having a particular type 
of noise model and input statistical properties. For a more detailed discussion of 
related models see (Linsker 1989). 
Since the computation of the MI (which involves the output entropy) is in general 
intractable for continuous-valued output vectors, previous work (and the present 
paper) makes use of a surrogate MI, which we will call the 'as-if-Gaussian' MI. This 
quantity is, by definition, computed as though the output vectors comprised a mul- 
tivariate Gaussian distribution having the same mean and covariance as the actual 
distribution of output vectors. Although expedient, this substitution has lacked a 
principled justification. The Appendix shows that, under certain conditions, using 
this 'surrogate MI' (and not the full MI) is indeed appropriate and justified. 
Denote the input vector by S _= {Si} (Si is the activity at input node i), the output 
vector by Z = {Zn), the matrix of connection weights by C _= {Cni), noise at the 
input nodes by N =- {Ni), and noise at the output nodes by , = {'n). Then our 
processing model is, in matrix form, Z - C($ 4- N) 4- . Assume that N and , are 
Gaussian random variables, (S) = (N = (,) = O, (_SN T } = (S ,T ) = (N ,T) = O, 
and, for the covariance matrices, (SS) = Q, (NN ') = 7I, (''?) = HI'. (Angle 
brackets denote an ensemble average, superscript T denotes transpose, and I and 
I' denote unit matrices on the input and output spaces, respectively.) In general, 
MI = Hz - (Hzls) where Hz is the output entropy and Hzls is the entropy of the 
output for given S. Replacing MI by the 'as-if-Gaussian' MI means replacing Hz 
by the expression for the entropy of a multivariate Gaussian distribution, which is 
(apart from an irrelevant constant term) Hz a = (1/2) In det Q', where Q' -- (zz T) = 
CQC ' + riCC ' +/3I' is the output covariance. Note that, when S is fixed, Z = 
CS+(CN+) is a Gaussian distribution centered on CS, so that we have (Hzls) = 
(1/2)lndetQ where Q = ((CN 4- ,)(CN 4- )) - ,CC  4- HI'. Therefore the 
Deriving Receptive Fields Using An Optimal Encoding Criterion 955 
'as-if-Gaussian' MI is 
MI' = (1/2)[lndet Q'- lndet Q]. 
(1) 
The variance of the output at node n (prior to adding noise v,) is V, = ([C($ + 
N)]}) = (CQC ' + CCa'),,. We will constrain the dynamic range of each output 
node (limiting the number of output values that can be discriminated from one 
another in the presence of output noise) by requiring that V, = 1 for each n. 
Subject to this constraint, we are to find a matrix C that maximizes MI'. For a 
local Hebbian algorithm that accomplishes this maximization, see (Linsker 1992). 
Here, in order to proceed analytically, we consider a special case of interest. 
Suppose that the input statistics are shift-invariant, so that the covariance (SiSj) 
is a function of (j - i). We then use a shift-invariant filter Ansatz, Cni = C(i - n). 
Infomax then determines the optimal filter gain as a function of spatial frequency; 
i.e., the magnitude of the Fourier components c(k) of C(i - n). The derivation is 
summarized below. 
Denote by q(k), q'(k), and q(k) the Fourier transforms of Q - i), Q'(m - n), 
and Q(m- n) respectively. Since Q' = CQCr+ yCC:+/5I', therefore 
q'(k) = [q(k) + r/) [ c(k) 12 +/5. Similarly, q(k) = r/ I c(k) 12 +/5. We obtain 
MI' = (1/2)Ea[lnq'(k) - lnq(k)]. Each node's output variance V is equal to 
v = V] K number of terms in the sum over k. 
To maximize MI' subject to the constraint on V we use the Lagrange multiplier 
method; that is, we mimize MI  MI' + (V - 1) with respect to each 
This yields an equation for each k that is quadratic in ]c(k) ]a. The unique solution 
is 
q(k) (1 + [1 2,K ]/a 
-1 + .] } (2) 
if the RHS is positive, and zero otherwise. The Lagrange multiplier (< 0) is chosen 
so that the {I c(k)I} satisfy V = 1. 
Starting from a differently-stated goal (that of reducing redundancy subject to a 
limit on information loss), which turns out to be closely related to infomax, (Atick 
& Redlich 1990a) found an expression for the optimal filter gain that is the same 
as that of Eq. 2 except for the choice of constraint. 
Filter properties found using this approach are related to those found in early stages 
of biological sensory processing. Smoothing and bandpass (contrast-enhancing) 
filters emerge as infomax solutions (Linsker 1989, Atick & Redlich 1990a) in certain 
cases, and good agreement with retinal controt sensitivity measurements has been 
found (Atick & Redlich 1990b). 
Nonetheless, the value of the infomax solution Eq. 2 is limited in two important 
ways. First, the phases of the {c(k)} are left undetermined. Any choice of phases is 
equally good at maximizing MIt in a linear network. Thus the real-space response 
function C(i - n), which determines the receptive field properties of the output 
nodes, is nonunique (and indeed may be highly nonlocalized in space). 
Second, it is useful to extend the solution Ansatz to allow a number of different filter 
types a = 1,..., A at each output site, while continuing to require that each type 
956 Linsker 
satisfy the shift-invariance condition Cni(a)  C(i - n; a). For example, one may 
want to model a topographic 'retinocortical' mapping in which each patch of cortex 
(each 'site') contains multiple filter types, yet each patch carries out the same set of 
processing functions on its input. For this Ansatz, one again obtains Eq. 2 (deriva- 
tion omitted here), but with [ c(k) [2 on the LHS replaced by Eap(a)[ c(k;a)]2, 
where c(k; a)is the F.T. of C(i-n; a), and p(a) is the fraction of the total number 
of filters (at each site) that are of type a. The partitioning of the overall (sum- 
squared) gain among the multiple filter types is thus left undetermined. 
The higher-order statistical structure of the input (beyond covariance) is not being 
exploited by infomax in the above analysis, because (1) the network is linear and (2) 
only pairwise correlations among the output activities enter into MI . We shall show 
that if we make the network even mildly nonlinear, MI  is no longer independent of 
the choice of phases or of the partitioning of gain among multiple filter types. 
3 NETWORK WITH WEAK NONLINEARITY 
We consider the weakly nonlinear input-output relation Zn = U + eU--EiCniNi-{- 
v, where U _= iCniSi, for small e. This differs from the linear network analyzed 
above by the term in U. (For simplicity, terms nonlinear in the noise are not 
included.) The cubic term increases the signal-to-noise ratio selectively when U, is 
large in absolute value. We maximize MI  as defined in Eq. 1. 
Heuristically, the new term will cause infomax to favor solutions in which some 
output nodes have large (absolute) activity values, over solutions in which all output 
nodes 
