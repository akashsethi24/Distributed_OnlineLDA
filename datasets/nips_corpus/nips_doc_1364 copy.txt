Receptive field formation in natural scene 
environments: comparison of single cell 
learning rules 
Brian S. Blais 
Brown University Physics Department 
Providence, RI 02912 
N. Intrator 
School of Mathematical Sciences 
Tel-Aviv University 
Ramat-Aviv, 69978 ISRAEL 
H. Shouval 
Institute for Brain and Neural Systems 
Brown University 
Providence, RI 02912 
Leon N Cooper 
Brown University Physics Department and 
Institute for Brain and Neural Systems 
Brown University 
Providence, RI 02912 
Abstract 
We study several statistically and biologically motivated learning 
rules using the same visual environment, one made up of natural 
scenes, and the same single cell neuronal architecture. This allows 
us to concentrate on the feature extraction and neuronal coding 
properties of these rules. Included in these rules are kurtosis and 
skewness maximization, the quadratic form of the BCM learning 
rule, and single cell ICA. Using a structure removal method, we 
demonstrate that receptive fields developed using these rules de- 
pend on a small portion of the distribution. We find that the 
quadratic form of the BCM rule behaves in a manner similar to a 
kurtosis maximization rule when the distribution contains kurtotic 
directions, although the BCM modification equations are compu- 
tationally simpler. 
424 B. S. BIais, iV. Intrator, H. ShouvaI and L iV. Cooper 
I INTRODUCTION 
Recently several learning rules that develop simple cell-like receptive fields in a 
natural image environment have been proposed (Law and Cooper, 1994; Olshausen 
and Field, 1996; Bell and Sejnowski, 1997). The details of these rules are different 
as well as their computational reasoning, however they all depend on statistics of 
order higher than two and they all produce sparse distributions. 
In what follows we investigate several specific modification functions that have 
the. general properties of BCM synaptic modification functions (Bienenstock et al., 
1982), and study their feature extraction properties in a natural scene environment. 
Several of the rules we consider are derived from standard statistical measures 
(Kendall and Stuart, 1977), such as skewness and kurtosis, based on polynomial 
moments. We compare these with the quadratic form of BCM (Intrator and Cooper, 
1992), though one should note that this is not the only form that could be used. 
By subjecting all of the learning rules to the same input statistics and retina/LGN 
preprocessing and by studying in detail the single neuron case, we eliminate possible 
network/lateral interaction effects and can examine the properties of the learning 
rules themselves. 
We compare the learning rules and the receptive fields they form, and introduce a 
procedure for directly measuring the sparsity of the representation a neuron learns. 
This gives us another way to compare the learning rules, and a more quantitative 
measure of the concept of sparse representations. 
2 MOTIVATION 
We use two methods for motivating the use of the particular rules. One comes 
from Projection Pursuit (Friedman, 1987) and the other is Independent Component 
Analysis (Comon, 1994). These methods are related, as we shall see, but they 
provide two different approaches for the current work. 
2.1 EXPLORATORY PROJECTION PURSUIT 
Diaconis and Freedman (1984) show that for most high-dimensional clouds (of 
points), most low-dimensional projections are approximately Gaussian. This find- 
ing suggests that important information in the data is conveyed in those directions 
whose single dimensional projected distribution is far from Gaussian. 
Intrator (1990) has shown that a BCM neuron can find structure in the input 
distribution that exhibits deviation from Gaussian distribution in the form of multi- 
modality in the projected distributions. This type of deviation is particularly useful 
for finding clusters in high dimensional data. In the natural scene environment, 
however, the structure does not seem to be contained in clusters. In this work we 
show that the BCM neuron can still find interesting structure in non-clustered data. 
The most common measures for deviation from Gaussian distribution are skewness 
and.kurtosis which are functions of the first three and four moments of the dis- 
tribution respectively. Rules based on these statistical measures satisfy the BCM 
conditions proposed in Bienenstock et al. (1982), including a threshold-based sta- 
bilization. The details of these rules and some of the qualitative features of the 
stabilization are different, however. In addition, there are some learning rules, 
such as the ICA rule of Bell and Sejnowski (1997) and the sparse coding algorithm 
of Olshausen and Field (1995), which have been used with natural scene inputs to 
produce oriented receptive fields. We do not include these in our comparison be- 
RF Formation in Natural Scenes: Comparison of Single Cell Learning Rules 425 
cause they are not single cell learning rules, and thus detract from our immediate 
goal of comparing rules with the same input structure and neuronal architecture. 
2.2 INDEPENDENT COMPONENT ANALYSIS 
Recently it has been claimed that the independent components of natural scenes 
are the edges found in simple cells (Bell and Sejnowski, 1997). This was achieved 
through the maximization of the mutual entropy of a set of mixed signals. Others 
(Hyvarinen and Oja, 1996) have claimed that maximizing kurtosis can also lead 
to the separation of mixed signals into independent components. This alternate 
connection between kurtosis and receptive fields leads us into a discussion of ICA. 
Independent Component Analysis (ICA) is a statistical signal processing technique 
whose goal is to express a set of random variables as a linear mixture of statistically 
independent variables. The problem of ICA is then to find the transformation from 
the observed mixed signals to the unmixed independent sources. The search 
for independent components relies on the fact that a linear mixture of two non- 
Gaussian distributions will become more Gaussian than either of them. Thus, 
by seeking projections which maximize deviations from Gaussian distribution, we 
recover the original (independent) signals. This explains the connection of ICA to 
the framework of exploratory projection pursuit. 
3 SYNAPTIC MODIFICATION RULES 
In this section we outline the derivation for the learning rules in this study. Neural 
activity is assumed to be a positive quantity, so for biological plausibility we denote 
by c the rectified activity a(d. m), where a(.) is a smooth monotonic function 
with a positive output (a slight negative output is also allowed). rr  denotes the 
derivative of the sigrnoidal. The rectification is required for all rules that depend 
on odd moments because these vanish in symmetric distributions such as natural 
scenes. We study the following measures(Kendall and Stuart, 1977, for review): 
Skewness 1 This measures the deviation from symmetry, and is of the form: 
S1 -- Z[c3]/zl'5[c2]. (1) 
A maximization of this measure via gradient ascent gives 
I 1 
VSl --' OM1. 5E [c (c E[cS]lE[c=]) od] OM1. sE [c (c E[cS]leM) od] (2) 
where Om is defined as E[c2]. 
Skewhess 2 Another skewness measure is given by 
S2 - E[c 3] - E1'5[c2]. (a) 
This measure requires a stabilization mechanism which we achieve by requiring that 
the vector of weights, denoted by m, has norm of 1. The gradient of S2 is 
VS2 = 3E [c 2- c] = 3E [c (c- OX/-M ) a'd], II m II=l (4) 
Kurtosis 1 Kurtosis measures deviation 
tails of the distribution. It has the form 
= s[ca]lS2[c 2] - a. 
This measure has a gradient of the form 
1 [c (c 2 E[c4]lE[c2]) a'd] = 1 E [c (c 2 - E[c4]IOM) a'd]. 
from Gaussian distribution mainly in the 
(5) 
(6) 
426 B. S. BIais, N. Intrator, H. Shouval and I_, N. Cooper 
Kurtosis 2 As before, there is a similar form which requires some stabilization: 
/2 '-- E[ C4] - 3E2[c2] � (7) 
This measure has a gradient of the form 
VK2: 4E [c 3 - cE[c2]] = 3E [c(c 2 - OM)]o'td], IIm II- l. 
Kurtosis 2 and ICA It has been shown that kurtosis, defined as 
= _ 
can be used for ICA(Hyvarinen and Oja, 1996). Thus, finding the extrema of 
kurtosis of the projections enables the estimation of the independent components. 
They obtain the following expression 
2 
m = X (E-1 [ddT] E [d(m. d) 3] - 3m). (9) 
which leads to an iterative fixed-point algorithm. 
Quadratic BCM The Quadratic BCM (QBCM) measure as given in (Intrator 
and Cooper, 1992) is of the form 
QBCM = E[ca]- �E2[c2]. 
Maximizing this form using gradient ascent gives the learning rule: 
VQBCM = E [c  - cE[c]] = E[c(c - OM)crtd]. 
(10) 
(11) 
4 METHODS 
We use 13113 circular patches from 12 images of natural scenes, presented to the 
neuron each iteration of the learning. The natural scenes are preprocessed either 
with a Difference of Gaussians (DOG) filter(Law and Cooper, 1994) or a whitening 
filter(Oja, 1995; Bell and Sejnowski, 1995), which eliminates the second order cor- 
relations. The moments of the output, c, are calculated iteratively, and when it is 
needed (i.e. K2 and S2) we also normalize the weights at each iteration. 
For Oja's fixed-point algorithm, the learning was done in batches of 1000 patterns 
over which the expectation values were performed. However, the covariance matrix 
was calculated over the entire set of input patterns. 
5 RESULTS 
5.1 RECEPTIVE FIELDS 
The resulting receptive fields (RFs) formed are shown in Figure 1 for both the 
DOGed and whitened images. Every learning rule developed oriented receptive 
fields, though some were more sensitive to the preprocessing than others. The 
additive versions of kurtosis and skewness, K2 and $2 respectively, developed RFs 
with a higher spatial frequency, and more orientations, in the whitened environment 
than in the DOGed environment. 
The multiplicative versions of kurtosis and skewness, K1 and S1 respectively, as 
well as QBCM, sampled from many orientations regardles
