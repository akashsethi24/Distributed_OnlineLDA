Neural Networks with Quadratic VC 
Dimension 
Pascal Koiran* 
Lab. de l'Informatique du Paralldlisme 
Ecole Normale Supdrieure de Lyon - CNRS 
69364 Lyon Cedex 07, France 
Eduardo D. Sontag* 
Department of Mathematics 
Rutgers University 
New Brunswick, NJ 08903, USA 
Abstract 
This paper shows that neural networks which use continuous acti- 
vation functions have VC dimension at least as large as the square 
of the number of weights w. This result settles a long-standing 
open question, namely whether the well-known O(w log w) bound, 
known for hard-threshold nets, also held for more general sigmoidal 
nets. Implications for the number of samples needed for valid gen- 
eralization are discussed. 
I Introduction 
One of the main applications of artificial neural networks is to pattern classification 
tasks. A set of labeled training samples is provided, and a network must be obtained 
which is then expected to correctly classify previously unseen inputs. In this context, 
a central problem is to estimate the amount of training data needed to guarantee 
satisfactory learning performance. To study this question, it is necessary to first 
formalize the notion of learning from examples. 
One such formalization is based on the paradigm of probably approximately correct 
(PAC) learning, due to Valiant (1984). In this framework, one starts by fitting some 
function f, chosen from a predetermined class jr, to the given training data. The 
class jr is often called the hypothesis class, and for purposes of this discussion it 
will be assumed that the functions in jr take binary values {0, 1) and are defined on a 
common domain X. (In neural networks applications, typically r corresponds to the 
set of all neural networks with a given architecture and choice of activation functions. 
The elements of X are the inputs, possibly multidimensional.) The training data 
consists of labeled samples (Xi,ei) , with each xi 6 X and each ei 6 {0, 1}, and 
*koiranlip. ens-lyon. fr. 
t sont aghilbert. rutgers. edu. 
198 P. KOIRAN, E. D. SONTAG 
fitting by an f means that f(zi) ---- ei for each i. Given a new example z, one 
uses f(x) as a guess of the correct classification of x. Assuming that both training 
inputs and future inputs are picked according to the same probability distribution 
on X, one needs that the space of possible inputs be well-sampled by the training 
data, so that f is an accurate fit. We omit the details of the formalization of 
PAC learning, since there are excellent references available, both in textbook (e.g. 
Anthony and Biggs (1992), Natarajan (1991)) and survey paper (e.g. Maass (1994)) 
form, and the concept is by now very well-known. 
After the work of Vapnik (1982) in statistics and of Blumer et. al. (1989) in com- 
putational learning theory, one knows that a certain combinatorial quantity, called 
the Vapnik-Chervonenkis (VC) dimension VC(:) of the class : of interest com- 
pletely characterizes the sample sizes needed for learnability in the PAC sense. (The 
appropriate definitions are reviewed below. In Valiant's formulation one is also in- 
terested in quantifying the computational effort required to actually fit a function 
to the given training data, but we are ignoring that aspect in the current paper.) 
Very roughly speaking, the number of samples needed in order to learn reliably is 
proportional to VC(:). Estimating VC(:) then becomes a central concern. Thus 
from now on, we speak exclusively of VC dimension, instead of the original PAC 
learning problem. 
The work of Cover (1988) and Baum and Haussler (1989) dealt with the computa- 
tion of VC(:) when the class : consists of networks built up from hard-threshold 
activations and having w weights; they showed that VC(:)= O(wlogw). (Con- 
versely, Maass (1993) showed that there is also a lower bound of this form.) It 
would appear that this definitely settled the VC dimension (and hence also the 
sample size) question. 
However, the above estimate assumes an architecture based on hard-threshold 
(Heaviside) neurons. In contrast, the usually employed gradient descent learning 
algorithms (backpropagation method) rely upon continuous activations, that is, 
neurons with graded responses. As pointed out in Sontag (1989), the use of ana- 
log activations, which allow the passing of rich (not just binary) information among 
levels, may result in higher memory capacity as compared with threshold nets. This 
has serious potential implications in learning, essentially because more memory ca- 
pacity means that a given function f may be able to memorize in a rote fashion 
too much data, and less generalization is therefore possible. Indeed, Sontag (1992) 
showed that there are conceivable (though not very practical) neural architectures 
with extremely high VC dimensions. Thus the problem of studying VC(:) for ana- 
log networks is an interesting and relevant issue. Two important contributions in 
this direction were the papers by Maass (1993) and by Goldberg and Jerrum (1995), 
which showed upper bounds on the VC dimension of networks that use piecewise 
polynomial activations. The last reference, in particular, established for that case 
an upper bound of O(w2), where, as before, w is the number of weights. However 
it was an open problem (specifically, open problem number 7 in the recent survey 
by Maass (1993) if there is a matching w 2 lower bound for such networks, and more 
generally for arbitrary continuous-activation nets. It could have been the case that 
the upper bound O(w ) is merely an artifact of the method of proof in Goldberg 
and Jerrum (1995), and that reliable learning with continuous-activation networks 
is still possible with far smaller sample sizes, proportional to O(w log w). But this is 
not the case, and in this paper we answer Maass' open question in the affirmative. 
Assume given an activation er which has different limits at -Vx, and is such that 
there is at least one point where it has a derivative and the derivative is nonzero 
(this last condition rules out the Heaviside activation). Then there are architec- 
tures with arbitrary large numbers of weights w and VC dimension proportional 
Neural Networks with Quadratic VC Dimension 199 
to w 2. The proof relies on first showing that networks consisting of two types of 
activations, Heavisides and linear, already have this power. This is a somewhat 
surprising result, since purely linear networks result in VC dimension proportional 
to w, and purely threshold nets have, as per the results quoted above, VC dimension 
bounded by wlog w. Our construction was originally motivated by a related one, 
given in Goldberg and Jerrum (1995), which showed that real-number programs (in 
the Blum-Shub-Smale (1989) model of computation) with running time T have VC 
dimension (T2). The desired result on continuous activations is then obtained, 
approximating Heaviside gates by a-nets with large weights and approximating lin- 
ear gates by a-nets with small weights. This result applies in particular to the 
standard sigmoid 1/(1 + e-). (However, in contrast with the piecewise-polynomial 
case, there is still in that case a large gap between our (w ) lower bound and 
the O(w 4) upper bound which was recently established in Karpinski and Macin- 
tyre (1995).) A number of variations, dealing with Boolean inputs, or weakening 
the assumptions on tr, are discussed. The full version of this paper also includes 
some remarks on thresholds networks with a constant number of linear gates, and 
threshold-only nets with shared weights. 
Basic Terminology and Definitions 
Formally, a (first-order, feedforward) architecture or network .A is a connected di- 
rected acyclic graph together with an assignment of a function to a subset of its 
nodes. The nodes are of two types: those of fan-in zero are called input nodes and 
the remaining ones are called computation nodes or gates. An output node is a node 
of fan-out zero. To each gate g there is associated a function ag ' ]1 -- ]1, called the 
activation or gate function associated to g. 
The number of weights or parameters associated to a gate g is the integer rg equal 
to the fan-in of g plus one. (This definition is motivated by the fact that each input 
to the gate will be multiplied by a weight, and the results are added together with 
a bias constant term, seen as one more weight; see below.) The (total) number 
of weights (or parameters) of Jl is by definition the sum of the numbers ha, over all 
the gates g of .A. The number of inputs m of .A is the total number of input nodes 
(one also says that .A has inputs in '); it is assumed that m > 0. The number 
of outputs p of Jl is the number of output nodes (unless otherwise mentioned, we 
assume by default that all nets considered have one-dimensional outputs, that is, 
p=l). 
Two examples of gate functions that are of particular interest are the identity or 
linear gate: Id(x) = x for all x, and the threshold or Heaviside function: H(x) - 1 
if z _ O, H(x) = 0 if x < 0. 
Let Jl be an architecture. Assume that nodes of .4 have been linearly ordered as 
rl,..., r,, gx, � �., g, where the rj's are the input nodes and the gj's the gates. For 
simplicity, write ri :-- rg,, for each i = 1,...,l. Note that the total number of 
I 
parameters is n = Ei=i/2i and the fan-in of each gi is/2i -- 1. To each architecture 
Jl (strictly speaking, an architecture together with such an ordering of nodes) we 
associate a function 
F � I  x I  - R e , 
where p is the number of outputs of Jl, defined by first assigning an output to 
each node, recursively on the distance from the the input nodes. Assume given 
an input x E 1 and a vector of weights w E  We partition w into blocks 
(wi,..., wl) of sizes hi,..., nl respectively. First the coordinates of x are assigned 
as the outputs of the input nodes r,..., rm respectively. For each of the other 
gat
