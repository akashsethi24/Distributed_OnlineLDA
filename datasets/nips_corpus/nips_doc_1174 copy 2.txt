Neural Models for Part-Whole Hierarchies 
Maximilian Riesenhuber Peter Dayan 
Department of Brain & Cognitive Sciences 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
{max, dayan}ai. mi. edu 
Abstract 
We present a connectionist method for representing images that ex- 
plicitly addresses their hierarchical nature. It blends data from neu- 
roscience about whole-object viewpoint sensitive cells in inferotem- 
poral cortex s and attentional basis-field modulation in V4 3 with 
ideas about hierarchical descriptions based on microfeatures. 5' l 
The resulting model makes critical use of bottom-up and top-down 
pathways for analysis and synthesis? We illustrate the model with 
a simple example of representing information about faces. 
1 Hierarchical Models 
Images of objects constitute an important paradigm case of a representational hi- 
erarchy, in which 'wholes', such as faces, consist of 'parts', such as eyes, noses and 
mouths. The representation and manipulation of part-whole hierarchical informa- 
tion in fixed hardware is a heavy millstone around connectionist necks, and has 
consequently been the inspiration for many interesting proposals, such as Pollack's 
RAAM.  
We turned to the primate visual system for clues. Anterior inferotemporal cortex 
(IT) appears to construct representations of visually presented objects. Mouths and 
faces are both objects, and so require fully elaborated representations, presumably 
at the level of anterior IT, probably using different (or possibly partially overlap- 
ping) sets of cells. The natural way to represent the part-whole relationship between 
mouths and faces is to have a neuronal hierarchy, with connections bottom-up from 
the mouth units to the face units so that information about the mouth can be used 
to help recognize or analyze the image of a face, and connections top-down from 
the face units to the mouth units expressing the generarive or synthetic knowledge 
that if there is a face in a scene, then there is (usually) a mouth too. There is little 
We thank Larry Abbott, Geoff Hinton, Bruno Olshausen, Tomaso Poggio, Alex Pouget, 
Emilio Salinas and Pawan Sinha for discussions and comments. 
18 M. Riesenhuber and P Dayan 
empirical support for or against such a neuronal hierarchy, but it seems extremely 
unlikely on the grounds that arranging for one with the correct set of levels for all 
classes of objects seems to be impossible. 
There is recent evidence that activities of cells in intermediate areas in the visual 
processing hierarchy (such as V4) are influenced by the locus of visual attention. 3 
This suggests an alternative strategy for representing part-whole information, in 
which there is an interaction, subject to attentional control, between top-down 
generative and bottom-up recognition processing. In one version of our example, 
activating units in IT that represent a particular face leads, through the top-down 
generafive model, to a pattern of activity in lower areas that is closely related to 
the pattern of activity that would be seen when the entire face is viewed. This 
activation in the lower areas in turn provides bottom-up input to the recognition 
system. In the bottom-up direction, the attentional signal controls which aspects of 
that activation are actually processed, for example, specifying that only the activity 
reflecting the lower part of the face should be recognized. In this case, the mouth 
units in IT can then recognize this restricted pattern of activity as being a particular 
sort of mouth. Therefore, we have provided a way by which the visual system can 
represent the part-whole relationship between faces and mouths. 
This describes just one of many possibilities. For instance, attentional control could 
be mainly active during the top-down phase instead. Then it would create in V1 (or 
indeed in intermediate areas) just the activity corresponding to the lower portion 
of the face in the first place. Also the focus of attention need not be so ineluctably 
spatial. 
The overall scheme is based on an hierarchical top-down synthesis and bottom-up 
analysis model for visual processing, as in the Helmholtz machine 6 (note that hi- 
erarchy here refers to a processing hierarchy rather than the part-whole hierarchy 
discussed above) with a synthetic model forming the effective map: 
'object'ï¿½ 'attentional eye-position' - 'image' 
(1) 
(shown in cartoon form in figure 1) where 'image' stands in for the (probabilities 
over the) activities of units at various levels in the system that would be caused by 
seeing the aspect of the 'object' selected by placing the focus and scale of attention 
appropriately. We use this generafive model during synthesis in the way described 
above to traverse the hierarchical description of any particular image. We use the 
statistical inverse of the synthetic model as the way of analyzing images to determine 
what objects they depict. This inversion process is clearly also sensitive to the 
attentional eye-position - it actually determines not only the nature of the object 
in the scene, but also the way that it is depicted (ie its instantiation parameters) 
as reflected in the attentional eye position. 
In particular, the bottom-up analysis model exists in the connections leading to 
the 2D viewpoint-selective image cells in IT reported by Logothetis et al s which 
form population codes for all the represented images (mouths, noses, etc). The 
top-down synthesis model exists in the connections leading in the reverse direction. 
In generalizations of our scheme, it may, of course, not be necessary to generate an 
image all the way down in V1. 
The map (1) specifies a top-down computational task very like the bottom-up one 
addressed using a multiplicatively controlled synaptic matrix in the shifter model 
Neural Models for Part-Whole Hierarchies 19 
layer 
3 o 
2 m 
attentional 
e =(e, % e) 
1 p 
Figure 1: Cartoon of the model. In the top-down, generative, direction, the model generates 
images of faces, eyes, mouths or noses based on an attentional eye position and a selection of a 
single top-layer unit; the bottom-up, recognition, direction is the inverse of this map. The response 
of the neurons in the middle layer is modulated sigmoidally (as illustrated by the graphs shown 
inside the circles representing the neurons in the middle layer) by the attentional eye position. See 
section 2 for more details. 
of Olshausen et al. 9 Our solution emerges from the control the attentional eye 
position exerts at various levels of processing, most relevantly modulating activity 
in V4. 3 Equivalent modulation in the parietal cortex based on actual (rather than 
attentional) eye position I has been characterized by Pouget & Sejnowski 13 and 
Salinas & Abbott 15 in terms of basis fields. They showed that these basis fields 
can be used to solve the same tasks as the shifter model but with neuronal rather 
than synaptic multiplicative modulation. In fact, eye-position modulation almost 
certainly occurs at many levels in the system, possibly including V1.12 Our scheme 
clearly requires that the modulating attentional eye-position must be able to become 
detached from the spatial eye-position - Connor et al. 3 collected evidence for part of 
this hypothesis; although the coordinate system(s) of the modulation is not entirely 
clear from their data. 
Bottom-up and top-down mappings are learned taking the eye-position modula- 
tion into account. In the experiments below, we used a version of the wake-sleep 
algorithm, 6 for its conceptual and computational simplicity. This requires learning 
the bottom-up model from generated imagery (during sleep) and learning the top- 
down model from assigned explanations (during observation of real input during 
wake). In the current version, for simplicity, the eye position is set correctly during 
recognition, but we are also interested in exploring automatic ways of doing this. 
2 Results 
We have developed a simple model that illustrates the feasibility of the scheme 
presented above in the context of recognizing and generating cartoon drawings of 
a face and its parts. Recognition involves taking an image of a face or a part 
thereof (the mouth, nose or one of the eyes) at an arbitrary position on the retina, 
20 M. Riesenhuber and P. Dayan 
a) 
b) 
face nse 
mouth eye 
Figure 2: a) Recognition: the left column of each pair shows the stimuli; the right shows the 
resulting activations in the top layer (ordered as face, mouth, nose and eye). The stimuli are faces 
at random positions in the retina. Recognition is performed by setting the attentional eye position 
in the image and setting the attentional scale, which creates a window of attention around the 
attended to position, shown by a circle of corresponding size and position. b) Generation: each 
panel shows the output of the generative pathway for a randomly chosen attentional eye position 
on activating each of the top layer units in turn. The focus of attention is marked by a circle 
whose size reflects the attentional scale. The name of the object whose neuronal representation in 
the top layer was activated is shown above each panel. 
and setting the appropriate top level unit to 1 (and the remaining units to zero). 
Generation involves imaging either a whole face or of one of its parts (selected by 
the active unit in the top layer) at an arbitrary position on the retina. 
The model (figure 1) consists of three layers. The lowest layer is a 32 x 32 'retina'. 
In the recognition direction, the retina feeds into a layer of 500 hidden units. These 
project to the top layer, which has four neurons. In the generative direction, the 
connectivity is reversed. The network is fully connected in both directions. The 
activity of each neuron based on input from the preceding (for recognition) or the 
following layer (for generation) is a linear function (weight matrices W r, V r in the 
recogni
