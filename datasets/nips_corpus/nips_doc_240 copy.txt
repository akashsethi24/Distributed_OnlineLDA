Non-Boltzmann Dynamics in Networks of Spiking Neurons 109 
Non-Boltzmann Dynamics in 
Spiking Neurons 
Networks of 
Michael C. Crair and William Bialek 
Department of Physics, and 
Department of Molecular and Cell Biology 
University of California at Berkeley 
Berkeley, CA 94720 
ABSTRACT 
We study networks of spiking neurons in which spikes are fired as 
a Poisson process. The state of a cell is determined by the instan- 
taneous firing rate, and in the limit of high firing rates our model 
reduces to that studied by Hopfield. We find that the inclusion 
of spiking results in several new features, such as a noise-induced 
asymmetry between on and off states of the cells and probabil- 
ity currents which destroy the usual description of network dynam- 
ics in terms of energy surfaces. Taking account of spikes also al- 
lows us to calibrate network parameters such as synaptic weights 
against experiments on real synapses. Realistic forms of the post 
synaptic response alters the network dynamics, which suggests a 
novel dynamical learning mechanism. 
I INTRODUCTION 
In 1943 McCulloch and Pitts introduced the concept of two-state (binary) neurons 
as elementary building blocks for neural computation. They showed that essentially 
any finite calculation can be done using these simple devices. Two-state neurons are 
of questionable biological relevance, yet much of the subsequent work on modeling of 
neural networks has been based on McCulloch-Pitts type neurons because the two- 
state simplification makes analytic theories more tractable. Hopfield (1982, 1984) 
110 Crair and Bialek 
showed that an asynchronous model of symmetrically connected two-state neurons 
was equivalent to Monte-Carlo dynamics on an 'energy' surface at zero temperature. 
The idea that the computational abilities of a neural network can be understood 
from the structure of an effective energy surface has been the central theme in much 
recent work. 
In an effort to understand the effects of noise, Amir, Gutfreund and Sompolinsky 
(Amir et M., 1985a; 1985b) assumed that Hopfield's 'energy' could be elevated to 
an energy in the statistical mechanics sense, and solved the Hopfield model at finite 
temperature. The problem is that the noise introduced in equilibrium statistical 
mechanics is of a very special form, and it is not clear that the stochastic properties 
of real neurons are captured by postulating a Boltzmann distribution on the energy 
surface. 
Here we try to do a slightly more realistic calculation, describing interactions among 
neurons through action potentials which are fired according to probabilistic rules. 
We view such calculations as intermediate between the purely phenomenological 
treatment of neural noise by Amir et al. and a fully microscopic description of 
neural dynamics in terms of ion channels and their associated noise. W'e find that 
even our limited attempt at biological realism results in some interesting deviations 
from previous ideas on network dynamics. 
2 THE MODEL 
We consider a model where neurons have a continuous firing rate, but the generation 
of action potentials is a Poisson process. This means that the state of each cell i 
is described by the instantaneous rate ri(t), and the probability that this cell will 
fire in a time interval [t, t + dt] is given by r(t)dt. Evidence for the near-Poisson 
character of neuronal firing can be found in the mammalian auditory nerve (Siebert, 
1965; 1968), and retinal ganglion cells (Teich et M., 1978, Teich and Saleh, 1981). 
To stay as close as possible to existing models, we assume that the rate r(t) of a 
neuron is a sigmoid function, g(x) = 1/(1+e-X), of the total input x to the neuron. 
The input is assumed to be a weighted sum of the spikes received from all other 
neurons, so that 
Jij is the matrix of connection strengths between neurons, r, is the maximum 
spike rate of the neuron, and Oi is the neuronal threshold. f(t) is a time weighting 
function, corresponding schematically to the time course of post-synaptic currents 
injected by a pre-synaptic spike; a good first order approximation for this function 
is f(t) ~ e -t/r, but we also consider functions with more than one time constant. 
(Aidley, 1980, Fetz and Gustafsson, 1983). 
We can think of the spike train from the i th neuron,  (t - t), as an approx- 
imation to the true firing rate ri(t); of course this approximation improves as the 
Non-Boltzmann Dynamics in Networks of Spiking Neurons 111 
spikes come closer together at high firing rates. If we write 
- = + i(t) (2) 
we have defined the noise r in the spike train. 
rates then become 
The equations of motion for the 
ri(t)=rrag I . Jijf�rj(t)-Oi+Ni(t) ] , 
(3) 
where Ni(t) = -]j JOrlj(t) and f o rj(t) is the convolution of f(t) with the spike 
rate r/(t). The statistics of the fluctuations in the spike rate rj(t) are (r/i(t)) = 
0, = 
3 DYNAMICS 
If the post-synaptic response f(t) is exactly exponential, we can invert Eq. (3) 
to obtain a first order equation for the normalized spike rate yi(t) -- ri(t)/r,. 
More precise descriptions of the post-synaptic response will yield higher order time 
derivatives with coefficients that depend on the relative time constants in f(t). We 
will comment later on the relevance of these higher order terms, but consider first 
the lowest order description. By inverting Eq. (3) we obtain a stochastic differential 
equation analogous to the Langevin equation describing Brownian motion: 
dg-(yi) dE 
dt 
where the deterministic forces are given by 
--- + (4a) 
dr g-l(yi) 
dyi' = r 
(4b) 
Note that Eq. (4) is nearly equivalent to the charging equation Hopfield (1984) 
assumed in his discussion of continuous neurons, except we have explicitly included 
the noise from the spikes. This system is precisely equivalent to the Hopfield two- 
state model in the limit of large spike rate (r,v = <x, Jii = constant), and no 
noise. In a thermodynamic system near equilibrium, the noise force Ni(t) is 
related to the friction coefficient via the fluctuation dissipation theorem. In this 
system however, there is no analogous relationship. 
A standard transformation, analogous to deriving Einstein's diffusion equation from 
the Langevin equation (Stratonovich, 1963, 1967), yields a probabilistic description 
for the evolution of the neural system, a form of Fokker-Planck equation for the time 
evolution of P({y}), the probability that the network is in a state described by the 
normalized rates {y}; we write the Fokker-Planck equation below for a simple case. 
112 Crair and Bialek 
A useful interpretation to consider is that the system, starting in a non-equilibrium 
state, diffuses or evolves in phase space, to a final stationary state. 
We can make our description of the post-synaptic response fit) more accurate 
by including two (or more) exponential time constants, corresponding roughly to 
the rise and fall time of the post synaptic potential. This inclusion necessitates 
the addition of a second order term in the Langevin equation (Eq. 4). This is 
analogous to including an inertial term in a diffusive description, so that the system 
is no longer purely dissipative. This additional complication has some interesting 
consequences. Adjusting the relative length of the rise to fall time of the post 
synaptic potential effects the rate of relaxation to local equilibrium of the system. 
In order to perform most efficaciously as an associative memory, a neural system 
will choose critical damping time constants, so that relaxation is fastest. Thus, 
by adjusting the time course of the post synaptic potential, the system can learn 
of a local stationary state, without adjusting the synaptic strengths. This novel 
learning mechanism could be a form of fine tuning of already established memories, 
or could be a unique form of dynamical short-term memory. 
4 QUALITATIVE RESULTS 
In order to understand the dynamics of our Fokker-Planck equation, we begin by 
considering the case of two neurons interacting with each other. There are two lim- 
iting behaviors. If the neurons are weakly coupled (J < Jc, Jc = 4/r,r), then the 
only stable state of the system is with both neurons firing at a mean firing rate, 
If the neurons are strongly (and positively) coupled (J > J), then isolated basins 
of attraction, or stationary states are formed, one stationary state corresponding 
to both neurons being active, the other state has both neurons relatively (but not 
absolutely) quiescent. In the strong coupling limit, one can reduce the problem 
to motion along the a collective coordinate connecting the two stable states. The 
resulting one dimensional Fokker-Planck equation is 
0 O [U(y)p(y,t)+yT(y)p(y,t)] ' (5) 
P(y,t) = Oy 
where U(y) is an effective potential energy, 
V'(y) = y(X - + - (c) 
7' 
1J2rmy3 . 
and T(y) is a spatially varying effective temperature, T(y) - 
One can solve to find the size of the stable regions, and the stationary probability 
distribution, 
B [ /�U'(Y)dy]. (7) 
P'(Y) = T(y) exp - T(y) 
We have done numerical simulations which confirm the qualitative predictions of the 
one dimensional Fokker-Planck equation. This analysis shows that the non-uniform 
Non-Boltzmann Dynamics in Networks of Spiking Neurons 113 
and asymmetric temperature distribution alters the relative stability of the stable 
states, in the favor of the 'off' state. This effect does have some biological pertinence, 
as it is well known that on average neurons are more likely to be quiescent then 
active. In our model the asymmetry is a direct consequence of the Poisson nature 
of the neuronal firing. 
Probability Current 
2 4 G 8 10 12 14 
rXta 
Figure 1: Probability current in the stationary state for two neurons that are 
strongly interacting. Computed as a ratio of the number of excess excursions in 
one direction to the total number of excursions, in percent. In thermodynamic 
equillibrium, detailed balance 
