Learning temporally persistent 
hierarchical representations 
Suzanna Becker 
Department of Psychology 
McMaster University 
Hamilton, Ont. LBS 4K1 
becker@mcmaster.ca 
Abstract 
A biologically motivated model of cortical self-organization is pro- 
posed. Context is combined with bottom-up information via a 
maximum likelihood cost function. Clusters of one or more units 
are modulated by a common contextual gating signal; they thereby 
organize themselves into mutually supportive predictors of abstract 
contextual features. The model was tested in its ability to discover 
viewpoint-invariant classes on a set of real image sequences of cen- 
tered, gradually rotating faces. It performed considerably better 
than supervised back-propagation at generalizing to novel views 
from a small number of training examples. 
I THE ROLE OF CONTEXT 
The importance of context effects  in perception has been demonstrated in many 
domains. For example, letters are recognized more quickly and accurately in the 
context of words (see e.g. McClelland & Rumelhart, 1981), words are recognized 
more efficiently when preceded by related words (see e.g. Neely, 1991), individual 
speech utterances are more intelligible in the context of continuous speech, etc. Fur- 
ther, there is mounting evidence that neuronal responses are modulated by context. 
For example, even at the level of the LGN in the thalamus, the primary source of 
visual input to the cortex, Murphy & Sillito (1987) have reported cells with end- 
stopped or length-tuned receptive fields which depend on top-down inputs from 
the cortex. The end-stopped behavior disappears when the top-down connections 
are removed, suggesting that the cortico-thalamic connections are providing contex- 
tual modulation to the LGN. Moving a bit higher up the visual hierarchy, yon der 
Heydt et al. (1984) found cells which respond to illusory contours, in the absence 
of a contoured stimulus within the cells' classical receptive fields. These exam- 
ples demonstrate that neuronal responses can be modulated by secondary sources 
of information in complex ways, provided the information is consistent with their 
expected or preferred input. 
We use the term context rather loosely here to mea any secondary source of input. 
It could be from a different sensory modality, a different input channel within the same 
modality, a temporal history of the input, or top-down information. 
Learning Temporally Persistent Hierarchical Representations 825 
Figure 1: Two sequences of 8 by 8 pixel images digitized with an IndyCam and prepro- 
cessed with a Sobel edge filter. Eleven views of each of four to ten faces were used in the 
simulations reported here. The alternate (odd) views of two of the faces are shown above. 
Why would contextual modulation be such a pervasive phenomenon? One obvious 
reason is that if context can influence processing, it can help in disambiguating or 
cleaning up a noisy stimulus. A less obvious reason may be that if context can 
influence learning, it may lead to more compact representations, and hence a more 
powerful processing system. To illustrate, consider the benefits of incorporating 
temporal history into an unsupervised classifier. Given a continuous sensory signal 
as input, the classifier must try to discover important partitions in its training 
data. If it can discover features that are temporally persistent, and thus insensitive 
to transformations in the input, it should be able to represent the signal compactly 
with a small set of features. Further, these features are more likely to be associated 
with the identity of objects rather than lower-level attributes. 
However, most classifiers group patterns together on the basis of spatial overlap. 
This may be reasonable if there is very little shift or other form of distortion between 
one time step and the next, but is not a reasonable assumption about the sensory 
input to the cortex. Pre-cortical stages of sensory processing, certainly in the visual 
system (and probably in other modalities), tend to remove low-order correlations in 
space and time, e.g. with centre-surround filters. Consider the image sequences of 
gradually rotating faces in Figure 1. They have been preprocessed by a simple edge- 
filter, so that successive views of the same face have relatively little pixel overlap. In 
contrast, identical views of different faces may have considerable overlap. Thus, a 
classifier such as k-means, which groups patterns based on their Euclidean distance, 
would not be expected to do well at classifying these patterns. So how are people 
(and in fact very young children) able to learn to classify a virtually infinite number 
of objects based on relatively brief exposures? It is argued here that the assumption 
of temporal persistence is a powerful constraining factor for achieving this, and is 
one which may be used to advantage in artificial neural networks as well. Not only 
does it lead to the development of higher-order feature analyzers, but it can result in 
more compact codes which are important for applications like image compression. 
Further, as the simulations reported here show, improved generalization may be 
achieved by allowing high-level expectations (e.g. of class labels) to influence the 
development of lower-level feature detectors. 
2 THE MODEL 
Competitive learning (for a review, see Becker 8z Plumbley, 1996) is considered 
by many to be a reasonably strong candidate model of cortical learning. It can 
be implemented, in its simplest form, by a Hebbian learning rule in a network 
826 S. Becker 
with lateral inhibition. However, a major limitation of competitive learning, and 
the majority of unsupervised learning procedures (but see the Discussion section), is 
that they treat the input as a set of independent identically distributed (iid) samples. 
They fail to take into account context. So they are unable to take advantage of the 
temporal continuity in signals. In contrast, real sensory signals may be better viewed 
as discretely sampled, continuously varying time-series rather than fid samples. 
The model described here extends maximum likelihood competitive learning 
(MLCL) (Nowlan, 1090) in two important ways: (i) modulation by context, and 
(ii) the incorporation of several canonical features of neocortical circuitry. The 
result is a powerful framework for modelling cortical self-organization. 
MLCL retains the benefits of competitive learning mentioned above. Additionally, 
it is more easily extensible because it maximizes a global cost function: 
L = y. log riYi (a) (1) 
a=l i=1 
where the i's are positive weighting coefficients which sum to one, and the yi's are 
the clustering unit activations: 
y?) = ri) (2) 
where I -) is the input vector for pattern c, and N() is the probability of I -a) under 
a Gaussian centred on the ith unit's weight vector, i, with covariance matrix 
Zi. For simplicity, Nowlan used a single glob variance parameter for all input 
dimensions, and allowed it to shrink during learning. MLCL actually mimizes 
the log likelihood (L) of the data under a mixture of Gaussians model, with mixing 
proportions equal to the ='s. L can be mimized by online gradient ascent  with 
learning rate : 
Thus, we have a Hebbian update rule with normalization of post-synaptic unit 
activations (which could be accomplished by shunting inhibition) and weight decay. 
2.1 Contextual modulation 
To integrate a contextual information source into MLCL, our first extension is to 
replace the mixing proportions (=i's) by the outputs of contextual gating units (see 
Figure 2). Now the i's are computed by separate processing units receiving their 
own separate stream of input, the context. The role of the gating signals here 
is analagous to that of the gating network in the (supervised) competing experts 
model (Jacobs et al., 1991)fi For the network shown in Figure 2, the context is 
simply a timedelayed version of the outputs of a module (explained in the next sub- 
section). However, more general forms of context are possible (see Discussion). In 
the simulations reported here, the context units computed their outputs according 
to a so, max function of their weighted summed inputs xi: 
=?) e() 
- (4) 
 e () 
We refer to the action of the gating units (the i's) as modulato because of the 
Nowlan (1990) used a slightly different on5ne weight update rule that more closely 
appromates the batch update rule of the EM gorithm (Dempster et al., 1977) 
SHowever, in the competing experts architecture, both the experts and gating network 
receive a common source of input. The competing experts model could be thought of as 
fitting a mixture model of the trning signal. 
Learning Temporally Persistent Hierarchical Representations 827 
input unlt 
Figure 2: The architecture used in the simulations reported here. Except where indicated, 
the gating units received all their inputs across unit delay lines with fixed weights of 1.0. 
multiplicative effect they have on the activities of the clustering units (the yi's). 
This multiplicative interaction is built into the cost function (Equation 1), and 
consequently, arises in the learning rule (Equation 3). Thus, clustering units are 
encouraged to discover features that agree with the current context signal they 
receive. If their context signal is weak or if they fail to capture enough of the 
activation relative to the other clustering units, they will do very little learning. 
Only if a unit's weight vector is sufficiently close to the current input vector and 
it's corresponding gating unit is strongly active will it do substantial learning. 
2.2 Modular, hierarchical architecture 
Our second modification to MLCL is required to apply it to the architecture shown 
in Figure 2, which is motivated by several ubiquitous features of the neocortex: a 
laminar structure, and a functional organization into cortica
