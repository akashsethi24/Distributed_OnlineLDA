Learning Curves: Asymptotic Values and 
Rate of Convergence 
Corinna Cortes, L. D. Jackel, Sara A. Solla, Vladimir Vapnik, 
and John S. Denker 
AT&T Bell Laboratories 
Holmdel, NJ 07733 
Abstract 
Training classifiers on large databases is computationally demand- 
ing. It is desirable to develop efficient procedures for a reliable 
prediction of a classifier's suitability for implementing a given task, 
so that resources can be assigned to the most promising candidates 
or freed for exploring new classifier candidates. We propose such 
a practical and principled predictive method. Practical because it 
avoids the costly procedure of training poor classifiers on the whole 
training set, and principled because of its theoretical foundation. 
The effectiveness of the proposed procedure is demonstrated for 
both single- and multi-layer networks. 
I Introduction 
Training classifiers on large databases is computationally demanding. It is desirable 
to develop efficient procedures for a reliable prediction of a classifier's suitability 
for implementing a given task. Here we describe such a practical and principled 
predictive method. 
The procedure applies to real-life situations with huge databases and limited re- 
sources. Classifier selection poses a problem because training requires resources -- 
especially CPU-cycles, and because there is a combinatorical explosion of classifier 
candidates. Training just a few of the many possible classifiers on the full database 
might take up all the available resources, and finding a classifier particular suitable 
for the task requires a search strategy. 
327 
328 Cortes, Jackel, Solla, Vapnik, and Denker 
test 
error 
I I I I I I training 
10,000 30,000 50,000 
set size 
Figure 1' Test errors as a function of the size of the training set for three different 
classifiers. A classifier choice based on best test error at trailting set size l0 = 10,000 
will result in an inferior classifier choice if the full database contains more than 
15,000 patterils. 
The naive solution to tile resource dilemma is to reduce tile size of the database to 
l = 10, so that it is feasible to train all classifier candidates. The performance of the 
classifiers is estimated froln an independently chosen test set after training. This 
makes up one point for each classifier in a plot of the test error as a function of the 
size I of the training set. The naive search strategy is to keep the best classifier at 
10, under the assumption that the relative ordering of the classifiers is unchanged 
when the test error is extrapolated from the reduced size l0 to the full database size. 
Such an assumption is questionable and could easily result in an inferior classifier 
choice as illustrated in Fig. 1. 
Our predictive method also utilizes extrapolation from medium sizes to large sizes 
of the training set, but it is based oll several data points obtained at various sizes 
of the training set in the intermediate size regime where the computational cost of 
training is low. A change in the representation of the measured data points is used 
to gain confidence in the extrapolation. 
2 A Predictive Method 
Our predictive method is based on a simple modeling of tile learning curves of a 
classifier. By learning curves we mean the expectation value of the test and training 
errors as a function of the training set size I. The expectation value is taken over 
all the possible ways of choosing a training set of a given size. 
A typical example of learning curves is shown ill Fig. 2. Tile test error is always 
larger than the training error, but asymptotically they reach a common value, a. 
We model the errors for large sizes of the training set as power-law decays to the 
Learning Curves: Asymptotic Values and Rate of Convergence 329 
error 
test error 
........... ..... training error 
training set size, 
Figure 2: Learning curves for a typical classifier. For all finite values of the 
training set size I the test error is larger than the training error. Asymptotically 
they converge to the same value a. 
asymptotic error value, a: 
b c 
Stest = a + l- and Strai n = a - l- 
where I is the size of the training set, and ( and  are positive exponents. From 
these two expressions the sum and difference is formed: 
b c 
Stest+Strai n - 2a+ l- --- (1) 
b c 
Stest -Strain = l - + l- (2) 
If we make the assumption  =  and b - c the equation (1) and (2) reduce to 
Stest + �train = 2a 
2b 
�test - �train = 
(3) 
These expressions suggest a log-log representation of the sum and difference of the 
test and training errors as a filnction of the the training set size l, resulting in 
two straight lines for large sizes of the training set: a constant ,- log(2a) for the 
sum, and a straight line with slope -o and intersection log(b + c) ,. log(2b) for the 
difference, as shown in Fig. 3. 
The assumption of equal alnplitudes b = c of the two convergent terms is a conve- 
nient but not crucial simplification of the model. YVe find experimentally that for 
classifiers where this approximation does not hold, the difference Stest -Strai n 
still forms a straight line in a log-log-plot. Froin this line the sum s = b + c 
can be extracted as the intersection, as indicated on Fig. 3. The weighted sum 
330 Cortes, Jackel, Solla, Vapnik, and Denker 
log(error) 
Iog(b+c) ~ 
Iog(2b) 
, 1og(�test +�train ) ~ log(2a) 
1 lt % - Etrain ) . 
log(training set size, /) 
Figure 3: Within the validity of the power-law modeling of the test and training 
errors, the sum and difference between the two errors as a function of training set 
size give two straight lines in a log-log-plot: a constant ~ log(2a) for the sum, and a 
straight line with slope -r and intersection log(b + c) ~ log(2b) for the difference. 
c � Stest + b � Strai n will give a constant for an appropriate choice of b and c, with 
b+c-s. 
The validity of the above model was tested on numerous boolean classifiers with 
linear decision surfaces. In all experiments we found good agreement with the model 
and we were able to extract reliable estimates of the three parameters needed to 
model the learning curves: the asymptotic value a, and the power c, and amplitude 
b of the power-law decay. An example is shown in Fig. 4, (left). The considered 
task is separation of handwritten digits 0-4 from the digits 5-9. This problem is 
unrealizable with the given database and classifier. 
The simple modeling of the test and training errors of equation (3) is only assumed 
to hold for large sizes of the training set, but it appears to be valid already at 
intermediate sizes, as seen in Fig. 4, (left). The predictive model suggested here is 
based on this observation, and it can be illustrated from Fig. 4, (left): with test 
and training errors measured for l _< 2560 it is possible to estimate the two straight 
lines, extract approximate values for the three parameters which characterize the 
learning curves, and use the resulting power-laws to extrapolate the learning curves 
to the full size of the database. 
The algorithm for the predictive method is therefore as follows: 
1. Measure Stest and Strai n for intermediate sizes of the training set. 
2. Plot log(Stest + Strain) and log(Stest - Strain) versus logl. 
3. Estimate the two straight lines and extract the asymptotic value a 
the amplitude b, and the exponent a. 
4. Extrapolate the learning curves to the full size of the database. 
Learning Curves: Asymptotic Values and Rate of Convergence 331 
-1 
-2 
-3 
irr) 
-test + qxain 0.25 
0.2 
0.15 
st- ffr--tra/n 
0.1, 
0.05 
0 1 2 log ( l/256) 0 
! ! I 
256 2560 25600 l  
error 
st error 
training error 
training set size, I 
---. points used for prediction 
.... predicted learning curves 
Figure 4: 
Left: Test of the model for a 256 dimensional boolean classifier trained by minimiz- 
ing a mean squared error. The sum and difference of the test and training errors 
are shown as a function of the normalized training set size in a log-log-plot (base 
10). Each point is the mean with standard deviation for ten different choices of a 
training set of the given size. The straight line with a = 1, corresponding to a 1/l 
decay, is shown as a reference. 
Right: Prediction of learning curves for a 256 dimensional boolean classifier trained 
by minimizing a mean squared error. Measured errors for training set size of 
I _< 2560 are used to fit the two proposed straight lines in a log-log plot. The 
three parameters which characterize the learning curves are extracted and used for 
extrapolation. 
A prediction for a boolean classifier with linear decision surface is illustrated in 
Fig. 4, (right). The prediction is excellent for this type of classifiers because the 
sum and difference of the test and training errors converge quickly to two straight 
lines in a log-log-plot. Unfortunately, linear decision surfaces are in general not 
adequate for many real-life applications. 
The usefulness of the predictive method proposed here can be judged from its per- 
formance on real-life sophisticated multi-layer networks. Fig. 5 demonstrates the 
validity of the model even for a fully-connected multi-layer network operating in its 
non-linear regime to implement an unrealizable digit recognition task. Already for 
intermediate sizes of the training set the sum and difference between the test and 
training errors are again observed to follow straight lines. 
The predictive method was finally tested on sparsely connected multi-layer net- 
works. Fig. 6, (left), shows the test and training errors for two networks trained 
for the recognition of handwritten digits. The network termed old is commonly 
referred to as LeNet [LCBD+90]. The network termed new is a modification of 
LeNet with additional feature maps. The full size of the database is 60,000 patterns, 
332 Cortes, Jackel, Solla, Vapnik, and Denker 
log(error) a 
-1 
-2 
-3 test,-- Etrain 
log (l/100) 
1000 10000 100000 I 
F
