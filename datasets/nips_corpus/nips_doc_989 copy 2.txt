Rapid Quality Estimation of Neural 
Network Input Representations 
Kevin J. Cherkauer Jude W. $havlik 
Computer Sciences Department, University of Wisconsin-Madison 
1210 W. Dayton St., Madison, WI 53706 
{ cherkauer, shavlik) @ cs. wisc. edu 
Abstract 
The choice of an input representation for a neural network can have 
a profound impact on its accuracy in classifying novel instances. 
However, neural networks are typically computationally expensive 
to train, making it difficult to test large numbers of alternative 
representations. This paper introduces fast quality measures for 
neural network representations, allowing one to quickly and ac- 
curately estimate which of a collection of possible representations 
for a problem is the best. We show that our measures for ranking 
representations are more accurate than a previously published mea- 
sure, based on experiments with three difficult, real-world pattern 
recognition problems. 
I Introduction 
A key component of successful artificial neural network (ANN) applications is an 
input representation that suits the problem. However, ANNs are usually costly to 
train, preventing one from trying many different representations. In this paper, 
we address this problem by introducing and evaluating three new measures for 
quickly estimating ANN input representation quality. Two of these, called ID$1eaves 
and Min(leaves), consistently outperform Rendell and Ragavan's (1993) blurring 
measure in accurately ranking different input representations for ANN learning on 
three difficult, real-world datasets. 
2 Representation Quality 
Choosing good input representations for supervised learning systems has been 
the subject of diverse research in both connectionist (Cherkauer & Shavlik, 1994; 
Kambhatla & Leen, 1994) and symbolic paradigms (Almuallim & Dietterich, 1994; 
4 6 K.J. CHERKAUER, J. W. SHAVLIK 
Caruana & Freitag, 1994; John et al., 1994; Kira & Rendell, 1992). Two factors 
of representation quality are well-recognized in this work: the ability to separate 
examples of different classes (sufficiency of the representation) and the number of 
features present (representational economy). We believe there is also a third impor- 
tant component that is often overlooked, namely the ease of learning an accurate 
concept under a given representation, which we call transparency. We define trans- 
parency as the density of concepts that are both accurate (generalize well) and 
simple (of low complexity) in the space of possible concepts under a given input 
representation and learning algorithm. Learning an accurate concept will be more 
likely if the concept space is rich in accurate concepts that are also simple, because 
simple concepts require less search to find and less data to validate. 
In this paper, we introduce fast transparency measures for ANN input represen- 
tations. These are orders of magnitude faster than the wrapper method (John 
et al., 1994), which would evaluate ANN representations by training and testing 
the ANNs themselves. Our measures are based on the strong assumption that, 
for a fixed input representation, information about the density of accurate, simple 
concepts under a (fast) decision-tree learning algorithm will transfer to the concept 
space of an ANN learning algorithm. Our experiments on three real-world datasets 
demonstrate that our transparency measures are highly predictive of representation 
quality for ANNs, implying that the transfer assumption holds surprisingly well for 
some pattern recognition tasks even though ANNs and decision trees are believed 
to work best on quite different types of problems (Quinlan, 1994).  In addition, our 
Exper. i shows that transparency does not depend on representational sufficiency. 
Exper. 2 verifies this conclusion and also demonstrates that transparency does not 
depend on representational economy. Finally, Exper. 3 examines the effects of re- 
dundant features on the transparency measures, demonstrating that the ID$1eaves 
measure is robust in the face of such features. 
2.1 Model-Based Transparency Measures 
We introduce three new model-based measures that estimate representational 
transparency by sampling instances of roughly accurate concept models from a 
decision-tree space and measuring their complexities. If simple, accurate models 
are abundant, the average complexity of the sampled models will be low. If they 
are sparse, we can expect a higher complexity value. 
Our first measure, avg(leaves), estimates the expected complexity of accurate con- 
cepts as the average number of leaves in n randomly constructed decision trees that 
correctly classify the training set: 
n 
avg(leaves) -- 1 t=l leaves(t) 
n 
where leaves(t) is the number of leaves in tree t. Random trees are built top-down; 
features are chosen with uniform probability from those which further partition the 
training examples (ignoring example class). Tree building terminates when each 
leaf achieves class purity (i.e., the tree correctly classifies all the training examples). 
High values of avg(leaves) indicate high concept complexity (i.e., low transparency). 
The second measure, min(leaves), finds the minimum number of leaves over the n 
randomly constructed trees instead of the average to reflect the fact that learning 
systems try to make intelligent, not random, model choices: 
rain(leaves) -- min {leaves(t)} 
1We did not preselect datasets based on whether our experiments upheld the transfer 
assumption. We report the results for all datasets that we have tested our transparency 
measures on. 
Rapid Quality Estimation of Neural Network Input Representations 4 7 
Table 1: Summary of datasets used. 
Dataset Examples Classes Cross Validation Folds 
DNA 20,000 6 4 
NIST 3,471 10 10 
Magellan 625 2 4 
The third measure, ID$1eaves, simply counts the number of leaves in the tree grown 
by Quinlan's (1986) ID3 algorithm: 
ID31eaves -- leaves(ID3 tree) 
We always use the full ID3 tree (100% correct on the training set). This measure 
assumes the complexity of the concept ID3 finds depends on the density of simple, 
accurate models in its space and thus reflects the true transparency. 
All these measures fix tree training-set accuracy at 100%, so simpler trees imply 
more accurate generalization (Fayyad, 1994) as well as easier learning. This lets us 
estimate transparency without the multiplicative additional computational expense 
of cross validating each tree. It also lets us use all the training data for tree building. 
2.2 Blurring as a Transparency Measure 
Rendell and Ragavan (1993) address ease of learning explicitly and present a met- 
ric for quantifying it called blurring. In their framework, the less a representation 
requires the use of feature interactions to produce accurate concepts, the more 
transparent it is. Blurring heuristically estimates this by measuring the average 
information content of a representation's individual features. Blurring is equivalent 
to the (negation of the) average information gain (Quinlan, 1986) of a representa- 
tion's features with respect to a training set, as we show in Cherkauer and Shavlik 
(1995). 
3 Evaluating the Transparency Measures 
We evaluate the transparency measures on three problems: DNA (predicting gene 
reading frames; Craven & Shavlik, 1993), NIST (recognizing handwritten digits; 
F13 distribution), and Magellan (detecting volcanos in radar images of the planet 
Venus; Burl et al., 1994). 2 The datasets are summarized in Table 1. 
To assess the different transparency measures, we follow these steps for each dataset 
in Exper. i and 2: 
1. Construct several different input representations for the problem. 
2. Train ANNs using each representation and test the resulting generalization 
accuracy via cross validation (CV). This gives us a (costly) ground-truth 
ranking of the relative qualities of the different representations. 
3. For each transparency measure, compute the transparency score of each 
representation. This gives us a (cheap) predicted ranking of the represen- 
tations from each measure. 
4. For each transparency measure, compute Spearman's rank correlation co- 
efficient between the ground-truth and predicted rankings. The higher this 
correlation, the better the transparency measure predicts the true ranking. 
2On these problems, we have found that ANNs generalize 1-6 percentage points better 
than decision trees using identical input representations, motivating our desire to develop 
fast measures of ANN input representation quality. 
4 8 K.J. CHERKAUER, J. W. SHAVLIK 
Table 2: User CPU seconds on a Sun SPARCstation 10/30 for the largest representation 
of each dataset. Parenthesized numbers are standard deviations over 10 runs. 
Dataset tl Blurring ID31eaves Min/Avg(leaves) I Backpro p 
DNA 1.68 (2.38) 1,245 (3.96) 13,444 (56.25) 212,900 
NIST 2.69 (2.31) 221 (2.75) 1,558 ( 5.00) 501,400 
Magellan 0.21 (0.15) 1 (0.07) 12 ( 0.13) 6,300 
In Exper. 3 we rank only two representations at a time, so instead of computing a 
rank correlation in step 4, we just count the number of pairs ranked correctly. 
We created input representations (step 1) with an algorithm we call RS (Repre- 
sentation Selector). RS first constructs a large pool of plausible, domain-specific 
Boolean features (5,460 features for DNA, 251,679 for NIST, 33,876 for Magellan). 
For each CV fold, RS sorts the features by information gain on the entire training 
set. Then it scans the list, selecting each feature that is not strongly pairwise de- 
pendent on any feature already selected according to a standard X e independence 
test using the X  statistic. 
This produces a single reasonable input representation, R. 3 To obtain the addi- 
tional representations needed for the ranking experiments, we ran RS several times 
with successively smaller subsets of the initial feature pool, created by deleting 
features whose training-set information gains were above differen
