Using Aperiodic Reinforcement for Directed 
Self-Organization During Development 
PR Montague 
P Dayan SJ Nowlan A Pouget 
CNL, The Salk Institute 
10010 North Torrey Pines Rd. 
La Jolla, CA 92037, USA 
readhelmholtz. sdsc. edu 
TJ Sejnowski 
Abstract 
We present a local learning rule in which Hebbian learning is 
conditional on an incorrect prediction of a reinforcement signal. 
We propose a biological interpretation of such a framework and 
display its utility through examples in which the reinforcement 
signal is cast as the delivery of a neuromodulator to its target. 
Three examples are presented which illustrate how this framework 
can be applied to the development of the oculomotor system. 
1 INTRODUCTION 
Activity-dependent accounts of the self-organization of the vertebrate brain have 
relied ubiquitously on correlational (mainly Hebbian) rules to drive synaptic learn- 
ing. In the brain, a major problem for any such unsupervised rule is that many 
different kinds of correlations exist at approximately the same time scales and each 
is effectively noise to the next. For example, relationships within and between 
the retinae among variables such as color, motion, and topography may mask one 
another and disrupt their appropriate segregation at the level of the thalamus or 
cortex. 
It is known, however, that many of these variables can be segregrated both within 
and between cortical areas suggesting that certain sets of correlated inputs are 
somehow separated from the temporal noise of other inputs. Some form of super- 
vised learning appears to be required. Unfortunately, detailed supervision and 
969 
970 Montague, Dayan, Nowlan, Pouget, and Sejnowski 
selection in a brain region is not a feasible mechanism for the vertebrate brain. The 
question thus arises: What kind of biological mechanism or signal could selectively 
bias synaptic learning toward a particular subset of correlations ? One answer lies 
in the possible role played by diffuse neuromodulatory systems. 
It is known that multiple diffuse modulatory systems are involved in the self- 
organization of cortical structures (eg Bear and Singer, 1986) and some of them 
appear to deliver reward and/or salience signals to the cortex and other structures 
to influence learning in the adult. Recent data (Ljunberg, et al, 1992) suggest that 
this latter influence is qualitatively similar to that predicted by Sutton and Barto's 
(1981,1987) classical conditioning theory. These systems innervate large expanses 
of cortical and subcortical turf through extensive axonal projections that originate 
in midbrain and basal forebrain nuclei and deliver such compounds as dopamine, 
serotonin, norepinephrine, and acetylcholine to their targets. The small number of 
neurons comprising these subcortical nuclei relative to the extent of the territory 
their axons innervate suggests that the nuclei are reporting scalar signals to their 
target structures. 
In this paper, these facts are synthesized into a single framework which relates 
the development of brain structures and conditioning in adult brains. We pos- 
tulate a modification to Hebbian accounts of self-organization: Hebbian learning 
is conditional on a incorrect prediction of future delivered reinforcement from a 
diffuse neuromodulatory system. This reinforcement signal can be derived both 
from externally driven contingencies such as proprioception from eye movements 
as well as from internal pathways leading from cortical areas to subcortical nuclei. 
The next section presents our framework and proposes a specific model for how 
predictions about future reinforcement could be made in the vertebrate brain uti- 
lizing the firing in a diffuse neuromodulatory system (figure 1). Using this model 
we illustrate the framework with three examples suggesting how mappings in the 
oculomotor system may develop. The first example shows how eye movement 
commands could become appropriately calibrated in the absence of visual experi- 
ence (figure 3). The second example demonstrates the development of a mapping 
from a selected visual target to an eye movement which acquires the target. The 
third example describes how our framework could permit the development and 
alignment of multimodal maps (visual and auditory) in the superior colliculus. In 
this example, the transformation of auditory signals from head-centered to eye- 
centered coordinates results implicitly from the development of the mapping from 
parietal cortex onto the colliculus. 
2 THEORY 
We consider two classes of reinforcement learning (RL) rule: static and dynamic. 
2.1 Static reinforcement learning 
The simplest learning rule that incorporates a reinforcement signal is: 
/lA/t --- 0XtUtlt 
(1) 
Using Aperiodic Reinforcement for Directed Self-Organization During Development 971 
where, all at times t, wt is a connection weight, xt an input measure, t3t an output 
measure, rt a reinforcement measure, and oc is the learning rate. 
In this case, r can be driven by either external events in the world or by cortical 
projections (internal events) and it picks out those correlations between x and t3 
about which the system learns. Learning is shut down if nothing occurs that is 
independently judged to be significant, i.e. events for which r is 0. 
2.2 Dynamic Reinforcement learning - learning driven by prediction error 
A more informative way to utilize reinforcement signals is to incorporate some 
form of prediction. The predictive form of RL, called temporal difference learning 
(TD, Sutton and Barto, 1981,1987), specifies weight changes according to: 
awt = + vd 
(2) 
where rt+l is the reward delivered in the next instant in time t + 1. V is called 
a value function and its value at any time t is an estimate of the future reward. 
This framework is closely related to dynamic programming (Barto et al, 1989) and 
a body of theory has been built around it. The prediction error [(rt+ + Vt+) - Vd, 
measures the degree to which the prediction of future reward Vt is higher or lower 
than the combination of the actual future reward rt+l and the expectation of reward 
from time t + 1 onward (Vt+). 
To place dynamic RL in a biological context, we start with a simple Hebbian rule 
but make learning contingent on this prediction error. Learning therefore slows as 
the predictions about future rewards get better. In contrast with static RL, in a TD 
account the value of r per se is not important, only whether the system is able to 
predict or anticipate the the future value of r. Therefore the weight changes are: 
zxw = + vd 
(3) 
including a measure of post-synaptic response, t3 t. 
3 MAKING PREDICTIONS IN THE BRAIN 
In our account of RL in the brain, the cortex is the structure that makes predictions of 
future reinforcement. This reinforcement is envisioned as the output of subcortical 
nuclei which deliver various neuromodulators to the cortex that permit Hebbian 
learning. Experiments have shown that various of these nuclei, which have access 
to cortical representations of complex sensory input, are necessary for instrumental 
and classical conditioning to occur (Ljunberg et al., 1992). 
Figure 1 shows one TD scenario in which a pattern of activity in a region of cortex 
makes a prediction about future expected reinforcement. At time t, the prediction 
of future reward Vt is viewed as an excitatory drive from the cortex onto one or 
more subcortical nuclei (pathway B). The high degree of convergence in B ensures 
that this drive predicts only a scalar output of the nucleus R. Consider a pattern 
of activity onto layer II which provides excitatory drive to R and concomitantly 
causes some output, say a movement, at time t + 1. This movement provides a 
separate source of excitatory drive rt+ to the same nucleus through independent 
972 Montague, Dayan, Nowlan, Pouget, and Sejnowski 
Layer I  
Layer II 
External _ C _1 R  
contingencies - l 
Figure 1: Making predictions about future reinforcement. Layer I is an array of units 
that projects topographically onto layer II. (A) Weights from I onto II develop according to 
equation 3 and represent the value function Vt. (B) The weights from II onto R are fixed. The 
prediction of future reward by the weights onto II is a scalar because the highly convergent 
excitatory drive from II to the reinforcement nucleus (R) effectively sums the input. (C) 
External events in the world provide independent excitatory drive to the reinforcement nu- 
cleus. (D) Scalar signal which results from the output firing of R and is broadcast throughout 
layer II. This activity delivers to layer II the neuromodulator required for Hebbian learning. 
The output firing of R is controlled by temporal changes in its excitatory input and habit- 
uates to constant or slowly varying input. This makes for learning in layer II according to 
equation 3 (see text). 
connections conveying information from sensory structures such as stretch recep- 
tors (pathway C). Hence, at time t + 1, the excitatory input to R is the sum of 
the 'immediate reward' rt+ and the new prediction of future reward Vt+. If the 
reinforcement nucleus is driven primarily by changes in its input over some time 
window, then the difference between the excitatory drive at time t and t + 1, ie 
[(rt+ + Vt+) - Vt] is what its output reflects. 
The output is distributed throughout a region of cortex (pathway D) and permits 
Hebbian weight changes at the individual connections which determine the value 
function Vt. The example hinges on two assumptions: 1) Hebbian learning in the 
cortex is contingent upon delivery of the neuromodulator, and 2) the reinforcement 
nucleus is sensitive to temporal changes in its input and otherwise habituates to 
constant or slowly varying input. 
Initially; before the system is capable of predicting future delivery of reinforcement 
correctly; the arrival of rt+ causes a large learning signal because the prediction 
error [
