Gradient 
Descent: Second-Order Momentum 
and Saturating Error 
Barak Pearlmutter 
Department of Psychology 
P.O. Box 11A Yale Station 
New Haven, CT 06520-7447 
pearlmutter-barak@yale.edu 
Abstract 
Batch gradient descent, Aw(/) -- -rldE/dw(t), converes to a minimum 
of quadratic form with a time constant no better than Amax/Ami n where 
Ami n and Amax are the minimum and maximum eigenvalues of the Hessian 
matrix of E with respect to w. It was recently shown that adding a 
momentum term Aw(t) ---- -rldE/dw(t ) q-cAw(t- 1) improves this to 
� X/Amax/Amin, although only in the batch case. Here we show that second- 
order momentum, Aw(/) -- -rldE/dw(t ) q- cAw(t- 1)q- fiAw(t- 2), can 
lower this no further. We then regard gradient descent with momentum 
as a dynamic system and explore a nonquadratic error surface, showing 
that saturation of the error accounts for a variety of effects observed in 
simulations and justifies some popular heuristics. 
1 INTRODUCTION 
Gradient descent is the bread-and-butter optimization technique in neural networks. 
Some people build special purpose hardware to accelerate gradient descent optimiza- 
tion of backpropagation networks. Understanding the dynamics of gradient descent 
on such surfaces is therefore of great practical value. 
Here we briefly review the known results in the convergence of batch gradient de- 
scent; show that second-order momentum does not give any speedup; simulate a 
real network and observe some effect not predicted by theory; and account for these 
effects by analyzing gradient descent with momentum on a saturating error surface. 
887 
888 Pearlmutter 
1.1 SIMPLE GRADIENT DESCENT 
First, let us review the bounds on the convergence rate of simple gradient descent 
without momentum to a minimum of quadratic form [11, 1]. Let w* be the minimum 
of E, the error, H = d2E/dw2(w*), and Ai, vi be the eigenvalues and eigenvectors 
of H. The weight change equation 
dE 
Aw = --r/dw (1) 
(where Af(t) = f(t + 1)- f(t)) is limited by 
0 < r/< 2/Amax 
(2) 
We can substitute r/= 2/Amax into the weight change equation to obtain conver- 
gence that tightly bounds any achievable in practice, getting a time constant of 
convergence of-1/log(1 - 2s) = (2s) - + O(1), or 
E-E* >- exp(-4st) (3) 
where we use s = Amin/Amax for the inverse eigenvalues spread of H and >- is read 
asymptotically converges to zero more slowly than. 
1.2 FIRST-ORDER MOMENTUM 
Sometimes a momentum term is used, the weight update (1) being modified to 
incorporate a momentum term c < 1 [5, equation 16], 
Aw(t) = -r/d-(t ) + aAw(t - 1). 
(4) 
The Momentum LMS algorithm, MLMS, has been analyzed by Shynk and Roy [6], 
who have shown that the momentum term can not speed convergence in the online, 
or stochastic gradient, case. In the batch case, which we consider here, Tugay and 
Tanik [9] have shown that momentum is stable when 
c< 1 and O<r/<2(c+l)/Amax (5) 
which speeds convergence to 
by 
E - E* >- exp(-(4v/ + O(s)) t) 
2- 4V/s(1- s ) 
(1 - 2s) 2 
- I = I - 4V5 q- O(s), 
(6) 
r]*-- 2(C* q- 1)/Ama x. (7) 
2 SECOND-ORDER MOMENTUM 
The time constant of asymptotic convergence can be changed from O(Amax/Amin) to 
O(V/Amax/Amin) by going from a first-order system, (1), to a second-order system, 
(4). Making a physical analogy, the first-order system corresponds to a circuit with 
Gradient Descent: Second Order Momentum and Saturating Error 889 
--2 
O.S 
Figure 1: Second-order momentum converges if Omax is less than the value plotted 
as eta, as a function of a and fl. The region of convergence is bounded by four 
smooth surfaces: three planes and one hyperbola. One of the planes is parallel 
to the r/ axis, even though the sampling of the plotting program makes it appear 
slightly sloped. Another is at r/= 0 and thus hidden. The peak is at 4. 
a resistor, and the second-order system adds a capacitor to make an RC oscillator. 
One might ask whether further gains can be had by going to a third-order system, 
dE 
Aw(t) = -r/ww + aAw(t - 1) + fiAw(t - 2). (8) 
For convergence, all the eigenvalues of the matrix 
Mi = 0 0 1 
-/3 -c + /3 1- rlAi + c 
in ((t- l) (t) (t + 1))'  ((t- ) (t - l) (t))' must have absolute 
value less than or equal to 1, which occurs precisely when 
0 < r I <_ 4(/3+1)/Ai 
,Ai/2- (1- fi) _ o < firlAi/2 + (1- fi). 
For fi _< 0 this is most restrictive for Amax, but for fi > 0 /min also comes into play. 
Taking the limit as Amin - 0, this gives convergence conditions for gradient descent 
with second-order momentum of 
when a _< 3/3+ 1 � 
0< 
when c >_ 3/3 + 1 � 
0< 
c 51-/3 
2 
< --(1 + c -/3) 
r/ - Amx 
(9) 
/3+1 ( +/3- 1) 
890 Pearlmutter 
a region shown in figure 1. 
Fastest convergence for Ami n within this region lies along the ridge a = 3fi + 1, 
r/= 2(1 + a - fi)/Amax. Unfortunately, although convergence is slightly faster than 
with first-order momentum, the relative advantage tends to zero as s -- 0, giving 
negligible speedup when Amax >> Amin. For small s, the optimal settings of the 
parameters are 
= 1 - 9vq+ o(s) 
/7., __ 3 
- o(s) 
r/** = 4(1-v/j)+O(s) 
(10) 
where a* is as in (7). 
3 SIMULATIONS 
We constructed a standard three layer backpropagation network with 10 input units, 
3 sigmoidal hidden units, and 10 sigmoidal output units. 15 associations between 
random 10 bit binary input and output vectors were constructed, and the weights 
were initialized to uniformly chosen random values between -1 and +1. Training 
was performed with a square error measure, batch weight updates, targets of 0 and 
1, and a weight decay coefficient of 0.01. 
To get past the initial transients, the network was run at r) - 0.45, c - 0 for 
150 epochs, and at r/: 0.3, c = 0.9 for another 200 epochs. The weights were then 
saved, and the network run for 200 epochs for r/ranging from 0 to 0.5 and c ranging 
from 0 to I from that starting point. 
Figure 3 shows that the region of convergence has the shape predicted by theory. 
Calculation of the eigenvalues of d2E/dw 2 confirms that the location of the bound- 
ary is correctly predicted. Figure 2 shows that momentum speeded convergence by 
the amount predicted by theory. Figure 3 shows that the parameter setting that 
give the most rapid convergence in practice are the settings predicted by theory. 
However, within the region that does not converge to the minimum, there appear 
to be two regimes: one that is characterized by apparently chaotic fluctuations 
of the error, and one which slopes up gradually from the global minimum. Since 
this phenomenon is so atypical of a quadratic minimum in a linear system, which 
either converges or diverges, and this phenomenon seems important in practice, we 
decided to investigate a simple system to see if this behavior could be replicated 
and understood, which is the subject of the next section. 
4 GRADIENT DESCENT WITH SATURATING ERROR 
The analysis of the sections above may be objected to on the grounds that it assumes 
the minimum to have quadratic form and then performs an analysis in the neigh- 
borhood of that minimum, which is equivalent to analyzing a linear unit. Surely 
our nonlinear backpropagation networks are richer than that. 
Gradient Descent: Second Order Momentum and Saturating Error 891 
0.69,5 
0.685 
0.680 
,=.9.25.,7=.o.2 ,rs. <=70 ?=.o.17 
3,50 400 ,*.50 500 550 
epoch 
Figure 2: Error plotted as a function of time for two settings of the learning param- 
eters, both determined empirically: the one that minimized the error the most, and 
the one with  = 0 that minimized the error the most. There exists a less aggressive 
setting of the parameters that converges nearly as fast as the quickly converging 
curve but does not oscillate. 
A clue that this might be the case was shown in figure 3. The region where the 
system converges to the minimum is of the expected shape, but rather than simply 
diverging outside of this region, as would a linear system, more complex phenomena 
are observed, in particular a sloping region. 
Acting on the hypothesis that this region is caused by Amax being maximal at 
the minimum, and gradually decreasing away from it (it must decrease to zero in 
the limit, since the hidden units saturate and the squared error is thus bounded) 
we decided to perform a dynamic systems analysis of the convergence of gradient 
descent on a one dimensional nonquadratic error surface. We chose 
1 
E=I (11) 
l+w 2 
which is shown in figure 4, as this results in a bounded E. 
Letting 
f(w) = w- rlE'(w ) = w(1 - 2r/+ 2w 2 + w 4) (12) 
(1 + w2) = 
be our transfer function, a local analysis at the minimum gives Ama x -' E//(0) ---- 2 
which limits convergence to r/ < 1. Since the gradient towards the minimum is 
always less than predicted by a second-order series at the minimum, such r/are in 
fact globally convergent. As r/passes I the fixedpoint bifurcates into the limit cycle 
w - :J:Fv/- - 1, (13) 
which remains stable until r/- 16/9 = 1.77777..., at which point the single sym- 
metric binary limit cycle splits into two asymmetric limit cycles, each still of period 
two. These in turn remain stable until r/- 2.0732261475-, at which point repeated 
period doubling to chaos occurs. This progression is shown in figure 7. 
892 Pearlmutter 
Figure 3: (Left) the error at epoch 550 as a function of the learning regime. Shading 
is based on the height, but most of the vertical scale is devoted to nonconvergent net- 
works in order to show the mysterious nonconvergent sloping region. The minimum, 
corresponding to the most darkly shaded point, is on the plateau of convergence 
at the location predicted by the theory. (Center) the region in which the network 
is convergent, as measured by a strictly monotonically decreasing error. Learning 
parameter settings for which the error was strictly decreasing have a low value while 
those for which it was not have a high one. The lip at r/= 0 has a value of 0, given 
where the error did not ch
