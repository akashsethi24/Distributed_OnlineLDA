The Belief in TAP 
Yoshiyuki Kabashima 
Dept. of Compt. Intl. &; Syst. Sci. 
Tokyo Institute of Technology 
Yokohama 226, Japan 
David Saad 
Neural Computing Research Group 
Aston University 
Birmingham B4 7ET, UK 
Abstract 
We show the similarity between belief propagation and TAP, for 
decoding corrupted messages encoded by Sourlas's method. The 
latter is a special case of the Gallager error-correcting code, where 
the code word comprises products of Ix' bits selected randomly from 
the original message. We examine the efficacy of solutions obtained 
by the two methods for various values of Ix' and show that solutions 
for I( > 3 may be sensitive to the choice of initial conditions in 
the case of unbiased patterns. Good approximations are obtained 
generally for I, = 2 and for biased patterns in the case of Ix' > 3, 
especially when Nishimori's temperature is being used. 
I Introduction 
Belief networks [1] are diagrammatic representations of joint probability distribu- 
tions over a set of variables. This set is usually represented by the vertices of 
a graph, while arcs between vertices represent probabilistic dependencies between 
variables. Belief propagation provides a convenient mathematical tool for calculat- 
ing iteratively joint probability distributions between variables and have been used 
in a variety of cases, most recently in the field of error correcting codes, for decoding 
corrupted messages [2] (for a review of graphical models and their use in the context 
of error-correcting codes see [3]). 
Error-correcting codes provide a mechanism for retrieving the original message after 
corruption due to noise during transmission. Of a particular interest to the current 
paper is an error-correcting code presented by Sourlas [4] which is a special case of 
the Gallager codes [5]. The latter have been recently re-discovered by MacKay and 
Neal [2] and seem to have a significant practical potential. 
In this paper we will examine the similarities between the belief propagation (BP) 
and TAP approaches, used to decode corrupted messaged encoded by Sourlas's 
method, and compare the solutions obtained by both approaches to the exact results 
obtained using the replica method [8]. The statistical mechanics approach will then 
The Belief in TAP 247 
allow us to draw some conclusion on the efficacy of the TAP/BP approach in the 
context of error correcting codes. 
The paper is arranged in the following manner: In section 2 we will introduce the 
encoding method and describe the decoding task. The Belief Propagation approach 
to the decoding process will be introduced in section 3 and will be compared to the 
TAP approach for diluted spin systems in section 4. Numerical solutions for various 
cases will be presented in section 5 and we will summarize our results and discuss 
their implications in section 6. 
2 The decoding problem 
In a general scenario, a message represented ly an N dimensional binary vector  
is encoded by a vector j0 which is then transmitted through a noisy channel with 
some flipping probability p per bit. The received message J is then decoded to 
retrieve the original message. Sourlas's code [4], is based on encoded message bits 
of the form Jiï¿½,i iK=ii ... iK, taking the product of different K message sites 
for each code wm:' bit. 
In the statistical mechanics approach we will attempt to retrieve the original mes- 
sage by exploring the ground state of the following Hamiltonian which corresponds 
to the preferred state of the system in terms of 'energy' 
'------ E 'A{ i,,...ig} J{i,...iK} Si...SiK- F//Sk, (1) 
(i ,...ic} 
where $ is an N dimensional binary vector of dynamical variables and .A is a sparse 
tensor with C unit elements per index (other elements are zero), which determines 
the components of j0. The last term on the right is required in the case of sparse 
(biased) messages and will require assigning a certain value to the additive field 
F//, related to the prior belief in the Bayesian framework. 
The statistical mechanical analysis can be easily linked to the Bayesian frame- 
work [4] in which one focuses on the posterior probability using Bayes theorem 
P($1J) 1-I P(Jul$) P0($) where y runs over the message components and 
represents the prior. Knowing the posterior one can calculate the typical retrieved 
message elements and their alignment, which correspond to the Bayes-optimal de- 
coding. The logarithms of the likelihood and prior terms are directly related to the 
first and second components of the Hamiltonian (Eq.1). 
One should also note that A{i,...i)J{i,...iK) represents a similar encoding scheme 
to that of Ref. [2] where a sparse matrix with Is; non-zero elements per row multiplies 
the original message  and the resulting vector, modulo 2, is transmitted. 
Sourlas analyzed this code in the cases of K = 2 and K - cx, where the ratio 
C/K -- cx, by mapping them onto the SK [9] and Random Energy [10] models 
respectively. However, the ratio R-K/C constitutes the code rate and the scenarios 
examined by Sourlas therefore correspond to the limited case of a vanishing code 
rate. The case of finite code rate, which we will consider here, has only recently 
been analyzed [8]. 
3 Decoding by belief propagation 
As our goal, of calculating the posterior of the system P($[J) is rather difficult, we 
resort to the methods of BP, focusing on the calculation of conditional probabilities 
when some elements of the system are set to specific values or removed. 
248 Y. Kabashirna and D. Saad 
The approach adopted in this case, which is quite similar to the practical approach 
employed in the case of Gallager codes [2], assumes a two layer system corresponding 
to the elements of the corrupted message J and the dynamical variables $ respec- 
tively, defining conditional probabilities which relate elements in the two layers: 
(2) 
where the index y represents an element of the received vector message J, con- 
stituted by a particular choice of indices i,...iK, which is connected to the cor- 
responding index of $ (l in the first equation), i.e., for which the corresponding 
element .A{i,...iK) is non-zero; the notation {S;t} refers to all elements of $, ex- 
cluding the/-th element, which are connected to the corresponding index of J (y 
in this case for the second equation); the index a: can take values of +1. The con- 
ditional probabilities qu and r x will enable us, through recursive calculations to 
ul 
obtain an approximated expression to the posterior. 
Employing Bayes rule and the assumption that the dependency of S on 
an element J`, is factorizable and vice versa: 79(Sh,S,...St,.l{J`,iu}) = 
K 
1-[=p(Sk]{J`,;u} ) and P({Jv#u} [St=x) - 
- I and rj  of the form 
one can rewrite a set of coupled equations for q, 
(3) 
-1 
where aut is a normalizing factor such that q + qut = 1 and p = 79 (St = a:) are 
our prior beliefs in the value of the source bits S. 
This set of equations can be solved iteratively [2] by updating a coupled set of differ- 
ql _-1 _,1 
ence equations for 6qua- u-qu  and 6rut derived for this specific model, 
-- ul-- ul , 
making use of the fact that the variables ru , and sub-sequentially the variables 
+l:(1-t-Srut)/2 and Eq.(3). At each 
can be calculated by exploiting the relation rut 
iteration we can also calculate the pseudo-posterior probabilities q[- atp' I-I,, 
where at are normalizing factors, to determine the current estimated value of 
Two points that are worthwhile noting: Firstly, the iterative solution makes use of 
I -1 
the normalization rut+rut = 1, which is not derived from the basic probability rules 
and makes implicit assumptions about the probabilities of obtaining S = +1 for all 
elements I. Secondly, the iterative solution would have provided the true posterior 
probabilities q if the graph connecting the message J and the encoded bits $ would 
have been free of cycles, i.e., if the graph would have been a tree with no recurrent 
dependencies among the variables. The fact that the framework provides adequate 
practical solutions has only recently been explained [13]. 
4 Decoding by TAP 
We will now show that for this particular problem it is possible to obtain a similar 
set of equations from the corresponding statistical mechanics framework based on 
Bethe approximation [11] or the TAP (Thouless-Anderson-Palmer) approach [12] 
to diluted systems 1 In the statistical mechanics approach we assign a Boltzmann 
i The terminology in the case of diluted systems is slightly vague. Unlike in the case 
of fully connected systems, self consistent equations of diluted systems cannot be derived 
The Belief in TAP 249 
weight to each set comprising an encoded message bit Jr and a dynamical vector $ 
wB (Jrl$) = e - g(Jl$) , (4) 
such that the first term of the system's Hamiltonian (Eq.1) can be rewritten as 
Er g (Jr I$), where the index y runs over all non-zero sites in the multidimensional 
tensor .A. We will now employ two straightforward assumptions to write a set of 
coupled equations for the mean field qrt - 7) (St[ {J;r}), which may be identified 
as the same variable as in the belief network framework (Eq.2), and the effective 
Boltzmann weight Weft(Jr[St , {J;r}): 
1) we assume a mean field behavior for the dependence of the dynamical variables 
S on a certain realization of the message sites J, i.e., the dependence is factorizable 
and may be replaced by a product of mean fields. 
2) Boltzmann weights (effective) for site St are factorizable with respect to 
The resulting set of equations are of the form 
Weft(J r ] St, {J;r)) = Tr{sk,) wB (Jr I S) 
Sk 
pf' Is,, (5) 
art H Weft ' 
where ar is a normalization factor and p? is our prior knowledge of the source's 
bias. Replacing the effective Boltzmann weight by a normalized field, which may 
s, of Eq.(2), we obtain 
be identified as the variable rrt 
rus = 7 ) (St ] Jr, {J;r}) - ar Weft(Jr I S, {J#r}) 
