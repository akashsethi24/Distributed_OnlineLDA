648 
LEARNING SEQUENTIAL STRUCTURE 
IN SIMPLE RECURRENT NETWORKS 
David Servan-Schreiber, Axel Cleeremans, and James L. McClelland 
Departments of Computer Science and Psycholgy 
Carnegie Mellon University 
Pittsburgh, PA 15213 
ABSTRACT 
We explore a network architecture introduced by Elman (1988) for 
predicting successive elements of a sequence. The network uses the 
pattern of activation over a set of hidden units from time-step t-l, 
together with element t, to predict element t+l. When the network is 
mined with strings from a particular finite-state grammar, it can learn 
to be a perfect finite-state recognizer for the grammar. Cluster analyses 
of the hidden-layer patterns of activation showed that they encode 
prediction-relevant information about the entire path traversed through 
the network. We illustrate the phases of learning with cluster analyses 
performed at different points during training. 
Several connectionist architectures that are explicitly constrained to capture 
sequential information have been developed. Examples are Time Delay 
Networks (e.g, Sejnowski & Rosenberg, 1986) -- also called 'moving 
window' paradigms -- or algorithms such as back-propagation in time 
(Rumelhart, Hinton & Williams, 1986). Such architectures use explicit 
representations of several consecutive events, if not of the entire history of 
past inputs. Recently, Elman (1988) has introduced a simple recurrent 
network (SRN) that has the potential to master an infinite corpus of 
sequences with the limited means of a learning procedure that is completely 
local in time (see Figure 1.). 
cr2),rl'EX'1' !.Igis I 
Figure 1. The simple recurrent network (Elman, 1988) 
644 Servan-Schreiber, Cleeremans and McClelland 
In the SRN, the pattern of activation on the hidden units at time t-l, 
together with the new input pattern, is allowed to influence the pattern of 
activation at time t. This is achieved by copying the pattern of activation on 
the hidden layer at time t-1 to a set of input units -- called the 'context units' 
-- at time t. The forward connections in the network are subject to training 
via back-propagation. but there is no backpropagation through time. 
In this paper, we show that the SRN can learn to mimic closely a finite state 
automaton, both in its behavior and in its state representations. In particular, 
we show that it can learn to process an infinite corpus of strings based on 
experience with a finite set of training exemplars. We then describe the 
phases through which the appropriate internal representations are discovered 
during training. 
MASTERING A FINITE STATE GRAMMAR 
In our first experiment, we asked whether the network could learn the 
contingencies implied by a small finite state grammar (see Figure 2). The 
network was presented with strings derived from this grammar, and was 
required to try to predict the next letter at every step. These predictions are 
context dependent since each letter appears twice in the grammar and is 
followed in each case by different successors. 
A single unit on the input layer represented a given letter (six input units in 
total; five for the letters and one for a begin symbol 'B'). Similar local 
representations were used on the output layer (with the 'begin' symbol 
being replaced by an end symbol E'). There were three hidden units. 
Start 
1 #2 x4 ï¿½ 
T 
Figure 2. The small finite-state grammar (Reber, 1967) 
Training. On each of 60,000 training trials, a string was generated from 
the grammar, starting with 'B'. Successive arcs were selected randomly 
from the 2 possible continuations with a probability of 0.5. Each letter was 
Learning Sequential Structure in Simple Recurrent Networks 645 
then presented sequentially to the network. The activations of the context 
units were reset to 0.5 at the beginning of each string. After each letter, the 
error between the network's prediction and the actual successor specified 
by the string was computed and back-propagated. The 60,000 randomly 
generated strings ranged from 3 to 30 letters (mean: 7; sd: 3.3). 
Performance. Three tests were conducted. First, we examined the 
network's predictions on a set of 70,000 random strings. During this test, 
the network is first presented with the start signal, and one of the five letters 
or E is then selected at random as a successor. If that letter is predicted by 
the network as a legal successor (i.e, activation is above 0.3 for the 
corresponding unit), it is then presented to the input layer on the next time 
step, and another letter is drawn at random as its successor. This procedure 
is repeated as long as each letter is predicted as a legal successor until the 
end signal is selected as the next letter. The procedure is interrupted as soon 
as the actual successor generated by the random procedure is not predicted 
by the network, and the string of letters is then considered 'rejected'. A 
string is considered 'accepted' if all its letters have been predicted as 
possible continuations up to, and including, the end signal. Of the 70,000 
random strings, 0.3 % were grammatical, and 99.7 % were ungrammatical. 
The network performed flawlessly, accepting all the grammatical strings and 
rejecting all the others. In a second test, we presented the network with 
20,000 generated at random from the grammar, i.e, all these strings were 
grammatical. Using the same criterion as above, all of these strings were 
correctly 'accepted'. Finally, we constructed a set of very long grammatical 
strings -- more than 100 letters long -- and verified that at each step the 
network correctly predicted all the possible successors (activations above 
0.3) and none of the other letters in the grammar. 
Analysis of internal representations. What kind of internal representations 
have developed over the set of hidden units that allow the network to 
associate the proper predictions to intrinsically ambiguous letters? One way 
to answer this question is to record the hidden units' activation patterns 
generated in response to the presentation of individual letters in different 
contexts. These activation vectors can then be used as input to a cluster 
analysis program. Figure 3.A. shows the results of such an analysis 
conducted on a small random set of grammatical strings. The patterns of 
activation are grouped according to the nodes of the grammar: all the 
patterns that are used to predict the successors of a given node are grouped 
together independently of the current letter. This observation sheds some 
light on the behavior of the network: at each point in a sequence, the pattern 
of activation stored over the context units provides information about the 
current node in the grammar. Together with information about the current 
letter (represented on the input layer), this contextual information is used to 
produce a new pattern of activation over the hidden layer, that uniquely 
specifies the next node. In that sense, the network closely approximates the 
finite-state automaton that would encode the grammar from which the 
training exemplars were derived. However, a closer look at the cluster 
analysis reveals that within a cluster corresponding to a particular node, 
patterns are further divided according to the path traversed before the node 
is reached. For example, looking at the bottom cluster -- node #5 -- patterns 
produced by a 'VV', 'PS', 'XS' or 'SXS' ending are grouped separately: 
646 Servan-Schreiber, Cleeremans and McClelland 
Figure 3. A. Hierarchical cluster analysis of u hid&n unit activation parterre after 
60,000 presentations of strings gea'ated at random from the finite-smt gmnm. 
B. Clus analysis of u H.U. activatio parterre following 2000 epochs of 
training on a set of 22 strings with a maximum length of eight letters. 
Learning Sequential Structure in Simple Recurrent Networks 647 
they are more similar to each other than to the abstract prototype of node #5. 
This tendency to preserve information about the path is not a characteristic 
of traditional finite-state automata. 
ENCODING PATH INFORMATION 
In a different set of experiments, we asked whether the SRN could learn to 
use the information about the path that is encoded in the hidden units' 
patterns of activation. In one of these experiments, we tested whether the 
network could master length constraints. When strings generated from the 
small finite-state grammar may only have a maximum of 8 letters, the 
prediction following the presentation of the same letter in position number 
six or seven may be different. For example, following the sequence 
I'SSSXXV', 'V' is the seventh letter and only another 'V' would be a legal 
successor. In contrast, following the sequence I'SSXXV', both 'V' and 
'P' are legal successors. A network with 15 hidden units was trained on a 
small set of length-limited (max. 8 letters) grammatical suings. It was able 
to use the small activation differences present over the context units - and 
due to the slightly different sequences presented - to master contingencies 
such as those illustrated above (see table 1). 
Table 1. Activation of each output unit following the presentation 
of V' as the 6th or 7th letter in the string 
T $ P X V E 
tssxxV 0.0 0.0 0.54 0.0 0.48 0.0 
tsssxxV 0.0 0.0 0.02 0.0 0.97 0.0 
A cluster analysis of all the patterns of activation on the hidden layer 
generated by each letter in each sequence demonstrates how the influence of 
the path is reflected in these patterns (see figure 3.B.)*. We labeled the arcs 
according to the letter being presented (the 'current letter') and its position in 
the grammar defined by Reber. Thus 'VI' refers to the first 'V' in the 
grammar and 'V2' to the second 'V' which immediately precedes the end of 
the string. 'Early' and l,ate' refer to whether the letter occurred early or late 
in the sequence (for example in 'PT..' '1' 2' occurs early; in 'PVPXT..' it 
occurs late). Finally, in t
