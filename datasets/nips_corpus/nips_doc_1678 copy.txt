Some Theoretical Results Concerning the 
Convergence of Compositions of Regularized 
Linear Functions 
Tong Zhang 
Mathematical Sciences Department 
IBM T.J. Watson Research Center 
Yorktown Heights, NY 10598 
tzhang@watson.ibm.com 
Abstract 
Recently, sample complexity bounds have been derived for problems in- 
volving linear functions such as neural networks and support vector ma- 
chines. In this paper, we extend some theoretical results in this area by 
deriving dimensional independent covering number bounds for regular- 
ized linear functions under certain regularization conditions. We show 
that such bounds lead to a class of new methods for training linear clas- 
sifiers with similar theoretical advantages of the support vector machine. 
Furthermore, we also present a theoretical analysis for these new meth- 
ods from the asymptotic statistical point of view. This technique provides 
better description for large sample behaviors of these algorithms. 
1 Introduction 
In this paper, we are interested in the generalization performance of linear classifiers ob- 
tained from certain algorithms. From computational learning theory point of view, such 
performance measurements, or sample complexity bounds, can be described by a quanti- 
ty called covering number [ 11, 15, 17], which measures the size of a parametric function 
family. For two-class classification problem, the covering number can be bounded by a 
combinatorial quantity called VC-dimension [12, 17]. Following this work, researchers 
have found other combinatorial quantities (dimensions) useful for bounding the covering 
numbers. Consequently, the concept of VC-dimension has been generalized to deal with 
more general problems, for example in [ 15, 11 ]. 
Recently, Vapnik introduced the concept of support vector machine [ 16] which has been 
successful applied to many real problems. This method achieves good generalization by 
restricting the 2-norm of the weights of a separating hyperplane. A similar technique has 
been investigated by Bartlett [3], where the author studied the performance of neural net- 
works when the 1-norm of the weights is bounded. The same idea has also been applied 
in [ 13] to explain the effectiveness of the boosting algorithm. In this paper, we will extend 
their results and emphasize the importance of dimension independence. Specifically, we 
consider the following form of regularization method (with an emphasis on classification 
problems) which has been widely studied for regression problems both in statistics and in 
Convergence of Regularized Linear Functions 3 71 
numerical mathematics: 
inf Ex,yL(w, z, y) = inf Ex,y f(w 'zy) + ,g(w), (1) 
where E,y is the expectation over a distribution of (z, y), and y E {-1, 1} is the binary 
label of data vector z. To apply this formulation for the purpose of training linear classifiers, 
we can choose f as a decreasing function, such that f(-) _> 0, and choose g(w) > 0 as 
a function that penalizes large w (lirn g(w) -+ oo). X is an appropriately chosen 
positive parameter to balance the two terms. 
The paper is organized as follows. In Section 2, we briefly review the concept of covering 
numbers as well as the main results related to analyzing the performance of learning algo- 
rithms. In Section 3, we introduce the regularization idea. Our main goal is to construct 
regularization conditions so that dimension independent bounds on covering numbers can 
be obtained. Section 4 extends results from the previous section to nonlinear composition- 
s of linear functions. In Section 5, we give an asymptotic formula for the generalization 
performance of a learning algorithm, which will then be used to analyze an instance of 
SVM. Due to the space limitation, we will only present the main results and discuss their 
implications. The detailed derivations can be found in [ 18]. 
2 Covering numbers 
We formulate the learning problem as to find a parameter from random observations to 
minimize risk: given a loss function L(a, z) and n observations X = {zx,... , z,} 
independently drawn from a fixed but unknown distribution D, we want to find a that 
minimizes the expected loss over z (risk): 
R(): E L(, )= f L(, ) dP(z). (2) 
The most natural method for solving (2) using a limited number of observations is by the 
empirical risk minimization (ERM) method (cf [15, 16]). We simply choose a parameter 
a that minimizes the observed risk: 
(3) 
) = 
i=1 
We denote the parameter obtained in this way as Oerm(X). The convergence behavior 
of this method can be analyzed by using the VC theoretical point of view, which relies 
on the uniform convergence of the empirical risk (the uniform law of large numbers): 
sups IR(a, X) - R(a)[. Such a bound can be obtained from quantities that measure 
the size of a Glivenko-Cantelli class. For finite number of indices, the family size can be 
measured simply by its cardinality. For general function families, a well known quantity to 
measure the degree of uniform convergence is the covering number which can be be dated 
back to Kolmogrov [8, 9]. The idea is to discretize (which can depend on the data X) the 
parameter space into N values a,... , aN so that each L(a, .) can be approximated by 
L(ai, .) for some i. We shall only describe a simplified version relevant for our purposes. 
Definition 2.1 Let B be a metric space with metric p. Given a norm p, observations X - 
[z, . . . , z,], and vectors f(a, X ) = [f(a, z ), . . . , f(a, z,)] E B ' parameterized by 
a, the covering number in p-norm, denoted as A/'p( f, e, X ), is the minimum number of a 
collection of vectors vt, . . . , v, E B ' such that Va, 3vi' [Ip(f(.,x?), 
We also denote Alp(f, e, n): maxx Alp (f, e, Xp). 
Note that from the definition and the Jensen's inequality, we have ./V'p _< ./V'q for p _< q. We 
will always assume the metric on R to be [zx - z21 if not explicitly specified otherwise. 
The following theorem is due to Pollard [11]' 
372 T. Zhang 
Theorem 2.1 ([lid �n, e > 0 and distribution D. 
_he 2 
P[supIR(a,X)- R(c)[ > e] <_ 8E[A/(L,e/S,X)]exp(128M2), 
where M = sup,,x L(a, z) - inf,.x L(a, ), and X[* = (, . . . , ,) are independently 
drawn from D. 
The constants in the above theorem can be improved for certain problems: see [4, 6, 15, ! 6] 
for related results. However, they yield very similar bounds. The result most relevant for 
this paper is a lemma in [3] where the 1-norm covering number is replaced by the cx>-norm 
covering number. The latter can be bounded by a scale-sensitive combinatorial dimension 
[ 1 ], which can be bounded from the 1-norm covering number if this covering number does 
not depend on n. These results can replace Theorem 2.1 to yield better estimates under 
certain circumstances. 
Since Bartlett's lemma in [3] is only for binary loss functions, we shall give a generalization 
so that it is comparable to Theorem 2.1' 
Theorem 2.2 Let fx and f2 be two functions.' R '  [0, 1] such that lyx - y2l _< implies 
f (y) <_ fa(y2 ) _< f2(Y ) where rs' R  -+ [0, 1] is a reference separating function, then 
--he 2 
P[sup[Efx(L(a,z)) - Exl f2(L(a,z))] > e] _< 4E[Afoo(L,'x,X)]exp(). 
Note that in the extreme case that some choice of a achieves perfect generalization: 
Ef2(L(a, z)) - 0, and assume that our choices of a(X) always satisfy the condition 
Ex? f2 (L(a, z)) = 0, then better bounds can be obtained by using a refined version of the 
Chernoffbound. 
3 Covering number bounds for linear systems 
In this section, we present a few new bounds on covering numbers for the following form 
of real valued loss functions: 
d 
L(w, z): z T w - E ziwi. (4) 
i:1 
As we shall see later, these bounds are relevant to the convergence properties of (1). Note 
that in order to apply Theorem 2.1, since . < A/'2, therefore it is sufficient to estimate 
A/'2(L, e, n) for e > 0. It is clear that A/'2(L, e, n- is not finite if no restrictions on z and w 
are imposed. Therefore in the following, we will assume that each is bounded, and 
study conditions of llwllq so that log A/'( f, e, n) is independent or weakly dependent of d. 
Our first result generalizes a theorem of Bartlett [3]. The original results is with p -- oo 
and q = 1, and the related technique has also appeared in [10, 13]. The proof uses a lemma 
that is attributed to Maurey (cf. [2, 7]). 
Theorem 3.1 r/llllp 5 b andllvollq a, where I/p+ 1/q = 1 and2 _< p _< oo, then 
a 2 b 2 
The above bound on the covering number depends logarithmically on d, which is already 
quite weak (as compared to linear dependency on d in the standard situation). However, the 
bound in Theorem 3.1 is not tight for p < oo. For example, the following theorem improves 
the above bound for p = 2. Our technique of proof relies on the SVD decomposition [5] 
for matrices, which improves a similar result in [14] by a logarithmic factor. 
Convergence of Regularized Linear Functions $ 73 
Theorem 3.2 Ifllill b and IIll a, the, 
log..M.(L, e, n)< [ 2a'b' ] log.(4a'b'/e+ 1). 
The next theorem shows that if 1/p + 1/q > 1, then the 2-norm covering number is also 
ndependent of dimension. 
Theorem 3.3 Let �(w,z) = z'w. �f'llillp 5 bandllwllq 5 a, where 1 <_ q _< 2 and 
5 = l/p + l/q- l > O. then 
.4a2b 2 
log2N'2(�,e,n)_< | -2 1�g2(2(2ab/e) /+ 1). 
One consequence of this theorem is a potentially refined explanation tbr the boosting al- 
gorithm. In [ 13], the boosting algorithm has been analyzed by using a technique related to 
results in [3] which essentially rely on Theorem 3.1 with p -- oo. Unfommately, the bound 
contains a logarithmic dependency on d (in the most general case) which does not seem to 
fully explain the fact that in many cases the performance of the boosting algorithm keeps 
improving as d increases. However, this seemingly mysterious behavior might be better 
understood from Theorem 3.3 under the assumption that the data is more restricted than 
simply being cr)-norm boun
