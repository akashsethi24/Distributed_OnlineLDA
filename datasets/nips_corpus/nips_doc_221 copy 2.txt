650 Lincoln and Skrzypek 
Synergy Of Clustering Multiple Back Propagation Networks 
William P. Lincoln* and Josef SkrzypekP 
UCLA Machine Perception Laboratory 
Computer Science Department 
Los Angeles, CA 90024 
ABSTRACT 
The properties of a cluster of multiple back-propagation (BP) networks 
are examined and compared to the performance of a single BP net- 
work. The underlying idea is that a synergistic effect within the cluster 
improves the performance and fault tolerance. Five networks were ini- 
tially trained to perform the same input-output mapping. Following 
training, a cluster was created by computing an average of the outputs 
generated by the individual networks. The output of the cluster can be 
used as the desired output during training by feeding it back to the indi- 
vidual networks. In comparison to a single BP network, a cluster of 
multiple BP's generalization and significant fault tolerance. It appear 
that cluster advantage follows from simple maxim you can fool some 
of the single BP's in a cluster all of the time but you cannot fool all of 
them all of the time {Lincoln} 
1 INTRODUCTION 
Shortcomings of back-propagation (BP) in supervised learning has been well docu- 
mented in the past {Soulie, 1987; Bemasconi, 1987}. Often, a network of a finite size 
does not leam a particular mapping completely or it generalizes poorly. Increasing the 
size and number of hidden layers most often does not lead to any improvements {Soulie, 
* also with Hughes Aircraft Company 
P to whom the correspondence should be addressed 
Synergy of Clustering Multiple Back Propagation Networks 651 
1987}. The central question that this paper addresses is whether a synergy of clustering 
multiple back-prop nets improves the properties of the clustered system over a compar- 
ably complex non-clustered system. We use the formulation of back-prop given in 
{Rumelhart, 1986}. A cluster is shown in figure 1. We start with five, three-layered, 
back propagation networks that learn to perform the same input-output mapping. Ini- 
tially the nets are given different starting weights. Thus after learning, the individual nets 
are expected to have different internal representations. An input to the cluster is routed to 
each of the nets. Each net computes its output and the judge uses these outputs, 9k to 
form the cluster output, 9. There are many ways of forming 9 but for the sake of simpli- 
city, in this paper we consider the following two rules: 
simple average :9 = 
K=I 
(1.1) 
N 
convex combination :9 =  Wkyk 
K=I 
(1.2) 
Cluster function 1.2 adds an extra level of fault tolerance by giving the judge the ability 
to bias the outputs based on the past reliability of the nets. The Wk are adjusted to take 
into account the recent reliability of the net. One weight adjustment rule is 
N 
W, = W,.G' e where e - 1  e, G is the gain of adjustment and 
e N k__ 
e = I 19 - 91 I is the network deviation from the cluster output. Also, in the absence 
of an initial training period with a perfect teacher the cluster can collectively self- 
organize. The cluster in this case is performing an averaging of the mappings that the 
individual networks perform based on their initial distribution of weights. Simulations 
have been done to verify that self organization does in fact occur. In all the simulations, 
convergence occurred before 1000 passes. 
Besides improved learning and generalization our clustered network displays other desir- 
able characteristics such as fault tolerance and self-organization. Feeding back the 
cluster's output to the N individual networks as the desired output in training endows the 
cluster with fault tolerance in the absence of a teacher. Feeding back also makes the clus- 
ter continuously adaptable to changing conditions. This aspects of clustering is similar to 
the tracking capabilities of adaptive equalizers. After the initial training period it is usu- 
ally assumed that no teacher is present, or that a teacher is present only at relatively infre- 
quent intervals. However, if the failure rate is large enough, the performance of a single, 
non-clustered net will degrade during the periods when no teacher is present. 
2 CLUSTERING WITH FEEDBACK TO INCREASE FAULT 
TOLERANCE IN THE ABSENCE OF A PERFECT TEACHER. 
When a teacher is not present,  can be used as the desired output and used to continu- 
ously train the individual nets. In general, the correct error that should b back- 
propagated, d = Y-9, will differ from the actual error, dk = 9 -  If d and d differ 
significantly, the error of the individual nets (and thus the cluster as a whole) can increase 
652 Lincoln and Skrzypek 
over time. This phenomenon is called drift. Because of drift, retraining using 9 as the 
desired output may seem disadvantageous when no faults exist within the nets. The possi- 
bility of drift is decreased by training the nets to a sufficiently small error. In fact under 
these circumstance with sufficiently small error, it is possible to see the error to decrease 
even further. 
It is when we assume that faults exist that retraining becomes more advantageous. If the 
failure rate of a network node is sufficiently low, the injured net can be retrained using 
the judge's output. By having many nets in the cluster the effect of the injured net's out- 
put on the cluster output can be minimized. Retraining using 9 adds fault tolerance but 
causes drift if the nets did not complete learning when the teacher was removed. 
Figure 1: A cluster of N back-prop nets. 
3 EXPERIMENTAL METHODS. 
To test the ideas outlined in this paper an abstract learning problem was chosen. This 
abstract problem was used because many neural network problems require similar 
separation and classification of a group of topologically equivalent sets in the process of 
learning [Lippman, 1987]. For instance, images categorized according to their charac- 
teristics. The input is a 3-dimensional point, P = (x,y,z). The problem is to categorize 
the point P into one of eight sets. The 8 sets are the 8 spheres of radius 1 centered at x = 
(+1), y = (+,1), z = (+,1) The input layer consists of three continuous nodes. The size of 
the output layer was 8, with each node trained to be an indicator function for its associ- 
ated sphere. One hidden layer was used with full connectivity between layers. Five nets 
with the above specifications were used to form a cluster. Generalization was tested using 
points outside the spheres. 
Synergy of Clustering Multiple Back Propagation Networks 653 
4 CLUSTER ADVANTAGE. 
The performance of a single net is compared to performance of a five net cluster when 
the nets are not retrained using 3. The networks in the cluster have the same structure and 
size as the single network. Average errors of the two systems are compared. A useful 
measure of the cluster advantage is obtained by taking the ratio of an individual net's 
error to the cluster error. This ratio will be smaller or larger than 1 depending on the rela- 
tive magnitudes of the cluster and individual net's errors. Figures 2a and 2b show the 
cluster advantage plotted versus individual net error for 256 and 1024 training passes 
respectively. It is seen that when the individual nets either learn the task completely or 
don't learn at all there is not a cluster advantage. However, when the task is learned even 
marginally, there is a cluster advantage. 
60 
5O 
40 
30 
20 
lO 
o 
0 A) Pass - 256 150 1  B) Pass  1024 
lO0 
II 
II 
tl 
1 p 
Error 
50 
, o 
3 0 
I 
I 
I 
I 
I 
I 
1 2 
Figure 2: Cluster Advantage versus Error. 
Data points from more than one learning task are shown. 
A) After 256 Ixaining passes. B) After 1024 Ixaining passes. 
The cluster's increased learning is based on the synergy between the individual networks 
and not on larger size of a cluster compared to an individual network. An individual net's 
error is dependent on the size of the hidden layer and the length of the training period. 
However, in general the error is not a decreasing function of the size of the hidden layer 
throughout its domain, i.e. increasing the size of the hidden layer does not always result 
in a decrease in the error. This may be due to the more direct credit assignment with the 
smaller number of nodes. Figures 4a and 4b show an individual net's error versus hidden 
layer size for different training passes. The point to this pedagogics is to counter the anti- 
cipated argument: a cluster should have a lower error based on the fact that it has more 
nodes. 
654 Lincoln and Skrzypek 
2 l 2 
A) Pass - 256 B) Pass -- 1024 
1 m 1 
,,' P 
o o 
0 20 40 60 80 100 0 20 40 60 80 100 
Number of Hidden Unite 
Number of Hidden Unit. 
Figure 3: Error of a single BP network is a nonlinear funtion 
of the number of hidden nodes. 
A) After 256 training passes B) After 1024 u'aining passes 
5 FAULT TOLERANCE. 
the judge's output as the desired output and retraining the individual networks, fault 
tolerance is added. The fault tolerant capabilities of a cluster of 5 were studied. The size 
of the hidden layer is 15. After the nets were trained, a failure rate of 1 link in the cluster 
per 350 inputs was introduced. This failure rate in terms of a single unclustered net is 1 
link per 1750 (=5.350) inputs. The link that is chosen to fail in the cluster was randomly 
selected from the links of all the networks in the cluster. When a link failed its weight 
was set to 0. The links from the nets to the judge are considered immune from faults in 
this comparison. A pass consisted of 1 presentation of a random point from each of the 8 
spheres. Figure 4 shows the fault tolerant capabilities of a cluster. By knowing the 
behavior of the single net in the presence of faults, the fault tolerant behavior of any con- 
ventional configuration (i.e. comparison and spares) of single nets can be determined, so 
that this form of fault tolerance can be compared with c
