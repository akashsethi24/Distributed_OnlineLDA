Hebb Learning of Features 
based on their Information Content 
Ferdinand Peper 
Communications Research Laboratory 
588-2, Iwaoka, Iwaoka-cho 
Nishi-ku, Kobe 651-24 
Japan 
peper@crl.go.jp 
Hideki Noda 
Kyushu Institute of Technology 
Dept. Electr., Electro., and Comp. Eng. 
1-1 Sensui-cho, Tobata-ku 
Kita-Kyushu 804, Japan 
noda@kawa.comp.kyutech.ac.jp 
Abstract 
This paper investigates the stationary points of a Hebb learning rule 
with a sigmoid nonlinearity in it. We show mathematically that when 
the input has a low information content, as measured by the input's 
variance, this learning rule suppresses learning, that is, forces the weight 
vector to converge to the zero vector. When the information content 
exceeds a certain value, the rule will automatically begin to learn a 
feature in the input. Our analysis suggests that under certain conditions 
it is the first principal component that is learned. The weight vector 
length remains bounded, provided the variance of the input is finite. 
Simulations confirm the theoretical results derived. 
I Introduction 
Hebb learning, one of the main mechanisms of synaptic strengthening, is induced 
by cooccurrent activity of pre- and post-synaptic neurons. It is used in artificial 
neural networks like perceptrons, associative memories, and unsupervised learning 
neural networks. Unsupervised Hebb learning typically employs rules of the form: 
laW(t) -- x(t)y(t) - d(x(t),y(t), w(t)), (1) 
where w is the vector of a neuron's synaptic weights, x is a stochastic input vector, 
y is the output expressed as a function of xTw, and the vector function d is a for- 
getting term forcing the weights to decay when there is little input. The integration 
constant p determines the learning speed and will be assumed i for convenience. 
The dynamics of rule (1) determines which features are learned, and, with it, the 
rule's stationary points and the boundedness of the weight vector. In some cases, 
weight vectors grow to zero or grow unbounded. Either is biologically implausible. 
Suppression and unbounded growth of weights is related to the characteristics of 
the input x and to the choice for d. Understanding this relation is important to 
enable a system, that employs Hebb learning, to learn the right features and avoid 
implausible weight vectors. 
Unbounded or zero length of weight vectors is avoided in [5] by keeping the total 
synaptic strength -i wi constant. Other studies, like [7], conserve the sum-squared 
Hebb Learning of Features based on their Information Content 247 
synaptic strength. Another way to keep the weight vector length bounded is to 
limit the range of each of the individual weights [4]. The effect of these constraints 
on the learning dynamics of a linear Hebb rule is studied in [6]. 
This paper constrains the weight vector length by a nonlinearity in a Hebb rule. It 
uses a rule of the form (1) with y - $(xTw -- h) and d(x, y, w) - c.w, the function 
$ being a smooth sigmoid, h being a constant, and c being a positive constant 
(see [1] for a similar rule). We prove that the weight vector w assumes a bounded 
nonzero solution if the largest eigenvalue  of the input covariance matrix satisfies 
, > c/$(-h). Furthermore, if  _ c/S(-h) the weight vector converges to the 
vector 0. Since  equals the variance of the input's first principal component, that 
is,  is a measure for the amount of information in the input, learning is enabled 
by a high information content and suppressed by a low information content. 
The next section describes the Hebb neuron and its input in more detail. After 
characterizing the stationary points of the Hebb learning rule in section 3, we ana- 
lyze their stability in section 4. Simulations in section 5 confirm that convergence 
towards a nonzero bounded solution occurs only when the information content of 
the input is sufficiently high. We finish this paper with a discussion. 
2 The Hebb Neuron and its Input 
Assume that the n-dimensional input vectors x presented to the neuron are gener- 
ated by a stationary white stochastic process with mean 0. The process's covariance 
matrix Y. = E[xx w] has eigenvalues A, ..., An (in order of decreasing size) and cor- 
responding eigenvectors u,..., un. Furthermore, E[[[x[[ '] is finite. This implies that 
the eigenvalues are finite because E[llxll = E[tr[xxT]] = tr[E[xxT]] = -i= Ai. It 
is assumed that the probability density function of x is continuous. Given an input 
x and a synaptic weight vector w, the neuron produces an output y = $(xTw -- h), 
where $: lR -- ]H. is a function that satisfies the conditions: 
C1. 
C2. 
C3. 
C4. 
is smooth, i.e., S is continuous and differentiable and S  is continuous. 
is sublinear, i.e., lim S(z)/z - lim S(z)/z -- O. 
Z'--)' OO 
is monotonically nondecreasing. 
has one maximum, which is at the point z - -h. 
Typically, these conditions are satisfied by smooth sigmoidal functions. This in- 
cludes sigmoids with infinite saturation values, like S(z) = sign(z)[z[ /2 (see [9]). 
The point at which a sigmoid achieves maximal steepness (condition C4) is called its 
base. Though the step function is discontinuous at its base, thus violating condition 
C1, the results in this paper apply to the step function too, because it is the limit 
of a sequence of continuous sigmoids, and the input density function is continuous 
and thus Lebesgue-integrable. The learning rule of the neuron is given by 
= xy - cw, (2) 
c being a positive constant. Use of a linear $(z) = az in this rule gives unstable 
dynamics: if a > c/,l, then the length of the weight vector w grows out of bound 
though ultimately w becomes collinear with u. It is proven in the next section 
that a sublinear $ prevents unbounded growth of w. 
248 E Peper and H. Noda 
3 Stationary Points of the Learning Rule 
To get insight into what stationary points the weight vector w ultimately converges 
to, we average the stochastic equation (2) over the input patterns and obtain 
(.) = E [xs (xT(w) - h) ] - c (w), 
(3) 
where (w / is the averaged weight vector and the expectation is taken over x, as 
with all expectations in this paper. Since the solutions of (2) correspond with the 
solutions of (3) under conditions described in [2], the averaged (w) will be referred 
to as w. Learning in accordance to (2) can then be interpreted [1] as a gradient 
descent process on an averaged energy function J associated with (3): 
S(w) = [v (xw - n) ] + 
with T(z)- S(v)dv. 
oo 
To characterize the solutions of (3) we use the following lemma. 
Lemma 1. Given a unit-length vector u, the function fu ' ]R - ]R is defined by 
_- 1 [uxS 
c 
and the constant u by u = E[uTxxXu]. The fixed points of f are as follows. 
1. If ,kS'(-h) _< c then f has one fixed point, i.e., z = 0. 
2. If ,kS(-h)  c then fu has three fixed points, i.e., z = 0, z = a +, and 
z = a, where a + (aS) is a positive (negative) value depending on u. 
Proof(Sketch; for a detailed proof see [11]). Function f is a smooth sigmoid, since 
conditions C1 to C4 carry over from S to f. The steepness of f in its base at z - 0 
depends on vector u. If S'(-h) _ c, function fu intersects the line h(z) -- z only 
at the origin, giving z - 0 as the only fixed point. If S(-h)  c, the steepness 
of fu is so large as to yield two more intersections: z -- a + and z = a. [] 
Thus characterizing the fixed points of f, the lemma allows us to find the fixed 
points of a vector function g: n _ ]Rn that is closely related to (3). Defining 
g(w) = _1 E[x$ (xTw- h)], 
c 
we find that a fixed point z = au of fu corresponds to the fixed point w = au of 
g. Then, since (3) can be written as v - c.g(w) - c.w, its stationary points are 
the fixed points of g, that is, w = 0 is a stationary point and for each u for which 
AS'(-h) > c there exists one bounded stationary point associated with a + and 
one associated with a. Consequently, if A _< c/S(-h) then the only fixed point 
of g is w = 0, because Ax >_ Au for all u. 
What is the implication of this result? The relation A 5 c/S'(-h) indicates a low 
information content of the input, because Ax--equaling the variance of the input's 
first principal component--is a measure for the input's information content. A low 
information content thus results in a zero w, suppressing learning. Section 4 shows 
Hebb Learning of Features based on their Information Content 249 
that a high information content results in a nonzero w. The turnover point of what 
is considered high/low information is adjusted by changing the steepness of the 
sigmoid in its base or changing constant c in the forgetting term. 
To show the boundedness of w, we consider an arbitrary point P: w =/u suffi- 
ciently far away from the origin 0 (but at finite distance) and calculate the compo- 
nent of v along the line OP as well as the components orthogonal to OP. Vector 
u has unit length, and  may be assumed positive since its sign can be absorbed 
by u. Then, the component along OP is given by the projection of v on u: 
uTr w=3u ---- uTcg(/U) -- uT�/U : --� [/ -- fu(/)]. 
This is negative for all/ exceeding the fixed points of fu because of the sigmoidal 
shape of fu. So, for any point P in IP n lying far enough from 0 the vector compo- 
nent of v in P along the line OP is directed towards 0 and not away from it. This 
component decreases as we move away from O, because the value of [ - fu()] 
increases as  increases (fu is sublinear). Orthogonal to this is a component given 
by the projection of v on a unit-length vector v that is orthogonal to u: 
I : �Tcg(/u) - �T�/u -- ��Tg(/u). 
�Tv 
This component increases as we move away from O; however, it changes at a slower 
pace than the component along OP, witness the quotient of both components: 
{ ��T g (/U) �T g (/U) // 
lim vTr = lim = lim = 0. 
Vector v thus becomes increasingly dominated by the component along OP as/ 
increases. So, the 
