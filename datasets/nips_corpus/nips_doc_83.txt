41 
ON PROPERTIES OF NETWORKS 
OF NEURON-LIKE ELEMENTS 
Pierre Baldi* and Santosh S. Venlmtesh I 
15 December 1987 
Abstract 
The complexity and computational capacity of multi-layered, feedforward 
neural networks is examined. Neural networks for special purpose (structured) 
functions are examined from the perspective of circuit complexity. Known re- 
sults in complexity theory are applied to the special instance of neural network 
circuits, and in particular, classes of functions that can be implemented in 
shallow circuits characterised. Some conclusions are also drawn about learning 
complexity, and some open problems raised. The dual problem of determining 
the computational capacity of a class of multi-layered networks with dynamics 
regulated by an algebraic Hamiltoninn is considered. Formal results are pre- 
sented on the storage capacities of programmed higher-order structures, and 
a tradeoff between ease of programming and capacity is shown. A precise de- 
termination is made of the static fixed point structure of random higher-order 
constructs, and phase-transitions (0-1 laws) are shown. 
1 INTRODUCTION 
In this article we consider two aspects of computation with neural networks. Firstly 
we consider the problem of the complexity of the network required to compute classes 
of specified (structured) functions. We give a brief overview of basic known com- 
plexity theorems for readers familiar with neural network models but less familiar 
with circuit complexity theories. We argue that there is considerable computational 
and physiological justification for the thesis that shallow circuits (i.e., networks with 
relatively few layers) are computationally more efficient. We hence concentrate on 
structured (as opposed to random) problems that can be computed in shallow (con- 
stant depth) circuits with a relatively few number (polynomial) of elements, and 
demonstrate classes of structured problems that are amenable to such low cost so- 
lutions. We discuss an allied problem--the complexity of learning--and close with 
some open problems and a discussion of the observed limitations of the theoretical 
approach. 
We next turn to a rigourous classification of how much a network of given 
structure can do; i.e., the computational capacity of a given construct. (This is, in 
*Department of Mathematics, University of California (San Diego), La Jolla, CA 92093 
tMoore School of Electrical Engineering, University of Pennsylvania, Philadelphia, PA 19104 
American Institute of Physics 1988 
42 
a sense, the mirror image of the problem considered above, where we were seeking 
to design a minimal structure to perform a given task.) In this article we restrict 
ourselves to the analysis of higher-order neural structures obtained from polynomial 
threshold rules. We demonstrate that these higher-order networks are a special class 
of layered neural network, and present formal results on storage capacities for these 
constructs. Specifically, for the case of prograznmed interactions we demonstrate 
that the storage capacity is of the order of n d where d is the interaction order. 
For the case of random interactions, a type of phase trartsition is observed in the 
distribution of fixed points as a function of attraction depth. 
2 COMPLEXITY 
There exist two broad classes of constraints on computations. 
1. Physical constraints. These are related to the hardware in which the computa- 
tion is embedded, and include among others time constants, energy limitations, 
volumes and geometrical relations in 3D space, and bandwidth capadties. 
2. Logical constraints: These can be further subdivided into 
� Computability constraints--for instance, there exist unsolvable problems, 
i.e., functions such as the halting problem which are not computable in 
an absolute sense. 
� Complexity constraints--usually giving upper and/or lower bounds on 
the amount of resources such as the time, or the number of gates re- 
quired to compute a given function. As an instance, the assertion There 
exists an exponential time algorithm for the Traveling Salesman Prob- 
lem, provides a computational upper bound. 
If we view brains as computational devices, it is not unreasonable to think 
that in the course of the evolutionary process, nature may have been faced several 
times by problems related to physical and perhaps to a minor degree logical con- 
straints on computations. If this is the case, then complexity theory in a broad 
sense could contribute in the future to our understanding of parallel computations 
and architectural issues both in natural and synthetic neural systems. 
A simple theory of parallel processing at the macro level (where the elements 
are processors) can be developed based on the ratio of the time spent on com- 
munications between processors [7] for different classes of problems and different 
processor architecture and interconnections. However, this approach does not seem 
to work for parallel processing at the level of circuits, especially if calculations and 
communications are intricately entangled. 
Recent neural or connectionist models are based on a common structure, that 
of highly interconnected networks of linear (or polynomial) threshold (or with sig- 
mold input-output function) units with adjustable interconnection weights. We shall 
therefore review the complexity theory of such circuits. In doing so, it will be some- 
times helpful to contrast it with the similar theory based on Boolean (AND, OR, 
NOT) gates. The presentation will be rather informal and technical complements 
can easily be found in the references. 
43 
Consider a circuit as being on a cyclic oriented graph connecting n Boolean 
inputs to one Boolean output. The nodes of the graph correspond to the gates 
(the n input units, the hidden units, and the output unit) of the circuit. The 
size of the circuit is the total number of gates and the depth is the length of the 
longest path connecting one input to the output. For a layered, feed-forward circuit, 
the width is the average number of computational units in the hidden (or interior) 
layers of elements. The first obvious thing when comparing Boolean and threshold 
logic is that they are equivalent in the sense that any Boolean function can be 
implemented using either logic. In fact, any such function can be computed in a 
circuit of depth two and exponential size. Simple counting arguments show that 
the fraction of functions requiring a circuit of exponential size approaches one as 
n  oo in both cases, i.e., a random function will in general require an exponential 
size circuit. (Paradoxically, it is very difficult to construct a family of functions 
for which we can prove that an exponential circuit is necessary.) Yet, threshold 
logic is more powerful than Boolean logic. A Boolean gate can compute only one 
function whereas a threshold gate can compute to the order of 2 an2 functions by 
varying the weights with 1/2 _< a _< 1 (see [19] for the lower bound; the upper 
bound is a classical hyperplane counting argument, see for instance [20,30]). It 
would hence appear plausible that there exist wide classes of problems which can be 
computed by threshold logic with circuits substantially smaller than those required 
by Boolean logic. An important result which separates threshold and Boolean logic 
from this point of view has been demonstrated by Yo [31] (see [10,24] for an elegant 
proof). The result is that in order to compute a function such as parity in a circuit 
of constant depth k, at least exp(cn /2k) Boolean gates with unbounded fanin are 
required. As we shall demonstrate shortly, a circuit of depth two and linear size is 
sufficient for the computation of such functions using threshold logic. 
It is not unusual to hear discussions about the tradeoffs between the depth 
and the width of a circuit. We believe that one of the main constributions of 
complexity analysis is to show that this tradeoff is in some sense minimal and that 
in fact there exists a very strong bias in favor of shallow (i.e., constant depth) 
circuits. There are multiple reasons for this. In general, for a fixed size, the number 
of different functions computable by a circuit of small depth exceeds the number 
of those computable by a deeper circuit. That is, if one had no a priori knowledge 
regarding the function to be computed and was given hidden units, then the optimal 
strategy would be to choose a circuit of depth two with the rn units in a single 
layer. In addition, if we view computations as propagating in a feedforward mode 
from the inputs to the output unit, then shallow circuits compute faster. And the 
deeper a circuit, the more difficult become the issues of time delays, synchronisation, 
and precision on the computations. Finally, it should be noticed that given overall 
responses of a few hundred milliseconds and given the known time scales for synaptic 
integration, biological circuitry must be shallow, at least within a module and 
this is corroborated by anatomical data. The relative slowness of neurons and their 
shallow circuit architecture are to be taken together with the analog factor and 
entropy factor [1] to understand the necessary high-connectivity requirements of 
neural systems. 
44 
From the previous analysis emerges an important class of circuits in threshold 
logic characterised by polynomial size and shallow depth. We have seen that, in 
general, a random function cannot be computed by such circuits. However, many 
interesting functions--the structured problems--are far from random, and it is then 
natural to ask what is the class of functions computable by such circuits? While 
a complete characterisation is probably difficult, there are several sub-classes of 
structural functions which are known to be computable in shallow poly-size circuits. 
The symmetric functions, i.e., functions which are invaxiant under any per- 
mutation of the n input vari
