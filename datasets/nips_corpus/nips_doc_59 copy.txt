412 
CAPACITY FOR PATTERNS AND SEQUENCES IN KANERVA'S SDM 
AS COMPARED TO OTHER ASSOCIATIVE MEMORY MODELS 
James D. Keeler 
Chemistry Department, Stanford University, Stanford, CA 94305 
and RIACS, NASA-AMES 230-5 Moffett Field, CA 94035. 
e.rnail: jdk hydra.riacs. edu 
ABSTRACT 
The information capacity of Kanerva's Sparse, Distributed Memory (SDM) and Hopfield-type 
neural networks is investigated. Under the approximations used here, it is shown that the to- 
tal information stored in these systems is proportional to the number connections in the net- 
work. The proportionality constant is the same for the SDM and Hopfield-type models in- 
dependent of the particular model, or the order of the model. The approximations are 
checked numerically. This same analysis can be used to show that the SDM can store se- 
quences of spatiotemporal patterns, and the addition of time-delayed connections allows the 
retrieval of context dependent temporal patterns. A minor modification of the SDM can be 
used to store correlated patterns. 
INTRODUCTION 
Many different models of memory and thought have been proposed by scientists over the 
years. In (1943) McCulloch and Pitts proposed a simple model neuron with two states of activity 
(on and off) and a large number of inputs.  Hebb (1949) considered a network of such neurons and 
postulated mechanisms for changing synaptic strengths 2 to learn memories. The learning rule 
considered here uses the outer-product of patterns of +Is and -Is. Anderson (1977) discussed the 
effect. of iterative feedback in such a system) Hopfield (1982) showed that for symmetric connec- 
tions, ' the dynamics of such a network is governed by an energy function that is analogous to the 
energy function of a spin glass? Numerous investigations have been carried out on similar 
modelsfi --a 
Several limitations of these binary interaction, outer-product models have been pointed out. 
For example, the number of patterns that can be stored in the system (its capacity) is limited to a 
fraction of the length of the pattern vectors. Also, these models are not very successful at storing 
correlated patterns or temporal sequences. 
Other models have been proposed to overcome these limitations. For example, one can 
allow higher-order interactions among the neurons. 9't� In the following, I focus on a model 
developed by Kanerva (1984) called the Spame, Distributed Memory (SDM) model.  The SDM 
can be viewed as a three layer network that uses an outer-product learning between the second and 
third layer. As discussed below, the SDM is more versatile than the above mentioned networks 
because the number of stored patterns can increased independent of the length of the pattern, and 
the SDM can be used to store spatiotemporal patterns with context retrieval, and store correlated 
patterns. 
The capacity limitations of outer-product models can be alleviated by using higher-order 
interaction models or the SDM, but a price must be paid for this added capacity in terms of an 
increase in the number of connections. How much information is gained per connection? It is 
shown in the following that the total information stored in each system is proportional to the 
number of connections in the network, and that the proportionality constant is independent of the 
particular model or the order of the model. This result also holds if the connections are limited to 
one bit of precision (clipped weights). The analysis presented here requires certain simplifying 
assumptions. The approximate results are compared numerically to an exact calculation developed 
by Chou.2 
SIMPLE OUTER-PRODUCT NEURAL NETWORK MODEL 
As an example or a simple first-order neural network Model, I consider in detail the model 
developed by Hopfieldfi This model will be used to introduce the mathematics and the concepts 
that will be generalized for the analysis of the $DM. The neurons are simple two-state 
American Institute of Physics 1988 
413 
threshold devices: The state of the i 'h neuron, ui, is either either +1 (on), or -1 (off). Consider a 
set of n such neurons with net input (local field), hi, to the i 'h neuron given by 
hi = Tii uj, (1) 
where T././ represents the interaction suength between the i ' neuron and the j''. The state of each 
neuron is updated asynchronously (at random) according to the rule 
Ui ('--g (hi), (2) 
where the function g is a simple threshold function g (x) = sign (x). 
Suppose we are given M randomly chosen patterns (strings of length n of :iris) which we 
wish to store in this system. Denote these M memory patterns as pattern vectors: 
p,X = (p  ,pX ..... pp), a -- 1,2,3 ..... M. For example, p might look like 
(+1,-1,+1,-I,-1 ..... +1). One method of storing these patterns is the outer-product (Hebbian) learn- 
ing rule: Start with T-0, and accumulate the outer-products of the pattern vectors. The resulting 
connection matrix is given by 
M 
r:j = rii = 0. (3) 
a=l 
The system described above is a dynamical system with attracting fixed points. To obtain 
an approximate upper bound on the total information stored in this network, we sidestep the issue 
of the basins of attraction, and we check to see if each of the pattsms stored by Eq. (3) is actually 
a fixed point of (2). Suppose we are given one of the patterns, p�, say, as the initial configuration 
of the neurons. I will show that p is expected to be a fixed point of Eq. (2). After inserting (3) 
for T into (1), the net input to the i' neuron becomes 
h: = f,pix[  p?p?]. (4) 
a= j 
The important term in the sum on a is the one for which a = [3. This term represents the sig- 
nal between the input p and the desired output. The rest of the sum represents noise result- 
ing from crosstalk with all of the other stored patterns. The expression for the net input becomes 
h i = signall + noisei where 
signali = p,.[' p? p?], (5) 
J 
noisei = ' plaX[ ' p? PPl. (6) 
Summing on all of the j} in (6) yields signall = (n-1)pi . Since n is positive, the sign of 
the signal term and pi It will be the same. Thus, ff the noise term were exacfiy zero, the signal 
would give the same sign as �: with a magnitude of = n a, and p would be a fixed point of (2). 
Moreover, patterns close to pVwould give nearly the same signal, so that p shotfid be an attract- 
ing fixed point. 
For randomly chosen patterns, <noise > = 0, where < > indicates statistical expectation, and 
its variance will be o 2 = (n-1)a(M-1). The probability that there will be an error on recall ofpi  
is given by the probability that the noise is greater than the signal. For n large, the noise distribu- 
tion is approximately gaussian, and the probability that there is an error in the i t' bit is 
1 
P' = 2do I e-2r:�edx' (7) 
I signal I 
INFORMATION CAPACITY 
The number of paRems that can be stored in the network is known as its capacity. 3'v* How- 
ever, for a fair comparison between all of the models discussed here, it is more relevant to com- 
pare the total number of bits (total information) stored in each model rather than the number of 
414 
patterm. This allows comparison of information storage in models with different lengths of the 
pattern vectors. If we view the memory model as a black box which receives input bit strings and 
outputs them with some small probability of error in each bit, then the definition of bit-capacity 
used here is exactly the definition of channel capacity used by Shannon? 
Define the bit-capacity as the number of bits that can be stored in a network with fixed pro- 
bability of getting an error in a recalled bit, i.e. Pe = constant in (10). Explicitly, the bit-capacity 
is given by TM 
B = bit capacity = nMq, (8) 
where q = (1 + pelog2p, + (1-p,)log2(1-p,)). Note that rl=l for p,=0. Setting p, to a constant is 
tantamount to keeping the signal-to-noise ratio (fidelity) constant, where the fidelity, R, is given by 
R = Isignal I/. Explicitly, the relation between (constant) pe and R, is just R = -l(1 -p,), 
where 
R 
I(R ) = (1/2Jr) 'A I e-t2/2dt' (9) 
Hence, the bit-capacity of these networks can be investigated by examining the fidelity of the 
models as a function of n, M, and R. From (8) and (9) the fidelity of the Hopfield model is is 
R 2 = n/(n(M-1)) � (n>l). Solving for M in terms of (fixed) R and q, the bit-capacity becomes 
B = q[(n 2/R 2)+n ]. 
The results above can be generalized to models with d '' order interactions? 't8 The resulting 
expression for the bit-capacity for d '' order interaction models is just 
It d+l 
B = (10) 
Hence, we see that the number of bits stored in the system increases with the order d. However, 
to store these bits, one must pay a price by including more connections in the connection tensor. 
To demonstrate the relationship between the number of connections and the information stored, 
define the information capacity, 7, to be the total information stored in the network divided by the 
number of bits in the connection tensor (note that thi.q is different than the definition used by Abu- 
Mostafa et al.)? Thus � is just the bit-capacity divided by the number of bits in the tensor T, 
and represents the efficiency with which information is stored in the network. Since T has n 
elements, the information capacity is found to be 
rl (11) 
�= R2b , 
where b is the number of bits of precision used per tensor element (b > 10g2M for no clipping of 
the weights). For large n, the information stored per neuronal coimection is �. q/R 2b, indepen- 
dent of the order of the model (compare this result to that of Peretto, et al.). ' To illustrate this 
point, suppose one decides that the maximum allowed probability of getting an error in a recalled 
bit is p, = 1/1000, then this would fix the !!ainirnum value of R at 3.1. Thus, to store 10,000 bits 
with a probability of getting an error of a recalled bit of 0.001, equation (15) states that it would 
take --'96,000b bits, independent of the order 
