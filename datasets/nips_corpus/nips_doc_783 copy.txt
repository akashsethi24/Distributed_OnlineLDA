Bayesian Backpropagation Over I-O Functions 
Rather Than Weights 
David H. Wolpert 
The Santa Fe Institute 
1660 Old Pecos Trail 
Santa Fe, NM 87501 
Abstract 
The conventional Bayesian justification of backprop is that it finds the 
MAP weight vector. As this paper shows, to find the MAP i-o function 
instead one must add a correction term to backprop. That term biases one 
towards i-o functions with small description lengths, and in particular fa- 
vors (some kinds of) feature-selection, pruning, and weight-sharing. 
INTRODUCTION 
In the conventional Bayesian view of backpropagation (BP) (Buntine and Weigend, 1991; 
Nowlan and Hinton, 1994; MacKay, 1992; Wolpert, 1993), one starts with the likelihood 
conditional distribution P(training set = t I weight vector w) and the prior distribution 
P(w). As an example, in regression one might have a Gaussian likelihood, P(t I w) o, 
exp[-z2(w, 0] -- IIi exp [-{net(w, tx(i)) - ty(i)} 2 / 202] for some constant c. (tx(i) and ty(i) 
are the successive input and output values in the training set respectively, and net(w, .) is 
the function, induced by w, taking input neuron values to output neuron values.) As another 
example, the weight decay (Gaussian) prior is P(w) o, exp(.a(w2)) for some constant a. 
Bayes' theorem tells us that P(w I 0 o P(t I w) P(w). Accordingly, the most probable weight 
given the data - the maximum a postefiofi (MAP) w - is the mode over w of P(t I w) P(w), 
which equals the mode over w of the cost function L(w, 0 -- ln[P(t I w)] + ln[P(w)]. So 
for example with the Gaussian likelihood and weight decay prior, the most probable w giv- 
en the data is the w minimizing Z2(w, 0 + ct w2. Accordingly BP with weight decay can be 
viewed as a scheme for trying to find the function from input neuron values to output neu- 
ron values (i-o function) induced by the MAP w. 
200 
Bayesian Backpropagation over I-O Functions Rather Than Weights 201 
One peculiar aspect of this justification of weight-decay BP is the fact that rather than the 
i-o function induced by the most probable weight vecwr, in practice one would usually pre- 
fer to know the most probable i-o function. (In few situations would one care more about a 
weight vector than about what that weight vector parameterizes.) Unfortunately, the differ- 
ence between these two i-o functions can be large; in general it is not true that the most 
probable output corresponds to the most probable parameter (Denker and LeCun, 1991). 
This paper shows that to find the MAP i-o function rather than the MAP w one adds a cor- 
rection term to conventional BP. That term biases one towards i-o functions with small 
description lengths, and in particular favors feature-selection, pruning and weight-sharing. 
In this that term constitutes a theoretical justification for those techniques. 
Although cast in terms of neural nets, this paper's analysis applies to any case where con- 
vention is to use the MAP value of a parameter encoding Z to estimate the value of Z. 
2 
BACKPROP OVER I-O FUNCTIONS 
Assume the net's architecture is fixed, and that weight vectors w live in a Euclidean vector 
space W of dimension IWl. Let X be the set of vectors x which can be loaded on the input 
neurons, and O the set of vectors o which can be read off the output neurons. Assume that 
the number of elements in X (IXl) is finite. This is always the case in the real world, where 
measuring devices have finite accuracy, and where the computers used to emulate neural 
nets are finite state machines. For similar reasons O is also finite in practice. However for 
now assume that O is very large and fine-grained, and approximate it as a Euclidean vec- 
tor space of dimension IOI. (This assumption usually holds with neural nets, where output 
values are treated as real-valued vectors.) This assumption will be relaxed later. 
Indicate the set of functions taking X to O by . (net(w, .) is an element of .) Any   � 
is an (IXl x IOI)-dimensional Euclidean vector. Accordingly, densities over W are related 
to densities over � by the usual rules for transforming densities between IWl-dimensional 
and (IXl x IOI)-dimensional Euclidean vector spaces. There are three cases to consider: 
1) IWl < IXl IOI. In general, as one varies over all w's the corresponding i-o func- 
tions net(w, .) map out a sub-manifold of � having lower dimension than . 
2) IWl > IXl IOI. There are an infinite number of w's corresponding to each . 
3) IWl - IXl IOI. This is the easiest case to analyze in detail. Accordingly I will deal 
with it first, deferring discussion of cases one and two until later. 
With some abuse of notation, let capital letters indicate random variables and lower case 
letters indicate values of random variables. So for example w is a value of the weight vector 
random variable W. Use 'p' to indicate probability densities. So for example Pn, IT( I t) is 
the density of the i-o function random variable , conditioned on the training set random 
variable T, and evaluated at the values � =  and T = t. 
In general, any i-o function not expressible as net(w, .) for some w has zero probability. For 
the other i-o functions, with 8(.) being the multivariable Dirac delta function, 
pn,(net(w, .)) = ldw' Pw(W') 8(net(w', .) - net(w, .)). 
When the mapping � = net(W, .) is one-to-one, we can evaluate equation (1) to get 
pT(net(w, .)l t) = PwiT(W I t) / Jl,,w(W), 
where Jn,,w(W) is the Jacobian of the W --) � mapping: 
(1) 
(2) 
202 Wolpert 
Jtl,,w(W) -- I det[/I i //}Wj ](w) I= I det[/} net(w, -)i //}wj ] I. 0) 
net(w, .)imeans the i'th component of the i-o function net(w, .). net(w, x) means the 
vector o mapped by net(w, .) from the input x, and net(w, X)kis the k'th component of o. 
So the i in net(w, .)i refers to a pair of values {x, k}. Each matrix value  //)wj is the 
partial derivative of net(w, X)k with respect to some weight, for some x and k. Jq,,w(w) can 
be rewduen as der it2 [gij(w)], where gij(w) -- Ek [( / wi) OR / wj)] is the metric of 
the W --> � mapping. This form of Jtl,.w(w) is usually more difficult to evaluate though. 
Unfortunately, { = net(w, .) is not one-to-one; where Jn,,w(W) ,e 0 the mapping is locally 
one-to-one, but there are global symmetries which ensure that more than one w corre- 
sponds to each {. (Such symmetries arise from things like permuting the hidden neurons 
or changing the sign of all weights leading into a hidden neuron - see (Fefferman, 1993) 
and references therein.) To circumvent this difficulty we must make a pair of assumptions. 
To begin, restrict attention to Winj, those values w of the variable W for which the Jaco- 
bian is non-zero. This ensures local injectivity of the map between W and . Given a par- 
ticular w  Winj, let k be the number of w'  Winj such that net(w, .) = net(w', .). (Since 
net(w, .) = net(w, .), k > 1.) Such a set of k vectors form an equivalence class, {w}. 
The first assumption is that for all w  Winj the size of {w} (i.e., k) is the same. This will 
be the case if we exclude degenerate w (e.g., w's with all first layer weights set to 0). The 
second assumption is that for all w' and w in the same equivalence class, PwlD (w I d) = 
PwI) (w' I d). This is usually the case. (For example, start with w' and telabel hidden neu- 
rons to get a new w  {w'}. If we assume the Gaussian likelihood and prior, then since 
neither differs for the two w's the weight-posterior is also the same for the two w's.) Given 
these assumptions, plT(net(w, .) I t) = k PWlT(W I t) / J.w(W). So rather than minimize the 
usual cost function, L(w, t), to find the MAP � BP should minimize L'(w, 0 -= L(w, t) + 
In[ Jn,,w(W) ]. The In[ Jn,.w(W) ] term constitutes a correction term to conventional BP. 
One should not confuse the correction term with the other quantities in the neural net liter- 
ature which involve partial derivative matrices. As an example, one way to characterize 
the quality of a local peak w' of a cost function involves the Hessian of that cost function 
(Buntine and Weigend, 1991). The correction term doesn't direcfiy concern the validity of 
such a Hessian-based quality measure. However it does concern the validity of some 
implementations of such a measure. In particular, the correction term changes the location 
of the peak w'. It also suggests that a peak's quality be measured by the Hessian of 
L'(w', 0 with respect to , rather than by the Hessian of L(w', 0 with respect to w. (As an 
aside on the subject of Hessians, note that some workers incorrectly use Hessians when 
they attempt to evaluate quantities like output-variances. See (Wolpert, 1994).) 
If we stipulate that the PqT({ I t) one encounters in the real world is independent of how 
one chooses to parameterize , then the probability density of our parameter must depend 
on how it gets mapped to . This is the basis of the correction term. As this suggests, the 
correction term won't arise if we use non-pqT(  I O-based estimators, like maximum-like- 
lihood estimators. (This is a basic difference between such estimators and MAP estimators 
with a uniform prior.) The correction term is also irrelevant if it we use an MAP estimate 
but J,t,,w(W) is independent of w (as when net (w, .) depends linearly on w). And for non- 
linear net(w, .), the correction term has no effect for some non-MAP-based ways to apply 
Bayesianism to neural nets, like guessing the posterior average � (Neal, 1993): 
Bayesian Backpropagation over I-O Functions Rather Than Weights 203 
E(, 10 -- $d paT( I 0  = ldw PWlT(W I 0 net(w, .), 
(4) 
so one can calculate E(d> I t) by working in W, without any concern for a correction term. 
(Loosely speaking, the Jacobian associated with changing integration variables cancels the 
Jacobian associated with changing the argument of the probability density. A formal deri- 
vation - applicable even when IWl ,e IXI x IOI - is in the appendix of (Wolpert, 1
