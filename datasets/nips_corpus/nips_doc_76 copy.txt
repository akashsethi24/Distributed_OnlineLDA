495 
REFLEX! VE ASSOCIATI VE MEMORIES 
Hendricus G. Loos 
Laguna Research Laboratory, Fallbrook, CA 92028-9765 
ABSTRACT 
In the synchronous discrete model, the average memory capacity of 
bidirectional associative memories (BAMs) is compared with that of 
Hopfield memories, by means of a calculation of the percentage of good 
recall for 100 random BAMs of dimension 64x64, for different numbers 
of stored vectors. The memory capacity is found to be much smaller than 
the Kosko upper bound, which is the lesser of the two dimensions of the 
BAM. On the average, a 64x64 BAM has about 68 % of the capacity of the 
corresponding Hopfield memory with the same number of neurons. Ortho- 
normal coding of the BAM increases the effective storage capacity by 
only 25 %. The memory capacity limitations are due to spurious stable 
states, which arise in BAMs in much the same way as in Hopfield 
memories. Occurrence of spurious stable states can be avoided by 
replacing the thresholding in the backlayer of the BAM by another 
nonlinear process, here called Dominant Label Selection (DLS). The 
simplest DLS is the winner-take-all net, which gives a fault-sensitive 
memory. Fault tolerance can be improved by the use of an orthogonal or 
unitary transformation. An optical application of the latter is a Fourier 
transform, which is implemented simply by a lens. 
INTRODUCTION 
A reflexive associative memory, also called bidirectional associa- 
tive memory, is a two-layer neural net with bidirectional connections 
between the layers. This architecture is implied by Dana Anderson's 
optical resonator 1 , and by similar configurations 2,3. Bart Kosko 4 coined 
the name Bidirectional Associative Memory (BAM), and investigated 
several basic properties 4-6. We are here concerned with the memory 
capacity of the BAM, with the relation between BAMs and Hopfield 
memories 7, and with certain variations on the BAM. 
American Institute of Physics 1988 
496 
BAMSTRUCTURE 
We will use the discrete model in which the state of a layer of 
neurons Is described by a bipolar vector. The Dirac notation 8 will be 
used, in which I> and <l denote respectively column and row vectors. <al 
and la> are each other transposes, <alb> is a scalar product, and !a><bl is 
an outer product. As depicted in Fig. 1, the BAM has two layers of 
neurons, a front layer of N neurons with state vector If>, and a back layer 
backlayer, P neurons 
state vector b 
frontlayer, N neurons 
state vector f 
back 
stroke 
forward 
stroke 
Fig. 1. BAM structure 
of P neurons with state vector 
lb>. The bidirectional connec- 
tions between the layers allow 
slgnal flow in two directions. 
The front stroke gives lb> = 
s(B!f>), where B is the connec- 
tion matrix, and s( ) is a thres- 
hold function, operating at 
zero. The back stroke results In an ugraded front state <f'l=s(<b!B), 
which also may be written as !f'>=s(B'!b>), where the superscript T 
denotes transposition. We consider the synchronous model, where all 
neurons of a layer are updated simultaneously, but the front and bacl( 
layers are updated at different times. The BAM action ls shown In Fig. 2. 
The forward stroke entails taking scalar products between a front 
state vector If> and the rows of B, and entering the thresholded results 
as elements of the back state vector lb>. !n the bacl< stroke we take 
f 
thresholding 
& reflection 
NxP 
Flg. 2. BAM action 
thresholding 
& reflection 
b 
V f ( 
, 
NxN 
j thresholding 
& 
feedback 
Fig. 3. Autoassociative 
memory action 
scalar products of lb> with column vectors of B, and enter the 
thresholded results as elements of an upgraded state vector If'>. !n 
contrast, the action of an autoassociative memory is shown in Figure 3. 
The BAM may also be described as an autoassociative memory 5 by 
497 
concatenating the front and back vectors into a slngle state vector 
Iv>=!f,b>,and by taking the (N+ P)x(N+ P) connection matrlx as shown in Flg. 
4. This autoassociative memory has the same number of neurons as our 
 zro 
b 
Fig. 4. BAM as autoasso- 
ciative memory 
memory 7 the connection 
BAM, viz. N+P. The BAM operation where 
initially only the front state is speci- 
fied may be obtained with the corres- 
ponding autoassociative memory by 
initially specifying lb> as zero, and by 
arranging the thresholding operation 
such that s(O) does not alter the state 
vector component. For a Hopfield 
matrix is 
M 
H=(lm><ml) -MI , (1) 
m=l 
where Im>, m=l to M, are stored vectors, and I is the identity matrix. 
Writing the N+P dimensional vectors Im> as concatenations Idm,Cm>, (1) 
takes the form 
M 
H-(  (!dm><dml+lcm><cml+ldm><cml+lcm><dml))-Ml, (2) 
m=l 
with proper block placing of submatrices understood. Writing 
M 
Hd=(  
m=l 
M 
K= ElCm><dml , (3) 
m=l M 
Idm> <dml)-Ml, Hc=( - Icm> <cml)-Ml, (4) 
m--1 
where the I are identities in appropriate subspaces, the Hopfield matrix 
be as 
H may partitioned shown in Fig. 5. K is just the BAM matrix given 
by Kosko 5, and previously used by Kohonen 9 for linear heteroassociative 
memories. Comparison of Figs. 4 and ,% shows that in the synchronous 
discrete model the BAM with connection matrix (3) is equivalent to a 
Hopfield memory in which the diagona! blocks H d and H c have been 
498 
deleted. Since the Hopfield memory is robust, this pruning may not 
affect much the associative recall of stored vectors, if M is small; 
however, on the average, pruning will not improve the memory capacity. 
It follows that, on the average, a discrete synchronous BAM with matrix 
(3) can at best have the capacity of a Hopfield memory with the same 
number of neurons. 
We have performed computations of the average memory capacity 
for 64x64 BAMs and for corresponding 128x128 Hopfield memories. 
Monte Carlo calculations were done for I O0 memories, each of which 
stores M random bipolar vectors. The straight recall of all these vectors 
was checked, allowing for 24 iterations. For the BAMs, the iterations 
were started with a forward stroke in which one of the stored vectors 
Idm> was used as input. The percentage of good recall and its standard 
deviation were calculated. The results plotted in Fig. 6 show that the 
square BAM has about 68% of the capacity of the corresponding Hopfield 
memory. Although the total number of neurons is the same, the BAM only 
needs 1/4 of the number of connections of the Hopfield memory. The 
storage capacity found is much smaller than the Kosko 6 upper bound, 
which is rain (N,P). oo- 
d T 
Fig. 5. Partitioned 
Hopfield matrix 
601 
4oi 
201 
10 20 30 40 50 60 
M, number of stored vectors 
Fig. 6. % of good recall versus M 
CODED BAM 
So far, we have considered both front and back states to be used for 
data. There is another use of the BAM in which only front states are used 
as data, and the back states are seen as providing a code, label, or 
pointer for the front state. Such use was anticipated in our expression 
(3) for the BAM matrix which stores data vectors Idm> and their labels or 
codes Icm>. For a square BAM, such an arrangement cuts the information 
contained in a single stored data vector in half. However, the freedom of 
499 
choosing the labels Icm> may perhaps be put to good use. Part of the 
problem of spurious stable states, whlch plagues BAMs as well as 
Hopfield memories as they are loaded up, is due to the lack of 
orthogonality of the stored vectors. In the coded BAM we have the 
opportunity to remove part of thls problem by choosing the labels as 
orthonormal. Such labels have been used previously by Kohonen 9 in linear 
heteroassociative memories. The question whether memory capacity can 
be improved In this manner was explored by taking 64x64 BAMs in which 
the labels are chosen as Hadamard vectors. The latter are bipolar vectors 
with Euclidean norm FP, which form an orthonormal set. These vectors 
are rows of a PxP Hadamard matrix; for a discussion see Hatwit and 
Sloane 10. The storage capacity of such Hadamard-coded BAHs was 
calculated as function of the number M of stored vectors for 100 cases 
for each value of M, in the manner discussed before. The percentage of 
good recall and its standard deviation are shown in Fig. 6. It is seen that 
the Hadamard coding gives about a factor 2.5 in M, compared to the 
ordinary 64x64 BAM. However, the coded BAM has only half the stored 
data vector dimension. Accounting for this factor 2 reduction of data 
vector dimension, the effective storage capacity advantage obtained by 
Hadamard coding comes to only 25 %. 
HALF BAH WITH HADAMARD CODING 
For the coded BAH there is the option of deleting the threshold 
operation in the front layer. The resulting architecture may be called 
half BAH. In the half BAM, thresholding is only done on the labels, and 
consequently, the data may be taken as analog vectors. Although such an 
arrangement diminishes the robustness of the memory somewhat, there 
are applications of interest. We have calculated the percentage of good 
recall for 100 cases, and found that giving up the data thresholding cuts 
the storage capacity of the Hadamard-coded BAH by about 60 %. 
SELECTIVE REFLEXIVE MEMORY 
The memory capacity limitations shown in Fig. 6 are due to the 
occurence of spurious states when the memories are loaded up. 
Consider a discrete BAM with stored data vectors Ira>, m = 1 to M, 
orthonormal labels Icm>, and the connection matrix 
500 
M 
K=. Icm><ml 
m=l 
(5) 
For an input data vector Iv> which is closest to the stored data vector 
II>, one has in the forward stroke 
M 
Ib>=s(clc 1 >+ Z amlcm>) ' (6) 
m=2 
where 
c=<11v>, and am=<mlv> (7) 
Although for m  1 
M 
am<C, for some vector component the sum 7 amlcm> 
m=2 
may accumulate to such a large value as to affect the thresholded result 
lb>. The problem would be avoided if the thresholding operation s( ) in the 
back layer of the BAM were to be replaced by another nonlinear operation 
which selects, from the linear combination 
M 
ClCl >
