Counting function heorem for 
multi-layer networks 
Adam Kowalczyk 
Telecom Australia, Research Laboratories 
770 Blackburn Road, Clayton, Vic. 3168, Australia 
(a.kowalczyk@trl.oz.au) 
Abstract 
We show that a randomly selected N-tuple  of points of R ' with 
probability > 0 is such that any multi-layer perceptron with the 
first hidden layer composed of hi threshold logic units can imple- 
of. > 
ment exactly 2 z_,i:0 - 
then such a perceptron must have all units of the first hidden layer 
fully connected to inputs. This implies the maximal capacities (in 
the sense of Cover) of 2n input patterns per hidden unit and 2 input 
patterns per synaptic weight of such networks (both capacities are 
achieved by networks with single hidden layer and are the same as 
for a single neuron). Comparing these results with recent estimates 
of VC-dimension we find that in contrast to the single neuron case, 
for sufficiently large n and hi, the VC-dimension exceeds Cover's 
capacity. 
I Introduction 
In the course of theoretical justification of many of the claims made about neural 
networks regarding their ability to learn a set of patterns and their ability to gen- 
eralise, various concepts of maximal storage capacity were developed. In particular 
Cover's capacity [4] and VC-dimension [12] are two expressions of this notion and 
are of special interest here. We should stress that both capacities are not easy 
to compute and are presently known in a few particular cases of feedforward net- 
works only. VC-dimension, in spite of being introduced much later, has been far 
375 
376 Kowalczyk 
more researched, perhaps due to its significance expressed by a well known relation 
between generalisation and learning errors [12, 3]. Another reason why Cover's ca- 
pacify gains less attention, perhaps, is that for the single neuron case it is twice 
higher than VC-dimension. Thus if one would hypothesise a similar relation to be 
true for other feedforward networks, he would judge Cover's capacity to be quite 
an unattractive parameter for generalisation estimates, where VC-dimenslon is be- 
lieved to be unrealistically big. One of the aims of this paper is to show that this 
last hypothesis is not true, at least for some feedforward networks with sufficiently 
large number of hidden units. In the following we will always consider multilayer 
perceptrons with r continuously-valued inputs, a single binary output, and one or 
more hidden layers, the first of which is made up of threshold logic units only. 
The derivation of Cover's capacity for a single neuron in [4] is based on the so-called 
Function Counting Theorem, proved for the linear function in the sixties (c.f. [4]), 
which states that for an N-tuple  of points in general position one can implement 
C'(N, n) ae 2 Y-i=0 Jv- different dichotomies of. Extension of this result to the 
multilayer case is still an open problem (c.f.T. Cover's address at NIPS'92). One of 
the complications arising there is that in contrast to the single neuron case even for 
percepttons with two hidden units the number of implementable dichotomies may 
be different for different N-tuples in general position [8]. Our first main result states 
that this dependence on  is relatively weak, that for a multilayer perceptron the 
number of implementable dichotomies (counting function) is constant on each of a 
finite number of connected components into which the space of N-tuples in general 
position can be decomposed. Then we show that for one of these components 
C'(N,rh) different dichotomies can be implemented, where h is the number of 
hidden units in the first hidden layer (all assumed to be linear threshold logic 
units). This leads to an upper bound on Cover's capacity of 2n input patterns per 
(hidden) neuron and 2 patterns per adjustable synaptic weight, the same as for a 
single neuron. Comparing this result with a recent lower bound on VC-dimension 
of multilayer perceptrons [10] we find that for for sufficiently large n and h the 
VC-dimension is higher than Cover's capacity (by a factor log2(hl)). 
The paper extends some results announced in [5] and is an abbreviated version of 
a forthcoming paper [6]. 
2 Results 
2.1 Standing assumptions and basic notation 
We recall that in this paper a multilayer ioercelotror means a layered feedforward 
network with one or more hidden layers, and the first hidden layer built exclusively 
from threshold logic units. 
A dickotomy of an N-tuple  = (xx,...,xnr)  (Rr) nr is a function $: {xx,...,xnr}  
{0,1}. For a multilayer perceptron F: R r ---} {0, 1} let  -* C'F() denote the 
number of different dichotomies of  which can be implemented for all possible 
selections of synaptic weights and biases. We shall call C'F() a courttire# furctior 
following the terminology used in [4]. 
Counting Function Theorem for Multi-Layer Networks 377 
) 
Example 1. C(g) = C(N, n) de__f 2 Ei=0 N-I for a single threshold logic unit 
R --, {0, x) 
Points of an N-tuple  6 (R') N are said to be in general position if there does not 
exist an l de= f min(N,n- 1)-dimensional affine hyperplane in R ' containing (l + 2) 
of them. We use a symbol {7P(n, N) C (R') N to denote that set of all N-tuples  
in general position. 
Throughout this paper we assume to be given a probability measure dp dc__f f dx on 
R ' such that the density f: R ' --, R is a continuous function. 
2.2 Counting function is locally constant 
We start with a basic characterisations of the subset 679(n, N) C (R') N. 
Theorem 1 (i) (77)(n,N) is an open and dense subset of (Rn) N with a finite 
number of connected components. 
(it) Any of these components is unbounded, has an infinite Lebesgue measure and 
has a positive probability measure. 
Proof outline. (i) The key point to observe is that G73(n, N) = { : p() =/: 0}, 
where p: (R') N ---} R is a polynomial on (R') N. This implies immediately that 
G73(n, N) is open and dense in (R') N. The finite number of connected components 
follows from the results of Milnor [7] (c.f. [2]). 
(it) This follows from an observation that each of the connected components C/has 
the property that if (x, ...,XN) 6 Ci and a > 0, then (axe, ...,aXN) 6 �i. [] 
As Example 1 shows, for a single neuron the counting function is constant on 
G7)(n, N). However, this may not be the case even for perceptrons with two hidden 
units and two inputs (c.f. [8, 0] for such examples and Corollary 8). Our first main 
result states that this dependence on  is relatively weak. 
Theorem 2 CF(X) is constant on connected components of G7)(n, N). 
Proof outline. The basic heuristic behind the proof of this theorem is quite simple. 
If we have an N-tuple  6 (R') N which is split into two parts by a hyperplane, 
then this split is preserved for any sufficiently small perturbation f � (R') N of , 
and vice versa, any split of : corresponds to a split of . The crux is to show that 
if  is in general position, then a minute perturbation f of  cannot allow a bigger 
number of splits than is possible for . We refer to [0] for details. [] 
The following corollary outlines the main impact of Theorem 2 on the rest of the 
paper. It reduces the problem of investigation of the function CF(X) on G7)(n, N) to 
a consideration of a set of individual, special cases of N-tuples which, in particular, 
are amenable to be solved analytically. 
Corollary 3 If   Q73(n,N), then CF() = CF(f) for a randomly selected N- 
tuple 2 6 (Rn) N with a probability > O. 
378 Kowalczyk 
2.3 A case of special component of gyP(n, N) 
The following theorem is the crux of the paper. 
Theorem 4 There exists a connected component CC C gYP(n, N) C (R') N such 
that 
cF() = = 2 - (for CC) 
i=o i 
with equality iff the input and first hidden/dyer are fully connected. The synaptlc 
weights to units not in the first hidden layer can be constant. 
Using now Corollary 3 we obtain: 
Corollary 5 C',() = C(N, nh) for  � (R') r with a probabilit.l > O. 
The component C'C C gyP(n, N) in Theorem 4 is defined as the connected compo- 
nent containing 
N de__f (c(t),c(t.),...,c(tN)) e (R') N, (1) 
where c : R  R ' is the curve defined as c(t) ae (t,t.,...,t) for t � R and 
0 < tx < t < -.. < tr are some numbers (this example has been considered 
previously in [11]). The essential part of the proof of Theorem 4 is showing the 
basic properties of the N-tuple 15v which will be described by the Lemma below. 
Any dichotomy  of the N-tuple fiv (c.f. 1) is uniquely defined by its value at 
(2 options) and the set of indices 1 < i < i. < .-- < ia < N of all transitional 
pairs (c(ti), c(ti+x)), i.e. all indices ii such that 5(c(ti)) y 5(c(ti+x)), where j = 
1,..., k, (additional (v-x) options). Thus it is easily seen that there exist altogether 
2(v -) different dichotomies of 15v for any given number k of transitional pairs, 
where0<k<N. 
Lemma6 Given integers n,N,h > 0, k > 0 and a dichotomy  Of N with k 
transitional pairs. 
(i) Ilk _< nh, then there exist hyperplanes H(w,,,,), (w, b) 6 R n x It, such that 
5(p) = O bo+ viO(wi .pj +hi) , (2) 
+ 0, (3) 
der der 
for i = 1, ..., h and j = 1, ..., N; here vi = 1 if n is even and vi = (--1) i if n is odd, 
der 
bo = -0.5 ifn is odd, h is even and 5(Po) = 1, and bo 
= 0.5, otherwise. 
(ii) Ilk = nh, then w  0 for j = 1,...,n and i = 1,...,h, where w = 
(wi, wia, ..., wi,). 
(iii) If k > nh, *hen (2) and (3) cannot be satised. 
The proof of Lemma  relies on usage of the Vandermonde determinant and its 
derivatives. It is quite technical and thus not included here (c.f. [] for details). 
Counting Function Theorem for Multi-Layer Networks 379 
10 4 
10 3 
10 2 
10 
2 
(Theorem 7 
(Mitchison & Burbin [10]]. 
(Huang & Huang 
[2], Sakurai [1 ..' 
(Akaho & Amari -' .. 
ts]) -- 
I 2 5 10 102 103 104 
Number of hidden units (hi) 
Figure 1: Some estimates of capacity. 
3 Discussion 
3.1 An upper bound on Cover's capacity
