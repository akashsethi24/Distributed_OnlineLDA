Model selection in clustering by uniform 
convergence bounds* 
Joachim M. Buhmann nd Marcus Held 
Institut fiir Informatik III, 
RSmerstrafie 164, D-53117 Bonn, Germany 
{jb,held)@cs.uni-bonn.de 
Abstract 
Unsupervised learning algorithms are designed to extract struc- 
ture from data samples. Reliable and robust inference requires a 
guarantee that extracted structures are typical for the data source, 
i.e., similar structures have to be inferred from a second sample 
set of the same data source. The overfitting phenomenon in max- 
imum entropy based annealing algorithms is exemplarily studied 
for a class of histogram clustering models. Bernstein's inequality 
for large deviations is used to determine the maximally achievable 
approximation quality parameterized by a minimal temperature. 
Monte Carlo simulations support the proposed model selection cri- 
terion by finite temperature annealing. 
I Introduction 
Learning algorithms are designed to extract structure from data. Two classes of 
algorithms have been widely discussed in the literature - supervised and unsuper- 
vised learning. The distinction between the two classes depends on supervision or 
teacher information which is either available to the learning algorithm or missing. 
This paper applies statistical learning theory to the problem of unsupervised learn- 
ing. In particular, error bounds as a protection against over fitting are derived for 
the recently developed Asymmetric Clustering Model (ACM) for co-occurrence 
data [6]. These theoretical results show that the continuation method determin- 
istic annealing yields robustness of the learning results in the sense of statistical 
learning theory. The computational temperature of annealing algorithms plays the 
role of a control parameter which regulates the complexity of the learning machine. 
Let us assume that a hypothesis class 7/ of loss functions h(x; c) is given. These 
loss functions measure the quality of structures in data. The complexity of 7/ is 
controlled by coarsening, i.e., we define a 'y-cover of 7/. Informally, the inference 
principle advocated by us performs learning by two inference steps: (i) determine 
the optimal approximation level 7 for consistent learning (in terms of large risk devi- 
ations); (ii) given the optimal approximation level 7, average over all hypotheses in 
an appropriate neighborhood of the empirical minimizer. The result of the inference 
*This work has been supported by the German Israel Foundation for Science and Re- 
search Development (GIF) under grant #1-0403-001.06/95. 
Model Selection in Clustering by Uniform Convergence Bounds 217 
procedure is not a single hypothesis but a set of hypotheses. This set is represented 
either by an average of loss functions or, alternatively, by a typical member of this 
set. This induction approach is named Empirical Risk Approximation (ERA) [2]. 
The reader should note that the learning algorithm has to return an average struc- 
ture which is typical in a -,-cover sense but it is not supposed to return the hypothesis 
with minimal empirical risk as in Vapnik's Empirical Risk Minimization (ERM) 
induction principle for classification and regression [9]. The loss function with mini- 
mal empirical risk is usually a structure with maximal complexity, e.g., in clustering 
the ERM principle will necessarily yield a solution with the maximal number of clus- 
ters. The ERM principle, therefore, is not suitable as a model selection principle to 
determine the number of clusters which are stable under sample fluctuations. The 
ERA principle with its approximation accuracy q, solves this problem by controlling 
the effective complexity of the hypothesis class. 
In spirit, this approach is similar to the Gibbs-algorithm presented for example in 
[3]. The Gibbs-algorithm samples a random hypothesis from the version space to 
predict the label of the l + lth data point xt+. The version space is defined as 
the set of hypotheses which are consistent with the first l given data points. In our 
approach we use an alternative definition of consistency, where all hypothesis in an 
appropriate neighborhood of the empirical minimizer define the version space (see 
also [4]). Averaging over this neighborhood yields a structure with risk equivalent to 
the expected risk obtained by random sampling from this set of hypotheses. There 
exists also a tight methodological relationship to [7] and [4] where learning curves 
for the learning of two class classifiers are derived using techniques from statistical 
mechanics. 
2 The Empirical Risk Approximation Principle 
The data samples Z = (zr  f, 1 _ r _ l) which have to be analyzed by the unsu- 
pervised learning algorithm are elements of a suitable object (resp. feature) space 
f. The samples are distributed according to a measure/z which is not assumed to 
be known for the analysis.  
A mathematically precise statement of the ERA principle requires several defini- 
tions which formalize the notion of searching for structure in the data. The qual- 
ity of structures extracted from the data set Z is evaluated by the empirical risk 
1 
7(c; Z) :-- ? '-r=l h(zr; c) of a structure a given the training set Z. The function 
h(z; a) is known as loss function in statistics. It measures the costs for processing a 
generic datum z with model a. Each value a  A parameterizes an individual loss 
function with A denoting the set of possible parameters. The loss function which 
minimizes the empirical risk is denoted by d � := argminae^ 7(a; Z). 
The relevant quality measure for learning is the expected risk T(c) := 
fa h(z; a)d/z(z). The optimal structure to be inferred from the data is a � :- 
argminae^ T(a). The distribution /z is assumed to decay sufficiently fast with 
bounded rth moments E, (lb(z; a) - T(a)l < r!w-2V. (h(z; a)), �a e A 
(r  2). E, (.) and V, (.) denote expectation and variance of a random vari- 
able, respectively. - is a distribution dependent constant. 
ERA requires the learning algorithm to determine a set hypotheses on the basis 
of the finest consistently learnable cover of the hypothesis class. Given a learning 
accuracy 7 a subset of parameters A v = (el,... ,alAl-1) U (&�) can be defined 
such that the hypothesis class 7/ is covered by the function balls with index sets 
By(a) := {a'' fa Ih(z;a')- h(z;a)[ dp(z) _< 7}, i.e. A C [JaeA B(a). The em- 
 Knowledge of covering numbers is required in the following analysis which is a weaker 
type of information than complete knowledge of the probability measure/ (see also [5]). 
218 J. M. Buhmann andM. HeM 
pirical minimizer &� has been added to the cover to simplify bounding arguments. 
Large deviation theory is used to determine the approximation accuracy /for learn- 
ing a hypothesis from the hypothesis class 7/. The expected risk of the empirical 
minimizer exceeds the global minimum of the expected risk T(c �) by ca* with a 
probability bounded by Bernstein's inequality [8] 
P {T(& �) -T(a �) > 
< P(sup17(a)-T(a)I> 1 
_ 2[A[exp-8+4,r(e_7/a.r) 
The complexity lay[ of the coarsened hypothesis class has to be small enough to 
guarantee with high confidence small e-deviations? This large deviation inequality 
weighs two competing effects in the learning problem, i.e. the probability of a 
large deviation exponentially decreases with growing sample size l, whereas a large 
deviation becomes increasingly likely with growing cardinality of the /-cover of the 
hypothesis class. According to (1) the sample complexity l0 (% e, 5) is defined by 
t0 2 
l�g IAvl- 8 + 4r(e-f/a 
2 
+1og =0. (2) 
With probability 1 - 5 the deviation of the empirical risk from the expected risk is 
bounded by � (e�Pt7 T -- /) --: /'PP. Averaging over a set of functions which exceed 
the empirical minimizer by no more than 2/'pp in empirical risk yields an average 
hypothesis corresponding to the statistically significant structure in the data, i.e., 
7(a�) - 7(& �) <_ T(a�) +/'P - (7(&�) -/') _< 2/'p since T(a �) < T(& �) by 
definition. The key task in the following remains to calculate the minimal precision 
e(/) as a function of the approximation /and to bound from above the cardinality 
[Av[ of the /-cover for specific learning problems. 
3 Asymmetric clustering model 
The asymmetric clustering model was developed for the analysis resp. grouping of 
objects characterized by co-occurrence of objects and certain feature values [6]. 
Application domains for this explorerive data analysis approach are for example 
texture segmentation, statistical language modeling or document retrieval. 
Denote by f - A' x y the product space of objects xi  A',I _< i _< n and 
features yj  y, 1 _< j _< f. The xi  3:' are characterized by observations 
Z = {zr} = {(xi(0,yj(0),r = 1,... ,l}. The sufficient statistics of how often 
the object-feature pair (xi, yj) occurs in the data set Z is measured by the set 
of frequencies {rli j � number of observations (xi, y j)/total number of observations}. 
Derived measurements are the frequency of observing object xi, i.e. r/i = Y]-=I r/ij 
and the frequency of observing feature yj given object xi, i.e. r/li = rli/rli. The 
asymmetric clustering model defines a generarive model of a finite mixture of com- 
ponent probability distributions in feature space with cluster-conditional distri- 
butions q = (qjlv), 1 _< j _< f, 1 _<  _< k (see [6]). We introduce indicator 
variables Mir  {0, 1} for the membership of object xi in cluster   {1,..., k}. 
Y].,x Mi = I �i � i _< i <_ n enforces the uniqueness constraint for assignments. 
2The maximal standard deviation a* := supe^ /V {h(z; a)} defines the scale to 
measure deviations of the empirical risk from the expected risk (see [2]). 
Model Selection in Clustering by Uniform Convergence Bounds 219 
Using these variables the observed data Z are distributed according to the genera- 
tive model over A' x y: 
i k 
P 
