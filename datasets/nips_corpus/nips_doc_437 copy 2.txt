Neural Network- Gaussian Mixture Hybrid for 
Speech Recognition or Density Estimation 
Yoshua Bengio 
Dept. Brain and Cognitive Sciences 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
Renato De Mori 
School of Computer Science 
McGill University 
Canada 
Giovanni F!ammia 
Speech Technology Center, 
Aalborg University, Denmark 
Rag Kompe 
Erlangen University, Computer Science 
Erlangen, Germany 
Abstract 
The subject of this paper is the integration of multi-layered Artificial Neu- 
ral Networks (ANN) with probability density functions such as Gaussian 
mixtures found in continuous density Hidden Markov Models (HMM). In 
the first part of this paper we present an ANN/HMM hybrid in which 
all the parameters of the the system are simultaneously optimized with 
respect to a single criterion. In the second part of this paper, we study 
the relationship between the density of the inputs of the network and the 
density of the outputs of the networks. A few experiments are presented 
to explore how to perform density estimation with ANNs. 
I INTRODUCTION 
This paper studies the integration of Artificial Neural Networks (ANN) with prob- 
ability density functions (pdf) such as the Gaussian mixtures often used in contin- 
uous density Hidden Markov Models. The ANNs considered here are multi-layered 
or recurrent networks with hyperbolic tangent hidden units. Raw or preprocessed 
data is fed to the ANN, and the outputs of the ANN are used as observations for 
a parametric probability density function such as a Gaussian mixture. One may 
view either the ANN as an adaptive preprocessor for the Gaussian mixture, or the 
Gaussian mixture as a statistical postprocessor for the ANN. A useful role for the 
ANN would be to transform the input data so that it can be more efficiently mod- 
eled by a Gaussian mixture. An interesting situation is one in which most of the 
input data points can be described in a lower dimensional space. In this case, it 
is desired that the ANN learns the possibly non-linear transformation to a more 
compact representation. 
175 
176 Bengio, De Mori, Flammia, and Kompe 
In the first part of this paper, we briefly describe a hybrid of ANNs and Hid- 
den Markov Models (HMM) for continuous speech recognition. More details on 
this system can be found in (Bengio 91). In this hybrid, all the free parameters 
are simultaneously optimized with respect to a single criterion. In recent years, 
many related combinations have been studied (e.g., Levin 90, Bridle 90, Bourlard 
&; Wellekens 90). These approaches are often motivated by observed advantages and 
disadvantages of ANNs and HMMs in speech recognition (Bourlard & Wellekens 89, 
Bridle 90). Experiments of phoneme recognition on the TIMIT database with the 
proposed ANN/HMM hybrid are reported. The task under study is the recogni- 
tion (or spotting) of plosive sounds in continuous speech. Comparative results on 
this task show that the hybrid performs better than the ANN alone, better than 
the ANN followed by a dynamic programming based postprocessor using duration 
constraints, and better than the HMM alone. Furthermore, a global optimization 
of all the parameters of the system also yielded better performance than a separate 
optimization. 
In the second part of this paper, we attempt to extend some of the findings of the 
first part, in order to use the same basic architecture (ANNs followed by Gaussian 
mixtures) to perform density estimation. We establish the relationship between 
the network input and output densities, and we then describe a few experiments 
exploring how to perform density estimation with this system. 
2 ANN/HMM HYBRID 
In a HMM, the likelihood of the observations, given the model, depends in a sim- 
ple continuous way on the observations. It is therefore possible to compute the 
derivative of an optimization criterion C, with respect to the observations of the 
HMM. For example, one may use the criterion of the Maximum Likelihood (ML) 
of the observations, or of the Maximum Mutual Information (MMI) between the 
observations and the correct sequence. If the observation at each instant is the 
oc 
vector output, Yt, of an ANN, then one can use this gradient, D-f7, to optimize the 
parameters of the ANN with back-propagation. See (Bridle 90, Bottou 91, Bengio 
91, Bengio et al 92) on ways to compute this gradient. 
2.1 EXPERIMENTS 
A preliminary experiment has been performed using a prototype system based on 
the integration of ANNs with HMMs. The ANN was initially trained based on 
a prior task decomposition. The task is the recognition of plosive phonemes pro- 
nounced by a large speaker population. The 1988 version of the TIMIT continuous 
speech database has been used for this purpose. SI and SX sentences from regions 
2, 3 and 6 were used, with 1080 training sentences and 224 test sentences, 135 train- 
ing speakers and 28 test speakers. The following 8 classes have been considered: 
/p/,/t/,/k/,/b/,/d/,/g/,/dx/,/all other phones/. Speaker-independent recognition 
of plosive phonemes in continuous speech is a particularly difficult task because 
these phonemes are made of short and non-stationary events that are often con- 
fused with other acoustically similar consonants or may be merged with other unit 
segments by a recognition system. 
Neural Network--Gaussian Mixture Hybrid for Speech Recognition or Density Estimation 177 
Level I Level 2 Level 3 
Initially ained to 
initially .tained specialized principal 
components 
to recogmze networks of lower  ï¿½ 
broad phonetic . 
classes . ....  levels 
7 f--'t ...... .......... 
SPEECH SIGNAANN 1]observations  ' , 
ANN3 
preprocessing   some sl?ecialized task 
'. e.g. plosive discrimation 
Figure 1: Architecture of the ANN/HMM Hybrid for the Experiments. 
The ANNs were trained with back-propagation and on-line weight update. As dis- 
cussed in (Bengio 91), speech knowledge is used to design the input, output, and 
architecture of the system and of each one of the networks. The experimental sys- 
tem is based on the scheme shown in Figure 1. The architecture is built on three 
levels. The approach that we have taken is to select different input parameters and 
different ANN architectures depending on the phonetic features to be recognized. 
At level 1, two ANNs are initially trained to perform respectively plosive recognition 
(ANN3) and broad classification ofphonemes (ANN2). ANN3 has delays and recur- 
rent connections and is trained to recognize static articulatory features of plosives 
in a way that depends of the place of articulation of the right context phoneme. 
ANN2 has delays but no recurrent connections. The design of ANN2 and ANN3 is 
described in more details in (Bengio 91). At level 2, ANN1 acts as an integrator of 
parameters generated by the specialized ANNs of level 1. ANN1 is a linear network 
that initially computes the 8 principal components of the concatenated output vec- 
tors of the lower level networks (ANN2 and ANN3). In the experiment described 
below, the combined network (ANNI+ANN2+ANN3) has 23578 weights. Level 3 
contains the HMMs, in which each distribution is modeled by a Gaussian mixture 
with 5 densities. See (Bengio et al 92) for more details on the topology of the 
HMM. The covariance matrix is assumed to be diagonal since the observations are 
initially principal components and this assumption reduces significantly the num- 
ber of parameters to be estimated. After one iteration of ML re-estimation of the 
HMM parameters only, all the parameters of the hybrid system were simultane- 
ously tuned to maximize the ML criterion for the next 2 iterations. Because of the 
simplicity of the implementation of the hybrid trained with ML, this criterion was 
used in these experiments. Although such an optimization may theoretically worsen 
performance l, we observed an marked improvement in performance after the final 
global tuning. This may be explained by the fact that a nearby local maximum of 
In section 3, we consider maximization of the likelihood of the inputs of the network, 
178 Bengio, De Mori, Flammia, and Kompe 
the likelihood is attained from the initial starting point based on prior and separate 
training of the ANN and the HMM. 
Table 1: Comparative Recognition Results. % recognized - 100 - % substitutions 
- % deletions. % accuracy - 100- % substitutions - % deletions-% insertions. 
%rec %ins %del %subs %ace 
ANNs alone 85 32 0.04 15 53 
HMMs alone 76 6.3 2.2 22.3 69 
ANNs+DP 88 16 0.01 11 72 
ANNs+HMM 87 6.8 0.9 12 81 
ANNs+HMM+global opt. 90 3.8 1.4 9.0 86 
In order to assess the value of the proposed approach as well as the improvements 
brought by the HMM as a post-processor for time alignment, the performance 
of the hybrid system was evaluated and compared with that of a simple post- 
processor applied to the outputs of the ANNs and with that of a standard dynamic 
programming postprocessor that models duration probabilities for each phoneme. 
The simple post-processor assigns a symbol to each output frame of the ANNs by 
comparing the target output vectors with actual output vectors. It then smoothes 
the resulting string to remove very short segments and merges consecutive segments 
that have the same symbol. The dynamic programming (DP) postprocessor finds 
the sequence of phones that minimizes a cost that imposes durational constraints 
for each phoneme. In the HMM alone system, the observations are the cepstrum 
and the energy of the signal, as well as their derivatives. Comparative results for 
the three systems are summarized in Table 1. 
3 DENSITY ESTIMATION WITH AN ANN 
In this section, we consider an extension of the system of the previous section. 
The objective is to perform density estimation of the inputs of the ANN. Instead 
of maximizing a criterion that depends on the density of the outputs of an ANN, 
we maximize the likelihood of inputs of the ANN. Hence the ANN is more than a 
preproce
