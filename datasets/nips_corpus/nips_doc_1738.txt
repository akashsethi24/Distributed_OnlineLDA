Constructing Heterogeneous Committees 
Using Input Feature Grouping: 
Application to Economic Forecasting 
Yuansong Liao and John Moody 
Department of Computer Science, Oregon Graduate Institute, 
P.O.Box 91000, Portland, OR 97291-1000 
Abstract 
The committee approach has been proposed for reducing model 
uncertainty and improving generalization performance. The ad- 
vantage of committees depends on (1) the performance of individ- 
ual members and (2) the correlational structure of errors between 
members. This paper presents an input grouping technique for de- 
signing a heterogeneous committee. With this technique, all input 
variables are first grouped based on their mutual information. Sta- 
tistically similar variables are assigned to the same group. Each 
member's input set is then formed by input variables extracted 
from different groups. Our designed committees have less error cor- 
relation between its members, since each member observes different 
input variable combinations. The individual member's feature sets 
contain less redundant information, because highly correlated vari- 
ables will not be combined together. The member feature sets con- 
tain almost complete information, since each set contains a feature 
from each information group. An empirical study for a noisy and 
nonstationary economic forecasting problem shows that commit- 
tees constructed by our proposed technique outperform committees 
formed using several existing techniques. 
I Introduction 
The committee approach has been widely used to reduce model uncertainty and 
improve generalization performance. Developing methods for generating candidate 
committee members is a very important direction of committee research. Good 
candidate members of a committee should have (1) good (not necessarily excellent) 
individual performance and (2) small residual error correlations with other members. 
Many techniques have been proposed to reduce residual correlations between mem- 
bers. These include resampling the training and validation data [3], adding ran- 
domness to data [7], and decorrelation training [8]. These approaches are only 
effective for certain models and problems. Genetic algorithms have also been used 
to generate good and diverse members [6]. 
Input feature selection is one of the most important stages of the model learning 
process. It has a crucial impact both on the learning complexity and the general- 
922 Y. Liao andJ. Moody 
ization performance. It is essential that a feature vector gives sufficient information 
for estimation. However, too many redundant input features not only burden the 
whole learning process, but also degrade the achievable generalization performance. 
Input feature selection for individual estimators has received a lot of attention 
because of its importance. However, there has not been much research on feature 
selection for estimators in the context of committees. Previous research found 
that giving committee members different input features is very useful for improving 
committee performance [4], but is difficult to implement [9]. The feature selection 
problem for committee members is conceptually different than for single estimators. 
When using committees for estimation, as we stated previously, committee members 
not only need to have reasonable performance themselves, but should also make 
decisions independently. 
When all committee members are trained to model the same underlying function, 
it is difficult for committee members to optimize both criteria at the same time. In 
order to generate members that provide a good balance between the two criteria, we 
propose a feature selection approach, called input feature grouping, for commit- 
tee members. The idea is to give each member estimator of a committee a rich but 
distinct feature sets, in the hope that each member will generalize independently 
with reduced error correlations. 
The proposed method first groups input features using a hierarchical clustering 
algorithm based on their mutual information, such that features in different groups 
are less related to each other and features within a group are statistically similar 
to each other. Then the feature set for each committee member is formed by 
selecting a feature from each group. Our empirical results demonstrate that forming 
a heterogeneous committee using input feature grouping is a promising approach. 
2 Committee Performance Analysis 
There are many ways to construct a committee. In this paper, we are mainly 
interested in heterogeneous committees whose members have different input feature 
sets. Committee members are given different subsets of the available feature set. 
They are trained independently, and the committee output is either a weighted or 
unweighted combination of individual members' outputs. 
In the following, we analyze the relationship between committee errors and average 
member errors from the regression point of view and discuss how the residual cor- 
relations between members affect the committee error. We define the training data 
7) = {(XZ, YZ);/3 = 1,2,...N} and the test data 7- = {(X,Y);/ = 1,2,...o}, 
where both are assumed to be generated by the model: Y = t(X) + e , e 
A/'(0, rr 2) . The data 7) and 7- are independent, and inputs are drawn from an 
unknown distribution. Assume that a committee has K members. Denote the 
available input features as X = [Xl,X2,... ,Xm], the feature sets for the i th and 
jth members as Xi = [Xil,Xi2,...,Xm] and Xj = [Xjl,Xj2,...,Xmj] respectively, 
where Xi  X, Xj  X and Xi  Xj, and the mapping function of the i th and jta 
member models trained on data from 7) as fi(Xi) and fj(Xj). Define the model 
t,=t t,_fi(X,) for a11/-1,2,3, . oandi=l,2, K. 
efTof'e i , .. , ..., 
Constructing Heterogeneous Committees for Economic Forecasting 923 
The MSE of a committee is 
and the average MSE made by the committee members acting individually is 
K 
Eave = (2) 
i----1 
where �[.] denotes the expectation over all test data T. Using Jensen's inequality, 
we get Ec _ Eave, which indicates that the performance of a committee is always 
equal to or better than the average performance of its members. 
We define the average model error correlation as C = 1 K [e i ej ] , and 
K(K--I) Eij �1 I I 
then have 
Ec = Eave + C = ( + q)Eave , (3) 
c We consider the following four cases of q: 
where q = Eave . 
Case 1:  < q < 0. In this case, the model errors between members 
K--1 -- 
are anti-correlated, which might be achieved through decorrelation training. 
Case 2: q = 0. In this case, the model errors between members are 
__ I Eave. That is to say, a committee can 
uncorrelated, and we have: Ec -  
do much better than the average performance of its members. 
Case 3:0 < q < 1. If Eave is bounded above, when the committee size 
K > o, we have Ec -- qEave � This gives the asymptotic limit of a 
committee's performance. As the size of a committee goes to infinity, the 
committee error is equal to the average model error correlation C. The 
difference between Ec and Eave is determined by the ratio q. 
Case 4: q = 1. In this case, Ec is equal to Eave. This happens only when 
�i -- e j, for all i,j = 1,..., K. It is obvious that there is no advantage to 
combining a set of models that act identically. 
It is clear from the analyses above that a committee shows its advantage when 
the ratio q is less than one. The smaller the ratio q is, the better the committee 
performs compared to the average performance of its members. For the commit- 
tee to achieve substantial improvement over a single model, committee members 
not only should have small errors individually, but also should have small residual 
correlations between each other. 
3 Input Feature Grouping 
One way to construct a feature subset for a committee member is by randomly 
picking a certain number of features from the original feature set. The advantage 
of this method is that it is simple. However, we have no control on each member's 
performance or on the residual correlation between members by randomly selecting 
subsets. 
924 Y. Liao andJ. Moody 
Instead of randomly picking a subset of features for a member, we propose an input 
feature grouping method for forming committee member feature sets. The input 
grouping method first groups features based on a relevance measure in a way such 
that features between different groups are less related to one another and features 
within a group are more related to one another. 
After grouping, there are two ways to form member feature sets. One method 
is to construct the feature set for each member by selecting a feature from' each 
group. Forming a member's feature set in this way, each member will have enough 
information to make decision, and its feature set has less redundancy. This is the 
method we use in this paper. 
Another way is to use each group as the feature set for a committee member. In this 
method each member will only have partial information. This is likely to hurt in- 
dividual member's performance. However, because the input features for different 
members are less dependent, these members tend to make decisions more inde- 
pendently. There is always a trade-off between increasing members' independence 
and hurting individual members' performance. If there is no redundancy among 
input feature representations, removing several features may hurt individual mem- 
bers' performance badly, and the overall committee performance will be hurt even 
though members make decisions independently. This method is currently under 
investigation. 
The mutual information I(xi; xj) between two input variables xi and xj is used as 
the relevance measure to group inputs. The mutual information I(x; xj), which is 
defined in equation 4, measures the dependence between the two random variables. 
I(xi;xj) = H(xi) - H(xilxj) -  p(xi,yi)log 
p(xi,xj) 
p(xi)p(xj) 
(4) 
If features xi and xj are highly dependent, 
