Learning a Hierarchical Belief Network of 
Independent Factor Analyzers 
H. Attias* 
hagai@gatsby. ucl.ac.uk 
Sloan Center for Theoretical Neurobiology, Box 0444 
University of California at San Francisco 
San Francisco, CA 94143-0444 
Abstract 
Many belief networks have been proposed that are composed of 
binary units. However, for tasks such as object and speech recog- 
nition which produce real-valued data, binary network models are 
usually inadequate. Independent component analysis (ICA) learns 
a model from real data, but the descriptive power of this model 
is severly limited. We begin by describing the independent factor 
analysis (IFA) technique, which overcomes some of the limitations 
of ICA. We then create a multilayer network by cascading single- 
layer IFA models. At each level, the IFA network extracts real- 
valued latent variables that are non-linear functions of the input 
data with a highly adaptive functional form, resulting in a hier- 
archical distributed representation of these data. Whereas exact 
maximum-likelihood learning of the network is intractable, we de- 
rive an algorithm that maximizes a lower bound on the likelihood, 
based on a variational approach. 
1 Introduction 
An intriguing hypothesis for how the brain represents incoming sensory informa- 
tion holds that it constructs a hierarchical probabilistic model of the observed data. 
The model parameters are learned in an unsupervised manner by maximizing the 
likelihood that these data are generated by the model. A multilayer belief net- 
work is a realization of such a model. Many belief networks have been proposed 
that are composed of binary units. The hidden units in such networks represent 
latent variables that explain different features of the data, and whose relation to the 
*Current address: Gatsby Computational Neuroscience Unit, University College Lon- 
don, 17 Queen Square, London WCiN 3AR, U.K. 
362 H. Attias 
data is highly non-linear. However, for tasks such as object and speech recognition 
which produce real-valued data, the models provided by binary networks are often 
inadequate. Independent component analysis (ICA) learns a generarive model from 
real data, and extracts real-valued latent variables that are mutually statistically 
independent. Unfortunately, this model is restricted to a single layer and the latent 
variables are simple linear functions of the data; hence, underlying degrees of free- 
dom that are non-linear cannot be extracted by ICA. In addition, the requirement 
of equal numbers of hidden and observed variables and the assumption of noiseless 
data render the ICA model inappropriate. 
This paper begins by introducing the independent factor analysis (IFA) technique. 
IFA is an extension of ICA, that allows different numbers of latent and observed 
variables and can handle noisy data. The paper proceeds to create a multilayer 
network by cascading single-layer IFA models. The resulting generarive model pro- 
duces a hierarchical distributed representation of the input data, where the latent 
variables extracted at each level are non-linear functions of the data with a highly 
adaptive functional form. Whereas exact maximum-likelihood (ML) learning in 
this network is intractable due to the difficulty in computing the posterior density 
over the hidden layers, we present an algorithm that maximizes a lower bound on 
the likelihood. This algorithm is based on a general variational approach that we 
develop for the IFA network. 
2 
Independent Component and Independent Factor 
Analysis 
Although the concept of ICA originated in the field of signal processing, it is actually 
a density estimation problem. Given an L' x 1 observed data vector y, the task is 
to explain it in terms of an L x 1 vector x of unobserved 'sources' that are mutually 
statistically independent. The relation between the two is assumed linear, 
y = Hx + u, (1) 
where H is the 'mixing' matrix; the noise vector u is usually assumed zero-mean 
Gaussian with a covariance matrix A. In the context of blind source separation 
[1]-[4], the source signals x should be recovered from the mixed noisy signals y with 
no knowledge of H, A, or the source densities p(xi), hence the term 'blind'. In the 
density estimation approach, one regards (1) as a probabilistic generative model for 
the observed p(y), with the mixing matrix, noise covariance, and source densities 
serving as model parameters. In principle, these parameters should be learned by 
ML, followed by inferring the sources via a MAP estimator. 
For Gaussian sources, (1) is the factor analysis model, for which an EM algorithm 
exists and the MAP estimator is linear. The problem becomes interesting and more 
difficult for non-Gaussian sources. Most ICA algorithms focus on square (L' = L), 
noiseless (y = Hx) mixing, and fix p(xi) using prior knowledge (but see [5] for the 
case of noisy mixing with a fixed Laplacian source prior). Learning H occurs via 
gradient-ascent maximization of the likelihood [1]-[4]. Source density parameters 
can also be adapted in this way [3] ,[4], but the resulting gradient-ascent learning is 
rather slow. This state of affairs presented a problem to ICA algorithms, since the 
ability to learn arbitrary source densities that are not known in advance is crucial: 
using an inaccurate p(xi) often leads to a bad H estimate and failed separation. 
This problem was recently solved by introducing the IFA technique [6]. IFA 
employs a semi-parametric model of the source densities, which allows learning 
them (as well as the mixing matrix) using expectation-maximization (EM). Specif- 
ically, p(xi) is described as a mixture of Gaussians (MOG), where the mixture 
Hierarchical IF.4 Belief Networks 3 63 
components are labeled by s = 1, ...,hi and have means /i,8 and variances 7i,8: 
p(xi) = Y p(si = s)G(xi - lui,, 7i,). I The mixing proportions are parametrized 
using the softmax form: p(si = s) = exp(ai,)/Y8, exp(ai,,). Beyond noiseless 
ICA, an EM algorithm for the noisy case (1) with any L, L' was also derived in 
[6] using the MOG description. 2 This algorithm learns a probabilistic model 
P(Y I W) for the observed data, parametrized by W = (H, A, {ai,, lui,, 7i,}). A 
graphical representation of this model is provided by Fig. 1, if we set n = 1 and 
o b},s 1 
yj = =vj,8=O. 
3 Hierarchical Independent Factor Analysis 
In the following we develop a multilayer generalization of IFA, by cascading dupli- 
cates of the generative model introduced in [6]. Each layer n = 1, ..., N is composed 
of two sublayers: a source sublayer which consists of the units x�, i = 1, ..., L, and 
an output sublayer which consists of yy, j = 1,..., L'. The two are linearly related 
via y = Hx  + u  as in (1); u  is a Gaussian noise vector with covariance A . 
The nth-layer source x? is described by a MOG density model with parameters a . 
/i,, and ')'i,s, in analogy to the IFA sources above. 
The important step is to determine how layer n depends on the previous layers. We 
choose to introduce a dependence of the ith source of layer n only on the ith output 
of layer n - 1. Notice that matching Ln = L'_ 1 is now required. This dependence 
is implemented by making the means and mixture proportions of the Gaussians 
which compose p(x?) dependent on y-l. Specifically, we make the replacements 
    -1 a? - a? + The resulting joint density for 
b  yn--1 
li, s ---> li, s q- bti,sY i and ,s ,s i,s i � 
layer n, conditioned on layer n - 1, is 
p(sn,x,y  l y W = 
, 
H P($i I n-1 n n n--1 X n 
 Yi )p(xi I$i,Yi )P(YI ), (2) 
i----1 
where W  are the parameters of layer n and 
b  y-X 
P($i $ I n-1 exp(a,s q- i,s i ) 
Yi )= 
, ,-1 , P(Y' I x') = (Y' - H'x', A') , 
E exp(as, + bi,s,i ) 
n n--1 n n n--1 n 
p(x? I 8i = 8, Yi ) = 6(X? -- i,s -- Pi,sYi , Ti,s) ' 
The full model joint density is given by the product of (2) over n = 1, ..., N (setting 
y0 = 0). A graphical representation of layer n of the hierarchical IFA network is 
given in Fig. 1. All units are hidden except y. 
To gain some insight into our network, we examine the relation between the nth- 
--1 
layer source x and the n - lth-layer output Yi . This relation is probabilistic 
and is determined by the conditional density p(x i I n-1 n n- )p(x i I 
n Yi ) : sP(Si [Yi 1 n 
8 n--1 
, Yi ). Notice from (2) that this is a MOG density. Its y-X-dependent mean is 
given by 
X? = i , i ) : p(8 i : 8 I Yi ) (lin, s q- bti,sYi , 
8 
(3) 
Throughout this paper, 6(x, ) =l 27r12 I -/2 exp(--xT1]-x/2). 
2However, for many sources the E-step becomes intractable, since the number 1-li ni 
of source state configurations s = (s, ..., s) depends exponentially on L. Such cases are 
treated in [6] using a variational approximation. 
364 H. Attias 
Figure 1: Layer n of the hierarchical ICA generative model. 
and is a non-linear function of y- due to the softmax form of P($i I n-1 
n 
By adjusting the parameters, the function f can assume a very wide range of 
forms: suppose that for state s? a? and b . are set so that p(s? = s I Y-) is 
significant only in a small, continuous range of y- values, with different ranges 
associated with different s's. In this range, f will be dominated by the linear 
n n n--1 n 
term i,s + i,si . Hence, a desired fi can be produced by placing oriented 
line segments at appropriate points above the --axis, then smoothly join them 
together by the p(s� 
i ). Using the algorithm below, the optimal form of fi 
will be learned from the data. Therefore, our model describes the data  as a 
potentially highly complex function of the top layer sources, produced by repeated 
application of linear mixing followed by a non-linearity, with noise allowed at each 
stage. 
4 
Learning and Inference by Variational EM 
The need for summing over an exponentially large number of source state config- 
urations (s, ..., s.), and integrating over the so
