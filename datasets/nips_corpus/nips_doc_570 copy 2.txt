Illumination and View Position in 3D Visual 
Recognition 
Amnon Shashua 
M.I.T. Artificial Intelligence Lab., NE43-737 
and Department of Brain and Cognitive Science 
Cambridge, MA 02139 
Abstract 
It is shown that both changes in viewing position and illumination con- 
ditions can be compensated for, prior to recognition, using combinations 
of images taken from different viewing positions and different illumina- 
tion conditions. It is also shown that, in agreement with psychophysical 
findings, the computation requires at least a sign-bit image as input 
contours alone are not sufficient. 
I Introduction 
The task of visual recognition is natural and effortless for biological systems, yet 
the problem of recognition has been proven to be very difficult to analyze from 
a computational point of view. The fundamental reason is that novel images of 
familiar objects are often not sufficiently similar to previously seen images of that 
object. Assuming a rigid and isolated object in the scene, there are two major 
sources for this variability: geometric and photometric. The geometric source of 
variability comes from changes of view position. A 3D object can be viewed from a 
variety of directions, each resulting with a different 2D projection. The difference is 
significant, even for modest changes in viewing positions, and can be demonstrated 
by superimposing those projections (see Fig. 4, first row second image). Much 
attention has been given to this problem in the visual recognition literature ([9], 
and references therein), and recent results show that one can compensate for changes 
in viewing position by generating novel views from a small number of model views 
of the object [10, 4, 8]. 
404 
Illumination and View Position in 3D Visual Recognition 405 
Figure 1: A 'Mooney' image. See text for details. 
The photometric source of variability comes from changing illumination conditions 
(positions and distribution of light sources in the scene). This has the effect of 
changing the brightness distribution in the image, and the location of shadows 
and specular reflections. The traditional approach to this problem is based on the 
notion of edge detection. The idea is that discontinuities in image brightness remain 
stable under changes of illumination conditions. This invariance is not complete 
and furthermore it is an open question whether this kind of contour information is 
sufficient, or even relevant, for purposes of visual recognition. 
Consider the image in Fig. 1, adopted from Mooney's Closure Faces Test [6]. Most 
observers show no difficulty in interpreting the shape of the object from the right- 
hand image, but cannot identify the object when presented with only the contours. 
Also, many of the contours are shadow contours and therefore critically rely on the 
direction of light source. In Fig. 2 four frontal images of a doll from four different 
illumination conditions are shown together with their intensity step edges. The 
change in the contour image is significant and is not limited to shadow contours 
-- some object edges appear or disappear as a result of the change in brightness 
distribution. Also shown in Fig. 4 is a sign-bit image of the intensity image followed 
by a convolution with a Difference of Gaussians. As with the Mooney image, it is 
considerably more difficult to interpret the image of a complex object with only the 
zero-crossing (or level-crossing) contours than when the sign-bits are added. 
It seems, therefore, that a successful recognition scheme should be able to cope 
with changes in illumination conditions, as well as changes in viewing positions, by 
working with a richer source of information than just contours (for a different point 
of view, see [1]). The minimal information that seems to be sufficient, at least for 
coping with the photometric problem, is the sign-bit image. 
The approach to visual recognition in this study is in line with the 'alignment' 
approach [9] and is also inspired by the work of Ullman and Basri [10] who show that 
the geometric source of variability can be handled by matching the novel projection 
to a linear combination of a small number of previously seen projections of that 
object. A recognition scheme that can handle both the geometric and photometric 
sources of variability is suggested by introducing three new results: (i) any image of a 
surface with a linear reflectance function (including Lambertian and Phong's model 
without point specularities) can be expressed as a linear combination of a fixed 
set of three images of that surface taken under different illumination conditions, 
(ii) from a computational standpoint, the coefficients are better recovered using the 
406 Shashua 
sign-bit image rather than the contour image, and (iii) one can compensate for both 
changes in viewing position and illumination conditions by using combinations of 
images taken from different viewing positions and different illumination conditions. 
2 Linear Combination of Images 
We start by assuming that view position is fixed and the only parameter that is 
allowed to change is the positions and distribution of light sources. The more general 
result that includes changes in viewing positions will be discussed in section 4. 
Proposition 1 All possible images of a surface, with a linear reflectance function, 
generated by all possible illumination conditions (positions and distribution of light 
sources) are spanned by a linear combination of images of the surface taken from 
independent illumination conditions. 
Proof.' Follows directly from the general result that if fj(x), x  R k, j = 1, ..., k, 
are k linear functions, which are also linearly independent, then for any linear 
function f(x), we have that f(x) = 5.j ajfj(x), for some constants aj.  
The simplest case for which this result holds is the Lambertian reflectance model 
under a point light source (observed independently by Yael Moses, personal com- 
munication). Let r be an object point projecting to p. Let nr represent the normal 
and albedo at r (direction and magnitude), and s represent the light source and 
its intensity. The brightness at p under the Lambertian model is I(p) = n,..s, 
and because s is fixed for all point p, we have I(p) = alii(p) q- a212(p) + aala(p) 
where Ij(p) is the brightness under light source sj and where Sl, s2, sa are linearly 
independent. This result generalizes, in a straightforward manner, to the case of 
multiple light sources as well. 
The Lambertian model is suitable for matte surfaces, i.e. surfaces that diffusely 
reflect incoming light rays. One can add a 'shininess' component to account for 
the fact that for non-ideal Lambertian surfaces, more light is reflected in a direc- 
tion making an equal angle of incidence with reflectance. In Phong's model of 
reflectance [7] this takes the form of (n. h) c where h is the bisector of s and 
the viewer's direction v. The power constant c controls the degree of sharpness of 
the point specularity, therefore outside that region one can use a linear version of 
Phong's model by replacing the power constant with a multiplicative constant, to 
get the following function: I(p) = n. Is + p(v + s)]. As before, the bracketed vector 
is fixed for all image points and therefore the linear combination result holds. 
The linear combination result suggests therefore that changes in illumination can 
be compensated for, prior to recognition, by selecting three points (that are visible 
to s, Sl, 82,83) to solve for al, a2, aa and then match the novel image I with I' = 
'.j aj Ij. The two images should match along all points p whose object points r are 
visible to si, s2, sa (even if n � s < 0, i.e. p is attached-shadowed); approximately 
match along points for which n,..sj < 0, for some j (Ij(p) is truncated to zero, 
geometrically s is projected onto the subspace spanned by the remaining basis light 
sources) and not match along points that are cast-shadowed in I (nr � s > 0 but 
r is not visible to s because of self occlusion). Coping with cast-shadows is an 
important task, but is not in the scope of this paper. 
Illumination and View Position in 3D Visual Recognition 407 
.:::i::::('.'Ss.': :i:: 
� '- ::-'::'.�. '5R: ::::''- 
.::-..:%.%.%%....'.. ...'..:.... 
::', ::...-..N.x;,,'?,:%:::.'-. ::::::: -.--, 
!::.','�':'-i..!k:i:':';i.,..' ,'.':!:!: :i>.: 
:ii}<,% .,...,..x. 
q::. -:.:,::' ,:-'. 
-i 
_.. I.=:m=. ,... 
Figure 2: Linear combination of model images taken from the same viewing position 
and under different illumination conditions. Row I,�: Three model images taken under 
a varying point light source, and the input image, and their brightness edges. Row 3: 
The image generated by the linear combination of the model images, its edges, and the 
difference edge image between the input and generated image. 
The linear combination result also implies that, for the purposes of recognition, one 
does not need to recover shape or light source direction in order to compensate for 
changes in brightness distribution and attached shadows. Experimental results, on 
a non-ideal Lambertian surface, are shown in Fig. 2. 
Coefficients from Contours and Sign-bits 
Mooney pictures, such as in Fig. 1, demonstrate that humans can cope well with 
situations of varying illumination by using only limited information from the input 
image, namely the sign-bits, yet are not able to do so from contours alone. This 
observation can be predicted from a computational standpoint, as shown below. 
Proposition 2 The coefficients that span an ,mage I by the basis of three other 
images, as described in proposition 1, can be solved, up to a common scale factor, 
408 Shashua 
Figure 3: Compensating for both changes in view and illumination. Row I: Three model 
images, one of which is taken from it different viewing direction (23 � apart), and the input 
image from it novel viewing direction (in 
