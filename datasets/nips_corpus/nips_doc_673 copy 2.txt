An 
Metamorphosis Networks: 
Alternative to Constructive Methods 
Brian V. Bonnlander Michael C. Mozer 
Department of Computer Science & 
Institute of Cognitive Science 
University of Colorado 
Boulder, CO 80309-0430 
Abstract 
Given a set of training examples, determining the appropriate num- 
ber of free parameters is a challenging problem. Constructive 
learning algorithms attempt to solve this problem automatically by 
adding hidden units, and therefore free parameters, during learn- 
ing. We explore an alternative class of algorithms--called meta- 
morphosis algorithms--in which the number of units is fixed, but 
the number of free parameters gradually increases during learning. 
The architecture we investigate is composed of RBF units on a lat- 
tice, which imposes flexible constraints on the parameters of the 
network. Virtues of this approach include variable subset selec- 
tion, robust parameter selection, multiresolution processing, and 
interpolation of sparse training data. 
1 INTRODUCTION 
Generalization performance on a fixed-size training set is closely related to the 
number of free parameters in a network. Selecting either too many or too few 
parameters can lead to poor generalization. Geman et al. (1991) refer to this 
problem as the bias/variance dilemma: introducing too many free parameters incurs 
high variance in the set of possible solutions, and restricting the network to too few 
free parameters incurs high bias in the set of possible solutions. 
Constructive learning algorithms (e.g., Fahlman & Lebiere, 1990; Platt, 1991) have 
131 
132 Bonnlander and Mozer 
output 
layer 
RBF trait 
layer 
illput 
layer 
Figure 1: Architecture of an RBF network. 
been proposed as a way of automatically selecting the number of free parameters in 
the network during learning. In these approaches, the learning algorithm gradually 
increases the number of free parameters by adding hidden units to the network. 
The algorithm stops adding hidden units when some validation criterion indicates 
that network performance is good enough. 
We explore an alternative class of algorithms--called metamorphosis algorithms-- 
for which the number of units is fixed, but heavy initial constraints are placed on 
the unit response properties. During learning, the constraints are gradually relaxed, 
increasing the flexibility of the network. Within this general framework, we devel- 
op a learning algorithm that builds the virtues of recursire partitioning strategies 
(Breiman et al., 1984; Friedman, 1991) into a Radial Basis Function (RBF) net- 
work architecture. We argue that this framework offers two primary advantages 
over constructive RBF networks: for problems with low input variable interaction, 
it can find solutions with far fewer free parameters, and it is less susceptible to noise 
in the training data. Other virtues include multiresolution processing and built-in 
interpolation of sparse training data. 
Section 2 introduces notation for RBF networks and reviews the advantages of 
using these networks in constructive learning. Section 3 describes the idea behind 
metamorphosis algorithms and how they can be combined with RBF networks. 
Section 4 describes the advantages of this class of algorithm. The final section 
suggests directions for further research. 
2 RBF NETWORKS 
RBF networks have been used successfully for learning difficult input-output map- 
pings such as phoneme recognition (Wettschereck & Dietterich, 1991), digit classi- 
fication (Nowlan, 1990), and time series prediction (Moody & Darken, 1989; Platt, 
1991). The basic architecture is shown in Figure 1. The response properties of each 
RBF unit are determined by a set of parameter values, which we'll call a pset. The 
pset for unit i, denoted ri, includes: the center location of the RBF unit in the 
and the strength of the connection(s) 
input space, t; the width of the unit, ai, 
from the RBF unit to the output unit(s), hi'. 
One reason why RBF networks work well with constructive algorithms is because 
Metamorphosis Networks: An Alternative to Constructive Methods 133 
the hidden units have the property of noninterference: the nature of their activation 
functions, typically Gaussian, allows new RBF units to be added without changing 
the global input-output mapping already learned by the network. 
However, the advantages of constructive learning with RBF networks diminish for 
problems with high-dimensional input spaces (Hartman & KeeleL 1991). For these 
problems, a large number of RBF units are needed to cover the input space, even 
when the number of input dimensions relevant for the problem is small. The rele- 
vant input dimensions can be different for different parts of the input space, which 
limits the usefulness of a global estimation of input dimension relevance, as in Pog- 
gio and Girosi (1990). Metamorphosis algorithms, on the other hand, allow RBF 
networks to solve problems such as these without introducing a large number of free 
parameters. 
3 METAMORPHOSIS ALGORITHMS 
Metamorphosis networks contrast with constructive learning algorithms in that the 
number of units in the network remains fixed, but degrees of freedom are gradually 
added during learning. While metamorphosis networks have not been explored in 
the context of supervised learning, there is at least one instance of a metamorphosis 
network in unsupervised learning: a Kohonen net. Units in a Kohonen net are 
arranged on a lattice; updating the weights of a unit causes weight updates of the 
unit's neighbors. Units nearby on the lattice are thereby forced to have similar 
responses, reducing the effective number of free parameters in the network. In one 
variant of Kohonen net learning, the neighborhood of each unit gradually shrinks, 
increasing the degrees of freedom in the network. 
3.1 MRBF NETWORKS 
We have applied the concept of metamorphosis algorithms to ordinary RBF net- 
works in supervised learning, yielding MRB? networks. Units are arranged on an 
n-dimensional lattice, where n is picked ahead of time and is unrelated to the dimen- 
sionality of the input space. The response of RBF unit i is constrained by deriving 
its pset, ri, from a collection of underlying psets, each denoted uj, that also reside 
on the lattice. The elements of uj correspond to those of ra: u = (/a3 , r?, h?). 
Due to the orderly arrangement of the uj, the lattice is divided into nonoverlap- 
ping hyperrectangular regions that are bounded by 2 u. Consequently, each ri is 
enclosed by 2 uj. The pset rl can then be derived by near interpolation of the 
enclosing underlying psets u, as shown in Figure 2 for a one-dimensional lattice. 
Learning in MRBF networks proceeds by minimizing an error function E in the u 
components via gradient descent: 
i6NEIGHi 
where NEIGHj is the set of RBF units whose values are affected by underlying pset 
j, and k indexes the input units of the network. The update expression is similar 
for r? and h. To better condition the search space, instead of optimizing the 
134 Bonnlander and Mozer 
(a) 
rl 
(b) 
r r2 r r4 : Input / 
U U 2 / 
Figure 2: Constrained RBF units. (a) Four RBF units with psets r-r4 are arranged 
on a one-dimensional lattice, enclosed by underlying psets ux and u2. (b) An input 
space representation of the constrained RBF units. RBF center locations, widths, 
and heights are linearly interpolated. 
r' directly, we follow Nowlan and Hinton's (1991) suggestion of computing each 
RBF unit width according to the transformation tr]' - ezp(Ti/2) and searching for 
the optimum value of 7i. This forces RBF widths to remain positive and makes it 
difficult for a width to approach zero. 
When a local optimum is reached, either learning is stopped or additional underlying 
psets are placed on the lattice in a process called metamorphosis. 
3.2 METAMORPHOSIS 
Metamorphosis is the process that gradually adds new degrees of freedom to the 
network during learning. For the MRBF network explored in this paper, introduc- 
ing new free parameters corresponds to placing additional underlying psets on the 
lattice. The new psets split one hyperrectangular region--an n-dimensional sub- 
lattice bounded by 2 underlying psets--into two nonoverlapping hyperrectangular 
regions. To achieve this, 2 - additional underlying psets, which we call the split 
group, are required (Figure 3). The splitting process implements a recursire par- 
titioning strategy similar to the strategies employed in the CART (Breiman et al., 
1984) and MARS (Friedman, 1991) statistical learning algorithms. 
Many possible rules for region splitting exist. In the simulations presented later, 
we consider every possible region and every possible split of the region into two 
subregions. For each split group k, we compute the tension of the split, defined as 
j6split group 
We then select the split group that has the greatest tension. This heuristic is based 
on the assumption that the error gradient at the point in weight space where a split 
would take place reflects the long-term benefit of that split. 
It may appear that this splitting process is computationally expensive, but it can be 
implemented quite efficiently; the cost of computing all possible splits and choosing 
Metamorphosis Networks: An Alternative to Constructive Methods 135 
Key 
ï¿½ RBF unit pst 
drivaliw of RBF pst 
O split group pset 
..,, drivalive of split group pst 
0 Una=yas pt 
::::::::: Lattic lion boundary 
Figure 3: Computing the tension of a split group. Arrows are meant to represent 
derivatives of corresponding pset components. 
the best one is hnear in the number of RBF units on the lattice. 
4 VIRTUES OF METAMORPHOSIS NETS 
4.1 VARIABLE SUBSET SELECTION 
One advantage of MRBF networks is that they can perform variable subset selection; 
that is, they can select a subset of input dimensions more relevant to the problem 
and ignore the other input dimensions. This is als
