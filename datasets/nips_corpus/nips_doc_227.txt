828 Cowan 
Neural networks: the early days 
J.D. Cowan 
Department of Mathematics, Committee on 
Neurobiology, and Brain Research Institute, 
The University of Chicago, 5734 S. Univ. Ave., 
Chicago, Illinois 60637 
ABSTRACT 
A short account is given of various investigations of neural network 
properties, beginning with the classic work of McCulloch & Pitts. 
Early work on neurodynamics and statistical mechanics, analogies with 
magnetic materials, fault tolerance via parallel distributed processing, 
memory, learning, and pattern recognition, is described. 
1 INTRODUCTION 
In this brief account of the early days in neural network research, it is not possible to be 
comprehensive. This article then is a somewhat subjective survey of some, but not all, of 
the developments in the theory of neural networks in the twent-five year period, from 
1943 to 1968, when many of the ideas and concepts were formulated, which define the 
field of neural network research. This comprises work on connections with automata 
theory and computability; neurodynamics, both deterministic and statistical; analogies 
with magnetic materials and spin systems; reliability via parallel and parallel distributed 
processing; modifiable synapses and conditioning; associative memory; and supervised 
and unsupervised learning. 
2 McCULLOCH-PITTS NETWORKS 
The modem era may be said to have begun with the work of McCulloch and Pitts (1943). 
This is too well-known to need commenting on. Let me just make some historical re- 
marks. McCulloch, who was by training a psychiatrist and neuroanatomist, spent some 
twenty years thinking about the representation of event in the nervous system. From 1941 
to 1951 he worked in Chicago. Chicago at that time was one of the centers of neural of 
Neural Networks: The Early Days 829 
Figurel: Warren McCulloch circa 1962 
network research, mainly through the work of the Rashevsky group in the Committee on 
Mathematical Biology at the University of Chicago. Rashevsky, Landahl, Rapaport and 
Shimbel, among others, carried out many early investigations of the dynamics of neural 
networks, using a mixture of calculus and algebra. In 1942 McCulloch was introduced to 
Walter Pitts, then a 17 year old student of Rashevsky's. Pitts was a mathematical prodigy 
who had joined the Committee sometime in 1941. There is an (apocryphal) story that 
Pitts was led to the Rashevsky group after a chance meeting with the philosopher 
Bertrand Russell, at that time a visitor to the University of Chicago. In any event Pitts 
was already working on algebraic aspects of neural networks, and it did not take him long 
to see the point behind McCulloch's quest for the embodiment of mind. In one of 
McCulloch later essays (McCulloch 1961) he describes the history of his efforts thus: 
My object, as a psychologist, was to invent a least psychic event, or 
psychon, that would have the following properties: First, it was to be 
so simple an event that it either happened or else it did not happen. 
Second, it was to happen only if its bound cause had happened-shades 
of Duns Scotus!-that is, it was to imply its temporal antecedent. 
Third it was to propose this to subsequent psychons. Fourth, these 
were to be compounded to produce the equivalents of more 
complicated propositions concerning their antecedents...In 1921 it 
dawned on me that these events might be regarded as the all-or- 
nothing impulses of neurons, combined by convergence upon the next 
neuron to yield complexes of propositional events. 
Their subsequent 1943 paper was remarkable in many respects. It is best appreciated 
within the zeitgeist of the era when it was written. As Papert has documented in his 
introduction to a collection of McCulloch's papers (McCulloch 1967), 1943 was a semi- 
830 Cowan 
nal year for the development of the science of the mind. Craik's monograph The Nature 
of Explanation and the paper Behavior, Purpose and Teleology, by Rosenbleuth, 
Wiener and Bigelow, were also published in 1943. As Papert noted, The common 
feature [of these publications] is their recognition that the laws governing the 
embodiment of mind should be sought among the laws governing information rather than 
energy or matter. The paper by McCulloch and Pitts certainly lies within this 
framework. 
Figure 2: Walter Pitts circa 1952 
McCulloch-Pitts networks (hence-forth referred to as MP networks), are finite state 
automata embodying the logic of propositions, with quantifiers, as McCulloch wished; 
and permit the framing of sharp hypotheses about the nature of brain mechanisms, in a 
form equivalent to computer programs. This was a remarkable achievement. It 
established once and for all, the validity of making formal models of brain mechanisms, 
if not their veridicality. It also established the possibility of a rigorous theory of mind, in 
that neural networks with feedback loops can exhibit purposive behavior, or as 
McCulloch and Pitts put it: 
both the formal and the final aspects of that activity which we are 
wont to call mental are rigorously deducible from present 
neurophysiology...[and] that in [imaginable networks]...Mind no 
longer goes more ghostly than a ghost. 
2.1 FAULT TOLERANCE 
MP networks were the first designed to perform specific logical tasks; and of course logic 
can be mapped into arithmetic. Landahl, McCulloch and Pitts (1943), for example, 
noted that the arithmetical operations +, 1-, and x can be obtained in MP networks via the 
logical operations OR, NOT, and AND. Thus the arithmetical expression a-a.b = a.(1-b) 
Neural Networks: The Early Days 831 
corresponds to the logical expression a AND NOT b, and more generally, all (finite) 
arithmetical calculations can be implemented in an MP network. But what happens if 
such a network malfunctions from time to time, or is damaged? It was this problem that 
attracted von Neumann. In 1951-2, following conversations with McCulloch, and with 
Bruckner and Gell-Mann, von Neumann took up the problem of designing MP networks 
to function reliably despite malfunctions and failures of their component neurons, or 
misconnected wires, or damage to a portion of the network (von Neumann 1956). 
Von Neumann solved the reliability problem in two differing ways. His first solution was 
to make use of the error correcting properties of majority logic elements. Such an 
element executes the logical function m(a,b,c) = (a AND b) OR (b AND c) OR (c AND 
a). The proceedure is to triplicate each logical function to be executed, i.e.; execute 
each logical function three times in parallel, and then feed the outputs through majority 
logic elements. Let  be the probability of a majority element malfunctioning, and let l 
be the probability of error on any of its input lines. The general result is that provided  
< 0.007, an output error l* equal to about 4 can be achieved. However the 
redundancy is high. Let t be the logical depth of the function to be computed, i.e.; the 
longest serial chain of MP units in the original network. Then about 3 MP neurons are 
required to achieve outputs whose probability of error is about four times the probability 
of error of the MP units. If x = 3 the requisite redundancy is 9:1, but if x = 6 it is about 
700:1, and if gt = 10 it is about 60, 000:1. Von Neumann's second solution to the 
reliability problem was to multiplex, i.e.; use N MP circuits to do the job of one. In such 
networks one bit of information (the choice between 1 and 0) is signaled not by the 
activation of one MP neuron, but instead by the synchronous activation of many MP 
neurons. Let A be a number between 0 and 1. Then 1 is signaled if , the fraction of 
activated MP neurons involved in any job, exceeds A; otherwise 0 is signalled. 
Evidently a multiplexed MP network will function reliably only if  is close to either 0 
or 1. Von Neumann achieved this with networks made up entirely of NAND logic 
elements. Let  now be the probability of such an element malfunctioning. Von 
Neumann then proved that with A = 0.07 and  < 0.0107, l* can be made to decrease 
with increasing N. With g = 0.005, von Neumann showed that 'fl* - aN-1/210-bN, 
where a = 6.4 and b = 8.6'10 -4. It follows that 'fl* can be made less than g provided N > 
2, 000. This is achieved with a redundancy of 3N: 1 that is independent of the logical 
depth gt; thus for logical computations of large depth the method of multiplexing is 
superior to majority logic decoding. 
2.2 PARALLEL DISTRIBUTED PROCESSING 
This solution to the problem of reliable computing with unreliable elements is general 
since the NAND logic element is universal. Is it biologically plausible? The answer 
seems to be negative since real neurons have thousands of synaptic contacts, so that it is 
not necessary to concatenate many NAND elements in circuits of large logical depth to 
implement logical functions of many variables. This suggests that real neurons can 
832 Cowan 
implement logical functions of many variables with a probability of error g not much 
greater than that of the NAND element. This observation motivated Winograd and I 
(1963) to study the limiting case in which g is independent of the logical complexity of 
the function to be implemented. In such a case it is possible to use error-correcting 
codes just as is done in noisy communication channels (Shannon 1948, Hamming 1950) 
in which a message comprising K symbols is transmitted via a signal comprising N 
signals, N - K of which are used by the receiver for error detection and correction. In the 
computing case this implies that K computations be implemented by an MP-network 
comprising N/K times as many elements as would be required in the error-free case. 
The overall effect of such an encoding scheme is to distribute the logical functions to be 
implemented, over the entire MP network. Such a scheme works most efficiently with 
large K and N; in effect in a parallel distributed archi
