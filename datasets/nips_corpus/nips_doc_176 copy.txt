348 
Further Explorations in Visually-Guided 
Reaching: Making MURPHY Smarter 
Bartlett W. Mel 
Center [or Complex Systems Research 
Beckman Institute, University of Illinois 
405 North Matheus Street 
Urbana, IL 61801 
ABSTRACT 
MURPHY is a vision-based kinematic controller and path planner 
based on a connectionist architecture, and implemented with a video 
camera and Rhino XR-series robot arm. Imitative of the layout of sen- 
sory and motor maps in cerebral cortex, MURPHY'S internal representa- 
tions consist of four coarse-coded populations of simple units represent- 
ing both static and dynamic aspects of the sensory-motor environment. 
In previously reported work [4], MURPHY first learned a direct kinematic 
model of his camera-arm system during a period of extended practice, 
and then used this mental model to heuristically guide his hand to 
unobstructed visual targets. MURPHY has since been extended in two 
ways: First, he now learns the inverse differential-kinematics of his arm 
in addition to ordinary direct kinematics, which allows him to push his 
hand directly towards a visual target without the need for search. Sec- 
ondly, he now deals with the much more difficult problem of reaching in 
the presence of obstacles. 
INTRODUCTION 
Visual guidance of a multi-link arm through a cluttered workspace is known to be 
an extremely difficult computational problem. Classical approaches in the field of 
robotics have typically broken the problem into pieces of manageable size, including 
modules for direct and inverse kinematics and dynamics [7], along with a variety 
of highly complex algorithms for motion planning in the configuration space of a 
multi-link arm (e.g. [3]). Workers in the field of robotics have rarely (until recently) 
emphasized neural plausibility at the level of representation and algorithm, opting 
instead for explicit mathematical computations or complex, multi-stage algorithms 
using general-purpose data structures. More peculiarly, very little emphasis has 
been placed on full use of the visual channel for robot control, other than as a 
source of target shape or coordinates. 
Further Explorations in Visually-Guided Reaching 349 
Fror  
Figure 1: MURPHY's Connectionist Architecture. Four interconnected populations 
of neuron-like units implement a variety of sensory-motor mappings. 
Much has been learned of the neural substrate for vision-guided limb control 
in humans and non-human primates (see [2] for review), albeit at a level too far 
removed from concrete algorithmic specification to be of direct engineering utility. 
Nonetheless, a number of general principles of cortical organization have inspired 
the current approach to vision-based kinematic learning and motion-planning. MUR- 
PHY's connectionist architecture has been based on the observation that a surpris- 
ingly large fraction of the vertebrate brain is devoted to the explicit representation 
of the animal's sensory and motor state [6]. During normal behavior, each of these 
neural representations carries behaviorally-relevant state information, some yoked 
to the sensory-epithelia, others to th6 motor system. The effect is a rich set of 
online associative learning opportunities. Moreover, the visual modality is by far 
the dominant in the primate brain by measures of sheer real-estate, including a 
number of areas that are known to be concerned with the representation of limb 
control in immediate extrapersonal space [2], suggesting that visual processing may 
overshadow what has usually been perceived as primarily a motor process. 
MURPHY's ORGANIZATION 
In the interests of space, we pesent here a highly reduced description of MUIPHY'S 
organization; the reader is referred to [5] for a much more comprehensive treat- 
350 Me! 
ment, including a lengthy discussion of the relevance of MURPHY'S structure and 
function to the psychology, motor-physiology, and neural-basis for visually-guided 
limb control in primates. 
The Physical Setup 
MURPHY'S physical setup consists of a 512 x 512 JVC video camera pointed at a 
Rhino XR-3 robotic arm, whose wrist, elbow, and shoulder rotate freely in the 
image plane of the camera. White spots are stuck to the arm in convenient places; 
when the image is thresholded, only the white spots appear in the image (see fig. 
2). This arrangement allows continuous control over the complexity of the visual 
image of the arm, which in turn affects computation time during learning. The arm 
is software controllable, with a stepper motor for each joint. Arm dynamics are not 
dealt with in this work. 
The Connectionist Architecture 
MURPHY is currently based on four interconnected populations of neuron-like units 
(fig. 1), encoding both static and dynamic aspects of the sensory-motor environment 
(only two were used in a previous report [4]). Visual Populations. The principal 
sensory population is organized as a rectangular, visuotopically-mapped 64 x 64 
grid of coarsely-tuned visual units, each of which responds when a visual feature 
(such as a white spot on the arm) falls into its receptive field (fig 1, upper left). 
The second population of 24 units encodes the direction of MURPHY's hand motion 
through the visual field (fig. 1, lower left)--vector magnitude is ignored at present. 
These units are thus fired only by the distinct visual image of the hand, but are 
selective for the direction of hand motion through the visual field as MURPHY moves 
his arm in the workspace. Joint Populations. The third population of 273 units 
consists of three subpopulations encoding the static joint configuration; the angle 
of each joint is value-coded individually in a subpopulation dedicated to that joint, 
consisting of units with overlapping triangular receptive fields. (fig. 1, upper right). 
The fourth and final population of 24 units also consists of three subpopulations, 
each value-coding the velocity of one of the three joints (fig. 1, lower right). 
During both his learning and performance phases to be described in subsequent 
sections, MURPHY is also able to carry out simple sequential operations that are 
driven by a control structure external to his connectionist architecture. 
MURPHY's Kinematics 
For a detailed discussion of the relation between MURPHY'S novel style of kine- 
matic representation and those used in other approaches to robot control, see [5]. 
Briefly, in reference to the four unit populations described above, MURPHY'S pri- 
mary workhorse is his direct kinematic mapping that relates static joint angles to 
the associated visual image of the arm. This smooth nonlinear mapping comprises 
both the kinematics of the arm and the optical parameters and global geometry of 
Further Explorations in Visually-Guided Reaching 351 
the camera/imaging system, and is learned and represented as a synaptic projec- 
tion from the joint-angle to visual-field populations (fig. 1). Post-training, MURPHY 
can assume an arbitrary joint posture mentally, i.e. by setting up the appropri- 
ate pattern of activation on his joint-angle population without allowing the arm to 
move. The learned mapping will then synaptically activate a mental image of the 
arm, in that configuration, on the post-synaptic visual-field population. Contem- 
plated movements of the arm can thus be evaluated without overt action--this is 
the heart of MURPHY'S mental model. 
MURPHY is also able to learn the inverse differential-kinematics of his arm, a 
mapping which translates a desired direction of motion through the workspace into 
the requisite commands to the joints, allowing MURPHY to guide his hand along 
a desired trajectory through his field of view. This mapping is learned and repre- 
sented as a synaptic projection originating from both i) the hand-vector population, 
encoding the desired visual-field direction, and ii) the joint-angle population encod- 
ing the current state of the arm, and terminating on the joint-move population, 
which specifies the appropriate pertubation to the joints (fig. 1, see arrows labelled 
Inverse Jacobjan). In the next section, we describe how this learning takes place. 
HOW MURPHY LEARNS 
As described in [4,5], MURPHY learns by doing. Thus, during an initial training pe- 
riod for the direct kinematics, MURPHY steps his arm systematically through a small 
representative sample of the 3.3 billion legal arm configurations (visiting 17,000 
configs. in 5 hours). Each step constitutes a standard connectionist training exam- 
ple between his joint-angle and visual-field populations. A novel synaptic learning 
scheme called sigma-pi Iearning is used for weight modification [4,5], described else- 
where in great detail [5]. This scheme essentially treats each post-synaptic sigma-pi 
neuron (see [5]) as an interpolating lookup table of the kind discussed by Albus 
and others [1], rather than as a standard linear threshold unit. Sigma-pi learning 
has been inspired by the physical structure and membrane properties of biological 
neurons, and yields several advantages in performance and simplicity of imple- 
mentation for the learning of smooth low-dimensional functions [5]. As an imple- 
mentation note, once the sigma-pi units have been appropriately trained, they are 
reimplemented using k-d trees, a much more efficient data-structure for a sequential 
computer (giving a speedup on the order of 50-100). 
MURPHY'S inverse-differential mapping is learned analogously, where each move- 
ment of the arm (rather than each state) is used as a training example. Thus, as the 
hand sweeps through the visual field during either physical or mental practice, each 
of the three relevant populations are activated (hand-vector and joint-angle as in- 
puts, joint-move as output), acting again as a single input-output training example 
for the learning procedure. 
352 Mel 
Figure 2: Four Visual Representations. The first frame shows the unprocessed 
camera view of MIAB.PHY'S axm. White s
