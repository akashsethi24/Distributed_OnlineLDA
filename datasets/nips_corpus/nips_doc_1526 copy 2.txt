Barycentric Interpolators for Continuous 
Space & Time Reinforcement Learning 
Rmi Munos & Andrew Moore 
Robotics Institute, Carnegie Mellon University 
Pittsburgh, PA 15213, USA. 
E-mail:{munos, awm)@cs.cmu.edu 
Abstract 
In order to find the optimal control of continuous state-space and 
time reinforcement learning (RL) problems, we approximate the 
value function (VF) with a particular class of functions called the 
barycentric interpolators. We establish sufficient conditions under 
which a RL algorithm converges to the optimal VF, even when we 
use approximate models of the state dynamics and the reinforce- 
ment functions. 
I INTRODUCTION 
In order to approximate the value function (VF) of a continuous state-space and 
time reinforcement learning (RL) problem, we define a particular class of functions 
called the barycentric interpolator, that use some interpolation process based on 
finite sets of points. This class of functions, including continuous or discontinuous 
piecewise linear and multi-linear functions, provides us with a general method for 
designing RL algorithms that converge to the optimal value function. Indeed these 
functions permit us to discretize the HJB equation of the continuous control problem 
by a consistent (and thus convergent) approximation scheme, which is solved by 
using some model of the state dynamics and the reinforcement functions. 
Section 2 defines the barycentric interpolators. Section 3 describes the optimal con- 
trol problem in the deterministic continuous case. Section 4 states the convergence 
result for RL algorithms by giving sufficient conditions on the applied model. Sec- 
tion 5 gives some computational issues for this method, and Section 6 describes the 
approximation scheme used here and proves the convergence result. 
Barycentric Interpolators for Continuous Reinforcement Learning 1025 
2 DEFINITION OF BARYCENTRIC INTERPOLATORS 
Let E  = {i}i be a set of points distributed at some resolution 5 (see (4) below) 
on the state space of dimension d. 
For any state x inside some simplex (1,.-., n), we say that x is the barycenter of 
the {i)i:.., inside this simplex with positive coefficients p(x[i)of sum 1, called 
the barycentric coordinates, if x = Y'4=i..n p(x[i).i. 
Let VS(i) be the value of the function at the points i. V  is a barycentric 
interpolator if for any state x which is the barycenter of the points {i}i=..n for 
some simplex (1, ...,n), with the barycentric coordinates p(x]i), we have: 
VS(x) = y.i=l..,p(xli).VS(i) 
(1) 
Moreover we assume that the simplex (1, ..., n) is of diameter 0(5). Let us describe 
some simple barycentric interpolators: 
Piecewise linear functions defined by some triangulation on the state 
space (thus defining continuous functions), see figure 1.a, or defined at any 
x by a linear combination of (d + 1) values at any points (, ...,+)  x 
(such functions may be discontinuous at some boundaries), see figure 1.b. 
Piecewise multi-linear functions defined by a multi-linear combination 
of the 2  values at the vertices of d-dimensional rectangles, see figure 1.c. 
In this case as well, we can build continuous interpolations or allow discon- 
tinuities at the boundaries of the rectangles. 
An important point is that the convergence result stated in Section 4 does not 
require the continuity of the function. This permits us to build variable resolution 
triangulations (see figure 1.b) or grid (figure 1.c) easily. 
(a) 
(c) 
Figure 1: Some examples of barycentric approximators. These are piecewise con- 
tinuous (a) or discontinuous (b) linear or multi-linear (c) interpolators. 
Remark 1 In the general case, for a given x, the choice of a simplex (1, ..., n)  X 
is not unique (see the two sets of grey and black points in figure 1.b and 1.c), and 
once the simplex (1, ..., n)  x is defined, if n > d + 1 (for example in figure 1.c), 
then the choice of the barycentric coordinates p(x]i) is also not unique. 
Remark 2 Depending on the interpolation method we use, the time needed for com- 
puting the values will vary. Following [Dav96], the continuous multi-linear interpo- 
lation must process 2  values, whereas the linear continuous interpolation inside a 
simplex processes (d + 1) values in O(dlogd) time. 
1026 R. Munos and A. W. Moore 
In comparison to [Gor95], the functions used here are averagers that satisfy the 
barycentric interpolation property (1). This additional geometric constraint permits 
us to prove the consistency (see (15) below) of the approximation scheme and thus 
the convergence to the optimal value in the continuous time case. 
3 THE OPTIMAL CONTROL PROBLEM 
Let us describe the optimal control problem in the deterministic and discounted case 
for continuous state-space and time variables and define the value function that we 
intend to approximate. We consider a dynamical system whose state dynamics 
depends on the current state x(t)  0 (the state-space, with O an open subset of 
ra) and control u(t)  U (compact subset) by a differential equation: 
dx 
dt = re(t), u(t)) (2) 
From equation (2), the choice of an initial state x and a control function u(t) leads 
to a unique trajectories x(t) (see figure 2). Let r be the exit time from O (with 
the convention that if x(t) always stays in O, then r = c). Then, we define the 
functional J as the discounted cumulative reinforcement' 
j0 T 
J(x; u(.)) = 3`tr(x(t), .(t))dt + 3`T 
where r(x, u) is the running reinforcement and R(x) the boundary reinforcement. 
3' is the discount factor (0 _< 3` < 1). We assume that f, r and R are bounded and 
Lipschitzian, and that the boundary 00 is C 2. 
RL uses the method of Dynamic Programming (DP) that introduces the value 
function (VF) � the maximal value of J as a function of initial state x � 
V(x) =supJ(x;u(.)). 
From the DP principle, we deduce that V satisfies a first-order differential equation, 
called the Hamilton-Jacobi-Bellman (HJB) equation (see [FS93] for a survey) � 
Theorem 1 If V is differentiable at x  O, let DV(x) be the gradient of V at x, 
then the following HJB equation holds at x. 
H(V, DV, x) d=f V(x) ln3` + sup[DV(x).f(x, u)+ r(x, u)] = 0 
uU 
(3) 
The challenge of RL is to get a good approximation of the VF, because from V 
we can deduce the optimal control: for state x, the control u* (x) that realizes the 
supremum in the HJB equation provides an optimal (feed-back) control law. 
The following hypothesis is a sufficient condition for V to be continuous within O 
(see [Bar94]) and is required for proving the convergence result of the next section. 
Hyp 1: For x G 00, let n--+(x) be the outward normal of O at x, we assume that' 
-If 3u G U, s.t. f(x, u). n--+(x) <_ 0 then 3v G U, s.t. f(x, v) n--+(x) <0. 
-IfBu  v, > 0 then By  v, > o. 
which means that at the states (if there exist any) where some trajectory is tangent 
to the boundary, there exists, for some control, a trajectory strictly coming inside 
and one strictly leaving the state space. 
Barycentric Interpolators for Continuous Reinforcement Learning 1027 
Figure 2: The state space and the set 
of points E  (the black dots belong to 
the interior and the white ones to the 
boundary). The value at some point ( 
is updated, at step n, by the discounted 
value at point r/, E (, 2, a). The main 
requirement for convergence is that the 
points r/, approximate r/ in the sense : 
p(rnli) - p(rli) + 0(5) (i.e. the On 
belong to the grey area). 
4 THE CONVERGENCE RESULT 
Let us introduce the set of points E  = {i}, composed of the interior (E  N O) 
and the boundary ((9E  = E�\ O), such that its convex hull covers the state space 
O, and performing a discretization at some resolution 5 � 
VxEO, inf IIx-,11<6 and vxEoo inf 11-:116 (4) 
,EZnO EOZ 
Moreover, we approximate the control spce U by some finite control spaces U 5 C U 
such that for   5 , U 5' C U 5 and li0 U 5 = U. 
We would like to update the value of any' 
- interior point   E 5  O with the discounted values at state (, u) (figure 2)' 
sup 
uU  
for some state qn(, u), some time dely vn(, u) and some reinforcement rn(, u). 
- boundary point   OZ 6 with some terminal reinforcement Rn() � 
V2+l() * n(e) (6) 
The following theorem states that the values V2 computed by a RL algorithm using 
the model (because of some  priori partial uncertainty of the state dynamics and 
the reinforcement functions) %(, u), vn(, u), rn(, u) and Rn() converge to the 
optimal value function as the number of iterations n   nd the resolution 5  0. 
Let us define the state ,(, u) (see figure 2) � 
= + u).f((, 
for some time dely v(, u) (with kid  v(, u)  k25 for some constants k ) 0 nd 
k2 > 0), and let p(,[i) (resp. p(%[i)) be the barycentric coordinate of  inside a 
simplex containing it (resp. qn inside the same simplex). We will write q, q, v, r, 
..., instead of ,(�, u), q (, u), v(�, u), r(, u), ... when no confusion is possible. 
Theorem 2 Assume that the hypotheses of the previous sections hold, and that for 
any resolution 5, we use barycentric interpolators V  defined on state spaces E  
(satisfying (4)) such that all points of E  ffl 0 are regularly updated with rule (5) 
and all points of (9E  are updated with rule (6) at least once. Suppose that rl, , r,, 
rn and -Rn approximate r h r, r and _R in the sense � 
1028 R. Munos and A. W. Moore 
then we have lim- Vn s = V uniformly on any compact fl C 0 (i.e. Ve > 0,Vfl 
5-- 0 
compact C O, 3A, BN , such that �5 _ A,�n _ N, supzrm ]Vd- V] _ e). 
Remark 3 For a given value orS, the rule (5) is not a DP updating rule for some 
Markov Decision Problem (MDP) since the values rn, rn, rn depend on n. This 
point is important in the RL framework since this allows on-line improvement of 
the model of the state dynamics and the reinforcement functions. 
Remark 4 This result extends the previous results of convergence obtained by 
Finite-Elemen
