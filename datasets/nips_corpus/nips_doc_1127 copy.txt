Information through a Spiking Neuron 
Charles F. Stevens and Anthony Zador 
Salk Institute MNL/S 
La Jolla, CA 92037 
zador@salk.edu 
Abstract 
While it is generally agreed that neurons transmit information 
about their synaptic inputs through spike trains, the code by which 
this information is transmitted is not well understood. An upper 
bound on the information encoded is obtained by hypothesizing 
that the precise timing of each spike conveys information. Here we 
develop a general approach to quantifying the information carried 
by spike trains under this hypothesis, and apply it to the leaky 
integrate-and-fire (IF) model of neuronal dynamics. We formu- 
late the problem in terms of the probability distribution p(T) of 
interspike intervals (ISis), assuming that spikes are detected with 
arbitrary but finite temporal resolution. In the absence of added 
noise, all the variability in the ISis could encode information, and 
the information rate is simply the entropy of the ISI distribution, 
H(T) = (-p(T)log2p(T)) , times the spike rate. H(T) thus pro- 
vides an exact expression for the information rate. The methods 
developed here can be used to determine experimentally the infor- 
mation carried by spike trains, even when the lower bound of the 
information rate provided by the stimulus reconstruction method 
is not tight. In a preliminary series of experiments, we have used 
these methods to estimate information rates of hippocampal neu- 
rons in slice in response to somatic current injection. These pilot 
experiments suggest information rates as high as 6.3 bits/spike. 
1 Information rate of spike trains 
Cortical neurons use spike trains to communicate with other neurons. The output 
of each neuron is a stochastic function of its inpu.t from the other neurons. It is of 
interest to know how much each neuron is telling other neurons about its inputs. 
How much information does the spike train provide about a signal? Consider noise 
n(t) added to a signal s(t) to produce some total input y(t) = s(t) + n(t). This 
is then passed through a (possibly stochastic) functional .' to produce the output 
spike train .'[y(t)] -. z(t). We assume that all the information contained in the 
spike train can be represented by the list of spike times; that is, there is no extra 
information contained in properties such as spike height or width. Note, however, 
that many characteristics of the spike train such as the mean or instantaneous rate 
76 C. STEVENS, A. ZADOR 
can be derived from this representation; if such a derivative property turns out to 
be the relevant one, then this formulation can be specialized appropriately. 
We will be interested, then, in the mutual information I(S(t); Z(t)) between the 
input signal ensemble S(t) and the output spike train ensemble Z(t). This is defined 
in terms of the entropy H(S) of the signal, the entropy H(Z) of the spike train, 
and their joint entropy H(S, Z), 
I(S;Z) = H(S) + H(Z)- H(S,Z). (1) 
Note that the mutual information is symmetric, I(S; Z) = I(Z; $), since the joint 
entropy H(S, Z) = H(Z, S). Note also that if the signal S(t) and the spike train 
Z(t) are completely independent, then the mutual information is 0, since the joint 
entropy is just the sum of the individual entropie H(S,Z) - H(S)+ H(Z). This is 
completely in line with our intuition, since in this case the spike train can provide 
no information about the signal. 
1.1 Information estimation through stimulus reconstruction 
Bialek and colleagues (Bialek et al., 1991) have used the reconstruction method 
to obtain a strict lower bound on the mutual information in an experimental set- 
ting. This method is based on an expression mathematically equivalent to eq. (1) 
involving the conditional entropy H(SIZ ) of the signal given the spike train, 
(S;Z) = (S)- (SlZ) 
>_ H(S)- Hest(SlZ), (2) 
where Hest(SIZ ) is an upper bound on the conditional entropy obtained from a 
reconstruction Ses t (t) of the signal. The entropy is estimated from the second order 
statistics of the reconstruction error e(t)  s(t)-Ses t (t); from the maximum entropy 
property of the Gaussian this is an upper bound. Intuitively, the first equation says 
that the information gained about the spike train by observing the stimulus is just 
the initial uncertainty of the signal (in the absence of knowledge of the spike train) 
minus the uncertainty that remains about the signal once the spike train is known, 
and the second equation says that this second uncertainty must be greater for any 
particular estimate than for the optimal estimate. 
1.2 Information estimation through spike train reliability 
We have adopted a different approach based an equivalent expression for the mutual 
information: 
(S; Z) = /(Z) - /(ZlS ). (a) 
The first term H(Z) is the entropy of the spike train, while the second H(ZIS) 
is the conditional entropy of the spike train given the signal; intuitively this like 
the inverse repeatability of the spike train given repeated applications of the same 
signal. Eq. (3) has the advantage that, if the spike train is a deterministic function 
of the input, it permits exact calculation of the mutual information. This follows 
from an important difference between the conditional entropy term here and in eq. 
2: whereas H(SIZ ) has both a deterministic and a stochastic component, H(ZIS ) 
has only a stochastic component. Thus in the absence of added noise, the discrete 
entropy H(ZI$ ): 0, and eq. (3) reduces to I($; Z) - H(Z). 
If ISis are independent, then the H(Z) can be simply expressed in terms of the 
entropy of the (discrete) ISI distribution p(T), 
H (T) = -  p(Ti ) log p(T/) (4) 
i=0 
Information Through a Spiking Neuron 77 
as H(Z) = nil(T), where n is the number of spikes in Z. Here v(T/) is the prob- 
ability that the spike occurred in the interval (/)At to (i + 1)At. The assumption 
of finite timing precision At keeps the potential information finite. The advantage 
of considering the ISI distribution p(T) rather than the full spike train distribution 
p(Z) is that the former is univariate while the latter is multivariate; estimating the 
former requires much less data. 
Under what conditions are ISis independent? Correlations between ISis can arise 
either through the stimulus or the spike generation mechanism itself. Below we shall 
guarantee that correlations do not arise from the spike-generator by considering the 
forgetful integrate-and-fire (IF) model, in which all information about the previous 
spike is eliminated by the next spike. If we further limit ourselves to temporally 
uncorrelated stimuli (i.e. stimuli drawn from a white noise ensemble), then we can 
be sure that ISis are independent, and eq. (4) can be applied. 
In the presence of noise, H(ZIT ) must also be evaluated, to give 
I(S; T) = H(T)- H(TIS ). (5) 
H(TIS ) is the conditional entropy of the ISI given the signal, 
H(TIS)=-(p(Tj,si(t))log2p(Tj,si(t)) ) (6) 
j=l si(t) 
where p(Tjlsi(t)) is the probability of obtaining an ISI of Tj in response to a par- 
ticular stimulus si(t) in the presence of noise n(t). The conditional entropy can be 
thought of as a quantification of the reliability of the spike generating mechanism: 
it is the average trial-to-trial variability of the spike train generated in response to 
repeated applications of the same stimulus. 
1.3 Maximum spike train entropy 
In what follows, it will be useful to compare the information rate for the IF neuron 
with the limiting case of an exponential ISI distribution, which has the maximum 
entropy for any point process of the given rate (Papoulis, 1984). This provides an 
upper bound on the information rate possible for any spike train, given the spike 
rate and the temporal precision. Let f(T) = e --r be an exponential distribution 
with a mean spike rate . Assuming a temporal precision of At, the entropy/spike 
is H(T) = log2 -, and the entropy/time for a rate  is H(T) = 1og -t' 
For example, if  = 1 Hz and At = 0.001 sec, this gives (11.4 bits/second) (1 
spike/second) = 11.4 bits/spike. That is, if we discretize a I Hz spike train into 
I msec bins, it is not' possible for it to transmit more than 11.4 bits/second. If 
we reduce the bin size two-fold, the rate increases by log 1/2 = I bit/spike to 
12.4 bits/spike, while if we double it we lose one bit/s to get 10.4 bit/s. Note 
that at a different firing rate, e.g.  = 2 Hz, halving the bin size still increases 
the entropy/spike by I bit/spike, but because the spike rate is twice as high, this 
becomes a 2 bit/second increase in the information rate. 
1.4 The IF model 
Now we consider the functional .' describing the forgetful leaky IF model of spike 
generation. Suppose we add some noise n(t) to a signal s(t), y(t) - n(t) + s(t), 
and threshold the sum to produce a spike train z(t) - .'[s(t) + n(t)]. Specifically, 
suppose the voltage v(t) of the neuron obeys )(t) - -v(t)/r + y(t), where r is the 
membrane time constant, both s(Q and n(t) have a white Gaussian distributions 
and y(t) has mean F and variance er . If the voltage reaches the threshold 00 at some 
time t, the neuron emits a spike at that time and resets to the initial condition v0. 
78 C. STEVENS, A. ZADOR 
In the language of neurobiology, this model can be thought of (Tuckwell, 1988) as 
the limiting case of a neuron with a leaky IF spike generating mechanism receiving 
many excitatory and inhibitory synaptic inputs. Note that since the input y(t) is 
white, there are no correlations in the spike train induced by the signal, and since 
the neuron resets after each spike there are no correlations induced by the spike- 
generating mechanism. Thus ISis are independent, and eq. (4) can be applied. 
We will estimate the mutual information I(S, Z) between the ensemble of input 
signals S and the ensemble of outputs Z. Since in this model ISis are independent by 
construction, we need only evaluate H(T) and H(TIS); for this we mu
