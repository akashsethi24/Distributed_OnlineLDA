Phonetic Classification and Recognition 
Using the Multi-Layer Perceptron 
Hong C. Leung, James R. Glass, 
Michael S. Phillips, and Victor W. Zue 
Spoken Language Systems Group 
Laboratory for Computer Science 
Massachusetts Institute of Technology 
Cambridge, Massachusetts 02139, U.S.A. 
Abstract 
In this paper, we will describe several extensions to our earlier work, utiliz- 
ing a segment-based approach. We will formulate our segmental framework 
and report our study on the use of multi-layer perceptrons for detection 
and classification of phoneroes. We will also examine the outputs of the 
network, and compare the network performance with other classifiers. Our 
investigation is performed within a set of experiments that attempts to 
recognize 38 vowels and consonants in American English independent of 
speaker. When evaluated on the TIMIT database, our system achieves an 
accuracy of 56%. 
I Introduction 
Thus far, the neural networks research community has placed heavy emphasis on 
the problem of pattern classification. In many applications, including speech recog- 
nition, one must also address the issue of detection. Thus, for example, one must 
detect the presence of phonetic segments as well as classify them. Recently, the 
community has moved more towards recognition of continuous speech. A network 
is typically used to label every frame of speech in a frame-based recognition sys- 
tem [Franzini 90, Morgan 90, Tebelskis 90]. 
Our goal is to study and exploit the capability of ANN for speech recognition, 
based on the premise that ANN may offer a flexible framework for us to utilize our 
248 
Phonetic Classification and Recognition Using the Multi-Layer Perceptron 249 
improved, albeit incomplete, speech knowledge. As an intermediate milestone, this 
paper extends our earlier work on phonetic classification to context-independent 
phonetic recognition. Thus we need to locate as well as identify the phonetic units. 
Our system differs from the majority of approaches in that r segmental framework is 
adopted. The network is used in conjunction with acoustic segmentation procedures 
to provide a phonetic string for the entire utterance. 
2 Segmental Formulation 
In our segmental framework, a phonetic unit is mapped to a segment explicitly 
delineated by a begin and end time in the speech signal. This is motivated by the 
belief that a segmental framework offers us more flexibility in applying our speech 
knowledge than is afforded by a frame-based approach. As a result, a segment-based 
approach could ultimately lead to superior modelling of the temporal variations in 
the realization of underlying phonological units. 
Let & denote the best sequence of phonetic units in an utterance. To simplify the 
problem, we assume that p(si) = p(silcj), where si stand for the i th time segment 
that has one and only one phoneme in it, and cj stands for the best phoneme label 
in si. Thus the probability of the best sequence, p(&), is: 
P(&) = p(cj)p(si); i j N (1) 
sie' 
where ' is any possible sequence of time segments consisting of {s,s2, ...), p(si) is 
the probability of a valid time segment, and N is the number of possible phonetic 
units. In order to perform recognition, the two probabilities in Equation i must be 
estimated. The first term, p(aj), is a set of phoneme probabilities and thus can be 
viewed as a classification problem. The second term, p(si), is a set of probabilities 
of valid time regions and thus can be estimated as a segmentation problem. 
2.1 Segmentation 
In order to estimate the segment probabilities, p(si), in Equation 1, we have formu- 
lated segmentation into a boundary classification problem. Let b! and br be the left 
and right boundary of a time segment, si, respectively, as shown in Figure la. Let 
{bl, b2, .., bc ) be the set of boundaries that might exist within si. These boundaries 
can be proposed by a boundary detector, or they can simply occur at every frame of 
speech. We define p(si) to be the joint probability that the left and right boundaries 
exist and all other boundaries within si do not exist. To reduce the complexity of 
the problem, assume bj is statistically independent of bk for �j  k. Thus, 
P($i) ---- P(DI, D1, D2, .., D/x', Dr_) 
= 
250 Leung, Glass, Phillips, and Zue 
bl bl b2 � � b K b r 
I I I I I 
(a) 
bi 
ti_2 ti_l ti ti+l ti+2 
(b) 
Figure 1: Schematic diagrams for estimation of (a) segment probability, p(si), and 
(b) boundary probability, p(bk). The boundaries can be proposed by a boundary 
detector, or they can simply occur at every frame. See text. 
where p(bi) and p(br) stand for the probability that the left and right boundary 
exist, respectively, p(bk) stands for the probability that the k TM boundary does not 
exist. As a result, the probability of a segment, p(si) can be obtained by computing 
the probabilities of the boundaries, p(bt), subsumed by the segment. As we will 
discuss in a later section, by using the time-aligned transcription, we can train the 
boundary probabilities in a supervised manner. 
2.2 Phonetic Classification 
Once the probability of a segment, p(sl), is obtained, we still need to classify it, 
i.e. compute the probabilities of the phonetic units in the segment, p(aj). Again, 
the time-aligned transcription can be used to train the probabilities in a supervised 
manner. We have discussed this in earlier papers [Leung 89, Leung 90]. In a later 
section, we will discuss some of our recent experimental results. 
3 Experiments 
3.1 Tasks and Corpora 
The experiments described in this paper deal with classification and recognition of 
38 phonetic labels representing 14 vowels, 3 semivowels, 3 nasals, 8 fricatives, 2 
affricates, 6 stops, i flap and i silence. Within the context of classification, the 
networks are given a segment of the speech signal, and are asked to determine 
its phonetic identity. Within the context of recognition, the networks are given 
an utterance, and are asked to determine the identity and locations of the pho- 
netic units in the utterance. All experiments were based on the sentences in the 
TIMIT database [Lamel 86]. As summarized in Table 1, Corpus I contains 1,750 sx 
sentences spoken by 350 male and female speakers, resulting in a total of 64,000 
phonetic tokens. Corpus II contains 4,400 sx and si sentences spoken by 550 male 
and female speakers, resulting in a total of 165,000 phonetic tokens. 
3.2 Phonetic Classification 
As previously discussed, estimation of the probability, p(aj) in Equation 1 can be 
viewed as a classification problem. Many statistical classifiers can be used. We have 
Phonetic Classification and Recognition Using the Multi-Layer Perceptron 251 
Corpus Set Speakers Sentences Tokens Type 
I training 300 1500 55,000 sx 
testing 50 250 9,000 sx 
II training 500 4000 150,000 sx/si 
testing 50 400 15,000 sx/si 
Table 1: Corpora I and II extracted from the TIMIT database. Corpus I contains 
only sx sentences, whereas Corpus II contains both sx and si sentences. The speakers 
in the testing sets for both Corpus I and Corpus II are the same. 
chosen to use the MLP, due to its discriminatory capability, as well as its flexibility 
in that it does not make assumptions about specific statistical distributions or 
distance metrics. In addition, earlier work shows that the outputs of MLP can 
approximate posterJori probabilities [Bourlard 88]. To train the network, we adopt 
procedures such as center initialization, input normalization, adaptive gain, and 
modular training [Leung 90]. The input representation was identical to that in the 
SUMMIT system, and consisted of 82 acoustic attributes [Zue 89]. These segmental 
attributes were generated automatically by a search procedure that uses the training 
data to determine the settings of the free parameters of a set of generic property 
detectors using an optimization procedure [Phillips 88]. 
3.3 Boundary Classification 
In our segmental framework formulated in Equation 1, the main difference between 
classification and recognition is the incorporation of a probability for each segment, 
p(si). As described previously in Equation 2, we have simplified the problem of 
estimating p(si) to one of determining the probability that a boundary exists, p(bk). 
To estimate p(bk), a MLP with two output units is used, one for the valid bound- 
aries and the other for the extraneous boundaries. By referencing the time-aligned 
phonetic transcription, the desired outputs of the network can be determined. In 
our current implementation p(bt) is determined using four abutting segments, as 
shown in Figure lb. These segments are proposed by the boundary detector in 
the SUMMIT system. Let ti stand for the time at which bl is located, and si stand 
for the segment between tl and tl+, where tl+ >ti. The boundary probability, 
p(bi), is then determined by using the average mean-rate response [Seneft 88] in 
s-2, sl_, sl, and si+ as inputs to the MLP. Thus the network has altogether 160 
input units. 
3.4 Results 
3.4.1 Phonetic Classification 
In the phonetic classification experiments, the system classified a token extracted 
from a phonetic transcription that had been aligned with the speech waveform. 
Since there was no detection involved in these experiments only substitution errors 
were possible. 
252 Leung, Glass, Phillips, and Zue 
I I Classifier Correct Parameters 
I SUMMIT 70% 2,200 
I Gaussian 70% 128,000 
I MLP 74% 15,000 
II MLP 76% 30,000 
Table 2: Phonetic classification results using the SUMMIT classifier, Gaussian clas- 
sifter, and MLP. Also shown are the number of parameters in the classifiers. 
In the first set of experiments, we compared results based on Corpus I, using dif- 
ferent classifiers. As Table 2 shows, the baseline speaker-independent classification 
performance of SUMMIT on the testing data was 70%. When Gaussian classifiers 
with full covariance matrices were used, we found that the performance is also
