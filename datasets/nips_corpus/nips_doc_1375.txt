Adaptive choice of grid and time in 
reinforcement learning 
Stephan Pareigis 
stp@numerik.uni-kiel.de 
Lehrstuhl Praktische Mathematik 
Christian-Albrechts-Universit/it Kiel 
Kiel, Germany 
Abstract 
We propose local error estimates together with algorithms for adap- 
tive a-posteriori grid and time refinement in reinforcement learn- 
ing. We consider a deterministic system with continuous state and 
time with infinite horizon discounted cost functional. For grid re- 
finement we follow the procedure of numerical methods for the 
Bellman-equation. For time refinement we propose a new criterion, 
based on consistency estimates of discrete solutions of the Bellman- 
equation. We demonstrate, that an optimal ratio of time to space 
discretization is crucial for optimal learning rates and accuracy of 
the approximate optimal value function. 
I Introduction 
Reinforcement learning can be performed for fully continuous problems by discretiz- 
ing state space and time, and then performing a discrete algorithm like Q-learning 
or RTDP (e.g. [5]). Consistency problems arise if the discretization needs to be 
refined, e.g. for more accuracy, application of multi-grid iteration or better starting 
values for the iteration of the approximate optimal value function. In [7] it was 
shown, that for diffusion dominated problems, a state to time discretization ratio 
k/h of Ch , 7 > 0 has to hold, to achieve consistency (i.e. k = o(h)). It can be 
shown, that for deterministic problems, this ratio must only be k/h = C, C a con- 
stant, to get consistent approximations of the optimal value function. The choice 
of the constant C is crucial for fast learning rates, optimal use of computer memory 
resources and accuracy of the approximation. 
We suggest a procedure involving local a-posteriori error estimation for grid refine- 
ment, similar to the one used in numerical schemes for the Bellman-equation (see 
[4]). For the adaptive time discretization we use a combination from step size con- 
Adaptive Choice of Grid and Time in Reinforcement Learning 1037 
trol for ordinary differential equations and calculations for the rates of convergence 
of fully discrete solutions of the Bellman-equation (see [3]). We explain how both 
methods can be combined and applied to Q-learning. A simple numerical example 
shows the effects of suboptimal state space to time discretization ratio, and provides 
an insight in the problems of coupling both schemes. 
2 Error estimation for adaptive choice of grid 
We want to approximate the optimal value function V: 9 -4 IR in a state space 
9 C IR a of the following problem: Minimize 
J(x, u(.)) :- e -p g(y,,(.)(r), u(r))dr, 
u(.): IR+ -4 A measurable, (1) 
where g: 9 x A -4 IR+ is the cost function, and y,(.)(.) is the solution of the 
differential equation 
)(t) - f(y(t), u(t)), y(O) = x. (2) 
As a trial space for the approximation of the optimal value function (or Q-function) 
we use locally linear elements on simplizes Si, i = 1,..., Ns which form a triangu- 
lation of the state space, Ns the number of simplizes. The vertices shall be called 
xi, i - 1,..., N, N the dimension of the trial space 1 . This approach has been used 
in numerical schemes for the Bellman-equation ([2], [4]). We will first assume, that 
the grid is fixed and has a discretization parameter 
k = m.axdiam{S/}. 
Other than in the numerical case, where the updates are performed in the vertices of 
the triangulation, in reinforcement learning only observed information is available. 
We will assume, that in one time step of size h > 0, we obtain the following 
information: 
� the current state y, G 9, 
� an action an  A, 
� the subsequent state Yn+l := Yyn,an(h) 
� the local cost rn = r(yn,an) = fo a e-Pg(yyn,a,(r),an(r))dr. 
The state y, in which an update is to be made, may be any state in . A shall be 
finite, and an locally constant. 
The new value of the fully discrete Q-function Q(y a) should be set to 
Q(y,a) shall be r + e-PaV(y+), 
where V(yn+)= mina Q(yn+,a). We call the right side the update function 
Pa(z,a, V) := r(z,a) + e-PaV(yz,a(a)), z e . (3) 
We will update Q in the vertices {xi }=x of the triangulation in one of the following 
two ways: 
i When an adaptive grid is used, then Ns and N depend on the refinement. 
1038 S. Parei g is 
Kaczmarz-update. Let A T = (A,..., AN) be the vector of barycentric coordi- 
nates, such that 
N 
Y=EAixi' 0_<Ai_< 1, for alli= 1,...,N. 
i----1 
Then update 
N ] 
+ . 
i--1 
(4) 
Kronecker-update. Let S 3 Yn and x be the vertex of S, closest to y (if there 
is a draw, then the update can be performed in all winners). Then update Q only 
in x according to 
Q(x, an) := rn + e-PnVn(yn+). (5) 
Each method has some assets and drawbacks. In our computer simulations the 
Kaczmarz-update seemed to be more stable over the Kronecker-update (see [6]). 
However, examples may be constructed where a (HSlder-)continuous bounded op- 
timal value function V is to be approximated, and the Kaczmarz-update produces 
an approximation with arbitrarily high [[.[[sup-norm (place a vertex x of the trian- 
gulation in a point where --V is infinity, and use as update states the vertex x in 
turn with an arbitrarily close state ). 
Kronecker-update will provide a bounded approximation if V is bounded. Let zn 
be the fully-discrete optimal value function 
zn(xi ) = min{r(xi, a) + e-PnVn(y,a(h)), i= 1,...,N. 
Then it can be shown, that an approximation_performed by Kronecker-update will 
k 
eventually be caught in an e-neighborhood of V (with respect to the I I- 
if the data points y0, y, y2,... are sufficiently dense. Under regularity conditions 
on V, e may be bounded by 2 
k 
e <_ + (6) 
As a criterion for grid refinement we choose a form of a local a posterJori error esti- 
mate as defined in [4]. Let Vn  (x) = mina Qn (x, a) be the current iterate of the op- 
timal value function. Let a G U be the minimizing control a = argminaQ(x , a). 
Then we define 
e(x) := [Vf (x)- P(x, a, Vf) I. (Z) 
If Vf is in the e-neighborhood of f, then it can be shown, that (for every x  f2 
and simplex S with z  S, a as above) 
0 _< e(x) <_ sup P(z,a, V) - inf P(z,a, V). 
zS zS: 
If zff is Lipschitz-continuous, then an estimate using only Gronwall's inequality 
bounds the right side and therefore e(x) by Cp-, where C depends on the Lipschitz- 
constants of zn and the cost g. 
2With respect to the results in [3] we assume, that also z _< C(h -1- n) can be shown. 
Adaptive Choice of Grid and Time in Reinforcement Learning 1039 
The value ej :- maxes iea (x) defines a function, which is locally constant on every 
simplex. We use e j, j = 1,..., N as an indicator function for grid refinement. The 
(global) tolerance value tolk for ej shall be set to 
Ns 
i--1 
where we have chosen 1 < C < 2. We approximate the function e on the simplizes 
in the following way, starting in some Yn  Sj' 
1. apply a control a  U constantly on [T, T + hi 
2. receive value rn and subsequent state yn+ 
3. calculate the update value Pa (x, a, V) 
4. if (IPa(x,a, V) - V(x)l >_ ej) then ej :- 
It is advisable to make grid refinements in one sweep. We also store (different to 
the described algorithm) several past values of ej in every simplex, to be able to 
distinguish between large ej due to few visits in that simplex and the large ej due to 
space discretization error. For grid refinement we use a method described in ([1]). 
3 A local criterion for time refinement 
Why not take the smallest possible sampling rate? There are two arguments for 
adaptive time discretization. First, a bigger time step h naturally improves (de- 
creases) the contraction rate of the iteration, which is e -pa . The new information 
is conveyed from a point further away (in the future) for big h, without the need 
to store intermediate states along the trajectory. It is therefore reasonable to start 
with a big h and refine where needed. 
The second argument is, that the grid and time discretization k and h stand in a 
certain relation. In [3] the estimate 
k 
iv(x)- v(x)l < C(h + ), for all x e , 
C a constant 
is proven (or similar estimates, depending on the regularity of V). For obvious 
reasons, it is desirable to start with a coarse grid (storage, speed), i.e. k large. 
Having a too small h in this case will make the approximation error large. Also 
here, it is reasonable to start with a big h and refine where needed. 
What can serve as a refinement criterion for the time step h? In numerical schemes 
for ordinary differential equations, adaptive step size control is performed by es- 
timating the local truncation error of the Taylor series by inserting intermediate 
points. In reinforcement learning, however, suppose the system has a large trunca- 
tion error (i.e. it is difficult to control) in a certain region using large h and locally 
constant control functions. If the optimal value function is nearly constant in this 
region, we will not have to refine h. The criterion must be, that at an intermediate 
point, e.g. at time hi2, the optimal value function assumes a value considerably 
smaller (better) than at time h. However, if this better value is due to error in the 
state discretization, then do not refine the time step. 
We define a function H on the simplices of the triangulation. H(S) > 0 holds the 
time-step which will be used when in simplex S. Starting at a state Yn  f, Yn  Sn 
at time T > 0, with the current iterate of the Q-function Q (V respectively) the 
following is performed: 
1040 S. Pareig is 
1. apply a control a � U constantly on IT, T q- h] 
2. take a sample at the intermediate state z = yy,,a(h/2) 
3. if (H($ < �*v/diam{s,}) then end. 
else 
4. compute V(z) -- min0 Q(z,b) 
5. compute Pn/2(yn,a, V) - rh/2(yn,a) q-e-Ph/V(z) 
6. compute P(y,a, V) - r6(y,,a) 
7. if (Pa/(yn,a, V) _< P(y,,a, Vff)-tol) update H($,) - H($n)/2 
The value � is currently se
