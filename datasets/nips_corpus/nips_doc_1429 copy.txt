Bidirectional 
Retrieval from Associative 
Memory 
Friedrich T. Sommer and Glinther Palm 
Department of Neural Information Processing 
University of Ulm, 89069 Ulm, Germany 
{ sommer, palm}� inf ormat ik. uni-ulm. de 
Abstract 
Similarity based fault tolerant retrieval in neural associative mem- 
ories (NAM) has not lead to wiedespread applications. A draw- 
back of the efficient Willshaw model for sparse patterns [Ste61, 
WBLH69], is that the high asymptotic information capacity is of 
little practical use because of high cross talk noise arising in the 
retrieval for finite sizes. Here a new bidirectional iterative retrieval 
method for the Willshaw model is presented, called crosswise bidi- 
rectional (CB) retrieval, providing enhanced performance. We dis- 
cuss its asymptotic capacity limit, analyze the first step, and com- 
pare it in experiments with the Willshaw model. Applying the very 
efficient CB memory model either in information retrieval systems 
or as a functional model for reciprocal cortico-cortical pathways 
requires more than robustness against random noise in the input: 
Our experiments show also the segmentation ability of CB-retrieval 
with addresses containing the superposition of pattens, provided 
even at high memory load. 
1 INTRODUCTION 
From a technical point of view neural associative memories (NAM) provide data 
storage and retrieval. Neural models naturally imply parallel implementation of 
storage and retrieval algorithms by the correspondence to synaptic modification 
and neural activation. With distributed coding of the data the recall in NAM 
models is fault tolerant: It is robust against noise or superposition in the addresses 
and against local damage in the synaptic weight matrix. As biological models NAM 
676 E T. Sommr and G. Palm 
have been proposed as general working schemes of networks of pyramidal cells in 
many places of the cortex. 
An important property of a NAM model is its information capacity, measuring 
how efficient the synaptic weights are used. In the early sixties Steinbuch realized 
under the name Lernmatrix a memory model with binary synapses which is now 
known as Willshaw model [Ste61, WBLH69]. The great variety of NAM models 
proposed since then, many triggered by Hopfield's work [Hop82], do not reach the 
high asymptotic information capacity of the Willshaw model. 
For finite network size, the Willshaw model does not optimally retrieve the 
stored information, since the inner product between matrix colurn and input 
pattern determines the activity for each output neuron independently. For au- 
toassociative pattern completion iterative retrieval can reduce cross talk noise 
[GM76, GR92, PS92, SSP96]. A simple bidirectional iteration - as in bidirectional 
associative memory (BAM) [Kos87] - can, however, not improve heteroassociative 
pattern mapping. For this task we propose CB-retrieval where each retrieval step 
forms the resulting activity pattern in an autoassociative process that uses the con- 
nectivity matrix twice before thresholding, thereby exploiting the stored information 
more efficiently. 
2 WILLSHAW MODEL AND CB EXTENSION 
Here pattern mapping tasks x v -+ yV are considered for a set of memory patterns: 
((x,y ) � x  E (0,1)n,y  E (0,1)m, = 1,...,M). The number of 1-components 
in a pattern is called pattern activity. The Willshaw model works efficiently, if 
the memories are sparse, i.e., if the memory patterns have the same activities: 
v = = = � a 
--' Ei=I Xi 0, [y[ b with << n and b << m. During 
learning the set of memory patterns is transformed to the weight matrix by 
Uij = min(1, x i yj ) -- sup X i yj. 
For a given initial pattern 5 the retrieval yields the output pattern  by forming 
in each neuron the dendritic sum [U2]j = Y'i Cj2� and by calculating the activity 
value by threshold comparison 
^u = H([C'u]j - 0) Vj, (1) 
Yj 
with the global threshold value 0 and H(x) denoting the Heaviside function. 
For finite sizes and with high memory load, i.e., 0 << P1 := Prob [Uij = 1] (< 0.5), 
the Willshaw model provides no tolerance with respect to errors in the address, see 
Fig. 1 and 2. A bidirectional iteration of standard simple retrieval (1), as proposed in 
BAM models [Kos87], can therefore be ruled out for further retrieval error reduction 
[SP97]. In the energy function of the Willshaw BAM 
E(x, y) - -  Cijxiyj + O' -. xi + !3  yj 
ij i j 
we now indroduce a factor accounting for the magnitudes of dendritic potentials at 
activated neurons 
E(x,y) = -  Cijxiyj a + b +  xi + !3 y'yj. (2) 
ij i j 
Bidirectional Retrieval from Associative Memory 677 
Differentiating the energy function (2) yields the gradient descent equations 
As new terms in (3) and (4) sums over pattern components weighted with the 
quantities w and w occur. w is the overlap between the matrix columns j and 
k conditioned by the pattern x, which we call a conditioned link between y-units. 
Restriction on the conditioned link terms yields a new iterative retrieval scheme 
which we denote as crosswise bidirectional (CB) retrieval 
y(r+l)j = H( E Cij[CTy(r-1)]i-O) (5) 
x(r+l)i = H( E Cij[Cx(r-1)]j-O') (6) 
jey(r) 
For r = 0 pattern y(r.-1) has to be replaced by H([Cx(O)] - 0), for r > 2 Boolean 
ANDing with results from timestep r - i can be applied which has been shown to 
improve iterative retrieval in the Willshaw model for autoassociation [SSP96]. 
3 MODEL EVALUATION 
Two possible retrieval error types can be distinguished: a miss error converts a 
1-entry in yU to '0' and a add error does the opposite. 
35 
30 
25 
20 
35 
30 
25 
20 
10 
5 
10 20 30 40 
simple r. ad error ..... ' ' ' / 
C B-.maidsd error  
_ CB-r error--11,i ' .,..,.,,,,..,,_ 
 I ....,..,,' ......... - ............... f ....... . ...... i 
10 15 20 25 30 
Figure 1: Mean retrieval error rates for n = 2000, M - 15000, a -- b = 10 
corresponding to a memory load of P1 - 0.3. The x-axes display the address 
activity: ]bu] - 10 corresponds to a errorfree learning pattern, lower activities are 
due to miss errors, higher activities due to add errors. Left: Theory - Add errors 
for simple retrieval, eq. (7) (upper curve) and lower bound for the first step of 
CB-retrieval, eq. (9). Right: Simulations - Errors for simple and CB retrieval. 
The analysis_ of simple retrieval from the address 5yields with optimal threshold 
setting 0 = k the add error rate, i.e, the expectation of spurious ones: 
& = (m-b)Prob [r >_ ], (7) 
678 E T. Sornmer and G. Palm 
with the binomial random variable Prob[r=l] = where B(n,p)t := 
()pt(1 _ p)n-t.  denotes the add error rate and k =  the number of 
correct 1-s in the address. 
For the first step of CB-retrieval a lower bound of the add error rate a(1) can be 
derived by the analysis of CB-retrieval with fixed address x(0): 2 and the perfect 
learning pattern yt, as starting patterns in the y-layer. In this case the add error 
rate is: 
a = (m - b)Prob [rl + r2 _> }b], (8) 
where the random variables rl and r2 have the distributions: 
Prob[rx = l/b] = B(,Px)t and Probit2 = 1] = B(b,(px)2)t. Thus, 
a(1) _> (m -b)E B(,P1)sB$ [b, (Px)2, ( - s)b], 
s----0 
(9) 
where B$[n,p,t] := Y'Lt B(n,p)t is the binomial sum. 
In Fig. 1 the analytic results for the first step (7) and (9) can be compared with 
simulations (left versus right_diagram). In the experiments simple retrieval is per- 
formed with threshold 0 -- k. CB-retrieval is iterated in the y-layer (with fixed 
address ) starting with three randomly chosen 1-s from the simple retrieval result 
. The iteration is stopped, if a stable pattern at threshold 0 = b is reached. 
The memory capacity can be calculated per pattern component under the assump- 
tion that in the memory patterns each component is independent, i.e., the proba- 
bilities for a 1 are p = a/n or q - b/m respectively, and the probabilities of an add 
and a miss error are simply the renormalized rates denoted by , ] and &,/ for 
x-patterns and by /, 5 for y-patterns. The information about the stored pattern 
contained in noisy initial or retrieved patterns is then given by the transinforma- 
tion t(p, c , fi) :- i(p) - i(p, c , ]), where i(p) is the Shannon information, and 
i(p, (, ]) the conditional information. The heteroassociative mapping is evaluated 
by the output capacity: A(,/ ) :- Mrn t(q,/,5)/mn (in units bit/synapse). It 
depends on the initial noise since the performance drops with growing initial errors 
and assumes the maximum, if no fault tolerance is provided, that is, with noiseless 
initial patterns, see Fig. 2. Autoassociative completion of a distorted x-pattern is 
evaluated by the completion capacity: C(& t,/)') := Mn(t(p, a',/)- t(p, &',/'))/rnn. 
A BAM maps and completes at the same time and should be therefore evaluated 
by the search capacity $ := C + A. 
The asymptotic capacity of the Willshaw model is strikingly high: The completion 
capacity (for autoassociation) is C + = ln[2]/4, the mapping capacity (for heteroas- 
sociation with input noise) is A + = ln[2]/2 bit/syn [Pa191], leading to a value for 
the search capacity of (3 ln[2])/4 = 0.52 bit/syn. To estimate $ for general retrieval 
procedures one can consider a recognition process of stored patterns in the whole 
space of sparse initial patterns; an initial pattern is recognized, if it is invari- 
ant under a bidirectional retrieval cycle. The so-called recognition capacity of this 
process is an upper bound of the completion capacity and it had been determined 
as ln[2]/2, see [PS92]. This is achieved again with parameters M,p,q providing 
A = ln[2]/2 yielding ln[2] bit/syn as upper bound of the asymptotic search capac- 
ity. In summary, we know about the asymptotic search capacity of the CB-model: 
0.52 _< S + _< 0.69 bit/syn. For experimental results, see Fig. 4. 
Bidirectional Retrieval from Associative Memory 679 
4 EXPERIMENTAL RESULTS 
The CB model has been tested in simulations an
