Learning in Compositional Hierarchies: 
Inducing the Structure of Objects from Data 
Joachim Utans 
Oregon Graduate Institute 
Department of Computer Science and Engineering 
P.O. Box 91000 
Portland, OR 97291-1000 
utans @cse.ogi.edu 
Abstract 
I propose a learning algorithm for learning hierarchical models for ob- 
ject recognition. The model architecture is a compositional hierarchy 
that represents part-whole relationships: parts are described in the lo- 
cal context of substructures of the object. The focus of this report is 
learning hierarchical models from data, i.e. inducing the structure of 
model prototypes from observed exemplars of an object. At each node 
in the hierarchy, a probability distribution governing its parameters must 
be learned. The connections between nodes reflects the structure of the 
object. The formulation of substructures is encouraged such that their 
parts become conditionally independent. The resulting model can be 
interpreted as a Bayesian Belief Network and also is in many respects 
similar to the stochastic visual grammar described by Mjolsness. 
1 INTRODUCTION 
Model-based object recognition solves the problem of invariant recognition by relying on 
stored prototypes at unit scale positioned at the origin of an object-centered coordinate 
system. Elastic matching techniques are used to find a correspondence between features of 
the stored model and the data and can also compute the parameters of the transformation the 
observed instance has undergone relative to the stored model. An example is the TRAFFIC 
system (Zemel, Mozer and Hinton, 1990) or the Frameville system (Mjolsness, Gindi and 
285 
286 Utans 
......  Human 
f------ I Arm 
I 
I 
I 
Lower Arm 
Figure 1: Example of a compositional 
hierarchy. The simple figure can be 
represented as hierarchical composi- 
tion of parts. The hierarchy can 
be represented as a graph (a tree in 
this case). Nodes represent parts and 
edges represent the structural relation- 
ship. Nodes at the bottom represent 
individual parts of the object; nodes 
at higher levels denote more complex 
substructures. The single node at the 
top of the tree represents the entire ob- 
ject. 
Anandan, 1989; Gindi, Mjolsness and Anandan, 1991; Utans, 1992). Frameville stores 
models as compositional hierarchies and by matching at each level in the hierarchy reduces 
the combinatorics of the match. 
The attractive feature of feed-forward neural networks for object recognition is the relative 
ease with which their parameters can be learned from training data. Multilayer feed-forward 
networks are typically trained on input/output pairs (supervised learning) and thus are tuned 
to recognize instances of objects as seen during training. Difficulties arise if the observed 
object appears at a different position in the input image, is scaled or rotated, or has been 
subject to distortions. Some of these problems can be overcome by suitable preprocessing or 
judicious choice of features. Other possibilities are weight sharing (LeCun, Boser, Denker, 
Henderson, Howard, Hubbard and Jackel, 1989) or invariant distance measures (Simard, 
LeCun and Denker, 1993). 
Few attempts have been reported in the neural network literature to learn the prototype 
models for model based recognition from data. For example, the Frameville system uses 
hand-designed models. However, models learned from data and reflecting the statistics of 
the data should be superior to the hand-designed models used previously. Segen (1988a; 
1988b) reports an approach to learning structural descriptions where features are clustered 
to substructures using a Minimum Description Length (MDL) criterion to obtain a sparse 
representation. Saund (1993) has proposed a algorithm for constructing tree presentation 
with multiple causes where observed data is accounted for by multiple substructures at 
higher levels in the hierarchy. Ueda and Suzuki (1993) have developed an algorithm for 
learning models from shape contours using multiscale convex/concave structure matching 
to find a prototype shape typical for exemplars from a given class. 
2 LEARNING COMPOSITIONAL HIERARCHIES 
The algorithm described here merges parts by means of grouping variables to form sub- 
structures. The model architecture is a compositional hierarchy, i.e. a part-whole hierarchy 
(an example is shown in Figure 1). The nodes in the graph represent parts and substruc- 
tures, the arcs describe the structure of the object. At each node a probability density for 
part parameters is stored. A prominent advocate of such models has been Marr (1982) 
and models of this type are used in the Frameville system (Mjolsness et al., 1989; Gindi 
et al., 1991; Utans, 1992). The nodes in the graph represent parts and substructures, the 
Learning in Compositional Hierarchies: Inducing the Structure of Objects from Data 287 
Figure 2: Examples of differ- 
ent compositional hierarchies for 
the same object (the digit 9 for 
a seven-segment LED display). 
One model emphasizes the paral- 
lel lines making up the square in 
the top part of the figure while for 
another model angles are chosen 
as intermediate substructures. The 
example on the right shows a hier- 
archy that reuses parts. 
arcs describe the structure of the object. The arcs can be regarded as part-of or ina 
relationships (similar to the notion used in semantic networks). At each node a probability 
density for part parameters such as position, size and orientation is stored. 
The model represents a typical prototypeobject at unit scale in an object-centered coordinate 
system. Parameters of parts are specified relative to parameters of the parent node in the 
hierarchy. Substructures thus provide a local context for their parts and decouple their parts 
from other parts and substructures in the model. The advantages of this representation are 
sparseness, invariance with respect to viewpoint transformations and the ability to model 
local deformations. In addition, the model explicitly represents the structure of an object 
and emphasizes the importance of structure for recognition (Cooper, 1989). 
Learning requires estimating the parameters of the distributions at each node (the mean and 
variance in the case of Gaussians) and finding the structure of model. The emphasis in this 
report is on learning structure from exemplars. The parameterization of substructures may 
be different than for the parts at the lowest level and become more complex and require more 
parameters as the substructures themselves become more complex. The representation as 
compositional hierarchy can avoid overfitting since at higher levels in the hierarchy more 
exemplars are available for parameter estimation due to the grouping of parts (Omohundro, 
1991). 
2.1 Structure and Conditional Independence: Bayesian Networks 
In what way should substructures be allocated? Figure 2 shows examples of different 
compositional hierarchies for the same object (the digit 9 for a seven-segment LED display). 
One model emphasizes the parallel lines making up the square in the top part of the figure 
while for another model angles are chosen as intermediate substructures. It is not clear 
which of these models to choose. 
The important benefit of a hierarchical representation of structure is that parts belonging to 
different substructures become decoupled, i.e. they are assigned to a different local context. 
The problem of constructing structured descriptions of data that reflect this independence 
relationship has been studied previously in the field of Machine Learning (see (Pearl, 1988) 
for a comprehensive introduction). The resulting models are Bayesian Belief Networks. 
Central to the idea of Bayesian Networks is the assumption that objects can be regarded 
as being composed of components that only sparsely interact and the network captures 
the probabilistic dependency of these components. The network can be represented as 
an interaction graph augmented with conditional probabilities. The structure of the graph 
represents the dependence of variables, i.e. connects them with and arc. The strength of the 
288 Utans 
Figure 3: Bayesian Networks and conditional 
independence (see text). 
ina p?, 
Oata 
Figure 4: The model architecture. Circles denote 
the grouping variables ina (here a possible valid 
model after learning is shown). 
dependence is expressed as forward conditional probability. The conditional independence 
is represented by the absence of an arc between two nodes and leads to the sparseness of 
the model. 
The notion of conditional independence in the context studied here manifest itself as follows. 
By just observing two parts in the image, one must assume that they, i.e. their parameters 
such as position, are dependent and must be modeled using their joint distribution. How- 
ever, if one knows that these two parts are grouped to form a substructure then knowing 
the parameters of the substructure, the parts become conditionally independent, namely 
conditioned on the parameters of the substructure. Thus, the internal nodes representing the 
substructures summarize the interaction of their child nodes. The correlation between the 
child nodes is summarized in the parent node and what remains is, for example, independent 
noise in observed instances of the child nodes. 
The probability of observing an instance can be calculated from the model by starting at 
the root node and multiplying with the conditional probabilities of nodes traversed until the 
leaf nodes are reached. For example, given the graph in Figure 3, the joint distribution can 
be factored as 
P(z], V], V2, z], z2, z3, z4) = 
P( z] )P(v] Iz] )P( z] Iv] )P( z] Iv] )P( z2lv] )P( z31v2 )P(z41v2) (1) 
(note that the hidden nodes are treated just like the nodes corresponding to observable parts). 
Note that the stochastic visual grammar described by Mjolsness (1991) is equivalent to this 
model. The model used there
