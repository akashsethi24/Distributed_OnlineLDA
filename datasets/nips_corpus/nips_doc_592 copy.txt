Integration of Visual and Somatosensory 
Information for Preshaping Hand 
in Grasping Movements 
Yoji Uno 
ATR Human Information Processing 
Research Laboratories 
2-2 Hikaridai, Seika-cho, Soraku-gun, 
Kyoto 619-02, Japan 
Naohiro Fukumura* 
Faculty of Engineering 
University of Tokyo 
7-3-1 Hongo, Bunkyo-ku, 
Tokyo 113, Japan 
Ryoji Suzuki 
Faculty of Engineering 
University of Tokyo 
7-3-1 Hongo, Bunkyo-ku, 
Tokyo 113, Japan 
Mitsuo Kawato 
ATR Human Information Processing 
Research Laboratories 
2-2 Hikaridai, Seika-cho, Soraku-gun, 
Kyoto 619-02, Japan 
Abstract 
The primate brain must solve two important problems in grasping move- 
ments. The first problem concerns the recognition of grasped objects: 
specifically, how does the brain integrate visual and motor information 
on a grasped object? The second problem concerns hand shape planning: 
specifically, how does the brain design the hand configuration suited to the 
shape of the object and the manipulation task? A neural network model 
that solves these problems has been developed. The operations of the net- 
work are divided into a learning phase and an optimization phase. In the 
learning phase, internal representations, which depend on the grasped ob- 
jects and the task, are acquired by integrating visual and somatosensory 
information. In the optimization phase, the most suitable hand shape for 
grasping an object is determined by using a relaxation computation of the 
network. 
*Present Address: Parallel Distributed Processing Research Dept., Sony Corporation, 
6-7-35 Kitashinagawa, Shinagawa-ku, Tokyo 141, Japan 
311 
312 Uno, Fukumura, Suzuki, and Kawato 
1 INTRODUCTION 
It has previously been established that, while reaching out to grasp an object, the human 
hand preshapes according to the shape of the object and the planned manipulation (Jean- 
nero& 1984; Arbib et al., 1985). The preshaping of the human hand suggests that prior to 
grasping an object the 3-dimensional form of the object is recognized and the most suitable 
hand configuration is preset depending on the manipulation task. 
It is supposed that the human recognizes objects using not only visual information but also 
somatosensory information when the hand grasps them. Visual information is made from 
the 2-dimensional image in the visual system of the brain. Somatosensory information 
is closely related to motor information, because it depends on the prehensile hand shape 
(i.e., finger configuration). We hypothesize that an internal representation of a grasped 
object is formed in the brain by integrating visual and somatosensory information. Some 
physiological studies support our hypothesis. For example, Taira et al. (1990) found that 
the activity of hand-movement-related neurons in the posterior parietal association cortex 
were highly selective to the shape and/or the orientation of manipulated switches. 
How can the neural network integrate different kinds of information? Merely uniting vi- 
sual image with somatosensory information does not lead to any interesting representation. 
Our basic idea is that information compression is applied to integrating different kinds of 
information. It is useful to extract the essential information by compressing the visual and 
somatosensory information. 
Irie & Kawato (1991) pointed out that multi-layered perceptrons have the ability to ex- 
tract features from the input signals by compressing the information from input signals. 
Katayama & Kawato (1990) proposed a learning schema in which an internal represen- 
tation of the grasped object was acquired using information compression. Developing the 
schema of Katayama et al., we have devised a neural network model for recognizing objects 
and planning hand shapes (e.g., Fukumura et al. 1991). This neural network consists of five 
layers of neurons with only forward connections as shown in Figure 1. The input layer (lst 
layer) and the output layer (5th layer) of the network have the same structure. There are 
fewer neurons in the 3rd layer than in the 1st and 5th layers. The operations of the network 
are divided into the learning phase, which is discussed in section 2 and the optimization 
phase, which is discussed in section 3. 
2 
INTEGRATION OF VISUAL AND SOMATOSENSORY 
INFORMATION USING NETWORK LEARNING 
In the learning phase, the neural network learns the relation between the visual information 
(i.e.,visual image) and the somatosensory information which, in this paper, is regarded as 
information on the prehensile hand configuration (i.e., finger configuration). 
Both vector x representing the visual image of an object and vector y representing the pre- 
hensile hand configuration to grasp it are fed into the 1 st layer (the input layer). The synaptic 
weights of the network are repeatedly adjusted so that the 5th layer outputs the same vec- 
tors x and y as are fed into the 1 st layer. In other words, the network comes to realize the 
identity map between the 1st layer and the 5th layer through a leaming process. The most 
important point of the neural network model is that the number of neurons in the 3rd layer 
is smaller than the number of neurons in the 1st layer (which is equal to the number of 
neurons in the 5th layer). Therefore, the information from x and y is compressed between 
Visual &: Somatosensory Information for Preshaping Hand in Grasping Movements 313 
y iX i// \\ 
(hand) / (hand) 
Figure 1: A neural network model for integrating visual image x and prehensile hand con- 
figuration y. The internal representation z of a grasped object is acquired in the third layer. 
the 1st layer and the 3rd layer, and restored between the 3rd layer and the 5th layer. Once 
the network leaming process is complete, visual image x and prehensile hand configuration 
y are integrated in the network. Consequently, the internal representation z of the grasped 
object, which should include enough information to reproduce x and y, is formed in the 3rd 
layer. 
Prehensile hand configuration in grasping movements were measured and the leaming of 
the network was simulated by a computer. In behavioral experiments, three kinds of wooden 
objects were prepared: five circular cylinders whose diameters were 3 cm, 4 cm, 5 cm, 6 cm 
and 7 cm; four quadrangular prisms whose side lengths were 3 cm, 4 cm, 5 cm and 6 cm; 
and three spheres whose diameters were 3 cm, 4 cm and 5 cm. Data input to the network 
was comprised of visual image x and prehensile hand configuration y. 
Visual images of objects are formed through complicated processes in the visual system of 
the brain. For simplicity, however, projections of objects onto a side plane and/or a bottom 
plane were used instead of real visual images. The area of each pixel of the projected 
image was fed into the network as an element of visual image x. A DataGlove TM (I?PL) 
was used to measure finger configurations in grasping movements. We attached sixteen 
optical fibers, whose outputs were roughly inversely proportional to finger joint-angles, to 
the DataGlove. The subject was instructed to grasp the objects on the table tightly with 
the palm and all the fingers. The subject grasped twelve objects thirty times each, which 
produced 360 prehensile patterns for use as training data for network learning. 
In the computer simulation, six neurons were set in the 3rd layer. The back-propagation 
learning method was applied in order to modify the synaptic weights in the network. Fig- 
ure 2 shows the activity of neurons in the 3rd layer after the learning had sufficiently been 
performed. Some interesting features of the internal representations were found in Figure 
2. The first is that the level of neuron activity in the 3rd layer increased monotonically as 
the size of the object increased. The second is that, except for the magnitude, the neuron 
activation patterns for the same kinds of objects were almost the same. Furthermore, the 
activation patterns were similar for circular cylinders and quadrangular prisms, but were 
quite different for spheres. In other words, similar representations were acquired for simi- 
larly shaped objects. We concluded that the internal representations were formed in the 3rd 
314 Uno, Fukumura, Suzuki, and Kawato 
Neuron activity 
1 2 3 4 $ 6 
N x of e 3rd 
a) Oimular linOor 
Neuron activity 
1.00 
� 3cm 
--o- -4cm 
-O- $�m 
'-O-m o.oc 
-C) -7�m 
Neuron activity 
g 0 1.00' 
i \ /  
\ I ,   Side Length 
Neon x of e 3 y 
----3cm 
-O-5cm 
-E]-6cm 0.00' 
b) Quadrangular prism 
 t 
� 
1 2 3 4 5 6 
Neuron index of the 3rd layer 
c) Sphere 
Diameter 
-- -4cm 
-- cm 
Figure 2: Internal representations of grasped objects. Graph a) shows the neuron activa- 
tion patterns for five circular cylinders whose diameters were 3 cm, 4 cm, 5 cm, 6 cm and 
7 cm. Graph b) shows the neuron activation patterns for four quadrangular prisms whose 
side lengths were 3 cm, 4 cm, 5 cm and 6 cm. Finally, Graph c) shows the neuron activa- 
tion patterns for three spheres whose diameters were 3 cm, 4 cm and 5 cm. The abscissa 
represents the index of the six neurons in the 3rd layer, while the ordinate represents their 
activity. These values were normalized from -1 to +1. 
layer and changed topologically according to the shapes and sizes of the grasped objects. 
3 DESIGN OF PREHENSILE HAND SHAPES 
The neural network that has completed the learning can design hand shapes to grasp any 
objects in the optimization phase. Determining prehensile hand shape (i.e., finger config- 
uration) is an ill-posed problem, because there are many ways to grasp any given object. 
In other words, prehensile hand configuration cannot be determined uniquely for any one 
object. In order to solve this indeterminacy, a criterion, a measure of performance for any 
possible prehensile configuration is introduced. 
The criterion should normally be defined based on the dynamics of the human hand and the 
manipulation task. However, for si
