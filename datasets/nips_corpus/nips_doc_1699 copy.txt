Emergence of Topography and Complex 
Cell Properties from Natural Images 
using Extensions of ICA 
Aapo HyvSrinen and Patrik Hoyer 
Neural Networks Research Center 
Helsinki University of Technology 
P.O. Box 5400, FIN-02015 HUT, Finland 
aapo. hyvarinenhut. f i, patrik. hoyerhut. f i 
http://www. cis. hut. f i/proj ects/ica/ 
Abstract 
Independent component analysis of natural images leads to emer- 
gence of simple cell properties, i.e. linear filters that resemble 
wavelets or Gabor functions. In this paper, we extend ICA to 
explain further properties of V1 cells. First, we decompose natural 
images into independent subspaces instead of scalar components. 
This model leads to emergence of phase and shift invariant fea- 
tures, similar to those in V1 complex cells. Second, we define a 
topography between the linear components obtained by ICA. The 
topographic distance between two components is defined by their 
higher-order correlations, so that two components are close to each 
other in the topography if they are strongly dependent on each 
other. This leads to simultaneous emergence of both topography 
and invariances similar to complex cell properties. 
I Introduction 
A fundamental approach in signal processing is to design a statistical generative 
model of the observed signals. Such an approach is also useful for modeling the 
properties of neurons in primary sensory areas. The basic models that we consider 
here express a static monochrome image I(x, y) as a linear superposition of some 
features or basis functions bi(x, y): 
I(x,y) -- E bi(x,y)si (1) 
i----1 
where the si are stochastic coefficients, different for each image I(x, y). Est'unation 
of the model in Eq. (1) consists of determining the values of si and bi(x, y) for all i 
and (x, y), given a sufficient number of observations of images, or in practice, image 
patches I(x, y). We restrict ourselves here to the basic case where the bi (x, y) form 
an invertible linear system. Then we can invert si = wi, I  where the wi denote 
the inverse filters, and  wi, I = --x,y wi(x,y)I(x,y) denotes the dot-product. 
828 A. Hyviirinen and P. Hoyer 
The wi(x, y) can then be identified as the receptive fields of the model simple cells, 
and the si are their activities when presented with a given image patch I(x, y). 
In the basic case, we assume that the si are nongaussian, and mutually independent. 
This type of decomposition is called independent component analysis (ICA) [3, 9, 
1, 8], or sparse coding [13]. Olshausen and Field [13] showed that when this model 
is estimated with input data consisting of patches of natural scenes, the obtained 
filters Wi(x,y) have the three principal properties of simple cells in VI: they are 
localized, oriented, and bandpass (selective to scale/frequency). Van Hateren and 
van der Schaaf [15] compared quantitatively the obtained filters wi(x, y) with those 
measured by single-cell recordings of the macaque cortex, and found a good match 
for most of the parameters. 
We show in this paper that simple extensions of the basic ICA model explain emer- 
gence of further properties of V1 cells: topography and the invariances of complex 
cells. Due to space limitations, we can only give the basic ideas in this paper. More 
details can be found in [6, 5, 7]. 
First, using the method of feature subspaces [11], we model the response of a com- 
plex cell as the norm of the projection of the input vector (image patch) onto a 
linear subspace, which is equivalent to the classical energy models. Then we maxi- 
mize the independence between the norms of such projections, or energies. Thus we 
obtain features that are localized in space, oriented, and bandpass, like those given 
by simple cells, or Gabor analysis. In contrast to simple linear filters, however, the 
obtained feature subspaces also show emergence of phase invariance and (limited) 
shift or translation invariance. Maximizing the independence, or equivalently, the 
sparseness of the norms of the projections to feature subspaces thus allows for the 
emergence of exactly those invariances that are encountered in complex cells. 
Second, we extend this model of independent subspaces so that we have overlapping 
subspaces, and every subspace corresponds to a neighborhood on a topographic grid. 
This is called topographic ICA, since it defines a topographic organization between 
components. Components that are far from each other on the grid are independent, 
like in ICA. In contrast, components that are near to each other are not independent: 
they have strong higher-order correlations. This model shows emergence of both 
complex cell properties and topography from image data. 
2 Independent subspaces as complex cells 
In addition to the simple cells that can be modelled by basic ICA, another important 
class of cells in V1 is complex cells. The two principal properties that distinguish 
complex cells from simple cells are phase invariance and (limited) shift invariance. 
The purpose of the first model in this paper is to explain the emergence of such 
phase and shift invariant features using a modification of the ICA model. The 
modification is based on combining the principle of invariant-feature subspaces [11] 
and the model of multidimensional independent component analysis [2]. 
Invariant feature subspaces. The principle of invariant-feature subspaces 
states that one may consider an invariant feature as a linear subspace in a feature 
space. The value of the invariant, higher-order feature is given by (the square of) the 
norm of the projection of the given data point on that subspace, which is typically 
spanned by lower-order features. A feature subspace, as any linear subspace, can 
always be represented by a set of orthogonal basis vectors, say wi(x, y), i = 1, ..., m, 
where ra is the dimension of the subspace. Then the value F(I) of the feature F 
with input vector I(x, y) is given by F(I) m 
-- i=1 < Wi' I , where a square root 
Emergence of 1 properties using Extensions of lCA 829 
might be taken. In fact, this is equivalent to computing the distance between the 
input vector I(x, y) and a general linear combination of the basis vectors (filters) 
wi(x, y) of the feature subspace [11]. In [11], it was shown that this principle, when 
combined with competitive learning techniques, can lead to emergence of invariant 
image features. 
Multidimensional independent component analysis. In multidimensional 
independent component analysis [2] (see also [12]), a linear generative model as in 
Eq. (1) is assumed. In contrast to ordinary ICA, however, the components (re- 
sponses) si are not assumed to be all mutually independent. Instead, it is assumed 
that the si can be divided into couples, triplets or in general m-tuples, such that 
the si inside a given m-tuple may be dependent on each other, but dependencies 
between different m-tuples are not allowed. Every m-tuple of si corresponds to m 
basis vectors bi(x, y). The m-dimensional probability densities inside the m-tuples 
of si is not specified in advance in the general definition of multidimensional ICA [2l. 
In the following, let us denote by J the number of independent feature subspaces, 
and by $j,j = 1, ..., J the set of the indices of the si belonging to the subspace of 
index j. 
Independent feature subspaces. Invariant-feature subspaces can be embedded 
in multidimensional independent component analysis by considering probability dis- 
tributions for the m-tuples of si that are spherically symmetric, i.e. depend only 
on the norm. In other words, the probability density pj(.) of the m-tuple with 
index j E { 1,..., J}, can be expressed as a function of the sum of the squares of the 
si, i  $j only. For simplicity, we assume further that the pj(.) are equal for all j, 
i.e. for all subspaces. 
Assume that the data consists of K observed image patches Ik(x,y), k = 1, ..., K. 
Then the logarithm of the likelihood L of the data given the model can be expressed 
K J 
logL(wi(x,y),i = 1...n) = E E 1ï¿½gp(E < wi,Ik >2) + Klog i det W] (2) 
k----1 j----1 iSj 
where P(..ie$j s) = pj(si,i  Sj) gives the probability density inside the j-th 
m-tuple of si, and W is a matrix containing the filters wi(x, y) as its columns. 
As in basic ICA, prewhitening of the data allows us to consider the wi(x, y) to be 
orthonormal, and this implies that log[detW[ is zero [6]. Thus we see that the 
likelihood in Eq. (2) is a function of the norms of the projections of I(x, y) on 
the subspaces indexed by j, which are spanned by the orthonormal basis sets given 
by wi(x,y),i  $j. Since the norm of the projection of visual data on practically 
any subspace has a supergaussian distribution, we need to choose the probability 
density p in the model to be sparse [13], i.e. supergaussian [8]. For example, we 
could use the following probability distribution 
logp( E s)=-ale 8] 1/2 -}-/, (3) 
is is 
which could be considered a multi-dimensional version of the exponential distribu- 
tion. Now we see that the estimation of the model consists of finding subspaces 
such that the norms of the projections of the (whitened) data on those subspaces 
have maximally sparse distributions. 
The introduced independent (feature) subspace analysis is a natural generalization 
of ordinary ICA. In fact, if the projections on the subspaces are reduced to dot- 
products, i.e. projections on 1-D subspaces, the model reduces to ordinary ICA 
830 A. Hyvarinen and P Hoyer 
(provided that, in addition, the independent components are assumed to have non- 
skewed distributions). It is to be expected that the norms of the projections on 
the subspaces represent some higher-order, invariant features. The exact nature of 
the invariances has not been specified in the model but will emerge from the input 
data, using only the prior information on their independence. 
When independent subspace analysis is applied to natural image data, we can iden-
