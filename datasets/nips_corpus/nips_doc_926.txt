Learning Local Error Bars 
for Nonlinear Regression 
David A. Nix 
Department of Computer Science 
and Institute of Cognitive Science 
University of Colorado 
Boulder, CO 80309-0430 
dnix@cs. colorado. edu 
Andreas S. Weigend 
Department of Computer Science 
and Institute of Cognitive Science 
University of Colorado 
Boulder, CO 80309-0430 
andreasc s. colorado. edu * 
Abstract 
We present a new method for obtaining local error bars for nonlinear 
regression, i.e., estimates of the confidence in predicted values that de- 
pend on the input. We approach this problem by applying a maximum- 
likelihood framework to an assumed distribution of errors. We demon- 
strate our method first on computer-generated data with locally varying, 
normally distributed target noise. We then apply it to laser data from the 
Santa Fe lme Series Competition where the underlying system noise is 
known quantization error and the error bars give local estimates of model 
misspecificafion. In both cases, the method also provides a weighted- 
regression effect that improves generalization performance. 
Learning Local Error Bars Using a Maximum Likelihood 
Framework: Motivation, Concept, and Mechanics 
Feed-forward artificial neural networks used for nonlinear regression can be interpreted as 
predicting the mean of the target distribution as a function of (conditioned on) the input 
pattern (e.g., Buntine & Weigend, 1991; Bishop, 1994), typically using one linear output unit 
per output variable. If parameterized, this conditional target distribution (CTD) may also be 
*http://www'cs'c�l�rad�'edu/andreas/H�me'html' 
This paper is avlable with figures in cobrs  ftp://ftp.cs.colorado.edu/pub/ 
Time-Series/MyPapers/nix.weigend_nips7.ps.Z 
490 David A. Nix, Andreas S. Weigend 
viewed as an error model (Rumelhart et al., 1995). Here, we present a simple method that 
provides higher-order information about the CTD than simply the mean. Such additional 
information could come from attempting to estimate the entire CTD with connectionist 
methods (e.g., Mixture Density Networks, Bishop, 1994; fractional binning, Srivastava 
& Weigend, 1994) or with non-connectionist methods such as a Monte Carlo on a hidden 
Markov model (Fraser & Dimitriadis, 1994). While non-parametric estimates of the shape 
of a CTD require large quantities of data, our less data-hungry method (Weigend & Nix, 
1994) assumes a specific parameterized form of the CTD (e.g., Gaussian) and gives us the 
value of the error bar (e.g., the width of the Gaussian) by finding those parameters which 
maximize the likelihood that the target data was generated by a particular network model. 
In this paper we derive the specific update rules for the Gaussian case. We would like to 
emphasize, however, that any parameterized unimodal distribution can be used for the CTD 
in the method presented here.  
! 
! 
hj 
O. O0 
Figure 1: Architecture of the network for estimating error bars using an auxiliary output unit. All 
weight layers have full connectivity. This architecture allows the conditional variance 2-unit access 
to both information in the input pattern itself and in the hidden unit representation formed while 
learning the conditional mean, ,0(x). 
We model the desired observed target value d as d(x) = y(x) + n(x), where y(x) is the 
underlying function we wish to approximate and n(x) is noise drawn from the assumed 
CTD. Just as the conditional mean of this CTD, V(x), is a function of the input, the 
variance rr 2 of the CTD, the noise level, may also vary as a function of the input x 
(noise heterogeneity). Therefore, not only do we want the network to learn a function 
.0(x) that estimates the conditional mean V(x) of the CTD, but we also want it to learn a 
function 82 (x) that estimates the conditional variance rr 2 (x). We simply add an auxiliary 
output unit, the b2-unit, to compute our estimate of rr 2 (x). Since rr 2 (x) must be positive, 
we choose an exponential activation function to naturally impose this bound: 82 (x) = 
exp [k wa2kh (x) +/3], where/3 is the offset (orbias), and wa2 is the weight between 
hidden unit k and the 2-unit. The particular connectivity of our architecture (Figure 1), 
in which the b2-unit has a hidden layer of its own that receives connections from both the 
.0-unit's hidden layer and the input pattern itself, allows great flexibility in learning 82 (x). 
In contrast, if the 2-unit has no hidden layer of its own, the 2-unit is constrained to 
approximate rr 2 (x) using only the exponential of a linear combination of basis functions 
(hidden units) already tailored to represent .0(x) (since learning the conditional variance 
b2(x) before learning the conditional mean .0(x) is troublesome at best). Such limited 
connectivity can be too constraining on the functional forms for ?r2 (x) and, in our experience, 
 The case of a single Gaussian to represent a unimodal distribution can also been generalized to a 
mixture of several Gaussians that allows the modeling of multimodal distributions (Bishop, 1994). 
Learning Local Error Bars for Nonlinear Regression 491 
produce inferior results. This is a significant difference compared to Bishop's (1994) 
Gaussian mixture approach in which all output units are directly connected to one set of 
hidden units. The other extreme would be not to share any hidden units at all, i.e., to 
employ two completely separate sets of hidden units, one to the 0(x)-unit, the other one to 
the 8 '2 (x)-unit This is the right thing to do ff there is indeed no overlap in the mapping 
from the inputs to y and from the inputs to 2. The two examples discussed in this paper are 
between these two extremes; this justifies the mixed architecture we use. Further discussion 
on shared vs. separate hidden units for the second example of the laser data is given by 
Kazlas & Weigend (1995, this volume). 
For one of our network outputs, the 0-unit, the target is easily available--it is simply given 
by d. But what is the target for the 32-unit? By maximizing the likelihood of our network - 
model iV given the data, P (iVI x, d), a target is invented as follows. Applying Bayes' mle 
and assuming statistical independence of the errors, we equivalently do gradient descent in 
the negative log likelihood of the targets d given the inputs and the network model, summed 
over all patterns i (see Rumelhart et al., 1995): C - - i in P(di Ixi, iV). Traditionally, 
the resulting form of this cost function involves only the estimate ,0(xi) of the conditional 
mean; the variance of the CrD is assumed to be constant for all xi, and the constant terms 
drop out after differentiation. In contrast, we allow the conditional variance to depend on 
x and explicitly keep these terms in C, approximating the conditional variance for x4 by 
.2 x 
(-). Given any network architecture and any parametric form for the CTD (i.e., any 
error model), the appropriate weight-update equations for gradient decent learning can be 
straighfforwardly derived. 
Assuming normally distributed errors around y(x) corresponds to a CTD density function 
[a,_y(x,)] 1 ^ 
of P(dilxi) = [27rrr2(xi)] -1/2 exp - 2,2(x,) j. Using the network output y(xi) m 
a 2 x, 2(xi) 
y(x4) to estimate the conditional mean and using the auxiliary output (.)  
to estimate the conditional variance, we ob!ain the monotonically related negative log 
[d-(x0] 2 
likelihood, - In P (di [xi, iV) = � In 2r3 '2 (xi) + 252(x,) . Summation over all patterns 
gives the total cost: 
. .2 x 
C- . a2(xi) +in ( .)+ln27r (1) 
To write explicit weight-update equations, we must specify the network unit transfer func- 
tions. Here we choose a linear activation function for the -unit, tanh functions for the 
hidden units, and an exponential function for the 3'2-unit. We can then take derivatives of 
the cost C with respect to the network weights. To update weights connected to the .0 and 
'2-units we have: 
1 
Awj = a2(x)[a - hj(x) (2) 
1 
y( hk(xi) (3) 
Awa2 k = r/232(xi ){[di- ^ X 2 
where r/is the learning rate. For weights not connected to the output, the weight-update 
equations are derived using the chain rule in the same way as in standard backpropagation. 
Note that Eq. (3) is equivalent to training a separate function-approximation network for 
'2(x) where the targets are the squared errors [di -- y(Xi)]2]. Note also that if 3'2(xi) is 
492 David A. Nix, Andreas S. Weigend 
constant, Eqs. (1)-(2) reduce to their familiar forms for standard backpropagation with a 
sum-squared error cost function. 
The 1/32(x) term in Eqs. (2)-(3) can be interpreted as a form of weighted regression, 
increasing the effective learning rate in low-noise regions and reducing it in high-noise 
regions. As a result, the network emphasizes obtaining small errors on those patterns where 
it can (low 32); it discounts learning patterns for which the expected error is going to be large 
anyway (large 32). This weighted-regression term can itself be highly beneficial where 
outliers (i.e., samples from high-noise regions) would ordinarily pull network resources 
away from fitting low-noise regions which would otherwise be well approximated. 
For simplicity, we use simple gradient descent learning for training. Other nonlinear mini- 
mization techniques could be applied, however, but only if the following problem is avoided. 
If the weighted-regression term described above is allowed a significant influence early in 
learning, local minima frequently result. This is because input patterns for which low errors 
are initially obtained are interpreted as low noise in Eqs. (2)-(3) and overemphasized 
in learning. Conversely, patterns for which large errors are initially obtained (because 
significant learning of .0 has not yet taken place) are erroneously discounted as being in 
high-noise regions and little subsequent learning takes place for these patterns, leading 
to highly-suboptimal solutions. Th
