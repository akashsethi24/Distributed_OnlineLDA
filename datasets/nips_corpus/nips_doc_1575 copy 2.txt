Semiparametric Support Vector and 
Linear Programming Machines 
Alex J. Smola, Thilo T. Friefl, and Bernhard Sch51kopf 
GMD FIRST, Rudower Chaussee 5, 12489 Berlin 
{smola, friess, bs} @first.gmd.de 
Abstract 
Semiparametric models are useful tools in the case where domain 
knowledge exists about the function to be estimated or emphasis is 
put onto understandability of the model. We extend two learning 
algorithms - Support Vector machines and Linear Programming 
machines to this case and give experimental results for SV ma- 
chines. 
I Introduction 
One of the strengths of Support Vector (SV) machines is that they are nonparamet- 
ric techniques, where one does not have to e.g. specify the number of basis functions 
beforehand. In fact, for many of the kernels used (not the polynomial kernels) like 
Gaussian rbf-kernels it can be shown [6] that SV machines are universal approxi- 
mators. 
While this is advantageous in general, parametric models are useful techniques in 
their own right. Especially if one happens to have additional knowledge about the 
problem, it would be unwise not to take advantage of it. For instance it might be 
the case that the major properties of the data are described by a combination of a 
small set of linear independent basis functions {tl ('),..., tn(')}. Or one may want 
to correct the data for some (e.g. linear) trends. Secondly it also may be the case 
that the user wants to have an understandable model, without sacrificing accuracy. 
For instance many people in life sciences tend to have a preference for linear models. 
This may be some motivation to construct semiparametric models, which are both 
easy to understand (for the parametric part) and perform well (often due to the 
nonparametric term). For more advocacy on semiparametric models see [1]. 
A common approach is to fit the data with the parametric model and train the non- 
parametric add-on on the errors of the parametric part, i.e. fit the nonparametric 
part to the errors. We show in Sec. 4 that this is useful only in a very restricted 
586 A. J. Smola, T. T. Friefi and B. Sch6lkopf 
situation. In general it is impossible to find the best model amongst a given class 
for different cost functions by doing so. The better way is to solve a convex op- 
timization problem like in standard SV machines, however with a different set of 
admissible functions 
n 
f(x): (w,�(x)) + (1) 
i=1 
Note that this is not so much different from the classical SV [10] setting where one 
uses functions of the type 
f(x):(w,�(x))+b. (2) 
2 Semiparametric Support Vector Machines 
Let us now treat this setting more formally. For the sake of simplicity in the 
exposition we will restrict ourselves to the case of SV regression and only deal with 
the e-insensitive loss function ]l = max{O, [l - e}. Extensions of this setting are 
straightforward and follow the lines of [7]. 
Given a training set of size �, X := {(x, y),..., (x, y)} one tries to find a function 
f that minimizes the functional of the expected risk  
R[f] -- / c(f(x) - y)p(x,y)dxdy. 
(3) 
Here c() denotes a cost function, i.e. how much deviations between prediction 
and actual training data should be penalized. Unless stated otherwise we will use 
= 
As we do not know p(x, y) we can only compute the empirical risk/emp[f] (i.e. the 
training error). Yet, minimizing the latter is not a good idea if the model class is 
sufficiently rich and will lead to overfitting. Hence one adds a regularization term 
T[f] and minirazes the regularized risk functional 
treg[f]--  �(f(Xi) -- Yi) q- AT[f] 
i=1 
with , > 0. (4) 
The standard choice in SV regression is to set T[f] =  12. 
This is the point of departure from the standard SV approach. While in the latter 
f is described by (2), we will expand f in terms of (1). Effectively this means that 
there exist functions �(-),... ,�,(.) whose contribution is not regularized at all. 
If n is sufficiently smaller than � this need not be a major concern, as the VC- 
dimension of this additional class of linear models is n, hence the overall capacity 
control will still work, provided the nonparametric part is restricted sufficiently. 
Figure I explains the effect of choosing a different structure in detail. 
Solving the optimization equations for this particular choice of a regularization 
term, with expansion (1), the e-insensitive loss function and introducing kernels 
More general definitions, mainly in terms of the cost function, do exist but for the 
sake of clarity in the exposition we ignored these cases. See [10] or [7] for further details 
on alternative definitions of risk functionals. 
Semiparametric Support Vector and Linear Programming Machines 587 
Figure 1: Two different nested subsets (solid and dotted lines) of hypotheses and the 
optimal model (+) in the realizable case. Observe that the optimal model is already 
contained in much a smaller (in this diagram size corresponds to the capacity of 
a subset) subset of the structure with solid lines than in the structure denoted by 
the dotted lines. Hence prior knowledge in choosing the structure can have a large 
effect on generalization bounds and performance. 
following [2] we arrive at the following primal optimization problem: 
minimize 
11wll 2 q- E i q- iF 
i=1 
subject to 
n 
(w, �(x)) + E jOj(xi) - y _<  + ? 
j=l 
n 
y- (w, �(x)) -  j(x) _<  +  
j=l 
Here k(x, x') has been written as ((x), (x')). Solving (5) for its Wolfe dual yields 
maximize 
(6) 
subject to 
{ (Oti--O?)j(Xi) -- 
i----1 
O i  0 E 
0 for all 1 _< j _< n 
[0, 1/] 
Note the similarity to the standard SV regression model. The objective function 
and the box constraints on the Lagrange multipliers ai, a? remain unchanged. The 
only modification comes from the additional unregularized basis functions. Whereas 
in the standard SV case we only had a single (constant) function b. I we now have 
an expansion in the basis ic)i('). This gives rise to n constraints instead of one. 
Finally f can be found as 
t n 
f(x): Z(a,-a.)k(x,,x) + y,,,(x) since w: 
i=1 i=1 i----1 
(7) 
The only difficulty remaining is how to determine fii. This can be done by exploiting 
the Karush-Kuhn-Tucker optimality conditions, or much more easily, by using an 
interior point optimization code [9]. In the latter case the variables i can be 
obtained as the dual variables of the dual (dual dual = primal) optimization problem 
(6) as a by product of the optimization process. This is also how these variables 
have been obtained in the experiments in the current paper. 
588 A. J. Smola, T. T. Friefi and B. SchOlkopf 
3 Semiparametric Linear Programming Machines 
Equation (4) gives rise to the question whether not completely different choices of 
regularization functionals would also lead to good algorithms. Again we will allow 
functions as described in (7). Possible choices are 
i=1 
or 
T[f] = y] Icti - ct:l (9) 
i=1 
or � n 
1 
r[S]: I-, - -;I + (10) 
i=1 i,j=l 
for some positive semidefinite matrix M. This is a simple extension of existing 
methods like Basis Pursuit [3] or Linear Programming Machines for classification 
(see e.g. [4]). The basic idea in all these approaches is to have two different sets 
of basis functions that are regularized differently, or where a subset may not be 
regularized at all. This is an efficient way of encoding prior knowledge or the 
preference of the user as the emphasis obviously will be put mainly on the functions 
with little or no regularization at all. Eq. (8) is essentially the SV estimation model 
where an additional linear regularization term has been added for the parametric 
part. In this case the constraints of the optimization problem (6) change into 
-1 < 
(ai - a;)oj(x) _< 1 
i----1 
oq, o? � [0, l/A] 
for all 1 _< j _< n 
(11) 
It makes little sense (from a technical viewpoint) to compute Wolfe's dual objective 
function in (10) as the problem does not get significantly easier by doing so. The 
best approach is to solve the corresponding optimization problem directly by some 
linear or quadratic programming code, e.g. [9]. Finally (10) can be reduced to the 
case of (8) by renaming variables accordingly and a proper choice of M. 
4 Why Backfitting is not sufficient 
One might think that the approach presented above is quite unnecessary and overly 
complicated for semiparametric modelling. In fact, one could try to fit the data to 
the parametric model first, and then fit the nonparametric part to the residuals. 
In most cases, however, this does not lead to finding the 'minimum of (4). We will 
show this at a simple example. 
Take a SV machine with linear kernel (i.e. k(x,x') = (x,x')) in one dimension and 
a constant term as parametric part (i.e. f(x) = wx + fi). This is one of the simplest 
semiparametric SV machinek possible. Now suppose the data was generated by 
Yi = Xi where xi _> 1 (12) 
without noise. Clearly then also Yi _> 1 for all i. By construction the best overall fit 
of the pair (/, w) will be arbitrarily close to (0, 1) if the regularization parameter 
is chosen sufficiently small. 
For backfitting one first carries out the parametric fit to find a constant , minimizing 
the term 'i=1 c(yi -/). Depending on the chosen cost function c(.),/ will be the 
mean (L2-error), the median (Ll-error), etc., of the set {Yl,... ,Y}. As all Yi _> 1 
Semiparametric Support l/ector and Linear Programming Machines 589 
+ 
1: 
Figure 2: Left: Basis functions used in the toy example. Note the different length 
scales of sin x and sinc 2rx. For convenience the functions were shifted by an offset 
of 2 and 4 respectively. Right: Training data denoted by '+', nonparametric (dash- 
dotted line), semiparametric (solid line), and parametric regression (dots). The 
regularization constant was set to  = 2. Observe that the semiparametric model 
picks up the characteristic wiggles of the original function. 
also  >_ 1 which is surely not the optimal
