Edges are the 'Independent Components' of 
Natural Scenes. 
Anthony J. Bell and Terrence J. Sejnowski 
Computational Neurobiology Laboratory 
The Salk Institute 
10010 N. Torrey Pines Road 
La Jolla, California 92037 
tony@salk.edu, terry@salk.edu 
Abstract 
Field (1994) has suggested that neurons with line and edge selectivities 
found in primary visual cortex of cats and monkeys form a sparse, dis- 
tributed representation of natural scenes, and Barlow (1989) has reasoned 
that such responses should emerge from an unsupervised learning algorithm 
that attempts to find a factorial code of independent visual features. We 
show here that non-linear 'infomax', when applied to an ensemble of nat- 
ural scenes, produces sets of visual filters that are localised and oriented. 
Some of these filters are Gabor-like and resemble those produced by the 
sparseness-maximisation network of Olshausen & Field (1996). In addition, 
the outputs of these filters are as independent as possible, since the info- 
max network is able to perform Independent Components Analysis (ICA). 
We compare the resulting ICA filters and their associated basis functions, 
with other decorrelating filters produced by Principal Components Analysis 
(PCA) and zero-phase whitening filters (ZCA). The ICA filters have more 
sparsely distributed (kurtotic) outputs on natural scenes. They also resem- 
ble the receptive fields of simple cells in visual cortex, which suggests that 
these neurons form an information-theoretic co-ordinate system for images. 
I Introduction. 
Both the classic experiments of Hubel & Wiesel [8] on neurons in visual cortex, and sev- 
eral decades of theorising about feature detection in vision, have left open the question 
most succinctly phrased by Barlow Why do we have edge detectors? That is: are there 
any coding principles which would predict the formation of localised, oriented receptive 
832 A. J. Bell and T. J. Sejnowski 
fields? Barlow's answer is that edges are suspicious coincidences in an image. Formalised 
information-theoretically, this means that our visual cortical feature detectors might be the 
end result of a redundancy reduction process [4, 2], in which the activation of each feature 
detector is supposed to be as statistically independent from the others as possible. Such a 
'factorial code' potentially involves dependencies of all orders, but most studies [9, 10, 2] 
(and many others) have used only the second-order statistics required for decorrelating the 
outputs of a set of feature detectors. Yet there are multiple decorrelating solutions, includ- 
ing the 'global' unphysiological Fourier filters produced by PCA, so further constraints are 
required. 
Field [7] has argued for the importance of sparse, or 'Minimum Entropy', coding [4], in which 
each feature detector is activated as rarely as possible. Olshausen & Field demonstrated [12] 
that such a sparseness criterion could be used to self-organise localised, oriented receptive 
fields. 
Here we present similar results using a direct information-theoretic criterion which maximises 
the joint entropy of a non-linearly transformed output feature vector [5]. Under certain 
conditions, this process will perform Independent Component Analysis (or ICA) which is 
equivalent to Barlow's redundancy reduction problem. Since our ICA algorithm, applied to 
natural scenes, does produce local edge filters, Barlow's reasoning is vindicated. Our ICA 
filters are more sparsely distributed than those of other decorrelating filters, thus supporting 
some of the arguments of Field (1994) and helping to explain the results of Olshausen's 
network from an information-theoretic point of view. 
2 Blind separation of natural images. 
A perceptual system is exposed to a series of small image patches, drawn from an ensemble 
of larger images. In the linear image synthesis model [12], each image patch, represented 
by the vector x, has been formed by the linear combination of N basis functions. The basis 
functions form the columns of a fixed matrix, A. The weighting of this linear combination 
(which varies with each image) is given by a vector, s. Each component of this vector 
has its own associated basis function, and represents an underlying 'cause' of the image. 
Thus: x-As. The goal of a perceptual system, in this simplified framework, is to linearly 
transform the images, x, with a matrix of filters, W, so that the resulting vector: u-- Wx, 
recovers the underlying causes, s, possibly in a different order, and rescaled. For the sake 
of argument, we will define the ordering and scaling of the causes so that W - A -1. But 
what should determine their form? If we choose decorrelation, so that (uu T) - I, then the 
solution for W must satisfy: 
wTw-- (xxT) -1 (1) 
There are several ways to constrain the solution to this: 
(1) Principal Components Analysis Wl (PCA), is the Orthogonal (global) solution 
[WW T = I]. The PCA solution to Eq.(1) is Wl = D-ï¿½E T, where D is the diagonal 
matrix of eigenvalues, and E is the matrix who's columns are the eigenvectors. The fil- 
ters (rows of Wl) are orthogonal, are thus the same as the PCA basis functions, and are 
typically global Fourier filters, ordered according to the amplitude spectrum of the image. 
Example PCA filters are shown in Fig. la. 
(2) Zero-phase Components Analysis Wz (ZCA), is the Symmetrical (local) solution 
[WW T = W2]. The ZCA solution to Eq.(1) is Wz = (xxT) -1/2 (matrix square root). 
ZCA is the polar opposite of PCA. It produces local (centre-surround type) whitening fil- 
Edges are the Independent Components of Natural Scenes 833 
ters, which are ordered according to the phase spectrum of the image. That is, each filter 
whitens a given pixel in the image, preserving the spatial arrangement of the image and 
flattening its frequency (amplitude) spectrum. Wz is related to the transforms described 
by Atick & Redlich [3]. Example ZCA filters and basis functions are shown in Fig.lb. 
(3) Independent Components Analysis W (ICA), is the Factorised (semi-local) solution 
[fu(U) - I-Ii fu, (ui)]. Please see [5] for full references. The 'infomax' solution we describe 
here is related to the approaches in [5, 1, 6]. 
As we will show, in Section 5, ICA on natural images produces decorrelating filters which 
are sensitive to both phase (locality) and spatial frequency information, just as in transforms 
involving oriented Gabor functions or wavelets. Example ICA filters are shown in Fig.ld 
and their corresponding basis functions are shown in Fig.le. 
3 An ICA algorithm. 
It is important to recognise two differences between finding an ICA solution, Wt, and other 
decorrelation methods. (1) there may be no ICA solution, and (2) a given ICA algorithm 
may not find the solution even if it exists, since there are approximations involved. In 
these senses, ICA is different from PCA and ZCA, and cannot be calculated analytically, for 
example, from second order statistics (the covariance matrix), except in the gaussian case. 
The approach Which we developed in [5] (see there for further references to ICA) was to 
maximise by stochastic gradient ascent the joint entropy, H[g(u)], of the linear transform 
squashed by a sigmoidal function, g. When the non-linear function is the same (up to scal- 
ing and shifting) as the cumulative density functions (c.d.f.s) of the underlying independent 
components, it can be shown (Nadal & Parga 1995) that such a non-linear 'infomax' pro- 
cedure also minimises the mutual information between the ui, exactly what is required for 
ICA. In most cases, however, we must pick a non-linearity, g, without any detailed knowledge 
of the probability density functions (p.d.f.s) of the underlying independent components. In 
cases where the p.d.f.s are super-gaussian (meaning they are peakier and longer-tailed than 
a gaussian, having kurtosis greater than 0), we have repeatedly observed, using the logistic 
or tanh nonlinearities, that maximisation of H[g(u)] still leads to ICA solutions, when they 
exist, as with our experiments on speech signal separation [5]. Although the infomax algo- 
rithm is described here as an ICA algorithm, a fuller understanding needs to be developed 
of under exactly what conditions it may fail to converge to an ICA solution. 
The basic infomax algorithm changes weights according to the entropy gradient. Defin- 
ing Yi -' g(ui) to be the sigmoidally transformed output variables, the stochastic gradient 
learning rule is: 
AW cr OH(y) = E = E[W -T + $rx T] (2) 
ow ow j 
In this, E denotes expected value, y = [g(ul)... g(uN)] T, and IJI is the absolute value of the 
determinant of the Jacobian matrix: J = det [Oyi/Oxj]ij , and  = [ ... N] T, the elements 
of which depend on the nonlinearity according to: i = O/Oyi(Oyi/Oui). 
Amari, Cichocki & Yang [1] have proposed a modification of this rule which utilises the 
natural gradient rather than the absolute gradient of H(y). The natural gradient exists for 
objective functions which are functions of matrices, as in this case, and is the same as the 
relative gradient concept developed by Cardoso & Laheld (1996). It amounts to multiplying 
834 A. J. Bell and T. J. Sejnowski 
the absolute gradient by wTw, giving, in our case, the following altered version of Eq.(2): 
0H(y) WW = (I + u')W (3) 
This rule has the twin advantages over Eq.(2) of avoiding the matrix inverse, and of con- 
verging several orders of magnitude more quickly, for data, x, that is not prewhitened. The 
speedup is explained by the fact that convergence is no longer dependent on the condi- 
tioning of the underlying basis function matrix, A. Writing Eq.(3) for one weight gives 
AWij (X Wij q- i Ek WkjUk. This rule is 'almost local' requiring a backwards pass. 
5 
7 
11 
15 
22 
37 
6O 
63 
89 
109 
144 
(a) (b) 
PCA ZCA 
(c) (d) 
W ICA 
(e) 
A 
Figure 1: Selected decorrelating filters 
and their basis functions extracted from 
the natural scene
