MLP can provably generalise much better 
than VC-bounds indicate. 
A. Kowalczyk and H. Ferrfi 
Telstra Research Laboratories 
770 Blackburn Road, Clayton, Vic. 3168, Australia 
((a.kowalczyk, h.ferra)@trl.oz.au) 
Abstract 
Results of a study of the worst case learning curves for a partic- 
ular class of probability distribution on input space to MLP with 
hard threshold hidden units are presented. It is shown in partic- 
ular, that in the thermodynamic limit for scaling by the number 
of connections to the first hidden layer, although the true learning 
curve behaves as  a - for a  1, its VC-dimension based bound 
is trivial (= 1) and its VC-entropy bound is trivial for a _< 6.2. It 
is also shown that bounds following the true learning curve can be 
derived from a formalism based on the density of error patterns. 
1 Introduction 
The VC-formalism and its extensions link the generalisation capabilities of a binary 
vlued neural network with its counting function  , e.g. via upper bounds implied by 
VC-dimension or VC-entropy on this function' [17, 18]. For linear perceptrons the 
counting function is constant for almost every selection of a fixed number of input 
samples [2], and essentially equal to its upper bound determined by VC-dimension 
and Sauer's Lemma. However, in the case for multilayer percepttons (MLP) the 
counting function depends essentially on the selected input samples. For instance, 
it has been shown recently that for MLP with sigmoidal units although the largest 
number of input samples which can be shattered, i.e. VC-dimension, equals fl(w 2) 
[6], there is always a non-zero probability of finding a (2w + 2)-element input sample 
which cannot be shattered, where w is the number of weights in the network [16]. 
In the case of MLP using Heaviside rather than sigmoidal activations (McCulloch- 
Pitts neurons), a similar claim can be made: VC-dimension is (wlog27-l) [13, 15], 
Known also as the partition function in computational learning theory. 
MLP Can Provably Generalize Much Better than VC-bounds Indicate 191 
where w is the number of weights to the first hidden layer of 7-/1 units, but there is 
a non-zero probability of finding a sample of size w + 2 which cannot be shattered 
[7, 8]. The results on these hard to shatter samples for the two MLP types 
differ significantly in terms of techniques used for derivation. For the sigmoidal 
case the result is existential (based on recent advances in model theory) while 
in the Heaviside case the proofs are constructive, defining a class of probability 
distributions from which hard to shatter samples can be drawn randomly; the 
results in this case are also more explicit in that a form for the counting function 
may be given [7, 8]. 
Can the existence of such hard to shatter samples be essential for generalisation 
capabilities of MLP? Can they be an essential factor for improvement of theoretical 
models of generalisation? In this paper we show that at least for the McCulloch- 
Pitts case with specific (continuous) probability distributions on the input space 
the answer is yes. We estimate directly the real learning curve in this case and 
show that its bounds based on VC-dimension or VC-entropy are loose at low learning 
sample regimes (for training samples having less than 12 x wl examples) even for 
the linear perceptron. We also show that a modification to the VC-formalism given 
in [9, 10] provides a significantly better bound. This latter part is a more rigorous 
and formal extension and re-interpretation of some results in [11, 12]. All the results 
are presented in the thermodynamic limit, i.e. for MLP with wl - cx and training 
sample size increasing proportionally, which simplifies their mathematical form. 
2 Overview of the formalism 
On a sample space X we consider a class H of binary functions h: X - {0, 1} 
which we shall call a hypothesis space. Further we assume that there are given a 
probability distribution/ on X and a target concept t: X - {0,1}. The quadruple 
� = (X,/, H, t) will be called a learning system. 
In the usual way, with each hypothesis h 6 H we associate the generalization error 
aef aef 1 Zi=l It(xi) - h(xi)l for 
eh - Ex lit(x) - h(x)l] and the training error eh,i = 
any training m-sample ' = (xl,..., Xm) 6 X m. 
Given a learning threshold 0 _  _ 1, let us introduce an auxiliary random variable 
eax() aej max(ca; h 6 H & ea,i _ ) for  6 X m, giving the worst general- 
ization error of all hypotheses with training error _  on the m-sample  6 X m. 2 
The basic objects of interest in this paper are the learning curve 3 defined as 
e,C(m) cted Exm [eax()]. 
2.1 Thermodynamic limit 
Now we introduce the thermodynamic limit of the learning curve. The underly- 
ing idea of such asymptotic analysis is to capture the essential features of learning 
In this paper max(S), where $ C 1, denotes the maximal element in the closure of $, 
or cx if no such element exists. Similarly, we understand min(S). 
aNote that our learning curve is determined by the worst generalisation error of accept- 
able hypotheses and in this respect differs from average generalisation error learning 
curves considered elsewhere, e.g. [3, 5]. 
192 A. Kowalczyk and H. Ferrd 
systems of very large size. Mathematically it turns out that in the thermodynamic 
limit the functional forms of learning curves simplify significantly and analytic char- 
acterizations of these are possible. 
We are given a sequence of learning systems, or shortly, �v - (Xv,/v,Hv, tv), 
N = 1, 2, ... and a scaling N  rv 6 R +, with the property rv - c; the scaling 
can be thought of as a measure of the size (complexity) of a learning system, e.g. 
VC-dimension of HN. The thermodynamic limit of scaled learning curves is defined 
for a > 0 as follows 4 
wc der 
exoo(a ) = limsupe',CN([arNJ), 
(1) 
Here, and below, the additional subscript N refers to the N-th learning system. 
2.2 Error pattern density formalism 
This subsection briefly presents a thermodynamic version of a modified VC formal- 
ism discussed previously in [9]; more details and proofs can be found in [10]. The 
main innovation of this approach comes from splitting error patterns into error shells 
and using estimates on the size of these error shells rather than the total number 
of error patterns. We shall see on examples discussed in the following section that 
this improves results significantly. 
The space {0, 1} ra of all binary m-vectors naturally splits into m + 1 error pattern 
shells E?, i = 0, 1, ..., m, with the i-th shell composed of all vectors with exactly i 
entries equal to 1 . For each h 6 H and :? = (x,..., xm)  X m, let ffh()  {0, 1} m 
denote a vector (error pattern) having 1 in the j-th position if and only if h(x)  
t(xj). As the i-th error shell has (.) elements, the average error pattern density 
falling into this error shell is 
An der (m) -1 
= i h 
(i = O, 1, ..., m), 
(2) 
where # denotes the cardinality of a set 5 . 
Theorem I Given a sequence of learning systems �N = (XN , !aN, HN , tN), a scal- 
ing rv and a function go: R + x (0, 1) -> R + such that 
m m 
In (Ai,v) _< -rNq , + o(rv), (3) 
for all m, N = 1, 2, ..., 0 < i < m. 
Then 
we 
xoo(a) _< ex(a), 
(4) 
4We recall that lxJ denotes the largest integer <_ x and limsup/v_ x/v is defined as 
lim/v- of the monotonic sequence N - max{xl,X2,..., xiv}. Note that in contrast to 
the ordinary limit, lim sup always exists. 
5Note the difference to the concept of error shells used in [4] which are partitions of the 
finite hypothesis space H according to the generalisation error values. Both formalisms 
are related though, and the central result in [4], Theorem 4, can be derived from our 
Theorem I below. 
MLP Can Provably Generalize Much Better than VC-bounds Indicate 193 
for any 0 _<  < i and a,  > O, where 
max 
e e (0, 1); 30<y< a(7/(y) +/7/(x)) - o [a + a, -- 
exl 
1+/ - 
and 7-/(y) e__/-Y In y - (1 - y) ln(1 - y) denotes the entropy function. 
3 Main results: applications of the formalism 
3.1 VC-bounds 
We consider a learning sequence �N = (Xv,laN,HN, tN), tNC Hv (realisable 
case) and the scaling of this sequence by VC-dimension [17], i.e. we assume rN -- 
dvc(HN) - oo. The following bounds for the N-th learning system can be derived 
for  - 0 (consistent learning case) [1, 17]: 
e',v(m) _ /0 rain 1,22-m/2 d,c'-N)J de. (5) 
In the thermodynamic limit, i.e. as N -, oo, we get for any a  1/e 
wc [ 2 log2(2ea) ] (6) 
Coo o(a) _ min 1, a ' 
Note that this bound is independent of probability distributions 
3.2 Piecewise constant functions 
Let PC(d) denote the class of piecewise constant binary functions on the unit 
segment [0, 1) with up to d _ 0 discontinuities and with their values defined as 
i at all these discontinuity points. We consider here the learning sequence �N -- 
([0, 1),/N, PC(dN),tN) where /zN is any continuous probability distributions on 
[0, 1), dN is a monotonic sequence of positive integers diverging to oo and targets 
tN  PC(dray) are such that the limit 5t ef limN-oo s exists. (Without loss of 
--- dt N 
generality we can assume that all/ are the uniform distribution on [0, 1).) 
For this learning sequence the following can be established. 
Claim 1. The following function det]ned for a > 1 and 0 _ x _ 1 as 
der 
and as O, otherwise, stisqes assumption (3) with respect to the scMing rv = dv. 
Claim 2. The following two sided bound on the learning curve holds: 
wc i (1+ ln(2a+)) 
i (l+ln(2a-)) < eXoo(a ) < 2- 
2c- -- -- 
(7) 
for a > 1, 0 _ ) _ 1 and 0 _ 6t _ a)/2, where 
der a 0 + de._.f 
194 A. Kowalczyk and H. Ferrd 
We outline the main steps of proof of these two claims now. 
For Claim i we start with a combinatorial argument establishing that in the par- 
ticular case of constant target 
A i ,mN -- i - ! 
1 
Edrl /2 ( ra--i--1 
for d + dt < min(2i,2(m - i)), 
otherwise. 
Next we obse
