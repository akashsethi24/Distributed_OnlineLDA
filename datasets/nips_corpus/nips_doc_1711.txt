Robust Neural Network Regression for Offiine 
and Online Learning 
Thomas Briege!* 
Siemens AG, Corporate Technology 
D-81730 Munich, Germany 
thomas.briegel@mchp.siemens.de 
Volker Tresp 
Siemens AG, Corporate Technology 
D-81730 Munich, Germany 
volker. tresp@mchp.siemens.de 
Abstract 
We replace the commonly used Gaussian noise model in nonlinear 
regression by a more flexible noise model based on the Student-t- 
distribution. The degrees of freedom of the t-distribution can be chosen 
such that as special cases either the Gaussian distribution or the Cauchy 
distribution are realized. The latter is commonly used in robust regres- 
sion. Since the t-distribution can be interpreted as being an infinite mix- 
ture of Gaussians, parameters and hyperparameters such as the degrees 
of freedom of the t-distribution can be learned from the data based on an 
EM-leaming algorithm. We show that modeling using the t-distribution 
leads to improved predictors on real world data sets. In particular, if 
outliers are present, the t-distribution is superior to the Gaussian noise 
model. In effect, by adapting the degrees of freedom, the system can 
learn to distinguish between outliers and non-outliers. Especially for 
online learning tasks, one is interested in avoiding inappropriate weight 
changes due to measurement outliers to maintain stable online learn- 
ing capability. We show experimentally that using the t-distribution as 
a noise model leads to stable online learning algorithms and outperforms 
state-of-the art online learning methods like the extended Kalman filter 
algorithm. 
1 INTRODUCTION 
A commonly used assumption in nonlinear regression is that targets are disturbed by inde- 
pendent additive Gaussian noise. Although one can derive the Gaussian noise assumption 
based on a maximum entropy approach, the main reason for this assumption is practica- 
bility: under the Gaussian noise assumption the maximum likelihood parameter estimate 
can simply be found by minimization of the squared error. Despite its common use it is far 
from clear that the Gaussian noise assumption is a good choice for many practical prob- 
lems. A reasonable approach therefore would be a noise distribution which contains the 
Gaussian as a special case but which has a tunable parameter that allows for more flexible 
distributions. In this paper we use the Student-t-distribution as a noise model which con- 
tains two free parameters - the degrees of freedom , and a width parameter o -2. A nice 
feature of the t-distribution is that if the degrees of freedom , approach infinity, we recover 
the Gaussian noise model. If , < c we obtain distributions which are more heavy-tailed 
than the Gaussian distribution including the Cauchy noise model with , - 1. The latter 
* Now with McKinsey & Company, Inc. 
408 T. Briegel and V. Tresp 
is commonly used for robust regression. The first goal of this paper is to investigate if the 
additional free parameters, e.g. v, lead to better generalization performance for real world 
data sets if compared to the Gaussian noise assumption with v = oc. The most common 
reason why researchers depart from the Gaussian noise assumption is the presence of out- 
liers. Outliers are errors which occur with low probability and which are not generated by 
the data-generation process that is subject to identification. The general problem is that a 
few (maybe even one) outliers of high leverage are sufficient to throw the standard Gaus- 
sian error estimators completely off-track (Rousseeuw & Leroy, 1987). In the second set of 
experiments we therefore compare how the generalization performance is affected by out- 
liers, both for the Gaussian noise assumption and for the t-distribution assumption. Dealing 
with outliers is often of critical importance for online learning tasks. Online learning is of 
great interest in many applications exhibiting non-stationary behavior like tracking, sig- 
nal and image processing, or navigation and fault detection (see, for instance the NIPS*98 
Sequential Learning Workshop). Here one is interested in avoiding inappropriate weight 
chances due to measurement outliers to maintain stable online learning capability. Outliers 
might result in highly fluctuating weights and possible even instability when estimating the 
neural network weight vector online using a Gaussian error assumption. State-of-the art 
online algorithms like the extended Kalman filter, for instance, are known to be nonrobust 
against such outliers (Meinhold & Singpurwalla, 1989) since they are based on a Gaussian 
output error assumption. 
The paper is organized as follows. In Section 2 we adopt a probabilistic view to outlier 
detection by taking as a heavy-tailed observation error density the Student-t-distribution 
which can be derived from an infinite mixture of Gaussians approach. In our work we use 
the multi-layer perceptron (MLP) as nonlinear model. In Section 3 we derive an EM algo- 
rithm for estimating the MLP weight vector and the hyperparameters offiine. Employing 
a state-space representation to model the MLP's weight evolution in time we extend the 
batch algorithm of Section 3 to the online learning case (Section 4). The application of the 
computationally efficient Fisher scoring algorithm leads to posterior mode weight updates 
and an online EM-type algorithm for approximate maximum likelihood (ML) estimation 
of the hyperparameters. In in the last two sections (Section 5 and Section 6) we present 
experiments and conclusions, respectively. 
2 THE t-DENSITY AS A ROBUST ERROR DENSITY 
We assume a nonlinear regression model where for the t-th data point the noisy target 
Yt  I is generated as 
Yt = g(xt;w) + v (1) 
and xt  k is a k-dimensional known input vector. g(.; wt) denotes a neural network 
model characterized by weight vector wt  n, in our case a multi-layer perceptron 
(MLP). In the offiine case the weight vector wt is assumed to be a fixed unknown constant 
vector, i.e. wt -- w. Furthermore, we assume that vt is uncorrelated noise with density 
Pvt (.). In the offiine case, we assume Pvt (.) to be independent of t, i.e. pv (.) _= p (.). In 
the following we assume that p (.) is a Student-t-density with , degrees of freedom with 
F(+i ( z2) ,+ 
\ 2  
p(z) = T(zlae, v) = aF(-) 1 + cr-5- , v,a > 0. (2) 
It is immediately appent at for v = 1 we recover e heavy-tailed Cauchy density. at 
is not so obvious is at for v   we obtain a Gaussi density. For e derivation of 
� e EM-leing rules in e next section it is important to note at the t-denstiy can be 
ought of as being  infinite mixture of Gaussians of the fore 
Robust Neural Network Regression for Offiine and Online Learning 409 
3 
2 
1 
o 
-1 
-2 
-3 
Boaton Housing data with additive outliers 
t.2 
0.71-    .,,,,,,, .I, l 
o -   ;o ; 2'0 2 
number of outller [%] 
Figure 1: Left: p(.)-functions for the Gaussian density (dashed) and t-densities with v = 
1, 4,15 degrees of freedom. Right: MSE on Boston Housing data test set for additive 
oufliers. The dashed line shows results using a Gaussian error measure and the continuous 
line shows the results using the Student-t-distribution as error measure. 
where T(zl a2, v) is the Student-t-density with v degrees of freedom and width parameter 
a 2, A/'(zl0, a/u) is a Gaussian density with center 0 and variance a/u and  
where X is a Chi-square distribution with v degrees of freedom evaluated at  > 0. 
To compare different noise models it is useful to evaluate the /,-function defined as (Hu- 
ber, 1964) 
l,(z) = -0 logp,(z)/Oz (4) 
i.e. the negative score-function of the noise density. In the case of i.i.d. samples the p- 
function reflects the influence of a single measurement on the resulting estimator. Assum- 
ing Gaussian measurement errors pv(z) = A/'(zlO, a ) we derive p(z) = z/a  which 
means that for Izl - o a single outlier z can have an infinite leverage on the estimator. In 
contrast, for constructing robust estimators West (1981) states that large outliers should not 
have any influence on the estimator, i.e. p(z) -+ 0 for Izl -* o. Figure 1 (left) shows p(z) 
for different v for the Student-t-distribution. It can be seen that the degrees of freedom v 
determine how much weight outliers obtain in influencing the regression. In particular, for 
finite v, the influence of outliers with Izl -. o approaches zero. 
3 ROBUST OFFLINE REGRESSION 
As stated in Equation (3), the t-density can be thought of as being generated as an infinite 
mixture of Gaussians. Maximum likelihood adaptation of parameters and hyperparameters 
can therefore be performed using an EM algorithm (Lange et al., 1989). For the t-th sample, 
a complete data point would consist of the triple (xt, Yt, ut) of which only the first two are 
known and at is missing. 
In the E-step we estimate for every data point indexed by t 
Ot: (t/�ld q- 1)/(Y �ld q- (it) 
(5) 
where at = E[utlYt, xt] is the expected value of the unknown ut given the available data 
(xt, Yt ) and where 6t = (Yt - g(xt ; w�ld)) 2/0.2,old ' 
In the M-step the weights w and the hyperparameters a 2 and v are optimized using 
T 
w new = argm2n{Eat(Ut-a(xt;w)) (6) 
t=l 
410 T. Briegel and I. Tresp 
T 
t=l 
/]new .__ argmax{ Tv v 
log - Tlog{r()} 
T T 
t=l t=l 
where 
t = DG ( v�ld + 1 
2 ) - og( + (9) 
wi e Digamma function DG(z) = OF(z)/Oz. Note at e M-step for v is a one- 
dimensional nonlinem optimization problem. Also note at e M-steps for e weights in 
� e MLP reduce to a weighted least squmes regression problem in which outliers tend to 
be weighted down. e exception of course is e Gaussi case wi v   in which all 
tes obtain equal weight. 
4 ROBUST ONLINE REGRESSION 
For robust online regression, we assume that the model Equation (1) is still valid but that 
w can change over time, i.e. w _= wt. In particular we assume that wt follows a first order 
random walk
