511 
CONVERGENCE AND PATTERN STABILIZATION 
IN THE BOLTZMANN MACHINE 
Moshe Kam 
Dept. of Electrical and Computer Eng. 
Drexel University, Philadelphia PA 19104 
Roger Cheng 
Dept. of Electrical Eng. 
Princeton University, NJ 08544 
ABSTRACT 
The Boltzmann Machine has been introduced as a means to perform 
global optimization for multimodal objective functions using the 
principles of simulated annealing. In this paper we consider its utility 
as a spurious-free content-addressable memory, and provide bounds on 
its performance in this context. We show how to exploit the machine's 
ability to escape local minima, in order to use it, at a constant 
temperature, for unambiguous associative pattern-retrieval in noisy 
environments. An association rule, which creates a sphere of influence 
around each stored pattern, is used along with the Machine's dynamics 
to match the machine's noisy input with one of the pre-stored patterns. 
Spurious fixed points, whose ragions of attraction are not recognized by 
the rule, are skipped, due to the Machine's finite probability to escape 
from any state. The results apply to the Boltzmann machine and to the 
asynchronous net of binary threshold elements (*rlopfield model'). They 
provide the network designer with worst-case and best-case bounds for 
the network's performance, and allow polynomial-time tradeoff studies 
of design parameters. 
I. INTRODUCTION 
The suggestion that artificial neural networks can be utilized for classification, pattern 
recognition and associative recall has been the main theme of numerous studies which 
appeared in the last decade (e.g. Rumelhart and McClelland (1986) and Grossberg (1988) - 
and their references.) Among the most popular families of neural networks are fully 
connected networks of binary threshold elements (e.g. Amari (1972), Hopfield (1982).) 
These structures, and the related family of fully connected networks of sigmoidal threshold 
elements have been used as error-correcting decoders in many applications, among which 
were interesting applications in optimization (Hopfield and Tank, 1985; Tank and 
Hopfield, 1986; Kennedy and Chua, 1987.) A common drawback of the many studied 
schemes is the abundance of 'spurious' local minima, which 'trap' the decoder in 
undesirable, and often non-interpretable, states during the process of input / stored-pattern 
association. It is generally accepted now that while the number of arbitrary binary patterns 
that can be stored in a fully-connected network is of the order of magnitude of N (N = 
number of the neurons in the network,) the number of created local attractors in the 
512 Kam and Cheng 
network's state space is exponential in N. 
It was proposed (Ackley et al., 1985; Hinton and Sejnowski, 1986) that fully-connected 
binary neural networks, which update their states on the basis of  
state-reassessment rules, could be used for global optimization when the objective 
function is multi-modal. The suggested architecture, the Boltzmann machine, is based on 
the principles of simulated annealing ( Kirkpatrick et al., 1983; Geman and Geman, 1984; 
Aarts et al., 1985; Szu, 1986,) and has been shown to perform interesting tasks of 
decision making and optimization. However, the learning algorithm that was proposed for 
the Machine, along with its cooling procedures, do not lend themselves to real-time 
operation. Most studies so far have concentrated on the properties of the Machine in 
global optimization and only few studies have mentioned possible utilization of the 
Machine (at constant 'temperature') as a content-addressable memory (e.g. for local 
optimization.) 
In the present paper we propose to use the Boltzmann machine for associative retrieval, 
and develop bounds on its performance as a content-addressable memory. We introduce a 
learning algorithm for the Machine, which locally maximizes the stabilization probability 
of learned patterns. We then proceed to calculate (in polynomial time) upper and lower 
bounds on the probability that a tuple at a given initial Hamming distance from a stored 
pattern will get attracted to that pattern. A proposed association rule creates a sphere of 
influence around each stored pattern, and is indifferent to 'spurious' attractors. Due to the 
fact that the Machine has a nonzero probability of escape from any state, the 'spurious' 
attractors are ignored. The obtained bounds allow the assessment of retrieval probabilities, 
different learning algorithms and necessary learning periods, network 'temperatures' and 
coding schemes for items to be stored. 
H. THE MACHINE AS A CONTENT ADDRESSABLE 
MEMORY 
The Boltzmann Machine is a multi-connected network of N simple processors called 
probabilistic binary neurons. The ith neuron is characterized by N-1 real numbers 
representing the synaptic weights (wij, j=l,2,...,i-l,i+l,...,N; wii is assumed to be zero 
for all i), a real threshold (xi) and a binary activity level (ui  B ={-1,1},) which we 
shall also refer to as the neuron's state. The binary N-tuple U = [Ul,U2,. � � ,UN] is called 
the network's state. We distinguish between two phases of the network's operation: 
a) The learning phase - when the network parameters wij and xi are determined. This 
determination could be achieved through autonomous learning of the binary pattern 
environment by the network (unsupervised learning); through learning of the environment 
with the help of a 'teacher' which supplies evaluafive reinforcement signals (supervised 
learning); or by an external fixed assignment of parameter values. 
b) Th? production phase - when the network's state U is determined. This determination 
could be performed synchronously by all neurons at the same predetermined time instants, 
or asynchronously - each neuron reassesses its state independently of the other neurons at 
random times. The decisions of the neurons regarding their states during reassessment can 
be arrived at deterministically (the set of neuron inputs determines the neuron's state) or 
Convergence and Pattern-Stabilization 513 
stochastically (the set of neuron inputs shapes a probability distribution for the 
state-selection law.) 
We shall describe first the (asynchronous and stochastic) production rule which our 
network employs. At random times during the production phase, asynchronously and 
independently of all other neurons, the i th neuron decides upon its next state, using the 
probabilistic decision rule 
1 with probabilty 
(1) 
-1 with probabilty 'ar where 
l+e-' 
N 
AEi= Z WijUj --'i 
is called the ith energy gap, and Te is a predetermined real number called temperature. 
The state-updating rule (1) is related to the network's energy level which is described by 
E--- i=l 
If the network is to find a local minimum of E in equation (2), then the ith neuron, when 
chosen (at random) for state updating, should choose deterministically 
u i = sgn Wij Uj -- 'r, i . (3) 
Lj=l,ji 
W'e note that rule in equation (3) is obtained from rule in equation (1) as Te - 0. This 
deterministic choice of ui guarantees, under symmetry conditions on the weights 
(wij=wji), that the network's state will stabilize at afixed point in the 2N-tuple state 
space of the network (Hopfield, 1982), where 
D�finition 1; A state Uf BN is called a fixed point in the state space of the N-neuron 
network ff 
PJ(n+l)--Uf I U(h)-Uf]= 1. (4) 
A fixed point found through iterations of equation (3) (with i chosen at random at each 
iteration) may not be the global minimum of the energy in equation (2). A mechanism 
which seeks the global minimum should avoid local-minimum traps by allowing 
'uphill' climbing with respect to the value of E. The decision scheme of equation (1) is 
devised for that purpose, allowing an increase in E with nonzero probability. This 
provision for progress in the locally 'wrong' direction in order to reap a 'global' advantage 
later, is in accordance with the principles of simulated annealing techniques, which are 
used in multimodal optimization. In our case, the probabilities of choosing the locally 
'fight' decision (equation (3)) and the locally 'wrong' decision are determined by the ratio 
and Oheng 
of the energy gap AF.i to the 'temperature' shaping constant Te. 
The Boltzmann Machine has been proposed for global minimization and a considerable 
amount of effort has been invested in devising a good cooling scheme, namely a means to 
control Te in order to guarantee the finding of a global minimum in a short time (Geman 
and Geman, 1984, Szu, 1987.) However, the network may also be used as a selective 
content addressable memory which does not suffer from inadvertently-installed spurious 
local minima. 
We consider the following application of the Boltzmann Machine as a scheme for pattern 
classification under noisy conditions: let an encoder emit a sequence of NX1 binary code 
vectors from a set of Q codewords (or 'patterns',) each having a probability of occurrence 
of rl m (m = 1,2,... ,Q). The emitted pattern encounters noise and distortion before it 
arrives at the decoder, resulting in some of its bits being reversed. The Boltzmann 
Machine, which is the decoder at the receiving end, accepts this distorted pattern as its 
initial state (U(0)), and observes the consequent time evolution of the network's state U. 
At a certain time instant no, the Machine will declare that the input pattern U(0) is to be 
associated with pattern Bm if U at that instant (u(n0)) is 'close enough' to Bin. For this 
purpose we define 
Definition 2: The dnmx-sphere of influence of pattern Bin, c( Bm, dmax) is 
O(Bm, dmax)= {U BN: HD(U, Bm) <- dmax}. (5) 
dmax is prespecified. 
Let Y_.(dmax) = u O(B m, Clma x ) and let n o be the smallest integer such that u(nO Y-.(dmax 
in 
Therule of association is' associate U 4�) with Bmat time n o, ifU (n0 which has evolved 
from U (�) satisfies: u4n0 o(B m, dmax). 
Due to the finite probability of escape from any minimum, 
