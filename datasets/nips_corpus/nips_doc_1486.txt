Outcomes of the Equivalence of Adaptive Ridge 
with Least Absolute Shrinkage 
Yves Grandvalet Stphane Canu 
Heudiasyc, UMR CNRS 6599, Universit de Technologie de Compigne, 
BP 20.529, 60205 Compigne cedex, France 
Yves. Grandvalet@hds .utc. fr 
Abstract 
Adaptive Ridge is a special form of Ridge regression, balancing the 
quadratic penalization on each parameter of the model. It was shown to 
be equivalent to Lasso (least absolute shrinkage and selection operator), 
in the sense that both procedures produce the same estimate. Lasso can 
thus be viewed as a particular quadratic penalizer. 
From this observation, we derive a fixed point algorithm to compute the 
Lasso solution. The analogy provides also a new hyper-parameter for tun- 
ing effectively the model complexity. We finally present a series of possi- 
ble extensions of lasso performing sparse regression in kernel smoothing, 
additive modeling and neural net training. 
1 INTRODUCTION 
In supervised learning, we have a set of explicative variables :e from which we wish to pre- 
dict a response variable y. To solve this problem, a learning algorithm is used to produce a 
predictor fx) from a learning set st = { (xi, Yi)}f=l of examples. The goal of prediction 
may be: 1) to provide an accurate prediction of future responses, accuracy being measured 
by a user-defined loss function; 2) to quantify the effect of each explicative variable in the 
response; 3) to better understand the underlying phenomenon. 
Penalization is extensively used in learning algorithms. It decreases the predictor variability 
to improve the prediction accuracy. It is also expected to produce models with few non-zero 
coefficients if interpretation is planned. 
Ridge regression and Subset Selection are the two main penalization procedures. The for- 
mer is stable, but does not shrink parameters to zero, the latter gives simple models, but is 
unstable [ 1 ]. These observations motivated the search for new penalization techniques such 
as Garrotte, Non-Negative Garrotte [ 1 ], and Lasso (least absolute shrinkage and selection 
operator) [ 10]. 
446 Y. Grandvalet and S. Canu 
Adaptive Ridge was proposed as a means to automatically balance penalization on different 
coefficients. It was shown to be equivalent to Lasso [4]. Section 2 presents Adaptive Ridge 
and recalls the equivalence statement. The following sections give some of the main out- 
comes of this connection. They concern algorithmic issues in section 3, complexity control 
in section 4, and some possible generalizations of lasso to non-linear regression in section 5. 
2 ADAPTIVE RIDGE REGRESSION 
For clarity of exposure, the formulae are given here for linear regression with quadratic loss. 
The predictor is defined as f(:e) = /3Y:e, with/3 y = (fi,... ,fid). Adaptive Ridge is a 
modification of the Ridge estimate, which is defined by the quadratic constraint y',j =  f? _< 
C applied to the parameters. It is usually computed by minimizing the Lagrangian 
g d d 
= Argmin- ( -fjZ,d- Yi)'+ A   , (1) 
 i=1 j=l 5=1 
where A is the Lagrange multiplier vawing with the bound C on the no of the pareters. 
When the ordina W least squares (OLS) estimate maximizes likelihood , the Ridge estimate 
may be seen as a maximum a posteriori estimate. The Bayes prior distribution is a centered 
nodal distribution, with variance propogional to 1/A. This prior distribution treats all co- 
variates similarly. It is not appropriate when we ow that all covariates are not equally 
relevant. 
O 
The gagotte estimate [ 1 ] is based on the OLS estimate . The standard quadratic constraint 
27/e  C. The coecients with smaller OLS estimate are thus 
is replaced by J=  ,_ ,_ _ 
more heavily penalized. Other modifications are better explained with the prior distribu- 
tion viewoint. Mixtures of Gaussians may be used to cluster different set of covariates. 
Several models have been proposed, with data dependent clusters [9], or classes defined a 
priori [7]. The Automatic Relevance Deteination model [8] ras in the latter Wpe. In [4], 
we propose to use such a mixture, in the fo 
t d d 
 in ( jxij - yi)   Aj (2) 
 i:1 j=l j=l 
Here, each coefficient has its own prior distribution. The priors are centered normal distri- 
butions with variances proportional to 1/A;. To avoid the simultaneous estimation of these 
d hyper-parameters by trial, the constraint 
j=l 
, A. > 0 (3) 
is applied on ,X = (A,..., Ad) T, where A is a predefined value. This constraint is a link 
between the d prior distributions. Their mean variance is proportional to 1/A. The values of 
Aj are automatically 2 induced from the sample, hence the qualifter adaptative. Adaptativity 
refers here to the penalization balance on {fj }, not to the tuning of the hyper-parameter A. 
 If {:e, } are independently and identically drawn from some distribution, and that some,O* exists, 
such that Y, - ,0' :rze, + s, where s is a centered normal random variable, then the empirical cost 
based on the quadratic loss is proportional to the log-likelihood of the sample. The OLS estimate/3 
is thus the maximum likelihood estimate 
2Adaptive Ridge, as Ridge or Lasso, is not scale invariant, so that the covariates should be nor- 
malized to produce sensible estimates. 
Equivalence of Adaptive Ridge with Least Absolute Shrinkage 44 7 
It was shown [4] that Adaptive Ridge and least absolute value shrinkage are equivalent, in 
the sense that they yield the same estimate. We remind that the Lasso estimate is defined by 
g d d 
 = Argmin - (-/35 a:i/- yi) 2 subject to I/jl < (4) 
 i=l 5=1 j=l 
The only difference in the definition of the Adaptive Ridge and the Lasso estimate is that 
the Lagrangian form of Adaptive Ridge uses the constraint (-]J=i I/3J [)2/d < tC e. 
3 OPTIMIZATION ALGORITHM 
Tibshirani [ 10] proposed to use quadratic programming to find the 1.asso solution, with 2d 
variables (positive and negative parts of/3) and 2d + 1 constraints (signs of positive and 
negative parts of/3 plus constraint (4)). Equations (2) and (3) suggest to use a fixed point 
(FP) algorithm. At each step s, the FP algorithm estimates the optimal parameters A} s) of 
the Bayes prior based on the estimate/3* - 1), and then maximizes the posterior to compute 
the current estimate .8 s) 
As the parameterization , ,k) may lead to divergent solutions, we define new variables 
7; = , and c j= for j= 1,...,d . 
The FP algorithm updates alternatively c and , as follows: 
(5) 
,() = 
d7}S-1) 2 
a (-i) 2 
-]k= 1 7k 
(diag(c())X r Xdiag(c ()) + AI) 
- i diag(c(S ))X t t 
(6) 
where z�ij ---- Xij , I is the identity matrix, and diag(c) is the square matrix with the vector 
� on its diagonal. 
The algorithm can be initialized by the Ridge or the OLS estimate. In the latter case, fl(1) is 
the garrotte estimate. 
Practically, 'if'}- l) is small compared to numerical accuracy, then c} ) is set to zero. In 
turn, 7} � is zero, and the system to be solved in the second step to determine , can be 
reduced to the other variables. If cj is set to zero at any time during the optimization pro- 
cess, the final estimate j will be zero. The computations are simplified, but it is not clear 
whether global convergence can be obtained with this algorithm. It is easy to show the con- 
vergence towards a local minimum, but we did not find general conditions ensuring global 
convergence. If these conditions exist, they rely on initial conditions. 
Finally, we stress that the optimality conditions for c (or in a less rigorous sense for ,k) do 
not depend on the first part of the cost minimized in (2). In consequence, the equivalence 
between Adaptive Ridge and lasso holds for any model or loss function. The FP algorithm 
can be applied to these other problems, without modifying the first step. 
4 COMPLEXITY TUNING 
The Adaptive Ridge estimate depends on the learning set st and on the hyper-parameter 
,X. When the estimate is defined by (2) and (3), the analogy with Ridge suggests A as the 
448 Y. Grandvalet and S. Canu 
natural hyper-parameter for tuning the complexity of the regressor. As , goes to zero,  
approaches the OLS estimate/3, and the number of effective parameters is d. As , goes to 
infinity,/3 goes to zero and the number of effective parameters is zero. 
When the estimate is defined by (4), there is no obvious choice for the hyper-parameter con- 
d d .--o 
trolling complexity. Tibshirani[10]proposedtousev 5-'j= [jl/ j As goes 
- I. 
to one, approaches; as v goes to iniw,goes to zero. 
The weaess of v is that it is explicitly defined from the OLS estimate. As a result, it is 
variable when the design matrix is badly conditioned. The estimation of v is thus harder, 
and the overall procedure looses in stabiliW. This is illustrated on an experiment following 
Breiman's benchark [ 1 ] with 30 highly co,elated predictors  (Xj X ) = plJ- 1, with 
p = 1 -- 10 -3. 
We generate 1000 i.i.d. samples of size  = 0. For each sample s, the modeling er- 
ror (ME) is computed for several values of v and X. We select v  d X  achieving the 
lowest ME. For one sample, there is a one to one mapping from v to X. Thus ME is the 
same for v  and X . Then, we compute v* and * achieving the best average ME on 
the 1000 samples. As v  and A achieve the lowest ME for s, the ME for s is higher 
or equal for v* and X*. Due to the wide spread of {v }, the average loss encountered is 
� W ��� (ME(s,v* - ME(s, 10 - and 
twice for * than for A* 1/1000 : ) )) = 4.6 , 
1/1000 pooo (ME(s A*) - ME(s, A)) = 2.3 10 -. The average modeling eor are 
k=l  
ME(v*) = 1.9 10 -t and ME(A*) = 1.7 10 -t 
The estimates of prediction eor, such as leave-one-out cross-validation tend to be variable. 
Hence, complexiW tuning is often based on the minimization of some estimate of the mean 
prediction eor (e.g bootstrap, K-fold cross-validation). Our experiment suppos that, re- 

