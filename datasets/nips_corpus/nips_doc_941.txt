Multidimensional Scaling and Data Clustering 
Thomas Hofmann & Joachim Buhmann 
Rheinische Friedrich-Wilhelms-Universitit 
Institut fiir Informatik HI, R6merstra13e 164 
D-53117 Bonn, Germany 
email:(h, jb}@cs. uni-bonn. de 
Abstract 
Visualizing and structuring pairwise dissimilarity data are difficult combinatorial op- 
timization problems known as multidimensional scaling or pairwise data clustering. 
Algorithms for embedding dissimilarity data set in a EuclidJan space, for clustering 
these data and for actively selecting data to support the clustering process are discussed 
in the maximum entropy framework. Active data selection provides a strategy to discover 
structure in a data set efficiently with partially unknown data. 
1 Introduction 
Grouping experimental data into compact clusters arises as a data analysis problem in psy- 
chology, linguistics, genetics and other experimental sciences. The data which are supposed 
to be clustered are either given by an explicit coordinate representation (central clustering) 
or, in the non-metric case, they are characterized by dissimilarity values for pairs of data 
points (pairwise clustering). In this paper we study algorithms (i) for embedding non-metric 
data in a D-dimensional Euclidian space, (ii) for simultaneous clustering and embedding of 
non-metric data, and (iii) for active data selection to determine a particular cluster structure 
with minimal number of data queries. All algorithms are derived from the maximum entropy 
principle (Hertz et al., 1991) which guarantees robust statistics (Tikochinsky et al., 1984). 
The data are given by a real-valued, symmetric proximity matrix D 6 R Nx N, Z)kt being 
the pairwise dissimilarity between the data points k, I. Apart from the symmetry constraint 
we make no further assumptions about the dissimilarities, i.e., we do not require D being a 
metric. The numbers Z)kt quite often violate the triangular inequality and the dissimilarity of 
a datum to itself could be finite. 
2 Statistical Mechanics of Multidimensional Scaling 
Embedding dissimilarity data in a D-dimensional Euclidian space is a non-convex optimiza- 
tion problem which typically exhibits a large number of local minima. Stochastic search 
methods like simulated annealing or its deterministic variants have been very successfully 
460 Thomas Hofmann, Joachim Buhmann 
applied to such problems. 
{xi -'� in a D-dimensional Euclidian space with minimal embedding costs 
Ji----I 
N 
74MD s 1 
= [Ix- 
The question in multidimensional scaling is to find coordinates 
(1) 
i,k=l 
N 
Without loss of generality we shift the center of mass in the origin (]k:l xk = 0). 
In the maximum entropy framework the coordinates {xi} are regarded as random variables 
which are distributed according to the Gibbs distribution P({xi}) = exp(-/ (74 MDs- Or). The 
inverse temperature . = 1/T controls the expected embedding costs (74 MDs) (expectation val- 
ues are denoted by (.)). To calculate the free energy Or for 74 MDs we approximate the coupling 
,X l)ikXiXk/�  =1 xihi with the mean fields hi = 4 ]]kN__ T)ik(Xk)/N ' 
term 2 
Standard techniques to evaluate the free energy Or yield the equations 
5oo oo D 
-cx -oo d,d'=l 
D N oo 
.?(7- MDS) ---- 2 Z '/'d' /N Zln dxiexp(-3f(xi)), (3) 
d,d'=l i=1 
N 
2 
f(xi) - Ixi14-lxi12Y])ik +4x'7gxi+x/T(hi--4r)- (4) 
k=l 
The integral in Eq. (2) is dominated by the absolute minimum of Or in the limit N --4 o. 
Therefore, we calculate the saddle point equations 
N 
7. = ;Z ((x/x/T) +  (Ixi12)I) and 0 - N 
i=1 i=1 
(Xi) = f xiexp(-flf(xi)dxi 
f exp(-flf(xi)dxi (6) 
Equation (6) has been derived by differentiating Or with respect to hi. I denotes the D x D 
unit matrix. In the low temperature limit/7 --4 c the integral in (3) is dominated by the 
minimum of f (x). Therefore, a new estimate of (xi) is calculated minimizing f with respect 
to xi. Since all explicit dependencies between the xi have been eliminated, this minimization 
can be performed independently for all i, 1 _< i _< N. 
In the spirit of the EM algorithm for Gaussian mixture models we suggest the following 
algorithm to calculate a meanfield approximation for the multidimensional scaling problem. 
initialize (xl) � randomly; t----O. 
while z_,i=,l<xi) �--<xl)('-'>l > � 
E-step: estimate (Xl) (t+) as a 
M-step: 
function of (xl) �, (t), 
calculate ('), h? and determine (t) such 
that the centroid condition is satisfied. 
29 (), h? ) 
Multidimensional Scaling and Data Clustering 461 
This algorithm was used to determine the embedding of protein dissimilarity data as shown in 
Fig. ld. The phenomenon that the data clusters are arranged in a circular fashion is explained 
by the lack of small dissimilarity values. The solution in Fig. ld is about a factor of two 
better than the embedding found by a classical MDS program (Gower, 1966). This program 
determines a (N - 1)- space where the ranking of the dissimilarities is preserved and uses 
principle component analysis to project this tentative embedding down to two dimensions. 
Extensions to other MDS cost functions are currently under investigation. 
3 Multidimensional Scaling and Pairwise Clustering 
Embedding data in a Euclidian space precedes quite often a visual inspection by the data 
analyst to discover structure and to group data into clusters. The question arises how both 
problems, the embedding problem and the clustering problem, can be solved simultaneously. 
The second algorithm addresses the problem to embed a data set in a Euclidian space such 
that the clustering structure is approximated as faithfully as possible in the maximum entropy 
sense by the clustering solution in this embedding space. The coordinates in the embedding 
space are the free parameters for this optimization problem. 
Clustering of non-metric dissimilarity data, also called pairwise clustering (Buhmann, Hof- 
mann, 1994a), is a combinatorial optimization problem which depends on Boolean assign- 
ments Mir  {0, 1 } of datum i to cluster u. The cost function for pairwise clustering with 
K clusters is 
K N N N 
1 1 
�[5(M) = E 2p,N E E M,Mt,V, with Pv = /E M:,,. (7) 
v=l k=l /=1 k=l 
In the meanfield approach we approximate the Gibbs distribution P(�) corresponding 
to the original cost function by a family of approximating distributions. The distribution 
which represents most accurately the statistics of the original problem is determined by 
the minimum of the Kullback-Leibler divergence to the original Gibbs distribution. In the 
pairwise clustering case we introduce potentials {�k} for the effective interactions, which 
define a set of cost functions with non-interacting assignments. 
K N 
(8) 
t/=l k=l 
The optimal potentials derived from this minimization procedure are 
{'--k} = arg min DKL (P�(�}-)IIP(�;D), (9) 
where P�(��c) is the Gibbs distribution corresponding to �., and VK&ll') is the KL- 
divergence. This method is equivalent to minimizing an upper bound on the free energy 
(Buhmann, Hofmann, 1994b), 
$-, c,o, < 0$'o(�-) + (Vx)o, with Vx pc _ �o (10) 
[cK / -- = Six' K  
(.)0 denoting the average over all configurations of the cost function without interactions. 
Correlations between assignment variables are statistically independent for po(�}.), i.e., 
{3irk,2V/1,j}0 = {Mtw}0{3fl,}0. The averaged potential Vt,-, therefore, amounts to 
K N K N 
1 
('V,,-) = E E (Mtv)(Mlv)2pvNV6l -- E E ('M/v)�tCv' (11) 
t/=l k,l=l u=l k=l 
462 Thomas Hofmann, Joachim Buhmann 
the subscript of averages being omitted for conciseness. The expected assignment variables 
are 
exp(-fl�i,) (12) 
(Mir) -- K � 
E,=, exp(--fi�i,) 
Minimizing the upper bound yields 
K O(Mi,) (�i - �i,) -- 0. (13) 
0�i. 
The optimal potentials 
depend on the given distance matrix, the averaged assignment variables and the cluster 
probabilities. They are optimal in the sense, that if we set 
�iv --- �i q- �i (15) 
the N, K stationarity conditions (13) are fulfilled for every i  { 1, ..., N}, t,  { 1, ..., K}. A 
simultaneous solution of Eq. (15) with (12) constitutes a necessary condition for a minimum 
of the upper bound for the free energy Y. 
The connection between the clustering and the multidimensional scaling problem is estab- 
lished, if we restrict the potentials �i to be of the form Ix/ - yl 2 with the centroids 
N 
- = Mk. We consider the coordinates xi as the variational param- 
y,, = Mk,,x,/ N 
eters. The additional constraints restrict the family of approximating distributions, defined 
by co to a subset. Using the chain rule we can calculate the derivatives of the upper bound 
(10), resulting in the exact stationary conditions for xi, 
K N K 
Np x 
c,v= 1 j=l c%v=l 
(xj- 
(16) 
where A�i = & - �'. The derivatives O(Mk)/Ox can be exactly calculated, since they 
are given as the solutions of an linear equation system with N x K unknowns for every xi. To 
reduce the computational complexity an approximation can be derived under the assumption 
Oya/Oxi  O. In this case the right hand side of (16) can be set to zero in a first order 
approximation yielding an explicit formula for xi, 
(17) 
with the covariance matrix ]Ci ((yyT)i (y)i(y)T) and (y)i K 
= - 
The derived system of transcendental equations given by (12), (17) and the centroid condi- 
tion explicitly reflects the dependencies between the clustering procedure and the Euclidian 
representation. Solving these equations simultaneously leads to an efficient algorithm which 
Multidimensional Scaling and Data Clustering 463 
a 
HA 
GGI 
MY 
HBX, 
HF, HE 
GP 
GGG 
HG,HE 
% HBX,HF, HE 
 MY 
 dP ' 
GG 1 x'x GG G 
x 
x 
+ 
HB  
++ + 
 + MY 
HG,HE,HF 
420 
380 
340 
300 
Random Selection 
2000 4000 6000 
# of selected D 
Figure 1: Similarity matrix of 145 protein sequencesof the globin family (a): dark gray levels 
correspond to high similarity values; (b): clustering with embedding in two dimensions; (c): 
multidimensional scaling solu
