Spectral Cues in Human Sound Localization 
Craig T. Jin 
Department of Physiology and 
Department of Electrical Engineering, 
Univ. of Sydney, NSW 2006, Australia 
Anna Corderoy 
Department of Physiology 
Univ. of Sydney, NSW 2006, Australia 
Simon Carlile 
Department of Physiology 
and Institute of Biomedical Research 
Univ. of Sydney, NSW 2006, Australia 
Andr6 van Schaik 
Department of Electrical Engineering, 
Univ. of Sydney, NSW 2006, Australia 
Abstract 
The differential contribution of the monaural and interaural spectral 
cues to human sound localization was examined using a combined psy- 
chophysical and analytical approach. The cues to a sound's location 
were correlated on an individual basis with the human localization re- 
sponses to a variety of spectrally manipulated sounds. The spectral cues 
derive from the acoustical filtering of an individual's auditory periphery 
which is characterized by the measured head-related transfer functions 
(HRTFs). Auditory localization performance was determined in virtual 
auditory space (VAS). Psychoacoustical experiments were conducted in 
which the amplitude spectra of the sound stimulus was varied indepen- 
dently at each ear while preserving the normal timing cues, an impossibil- 
ity in the free-field environment. Virtual auditory noise stimuli were gen- 
erated over earphones for a specified target direction such that there was 
a false flat spectrum at the left eardrum. Using the subject's HRTFs, 
the sound spectrum at the right eardrum was then adjusted so that either 
the true right monaural spectral cue or the true interaural spectral cue 
was preserved. All subjects showed systematic mislocalizations in both 
the true right and true interaural spectral conditions which was absent in 
their control localization performance. The analysis of the different cues 
along with the subjects' localization responses suggests there are signif- 
icant differences in the use of the monaural and interaural spectral cues 
and that the auditory system's reliance on the spectral cues varies with 
the sound condition. 
1 Introduction 
Humans are remarkably accurate in their ability to localize transient, broadband noise, an 
ability with obvious evolutionary advantages. The study of human auditory localization has 
a considerable and rich history (recent review [ 1]) which demonstrates that there are three 
general classes of acoustical cues involved in the localization process: (l) interaural time 
differences, ITDs; (2) interaural level differences, ILDs; and (3) the spectral cues resulting 
Spectral Cues in Human Sound Localization 769 
from the auditory periphery. It is generally accepted that for humans, the ITD and ILD 
cues only specify the location of the sound source to within a cone of confusion [1], 
i.e., a locus of points approximating the surface of a cone symmetric with respect to the 
interaural axis. It remains, therefore, for the localization system to extract a more precise 
sound source location from the spectral cues. 
The utilization of the outer ear spectral cues during sound localization has been analyzed 
both as a statistical estimation problem, (e.g., [2]) and as optimization problem, often using 
neural networks, (e.g., [3]). Such computational models show that sufficient localization 
information is provided by the spectral cues to resolve the cone of confusion ambiguity 
which corroborates the psychoacoustical evidence. Furthermore, it is commonly argued 
that the interaural spectral cue, because of its natural robustness to level and spectral vari- 
ations, has advantages over the monaural spectral cues alone. Despite these observations, 
there is still considerable contention as to the relative role or contribution of the monaural 
versus the interaural spectral cues. 
In this study, each subject's spectral cues were characterized by measuring their head re- 
lated transfer functions (HRTFs) for 393 evenly distributed positions in space. Measure- 
ments were carried out in an anechoic chamber and were made for both ears simultane- 
ously using a blocked ear technique [ 1 ]. Sounds filtered with the HRTFs and played over 
earphones, which bypass the acoustical filtering of the outer ear, result in the illusion of 
free-field sounds which is known as virtual auditory space (VAS). The HRTFs were used to 
generate virtual sound sources in which the spectral cues were manipulated systematically. 
The recorded HRTFs along with the Glasberg and Moore cochlear model [4] were also 
used to generate neural excitation patterns (frequency representations of the sound stim- 
ulus within the auditory nerve) which were used to estimate the different cues available 
to the subject during the localization process. Using this analysis, the interaural spectral 
cue was characterized and the different localization cues have been correlated with each 
subjects' VAS localization responses. 
2 VAS Sound Localization 
The sound localization performance of four normal hearing subjects was examined in VAS 
using broadband white noise (300 - 14 000 Hz). The stimuli were filtered under three 
differing spectral conditions. (1) control: stimuli were filtered with spectrally correct left 
and right ear HRTFs for a given target location, (2) veridical interaural: stimuli at the 
left ear were made spectrally flat with an appropriate dB sound level for the given target 
location, while the stimuli at the right ear were spectrally shaped to preserve the correct 
interaural spectrum, (3) veridical right monaural: stimuli at the left ear were spectrally 
flat as in the second condition, while the stimuli at the right ear were filtered with the 
correct HRTF for the given target location, resulting in an inappropriate interaural spectral 
difference. For each condition, a minimum-phase filter spectral approximation was made 
and the interaural time difference was modeled as an all-pass delay [5]. Sounds were 
presented at approximately 70 dB SPL and with duration 150 ms (with 10 ms raised-cosine 
onset and offset ramps). Each subject performed five trials at each of 76 test positions for 
each stimulus condition. Detailed sound localization methods can be found in [ 1 ]. A short 
summary is presented below. 
2.1 Sound Localization Task 
The human localization experiments were carded out in a darkened anechoic chamber. 
Virtual auditory sound stimuli were presented using earphones (ER-2, Etymbtic Research, 
with a flat frequency response, within 3 dB, between 200-16 000 Hz). The perceived 
location of the virtual sound source was indicated by the subject pointing his/her nose in 
770 C. T. din, A. Corderoy, S. Carlile and A. v. Schaik 
the direction of the perceived source. The subject's head orientation and position were 
monitored using an electromagnetic sensor system (Polhemus, Inc.). 
2.2 Human Sound Localization Performance 
The sound localization performance of two subjects in the three different stimulus condi- 
tions are shown in Figure 1. The pooled data across 76 locations and five trials is presented 
for both the left (L) and right (R) hemispheres of space from the viewpoint of an outside 
observer. The target location is shown by a cross and the centroid of the subjects responses 
for each location is shown by a black dot with the standard deviation indicated by an ellipse. 
Front-back confusions are plotted, although, they were removed for calculating the standard 
deviations. The subjects localized the control broadband sounds accurately (Figure 1 a). In 
contrast, the subjects demonstrated systematic mislocalizations for both the veridical inter- 
aural and veridical monaural spectral conditions (Figures lb,c). There is clear pulling of 
the localization responses to particular regions of space with evident intersubject variations. 
(a) Subject 1: Broadband Control 
Nose Cross: Target location 
(b) Subject 1: Veridical Interaural Spectrum 
(c) Subject 1: Veridical Right Monaural Spectrum 
Subject 2: Broadband Control 
Dot. Centtold location 
of Responses 
Ellipse: Standard Deviation 
K.: of Responses 
: R 
.... :._ _ .-'T.  '+' :, 
Subject 2: Veridical Interaural Spectrum 
Subject 2: Veridical Right Monaural Spectrum 
Figure 1: Localization performance for two subjects in the three sound conditions: (a) 
control broadband; (b) veridical interaural; (c) veridical monaural. See text for details. 
3 Extraction of Acoustical Cues 
With accurate measurements of each individual's outer ear filtering, the different acousti- 
cal cues can be compared with human localization performance on an individual basis. In 
order to extract the different acoustical cues in a biologically plausible manner, a model 
of peripheral auditory processing was used. A virtual source sound stimulus was prepared 
as described in Secion 2 for a particular target location. The stimulus was then filtered 
using a cochlear model based on the work of Glasberg and Moore [4]. This cochlear model 
consisted of a set of modified rounded-exponential auditory filters. The width and shape 
of the auditory filters change as a function of frequency (and sound level) in a manner 
Spectral Cues in Human Sound Localization 771 
consistent with the known physiological and psychophysical data. These filters were log- 
arithmically spaced on the frequency axis with a total of 200 filters between 300 Hz and 
14 kHz. The cochlea's compressive non-linearity was modelled mathematically using a 
logarithmic function. Thus the logarithm of the output energy of a given filter indicated the 
amount of neural activity in that particular cochlear channel. 
The relative activity across the different cochlear channels was representative of the neu- 
ral excitation pattern (EP) along the auditory nerve and it is from this excitation pattern 
that the different spectral cues were estimated. For a given location, the left and right EPs 
themselves represent the monaural spectral cues. The difference in 
