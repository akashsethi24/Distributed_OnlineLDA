Analytical Mean Squared Error Curves 
in Temporal Difference Learning 
Satinder $ingh 
Department of Computer Science 
University of Colorado 
Boulder, CO 80309-0430 
bavej a@cs. colorado.edu 
Peter Dayan 
Brain and Cognitive Sciences 
E25-210, MIT 
Cambridge, MA 02139 
bertsekas@lids.mit.edu 
Abstract 
We have calculated analytical expressions for how the bias and 
variance of the estimators provided by various temporal difference 
value estimation algorithms change with offline updates over trials 
in absorbing Markov chains using lookup table representations. We 
illustrate classes of learning curve behavior in various chains, and' 
show the manner in which TD is sensitive to the choice of its step- 
size and eligibility trace parameters. 
I INTRODUCTION 
A reassuring theory of asymptotic convergence is available for many reinforcement 
learning (RL) algorithms. What is not available, however, is a theory that explains 
the finite-term learning curve behavior of RL algorithms, e.g., what are the different 
kinds of learning curves, what are their key determinants, and how do different 
problem parameters effect rate of convergence. Answering these questions is crucial 
not only for making useful comparisons between algorithms, but also for developing 
hybrid and new RL methods. In this paper we provide preliminary answers to some 
of the above questions for the case of absorbing Markov chains, where mean square 
error between the estimated and true predictions is used as the quantity of interest 
in learning curves. 
Our main contribution is in deriving the analytical update equations for the two 
components of MSE, bias and variance, for popular Monte Carlo (MC) and TD() 
(Sutton, 1988) algorithms. These derivations are presented in a larger paper. Here 
we apply our theoretical results to produce analytical learning curves for TD on 
two specific Markov chains chosen to highlight the effect of various problem and 
algorithm parameters, in particular the definite trade-offs between step-size, a, and 
eligibility-trace parameter, A. Although these results are for specific problems, we 
Analytical MSE Curves for TD Learning 1055 
believe that many of the conclusions are intuitive or have previous empirical support, 
and may be more generally applicable. 
2 ANALYTICAL RESULTS 
A random walk, or trial, in an absorbing Markov chain with only terminal payoffs 
produces a sequence of states terminated by a payoff. The prediction task is to 
determine the expected payoff as a function of the start state i, called the optimal 
value function, and denoted v*. Accordingly, v = E{rlsl = i}, where st is the 
state at step t, and r is the random terminal payoff. The algorithms analysed are 
iterative and produce a sequence of estimates of v* by repeatedly combining the 
result from a new trial with the old estimate to produce a new estimate. They have 
the form: vi(t) = vi(t- 1) + a(t)Si(t) where v(t) = {vi(t)} is the estimate of the 
optimal value function after t trials, 5i(t) is the result for state i based on random 
trial t, and the step-size c(t) determines how the old estimate and the new result 
are combined. The algorithms differ in the 5s produced from a trial. 
Monte Carlo algorithms use the final payoff that results from a trial to define the 
5i(t) (e.g., Barto & Duff, 1994). Therefore in MC algorithms the estimated value of a 
state is unaffected by the estimated value of any other state. The main contribution 
of TD algorithms (Sutton, 1988) over MC algorithms is that they update the value 
of a state based not only on the terminal payoff but also on the the estimated 
values of the intervening states. When a state is first visited, it initiates a short- 
term memory process, an eligibility trace, which then decays exponentially over time 
with parameter A. The amount by which the value of an intervening state combines 
with the old estimate is determined in part by the magnitude of the eligibility trace 
at that point. 
In general, the initial estimate v(0) could be a random vector drawn from some 
distribution, but often v(0) is fixed to some initial value such as zero. In either case, 
subsequent estimates, v(t); t > 0, will be random vectors because of the random 
The random vector v(t) has a bias vector b(t) de=/ E{v(t)- v*} and a covariance 
matrix C(t) de__1 E{(v(t)- E{v(t)})(v(t)- E{v(t)})T}. The scalar quantity of 
interest for learning curves is the weighted MSE as a function of trial number t, and 
is defined as follows: 
MSr(t) = = 
where Pi -- (#T[I--Q]-I)i/E(r[- is the weight for state i, which is the 
expected number of visits to i in a trial divided by the expected length of a trial 1 
(#/' is the probability of starting in state i; Q is the transition matrix of the chain). 
In this paper we present results just for the standard TD(A) algorithm (Sutton, 
1988), but we have analysed (Singh & Dayan, 1996) various other TD-like algorithms 
(e.g., Singh & Sutton, 1996) and comment on their behavior in the conclusions. Our 
analytical results are based on two non-trivial assumptions: first that lookup tables 
are used, and second that the algorithm parameters a and A are functions of the 
trial number alone rather than also depending on the state. We also make two 
assumptions that we believe would not change the general nature of the results 
obtained here: that the estimated values are updated offiine (after the end of each 
trial), and that the only non-zero payoffs are on the transitions to the terminal 
states. With the above caveats, our analytical results allow rapid computation of 
exact mean square error (MSE) learning curves as a function of trial number. 
 Other reasonable choices for the weights, p,, would not change the nature of the results 
presented here. 
1056 S. Singh and P. Dayan 
2.1 BIAS, VARIANCE, And MSE UPDATE EQUATIONS 
The analytical update equations for the bias, variance and MSE are complex and 
their details are in Singh 2 Dayan (1996) -- they take the following form in outline: 
b(t) = a -- Bb(t- 1) (1) 
C(t) = A s -- BsC(t - 1)-- fS(b(t- 1)) (2) 
where matrix B depends linearly on a(t) and B s and fs depend at most quadrat- 
ically on a(t). We coded this detail in the ï¿½ programming language to develop a 
software tool 2 whose rapid computation of exact MSE error curves allowed us to ex- 
periment with many different algorithm and problem parameters on many Markov 
chains. Of course, one could have averaged together many empirical MSE curves 
obtained via simulation of these Markov chains to get approximations to the an- 
alytical MSE error curves, but in many cases MSE curves that take minutes to 
compute analytically take days to derive empirically on the same computer for five 
significant digit accuracy. Empirical simulation is particularly slow in cases where 
the variance converges to non-zero values (because of constant step-sizes) with long 
tails in the asymptotic distribution of estimated values (we present an example in 
Figure lc). Our analytical method, on the other hand, computes exact MSE curves 
for L trials in O(Istate spacel3L) steps regardless of the behavior of the variance 
and bias curves. 
2.2 ANALYTICAL METHODS 
Two consequences of having the analytical forms of the equations for the update 
of the mean and variance are that it is possible to optimize schedules for setting a 
and A and, for fixed A and a, work out terminal rates of convergence for b and C. 
Computing one-step optimal a's: Given a particular A, the effect on the MSE 
of a single step for any of the algorithms is quadratic in a. It is therefore straight- 
forward to calculate the value of a that minimises MSE(t) at the next time step. 
This is called the greedy value of a. It is not clear that if one were interested 
in minimising MSE(t + t'), one would choose successive a(u) that greedily min- 
imise MSE(t); MSE(t + 1); .... In general, one could use our formulee and dynamic 
programming to optimise a whole schedule for a(u), but this is computationally 
challenging. 
Note that this technique for setting greedy a assumes complete knowledge about 
the Markov chain and the initial bias and covariance of v(0), and is therefore not 
directly applicable to realistic applications of reinforcement learning. Nevertheless, 
it is a good analysis tool to approximate omniscient optimal step-size schedules, 
eliminating the effect of the choice of a when studying the effect of the A. 
Computing one-step optimal A's: Calculating analytically the A that would 
minimize MSE(t) given the bias and variance at trial t - 1 is substantially harder 
because terms such as [I- A(t)Q] -1 appear in the expressions. However, since it is 
possible to compute MSE(t) for any choice of A, it is straightforward to find to any 
desired accuracy the Ag(t) that gives the lowest resulting MSE(t). This is possible 
only because MSE(t) can be computed very cheaply using our analytical equations. 
The caveats about greediness in choosing ag(t) also apply to Ag(t). For one of the 
Markov chains, we used a stochastic gradient ascent method to optimise A(u) and 
2The analytical MSE error curve software is available via anonymous ftp from the 
following address: ftp.cs.colorado.edu /users/baveja/AUse.tar. Z 
Analytical MSE Curves for TD Learning 1057 
a(u) to minimise MSE(t + t') and found that it was not optimal to choose Ag(t) 
and ag(t) at the first step. 
Computing terminal rates of convergence: In the update equations I and 2, 
b(t) depends linearly on b(t- 1) through a matrix Bin; and C(t) depends linearly 
on C(t - 1) through a matrix B s. For the case of fixed a and A, the maximal and 
minimal eigenvalues of B m and B s determine the fact and speed of convergence 
of the algorithms to finite endpoints. If the modulus of the real part of any .of 
the eigenvalues is greater than 1, then the algorithms will not converge in general. 
We observed that the mean update is more stable than the mean squ
