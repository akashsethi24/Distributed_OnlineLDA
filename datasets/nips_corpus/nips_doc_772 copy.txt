Structured Machine Learning For 'Soft' 
Classification with Smoothing Spline 
ANOVA and Stacked Tuning, Testing 
and Evaluation 
Grace Wahba 
Dept of Statistics 
University of Wisconsin 
Madison, WI 53706 
Yuedong Wang 
Dept of Statistics 
University of Wisconsin 
Madison, WI 53706 
Chong Gu 
Dept of Statistics 
Purdue University 
West Lafayette, IN 47907 
Ronald Klein, MD 
Dept of Ophthalmalogy 
University of Wisconsin 
Madison, WI 53706 
Barbara Klein, MD 
Dept of Ophthalmalogy 
University of Wisconsin 
Madison, WI 53706 
Abstract 
We describe the use of smoothing spline analysis of variance (SS- 
ANOVA) in the penalized log likelihood context, for learning 
(estimating) the probability p of a '1' outcome, given a train- 
ing set with attribute vectors and outcomes. p is of the form 
p(t) - el(t)/(1 + el(t)), where, if t is a vector of attributes, f 
is learned as a sum of smooth functions of one attribute plus a 
sum of smooth functions of two attributes, etc. The smoothing 
parameters governing f are obtained by an iterative unbiased risk 
or iterative GCV method. Confidence intervals for these estimates 
are available. 
1. Introduction to 'soft' classification and the bias-variance tradeoff. 
In medical risk factor analysis records of attribute vectors and outcomes (0 or 1) 
for each example (patient) for n examples are available as training data. Based on 
the training data, it is desired to estimate the probability p of the i outcome for any 
415 
416 Wahba, Wang, Gu, Klein, and Klein 
new examples in the future, given their attribute vectors. In 'soft' classification, the 
estimate p of p is of particular interest, and might be used, say, by a physician to 
tell a patient that if he reduces his cholesterol from t to t', then he will reduce his 
risk of a heart attack from p(t) to/5(t'). We assume here that p varies 'smoothly' 
with any continuous attribute (predictor variable). 
It is long known that smoothness penalties and Bayes estimates are intimately re- 
lated (see e.g. Kimeldorf and Wahba(1970, 1971), Wahba(1990) and references 
there). Our philosophy with regard to the use of priors in Bayes estimates is to 
use them to generate families of reasonable estimates (or families of penalty func- 
tionals) indexed by those smoothing or regularization parameters which are most 
relevant to controlling the generalization error. (See Wahba(1990) Chapter 3, also 
Wahba(1992)). Then use cross-validation, generalized cross validation (GCV), un- 
biased risk estimation or some other performance oriented method to choose these 
parameter(s) to minimize a computable proxy for the generalization error. A person 
who believed the relevant prior might use maximum likelihood (ML) to choose the 
parameters, but ML may not be robust against an unrealistic prior (that is, ML 
may not do very well from the generalization point of view if the prior is off), see 
Wahba(1985). One could assign a hyperprior to these parameters. However, except 
in cases where real prior information is available, there is no reason to believe that 
the use of hyperpriors will beat out a performance oriented criterion based on a good 
proxy for the generalization error, assuming, of course, that low generalization error 
is the true goal. 
O'Sullivan et a/(1986) proposed a penalized log likelihood estimate of f, this work 
was extended to the SS-ANOVA context in Wahba, Gu, Wang and Chappell(1993), 
where numerous other relevant references are cited. This paper is available by 
ftp from ftp. stat.wisc.edu, cd pub/wahba in the file soft-class.ps.Z. An 
extended bibliography is available in the same directory as ml-bib.ps. The SS- 
ANOVA allows a variety of interpretable structures for the possible relationships 
between the predictor variables and the outcome, and reduces to simple relations 
in some of the attributes, or even, to a two-layer neural net, when the data suggest 
that such a representation is adequate. 
2. Soft classification and penalized log likelihood risk factor estimation 
To describe our 'worldview', let t be a vector of attributes, t  g2  T, where g2 is 
some region of interest in attribute space T. Our 'world' consists of an arbitrarily 
large population of potential examples, whose attribute vectors are distributed in 
some way over g2 and, considering all members of this 'world' with attribute vectors 
in a small neighborhood about t, the fraction of them that are 1's is p(t). Our 
training set is assumed to be a random sample of n examples from this population, 
whose outcomes are known, and our goal is to estimate p(t) for any t  fl. In 'soft' 
classification, we do not expect one outcome or the other to be a 'sure thing', that 
is we do not expect p(t) to be 0 or i for large portions of g2. 
Next, we review penalized log likelihood risk estimates. Let the training data be 
{Yi, t(i), i = 1, ...n} where yi has the value 1 or 0 according to the classification of 
example i, and t(i) is the attribute vector for example i. If the n examples are a 
random sample from our 'world', then the likelihood function of this data, given 
Soft Classification with Smoothing Spline ANOVA 417 
v('), is 
l ikel ihood { y, p } = II= xp( t ( i) )u' (1 - p( t ( i) ) ) -u ' , (1) 
which is the product of n Bernoulli likelihoods. Define the logit f(t) by f(t) = 
log[p(t)/(1 - p(t))], then p(t) = e.t(t)/(1 + e.t(t)). Substituting in f and taking logs 
gives 
- log likelihood{y,f) -- �(y,f) =  log(1 + e l(t(i))) - yif(t(i)). (2) 
i=1 
We estimate f assuming that it is in some space 7/of smooth functions. (Technically, 
7/ is a reproducing kernel Hilbert space, see Vahba(1990), but you don't need to 
know what this is to read on). The fact that f is assumed 'smooth' makes the 
methods here very suitable for medical data analysis. The penalized log likelihood 
estimate fx of f will be obtained as the minimizer in 7/of 
�(y, f) + xJ(f) 
(a) 
where J(f) is a suitable 'smoothness' penalty. A simple example is, T = [0, 1] and 
J(f) = fo(f(m)(t)?dt, in which case fx is a polynomial spline of degree 2m- 1. If 
Jam(f)= -- ax!:7]aa! ' ,Ox...Oxj d Ildxj. (4) 
o +...+o,=m c c j 
then fx is a thin plate spline. The thin plate spline is a linear combination of 
polynomials of degree m or less in d variables, and certain radial basis functions. 
For more details and other penalty functionals which result in rbf's, see Wahba(1980, 
1990, 1992). 
The likelihood function �(y, f) will be maximized if p(t(i)) is 1 or 0 according as 
yi is i or 0. Thus, in the (full-rank) spline case, as A -+ 0, fx tends to 
at the data points. Therefore, by letting A be small, we can come close to fitting 
the data points exactly, but unless the 1's and O's are well separated in attribute 
space, fx will be a very 'wiggly' function and the generalization error (not precisely 
defined yet) may be large. 
The choice of A represents a tradeoff betwcen overfitting and underfitting the data 
(bias-variance tradeoff). It is important in practice good value of A. We now define 
what we mean by a good value of X. Given the family px, A >_ 0, we want to choose 
X so that px is close to the 'true' but unknown p so that, if new examples arrive with 
attribute vector in a neighborhood of t, px(t) will be a good estimate of the fraction 
of them that are 1's. 'Closeness' can be defined in various reasonable ways. We use 
the Kullbach-Leibler (KL) distance (not a real distance!). The KL distance between 
two probability measures (g,�) is defined as KL(g,�) = Ea[log(g/�)] , where E a 
means expectation given g is the true distribution. If (t) is some probability 
measure on T, (say, a proxy for the distribution of the attributes in the population), 
then define KL (p, Px) (for Bernoulli random variables) with respect to  as 
KL.(p, px) = f [p(t)log (p(t)  (1-p(t) 
[,p-) ] + (1 -- p(t))log )] dr(t). 
,1 - px(t) 
(5) 
418 Wahba, Wang, Gu, Klein, and Klein 
Since KLv is not computable from the data, it is necessary to develop a computable 
proxy for it. By a computable proxy is meant a function of A that can be calculated 
from the training set which has the property that its minimizer is a good estimate 
of the minimizer of KLv. By letting px('t) - eld(t)/(1 q- e l(t)) it is seen that to 
minimize KL, it is only necessary to minimize 
[log(1 q- e Ix(t)) - p(t)f(t)]dy(t) (6) 
over A since (5) and (6) differ by something that does not depend on A. Leaving- 
out-half cross validation (�CV) is one conceptually simple and generally defensible 
(albeit possibly wasteful) way of choosing A to minimize a proxy for KL(p, px). 
The n examples are randomly divided in half and the first n/2 examples are used 
to compute Px for a series of trial values of A. Then, the remaining n/2 examples 
are used to compute 
A 2 Z [log(1 q- e lx(t(i))) -- yifx(t(i))] (7) 
KL�cv( ) = . 
,=+1 
for the trial values of A. Since the expected value of Yi is p(t(i)), (7) is, for each A an 
unbiased estimate of (�) with dr the sampling distribution of the {t(1), ..., t(n/2)}. 
A would then be chosen by minimizing (7) over the trial values. It is inappropriate to 
just evaluate (7) using the same data that was used to obtain fx, as that would lead 
to overfitting the data. Variations on (7) are obtained by successively leaving out 
groups of data. Leaving-out-one versions of (7) may be defined, but the computation 
may be prohibitive. 
3. Newton-Raphson Iteration and the Unbiased Risk estimate of A. 
We use the unbiased risk estimate given in Craven and Wahba(1979) for smoothing 
spline estimation with Gaussian errors, which has been adapted by Gu(1992a) for 
the Bernoulli case. To describe the estimate we need to describe the Newton- 
Raphson iteration for minimizing (3). Let b(f) -- log(1 q- el), then �(y,f) - 
y4n=l[b(f(t(i)) - yif(t(i))]. It is easy to show that Eyi = f(t(i)) = b'(f(t(i)) and 
vat yi = p(t(i))(1- p(t(i)) = b(f(t(i)). Represent f either exactly by using a 
basis for the (k
