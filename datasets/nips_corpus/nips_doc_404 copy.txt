Using Genetic Algorithms to Improve 
Pattern Classification Performance 
Eric I. Chang and Richard P. Lippmann 
Lincoln Laboratory, MIT 
Lexington, MA 02173-9108 
Abstract 
Genetic algorithms were used to select and create features and to select 
reference exemplar patterns for machine vision and speech pattern classi- 
fication tasks. For a complex speech recognition task, genetic algorithms 
required no more computation time than traditional approaches to feature 
selection but reduced the number of input features required by a factor of 
five (from 153 to 33 features). On a difficult artificial machine-vision task, 
genetic algorithms were able to create new features (polynomial functions 
of the original features) which reduced classification error rates from 19% 
to almost 0%. Neural net and k nearest neighbor (KNN) classifiers were 
unable to provide such low error rates using only the original features. Ge- 
netic algorithms were also used to reduce the number of reference exemplar 
patterns for a KNN classifier. On a 338 training pattern vowel-recognition 
problem with 10 classes, genetic algorithms reduced the number of stored 
exemplars from 338 to 43 without significantly increasing classification er- 
ror rate. In all applications, genetic algorithms were easy to apply and 
found good solutions in many fewer trials than would be required by ex- 
haustive search. Run times were long, but not unreasonable. These results 
suggest that genetic algorithms are becoming practical for pattern classi- 
fication problems as faster serial and parallel computers are developed. 
1 INTRODUCTION 
Feature selection and creation are two of the most important and difficult tasks in 
the field of pattern classification. Good features improve the performance of both 
conventional and neural network pattern classifiers. Exemplar selection is another 
task that can reduce the memory and computation requirements of a KNN classifier. 
These three tasks require a search through a space which is typically so large that 
797 
798 Chang and Lippmann 
exhaustive search is impractical. The purpose of this research was to explore the 
usefulness of Genetic search algorithms for these tasks. Details concerning this 
research are available in (Chang, 1990). 
Genetic algorithms depend on the generation-by-generation development of possible 
solutions, with selection eliminating bad solutions and allowing good solutions to 
replicate and be modified. There are four stages in the genetic search process: cre- 
ation, selection, crossover, and mutation. In the creation stage, a group of possible 
solutions to a search problem is randomly generated. In most genetic algorithm 
applications, each solution is a bit string with each bit initially randomly set to 1 
or 0. 
After the creation stage, each solution is evaluated using a fitness function and as- 
signed a fitness value. The fitness function must be tightly linked to the eventual 
goal. The usual criterion for success in pattern classification tasks is the percentage 
of patterns classified correctly on test data. This was approximated in all experi- 
ments by using a leave-one-out cross-validation measure of classification accuracy 
obtained using training data and a KNN classifier. After solutions are assigned 
fitness values, a selection stage occurs, where the fitter solutions are given more 
chance to reproduce. This gives the fitter solutions more and more influence over 
the changes in the population so that eventually fitter solutions dominate. 
A crossover operation occurs after two fitter solutions (called parent solutions) have 
been selected. During crossover, portions of the parent solutions are exchanged. 
This operation is performed in the hope of generating new solutions which will 
contain the useful parts of both parent solutions and be even better solutions. 
Crossover is responsible for generating most of the new solutions in genetic search. 
When all solutions are similar, the crossover operation loses its ability to generate 
new solutions since exchanging portions of identical solutions generates the same 
solutions. Mutation (randomly altering bits) is performed on each new solution 
to prevent the whole population from becoming similar. However, mutation does 
not generally improve solutions by itself. The combination of both crossover and 
mutation is required for good performance. 
There are many varieties of genetic algorithms. A relatively new incremental static 
population model proposed by (Whitley, 1989) was used in all experiments. In 
the regular genetic algorithm model, the whole population undergoes selection and 
reproduction, with a large portion of the strings replaced by new strings. It is thus 
possible for good strings to be deleted from the population. In the static population 
model, the population is ranked according to fitness. At each recombination cycle, 
two strings are picked as parents according to their fitness values, and two new 
strings are produced. These two new strings replace the lowest ranked strings in 
the original population. This model automatically protects the better strings in the 
population. 
2 FEATURE SELECTION 
Adding more input features or input dimensions to a pattern classifier often degrades 
rather than improves performance. This is because as the number of input features 
increases, the number of training patterns required to maintain good generalization 
and adequately describe class distributions also often increases rapidly. Performance 
with limited training data may thus degrade. Feature selection (dimensionality 
Using Genetic Algorithms to Improve Pattern Classification Performance 799 
A) CLASSIFICATION ERROR RATE 
30 
o 
0 
Testing Set 
g Set 
ï¿½ 'o 2oo 
 160 
120 
B) NUMBER OF FEATURES USED 
I 
Initial Number of Features 
--,.., Features Selected by Genetic Search 
I 
100(0) 20000 
Number of Features Recombinations 
Figure 1' Progress Of a Genetic Algorithm Search For Those Features From an 
Original 153 Features That Provide High Accuracy in E Set Classification For 
One Female Talker: (A) Classification Error Rate and (B) Number Of Features 
Used. 
reduction) is often required when training data is limited to select the subset of 
features that best separates classes. It can improve performance and/or reduce 
computation requirements. 
Feature selection is difficult because the number of possible combinations of features 
grows exponentially with the number of original features. For a moderate size 
problem with 64 features, there are 264 possible subsets of features. Clearly an 
exhaustive evaluation of each possible combination is impossible. Frequently, finding 
a near optimal feature subset is adequate. An overview of many different approaches 
to feature selection is available in (Siedlecki and Sklansky, 1988). 
This work applies genetic search techniques to the problem of feature selection. 
Every feature set is represented by a bit string with d bits, where d is the maximum 
800 Chang and Lippmann 
input dimension. Each bit determines whether a feature is used. The accuracy of 
a KNN classifier with the leave-one-out approach to error rate estimation was used 
as an evaluation function as described above. A KNN classifier has the advantage 
of requiring no training time and providing results directly related to performance. 
E-set words (9 letters from the English alphabet that rhyme with the letter E) 
taken from a Texas Instruments 46-word speech database were used for experiments. 
Waveforms were spectrally analyzed and encoded with a hidden Markov Model 
speech recognizer as described in (Huang and Lippmann, 1990). Features were 
the average log likelihood distance and duration from all the hidden Markov nodes 
determined using Viterbi decoding. The final output of the hidden Markov model 
was also included in the feature set. This resulted in 17 features per word class. 
The 9 different word classes result in a total of 153 features. For each talker there 
were 10 patterns in the training set and 16 patterns in the testing set per word 
class. All experiments were talker dependent. 
An experiment was performed using the data from one female talker. More conven- 
tional sequential forward and backward searches for the best feature subset were 
first performed. The total number of KNN evaluations for each sequential search 
was 11,781. The best feature subset found with sequential searches contained 33 
features and the classification error rates were 2.2% and 18.5% on training and test- 
ing sets respectively. Genetic algorithms provided a lower error rate on the testing 
set with fewer than half as many features. Fig. 1 shows the progress of the genetic 
search. The bottom plot shows that near recombination 12,100, the number of 
features used was reduced to 15. The top plot shows that classification error rates 
were 3.3% and 17.5% for the training and testing sets respectively. 
3 FEATURE CREATION 
One of the most successful techniques for improving pattern classification perfor- 
mance with limited training data is to find more effective input features. An ap- 
proach to creating more effective input features is to search through new features 
that are polynomial functions of the original features. This difficult search problem 
was explored using genetic algorithms. The fitness function was again determined 
using the performance of a KNN classifier with leave-one-out testing. 
Polynomial functions of the original features taken two at a time were created as 
new features. New features were represented by a bit string consisting of substrings 
identifying the original features used, their exponents, and the operation to be 
applied between the original features. A gradual buildup of feature complexity over 
multiple stages was enforced by limiting the complexity of the created features. 
Once the accuracy of a KNN classifier had converged at one stage, another stage 
was
