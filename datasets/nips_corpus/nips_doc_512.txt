Principled Architecture Selection 
for Neural Networks: 
Application to Corporate Bond Rating Prediction 
John Moody 
Department of Computer Science 
Yale University 
P.O. Box 2158 Yale Station 
New Haven, CT 06520 
Joachim Utans 
Department of Electrical Engineering 
Yale University 
P. O. Box 2157 Yale Station 
New Haven, CT 06520 
Abstract 
The notion of generalization ability can be defined precisely as the pre- 
diction risk, the expected performance of an estimator in predicting new 
observations. In this paper, we propose the prediction risk as a measure 
of the generalization ability of multi-layer perceptton networks and use it 
to select an optimal network architecture from a set of possible architec- 
tures. We also propose a heuristic search strategy to explore the space of 
possible architectures. The prediction risk is estimated from the available 
data; here we estimate the prediction risk by v-fold cross-validation and 
by asymptotic approximations of generalized cross-validation or Akaike's 
final prediction error. We apply the technique to the problem of predicting 
corporate bond ratings. This problem is very attractive as a case study, 
since it is characterized by the limited availability of the data and by the 
lack of a complete a priori model which could be used to impose a structure 
to the network architecture. 
1 Generalization and Prediction Risk 
The notion of generalization ability can be defined precisely as the prediction risk, 
the expected performance of an estimator is predicting new observations. Consider 
a set of observations D - {(ï¿½j,tj);j: 1...N} that are assumed to be generated 
683 
684 Moody and Utans 
where It(x) is an unknown function, the inputs xj are drawn independently with 
an unknown stationary probability density function p(x), the e are independent 
2 and the t are the 
random variables with zero mean (g = 0) and variance 
observed target values. 
The learning or regression problem is to find an estimate /)x(x; D) of it(x) given 
the data set D from a class of predictors or models Itx(x) indexed by A. In general, 
A 6 A = (S, A, W), where S C X denotes a chosen subset of the set of available 
input variables X, A is a selected architecture within a class of model architectures 
.4, and W are the adjustable parameters (weights) of architecture A. 
The prediction risk P(A) is defined as the expected performance on future data and 
can be approximated by the expected performance on a finite test set: 
N 
j=l 
where (xj,tj) are new observations that were not used in constructing /x(x). In 
what follows, we shall use P(A) as a measure of the generalization ability of a model. 
See [4] and [6] for more detailed presentations. 
2 Estimates of Prediction Risk 
Since we cannot directly calculate the prediction risk Px, we have to estimate it 
from the available data D. The standard method based on test-set validation is 
not advisable when the data set is small. In this paper we consider such a case; 
the prediction of corporate bond ratings from a database of only 196 firms. Cross- 
validation (CV) is a sample re-use method for estimating prediction risk; it makes 
maximally efficient use of the available data. Other methods are the generalized 
cross-validation (GCV) and the final prediction error (FPE) criteria, which combine 
the average training squared error ASE with a measure of the model complexity. 
These will be discussed in the next sections. 
2.1 Cross Validation 
Cross-Validation is a method that makes minimal assumptions on the statistics of 
the data. The idea of cross validation can be traced back to Mosteller and Tukey [7]. 
For reviews, see Stone [8, 9], Geisser [5] and Eubank [4]. 
Let fx(j)(x) be a predictor trained using all observations except (xj, tj) such that 
ftx(j)(x) minimizes 
1 
AEj - (N - 1)  (t -/2x(j)(x)) 2 
Then, an estimator for the prediction risk P(A) is the cross validation average 
Principled Architecture Selection for Neural Networks 685 
squared error 
N 
1 
cv() = (rs - (a) 
j=l 
This form of CV(X) is known as leave-one-out cross-validation. 
However, CV(X) in (3) is expensive to compute for neural network models; it in- 
volves constructing N networks, each trained with N - 1 patterns. For the work 
described in this paper we therefore use a variation of the method, v-fold cross- 
validation, that was introduced by Getsset [8] and Wahba et al [12]. Instead of 
leaving out only one obserwtion for the computation of the sum in (3) we delete 
larger subsets of D. 
Let the data D be divided into v randomly selected disjoint subsets P5 of roughly 
equal size: U=P5 = W andVij, PP = . Let N 5 denote the number of 
obserwtions in subset P5' Let x(v)(z) be an estimator trMned on all data except 
for (z,Q  PS. Then, the cross-wlidation average squared error for subset j is 
defined as 
and 
CVp(A) = 1 E CVs(A)' (4) 
Typical choices for v are 8 and 10. Note that leave-one-out CV is obtained in the 
limit v = N. 
2.2 Generalized Cross-Validation and Final Prediction Error 
For linear models, two useful criteria for selecting a model architecture are general- 
ized cross-validation (GCV) (Wahba [11]) and Akaike's final prediction error (FPE) 
acv() = 
([1]): 
(1- S(NX) ) 
FpE()=ASE()( l+s--(' ) 
N 
1- 
N 
S(A) denotes the number of weights of model X. See [4] for a tutorial treatment. 
Note that although they are slightly different for small sample sizes, they are asymp- 
totically equivalent for large N: 
b(A) = ASE(A) (1 q- 2 S(A) )  OCV(A)  FPE(A) (5) 
N 
We shall use this asymptotic estimate for the prediction risk in our analysis of the 
bond rating models. 
It has been shown by Moody [6] that FPE and therefore/3(A) is an unbiased estimate 
of the prediction risk for the neural network models considered here provided that 
(1) the noise ej in the observed targets tj is independent and identically distributed, 
686 Moody and Utans 
(2) weight decay is not used, and (3) the resulting model is unbiased. (In practice, 
however, essentially all neural network fits to data will be biased (see Moody [6]).) 
FPE is a special case of Barron's PSE [2] and Moody's GPE [6]. Although FPE 
and P(A) are unbiased only under the above assumptions, they are much cheaper 
to compute than CVs, since no retraining is equired. 
3 A Case Study: Prediction of Corporate Bond Ratings 
A bond is a debt security which constitutes a promise by the issuing firm to pay a 
given rate of interest on the original issue price and to redeem the bond at face value 
at maturity. Bonds are rated according to the default risk of the issuing firm by 
independent rating agencies such as Standard & Poors (S&P) and Moody's Investor 
Service. The firm is in default if it is not able make the promised interest payments. 
I Representation of S&P Bond Ratings 
ccc B- B I... I BBB+ 
2 I 3 I 4 I ... ] 11 
high default risk 
low default risk 
Table 1: Key to S&P bond ratings. We only used the range from 'AAA' or 'very low default risk' to 
'CCC' meaning 'very high default risk'. (Note that AAA- is a not a standard category; its inclusion was 
suggested to us by a Wall Street analyst.) Bonds with rating BBB- or better are investment grade 
while junk bonds have ratings BB+ or below. For our output representation, we assigned an integer 
number to each rating as shown. 
S&P and Moody's determine the rating from various financial variables and possibly 
other information, but the exact set of variables is unknown. It is commonly believed 
that the rating is at least to some degree judged on the basis of subjective factors 
and on variables not directly related to a particular firm. In addition, the method 
used for assigning the rating based on the input variables is unknown. The problem 
we are considering here is to predict the S&P rating of a bond based on fundamental 
financial information about the issuer which is publicly available. Since the rating 
agencies update their bond ratings infrequently, there is considerable value to being 
able to anticipate rating changes before they are announced. A predictive model 
which maps fundamental financial factors onto an estimated rating can accomplish 
this. 
The input data for our model consists of 10 financial ratios reflecting the fundamen- 
tal characteristics of the firms. The database was prepared for us by analysts at a 
major financial institution. Since we did not attempt to include all information in 
the input variables that could possibly be related to a firms bond rating (e.g. all 
fundamental or technical financial factors, or qualitative information such as quality 
of management), we can only attempt to approximate the S&P rating. 
3.1 A Linear Bond Rating Predictor 
For comparison with the neural network models, we computed a standard linear 
regression model. All input variables were used to predict the rating which is 
represented by a number in [0, 1]. The rating varies continuously from one category 
to the next higher or next lower one and this smoothness is captured in the single 
output representation and should make the task easier. To interpret the network 
Principled Architecture Selection for Neural Networks 687 
'.00 
Cross Validation Error vs. 
Number of Hidden Units 
' 
Nmber of Hidden Units 
1.8 
P VS. 
NuTbet of Hidden Units 
NUTbet of Hidden Units 
Figure 1: Cross validation error CVp(A) and P(A) versus number of hidden units. 
response, the output was rescaled from [0, 1] to [2, 19] and rounded to the nearest 
integer; 19 corresponds to a rating of'AAA' and 2 to 'CCC' and below (see Table 1). 
The input variables were normalized to the interval [0, 1] since the original financial 
ratios differed widely in magnitude. The model predicted the rating of 21.4 % of 
the firms correctly, for 37.2 % the error was one notch and for 21.9 % two notches 
(thus predicting 80.5 % of the data within two notches from the correct target). 
The RMS training error was 1.93 and the estim
