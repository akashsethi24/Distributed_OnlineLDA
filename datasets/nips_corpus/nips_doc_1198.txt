The effect of correlated input data on the 
dynamics of learning 
Sren Halkjar and Ole Winther 
CONNECT, The Niels Bohr Institute 
Blegdamsvej 17 
2100 Copenhagen, Denmark 
halkj aer, winherQconnec. nbi. dk 
Abstract 
The convergence properties of the gradient descent algorithm in the 
case of the linear perceptton may be obtained from the response 
function. We derive a general expression for the response function 
and apply it to the case of data with simple input correlations. It 
is found that correlations severely may slow down learning. This 
explains the success of PCA as a method for reducing training time. 
Motivated by this finding we furthermore propose to transform the 
input data by removing the mean across input variables as well as 
examples to decrease correlations. Numerical findings for a medical 
classification problem are in fine agreement with the theoretical 
results. 
1 INTRODUCTION 
Learning and generalization are important areas of research within the field of neu- 
ral networks. Although good generalization is the ultimate goal in feed-forward 
networks (perceptrons), it is of practical importance to understand the mechanism 
which control the amount of time required for learning, i.e. the dynamics of learn- 
ing. This is of course particularly important in the case of a large data set. An exact 
analysis of this mechanism is possible for the linear perceptron and as usual it is 
hoped that the results to some extend may be carried over to explain the behaviour 
of non-linear perceptrons. 
We consider N dimensional input vectors x G T�N and scalar output y. The linear 
170 S. Halkjcer and O. Winther 
perceptron is parametrized by the weight vector w  T N 
1 wT x (1) 
Y(x) = vW 
Let the training set be {(x, y),/- 1,...,p) and the training error be the usual 
1 
squared error, E(w) -  -. (y- y(x)) 2. We will use the well-known gradient 
descent algorithm 1 w(k q- 1) - w(k)- r/VE(w(k)) to estimate the minimum points 
w* of E. Here r/ denotes the learning parameter. Collecting the input examples 
in the N x p matrix X and the corresponding output in y, the error function is 
I 1 
written E(w) - �(wTRw--2qTw+c), where R _= N Z XI(X/)T' q = Xy and 
c = yTy. As in (Le Cun �t al., 1991) the convergence properties of the minimum 
points w* are examined in the coordinate system where R is diagonal. Let U denote 
the matrix whose columns are the eigenvectors of R and A = diag(A1,..., AN) the 
diagonal matrix containing the eigenvalues of R. The new coordinates then become 
v = U T(w - w*) with corresponding error function 2 
S(v) = ivYv + s0 = 1 
+ z0 
i 
where E0 = E(w*). Gradient descent now leads to the decoupled equations 
vi(k + 1)= (1 - rlAi)vi(k ) = (1 - rlAi)kvi(O) (3) 
with i = 1,..., N. Clearly, v -+ 0 requires ]1 - r/Ai I < i for all i, so that r/must 
be chosen in the interval 0 < r/ < 2/Amax. In the extreme case Ai = A we will 
have convergence in one step for r/= 1/A. However, in the usual case of unequal Ai 
the convergence for large k will be exponential vi(k): exp(-rlAik)vi(O). (r/Ai) -1 
therefore defines the time constant of the i'th equation giving a slowest time constant 
(rlAmin) -. A popular choice for the learning parameter is r/= 1/Amax resulting in 
a slowest time constant ,Xrna/,X,in called the learning time r in the following. The 
convergence properties of the gradient descent algorithm is thus characterized by 
r. In the case of a singular matrix R, one or more of the eigenvalues will be zero, 
and there will be no convergence along the corresponding eigendirections. This has 
however no influence on the error according to (2). Thus, Amin will in the following 
denote the smallest non-zero eigenvalue. 
We will in the article calculate the eigenvalue spectrum of R in order to obtain the 
learning time of the gradient descent algorithm. This may be done by introducing 
the response function 
-- i - / i / 
G. --G(L,H)= TrL 1-RH - L 1-':RH (4) 
where L, H are arbitrary N x N matrices. Using a standard representation of the 
Dirac 6-function (Krogh, 1992) we may write the eigenvalue spectrum of R as 
1 , 
= N - = --lIm (5) 
i 
The Newton-Raphson method, w(k + 1) = w(k)- VE(w(k))(V2E(w(k))) - is of 
course much more effective in the linear case since it gives convergence in one step. This 
method however requires an inversion of the Hessian matrix. 
2Note that this analysis is valid for any part of an error surface in which a quadratic 
approximation is valid. In the general case R should be exchanged with the Hessian 
WE(w*). 
The Effect of Correlated Input Data on the Dynamics of Learning 171 
where  has an infinitesimal imaginary part which is set equal to zero at the end of 
the calculation. 
In the 'thermodynamic limit' N -+ oc keeping a = n z constant and finite, G 
(and thus the eigenvalue spectrum) is a self-averaging quantity (Sollich, 1996) i. 
e. G -  = O(N-), where U is defined as the response function averaged over 
the input distribution. Previously U has been calculated for independent input 
variables (Hertz et al., 1989; Sollich, 1996). In section 2 we derive an implicit 
equation for the averaged response function for arbitrary correlations using random 
matrix techniques (Brody et al., 1981). This equation is solved showing that simple 
input correlations may slow down learning significantly. Based on this finding we 
propose in section 3 data transformations for improving the learning speed and test 
the transformation numerically on a medical classification problem in section 4. We 
conclude in section 5 with a discussion of the results. 
2 THE RESPONSE FUNCTION 
The method for deriving the averaged response function is based on the fact that the 
response function (4)may be written as a geometrical series Gr. = -]r��__0 (L(RH)r). 
We will assume that the input examples x  are drawn independently from a 
Gaussian distribution with means rni and correlations xixj -rnirnj -- (fij, i.e. 
x(x )r = 6Z and  = aZ where Z = C +mm r. The Gaussian distribution 
has the property that the average of products of x's can be calculated by making 
all possible pair correlations, e.g. xixjxxl = ZijZi + ZiZjt + ZaZj. To take 
the average of (L(RH)), we must therefore make all possible pairs of the x's and 
exchange each pair 3cixj with Zij. This combinatorial problem will be solved below 
in a recursive fashion leading to an implicit equation for G�. Using underbraces to 
indicate pairings of x's, we get for r >_ 2 
<L(RH)> 
(6) 
Resumming this we get the response function 
L = <L) + c  <LZH(RH) r ) +   <L(RH) r--l) <ZH(RH) ) 
r=0 r=0 s=0 
(7) 
Exchanging the order of summation in the last term we can write everything in 
terms of the response function 
= <L)+ ar.ZH +   <L(RH) -) <ZH(RH) ) 
s=O r=s+l 
: (L) + aGr.zH + (Gr. - 
172 S. Halkjcer and O. Winther 
OGLZH 
(L) + 1 - ZH (8) 
Using (8) recursively setting L equal to LZH, L(ZH) 2 etc. one obtains 
GL =  L = L ZH 
=o 1 -- ZH 1 -- 1-G--zH 
(9) 
This is our main result for the response function. To get the response function 
_ = G(A -, A -1) requires two steps, first set L = ZH and solve for ZH and then 
solve for Gz. If Z has a particularly simple form (9) may be solved analytically, 
but in general it must be solved numerically. 
In the following we will calculate the eigenvalue spectrum for a correlation matrix 
on the form C = nn T + rI and general mean m. To ensure that C is positive 
semi definite r > 0 and Inl 2 + r >_ 0 where Inl 2 -- n.h. The eigenvalues of 
Z = nn T + turn T + rI are straight forwardly shown to be a = r (with multiplicity 
N -2), 32 =r+ [In[2 + [ml 2- v/-]/2 and 33:rq- []n]2 + ]ml2+ V/-] /2 with 
D = (Inl 2 - Im12) 2 + 4(n. m) 2. Carrying out the trace in eq. (9) we get 
N-2 i i i i 1 
Gx = + -- + -- (10) 
 N A a NA- a NA- a 
1--GzH 1--GzH 1--GzH 
This expression suggests that we may solve G k in powers of 1IN (see e.g. (Sollich, 
1996)). However for purposes of the discussion of learning times the only -term 
that will be of importance is the last term above. We therefore only need to solve 
for GZH (setting L = ZH in (9)) to leading order 
A + al(1 - c 0 - v/(A + al(1 - c02) - 4a 
ZH -- 2A 
(11) 
Note that GZH will vanish for large A implying that the last term in (10) to leading 
order is singular for A = aaa. Inserting the result in (10) gives 
1 A+a'(1-a)-x/(A+a'(1-a))2-4Aa2 + NA cta 
Gk = 2Aa--- - 
According to (__5) the eigenvalue spectrum is determined by the imaginary part and 
poles of Gz. Gz has an imaginary part for A_ < A < A+ where A+ = a(1 4- x/-a-) 2 
and the poXles A x = 0, A = aaa. The poles contribute each with a &function such 
that the eigenvalue spectrum up to corrections of order  becomes 
(A) = (1 - a)O(1 - a)6(A) + 
1 6(A - aa3) q- -- 
27rAa1 
v/(A+ - A)(A- A_) (13) 
where O(x) = 1 for x > 0 and 0 otherwise. The first term expresses the trivial fact 
that for p < N the whole input space is not spanned and R will have a fraction of 
1 - a zero-eigenvalues. The continuous spectrum (the root term) only contributes 
for A_ < A < A+. Numerical simulations has been performed to test the validity 
of the spectrum (13) (Hulkjeer, 1996). They are in good agreement with predicted 
results indicating that finite size effects are unimportant. The continuous spectrum 
'n (13) has also been calculated using the replica method (Halkjaer, 1996). 
The Effect of Correlated Input Data on the Dynamics of Learning 173 
From the spectrum the learning time r may be read of directly 
(A+ aaa) ((l+v/) 2 
aaa (14) 
r = max __, __ = max 1 --  'a 1 (1 - )2 
To illustrate how input correlations and bi may affect learning time consider 
simple correlations Cij = 6ijv(1 - c) + vc and mi = m. With this special choice 
N(m+ vc) m 2 
r  v(X_c)(x_). For + cv > 0, i.e. for non-zero mean or positive correlations, 
the convergence time will blow up by a factor proportional to N. The input bi 
effect h previou
