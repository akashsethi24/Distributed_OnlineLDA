133 
TRAINING MULTILAYER PERCEPTRONS WITH THE 
EXTENDED KALMAN ALGORITHM 
Sharad Singhal and Lance Wu 
Bell Communications Research, Inc. 
Morristown, NJ 07960 
ABSTRACT 
A large fraction of recent work in artificial neural nets uses 
multilayer perceptrons trained with the back-propagation 
algorithm described by Rumelhart et. al. This algorithm 
converges slowly for large or complex problems such as 
speech recognition, where thousands of iterations may be 
needed for convergence even with small data sets. In this 
paper, we show that training multilayer perceptrons is an 
identification problem for a nonlinear dynamic system which 
can be solved using the Extended Kalman Algorithm. 
Although computationally complex, the Kalman algorithm 
usually converges in a few iterations. We describe the 
algorithm and compare it with back-propagation using two- 
dimensional examples. 
INTRODUCTION 
Multilayer perceptrons are one of the most popular artificial neural net 
structures being used today. In most applications, the back propagation 
algorithm [Rumelhart et al, 1986] is used to train these networks. Although 
this algorithm works well for small nets or simple problems, convergence is 
poor if the problem becomes complex or the number of nodes in the network 
become large [Waibel et al, 1987]. In problems such as speech recognition, 
tens of thousands of iterations may be required for convergence even with 
relatively small data-sets. Thus there is much interest [Prager and Fallside, 
1988; Irie and Mivake, 1988] in other training algorithms which can 
compute the parameters faster than back-propagation and/or can handle nmch 
more complex problems. 
In this paper, we show that trailting multilayer perceptrons eau be viewed as 
an identification problem for a nonlinear clylmmic system. For linear dynamic 
Copyright 1989. Bell Communications Research. Inc. 
134 Singhal and Wu 
with white input and observation noise, the Kalman algorithm 
systems 
[Kalman, 1960] is known to be an optimum algorithm. 
the Kalman algorithm can be applied to nonlinear 
linearizing the system around the current estimate 
Although computationally complex, this algorithm 
consistent with all previously seen data and usually 
iterations. In the following sections, we describe how this algorithm can be 
applied to multilayer perceptrons and compare its performance with back- 
propagation using some two-dimensional examples. 
Extended versions of 
dynamic systems by 
of the parameters. 
updates parameters 
converges in a few 
THE EXTENDED KALMAN FILTER 
In this section we briefly outline the Extended Kalman filter. Mathematical 
derivations for the Extended Kalman filter are widely available in the 
literature [Anderson and Moore, 1979; Gelb, 1974] and are beyond the scope 
of this paper. 
Consider a nonlinear finite dimensional discrete time system of the form: 
x(n+l) = fn(x(n)) + g,(x(n))w(n), 
8 (n) = h (x (n))+v (n). 
(1) 
Here the vector x (n) is the state of the system at time n, w (n) is the input, 
d(n) is the observation, v(n) is observation noise and f,(.), g,-), and h,.) 
are nonlinear vector functions of the state with the subscript denoting possible 
dependence on time. We assume that the initial state, x(0), and the 
sequences {v(n)} and {w(n)} are independent and gaussian with 
(2) 
E [x (0)]: (0), E {[x (0)-- (0)]Ix (0)-- (0)] t ) = P (0), 
E[w(/'/)] - O, E[w(J )IM t(l)] = Q(FI )nl, 
Ep(n)] = 0, E[ (n)'(l)] = 
where 8nt is the Kronecker delta. Our problem is to find an estimate i(n +1) 
of x(n+l) given d(j) , O<_j<_n. We denote this estimate by (n+lln ). 
If the nonlinearities in (1) are sufficiently smooth, we can expand thein using 
Taylor series about the state estimates � (n In) and i (n In-1) to obtain 
where 
f,(x(n)) = f,,(.f(n In)) + F (n )[x (n )-� (n In)] + .-. 
g, (x (n )) = g,, (i (n [n))+ -.. = G(n)+ ... 
Oh,,(x) 
, H' (n) - 
I ) Ox 
F (n) - Ox ) 
is the value of the 
components of F(n) and H'(n) are the partial derivatives of the ith 
components of f,, (.) and h,,(.) respectively with respect to the jth component 
of x (n) at the points indicated. Neglecting higher order terms and assuming 
i.e. G(n) function g,,(.) at �(n In) and the ijth 
Training Multilayer Perceptrons 135 
knowledge of ./(n In) and  (n In-1), the system in (3) can be approximated 
as 
x(n+l) = F(n)x(n) + G(n)w(n) + u(n) 
z (n) = W (n)x (n)+v (n) + y (n), 
(4) 
where 
u(n) = f.((n In))- F(n)(n In) 
y(n) = h.((n In-0)- H'(n):e(n In-1). 
It can be shown [Anderson and Moore, 1979] that the desired estimate 
: (n +1 In ) can be obtained by the recursion 
(n+l In) = f,,(�(n In)) 
:e(n In)= 2e(n In-1) + K(n)[a(n)- h,,(�(n In-1))] 
g (n) -- P(n I n -1)S (n)JR (n)+S' (n)P (n In-0H (n)]- 
P(n+l[n) = F(n)P(n In)Ft(n) + G(n)Q(n)Gt(n) 
P(n In)= P(n In-1)- K (n )Ht (n )P (n In-1) 
(6) 
(7) 
(8) 
(9) 
(10) 
with P(ll0 ) = P(0). K(n) is known as the Kahnan gain. In case of a linear 
system, it can be shown that P(n) is the conditional error covariance matrix 
associated with the state and the estimate �(n +11n) is optimal in the sense 
that it approaches the conditional mean E[x(n+l) ld(O)... d(n)] for large 
n. However, for nonlinear systems, the filter is not optimal and the estimates 
can only loosely be termed conditional means. 
TRAINING MULTILAYER PERCEPTRONS 
The network under consideration is a L layer perceptton  with the i th input 
of the k th weight layer labeled as z/-'(n ), the j th output being zf(n) and the 
weight connecting the ith input to the ]th output being 0j. We assume that 
the net has m inputs and I outputs. Thresholds are implemented as weights 
connected from input nodes 2 with fixed unit strength inputs. Thus, if there 
are N(k) nodes in the kth node layer, the total number of weights in the 
system is 
L 
M = ZN(k-I)[N(k)-I]. (11) 
k=l 
Although the inputs and outputs are dependent on time n, for notational 
brevity, we will not show this dependence unless explicitly needed. 
I. We use the convention that the number of layers is equal to the number of weight layers. Thus 
we have L layers of weights labeled I � L and L+I layers of nodes (including the input and 
output nodes) labeled 0. � � L. We will refer to the kth weight layer or the kth node layer 
unless the context is clear. 
' We adopt the convention that the 1st input node is the threshold i.e. 8 ' is the threshold for 
.. . 14 
the jth output node from the kth weight layer. 
136 Singhal and Wu 
In order to cast the problem in a form for recursive estimation, we let the 
weights in the network constitute the state x of the nonlinear system, i.e. 
x = [0 L 0 L --  t 
,2, ,3 ' 0v(0),v0)] � (12) 
The vector x thus consists of all weights arranged in a linear array with 
dimension equal to the total number of weights M in the system. The system 
model thus is 
x(n) n>0, (13) 
d (n) = z z' (n) + v (n) = hn (x (n),z�(n )) + v (n), (14) 
where at time n, z�(n) is the input vector from the training set, d (n) is the 
corresponding desired output vector, and zZ'(n) is the output vector 
produced by the net. The components of hn(') define the nonlinear 
relationships between the inputs, weights and outputs of the net. If F(.) is the 
nonlinearity used, then z  (n) = h n (x (r/),go(r/)) is given by 
z L (n -- r{(0 � � � F{(Oi)tz�(n )}. � � }}, (15) 
where F applies componentwise to vector arguments. Note that the input 
vectors appear only implicitly through the observation function hn (') in (14). 
The initial state (before training) x (0) of the network is defined by populating 
the net with gaussian randon variables with a N(7(0),P(0)) distribution where 
(0) and P (0) reflect any apriori knowledge about the weights. In the absence 
of any such knowledge, a N(0,1/(I) distribution can be used, where ( is a 
small number and I is the identity matrix. For the system in (13) and (14), 
the extended Kahnan filter recursion simplifies to 
� (n + 1) = 27 (n + K (n)[d (n) - hn (2? (n),z�(n ))] (16) 
K (n) = P(n )H (n)[R (n)+Ht(n )P(n )H(n )]-I (17) 
?(n+l) = P(n)- K (n )H' (n )P (n ) (18) 
where P(n) is the (approximate) conditional error covariance matrix. 
Note that (16) is sinfilar to the weight update equation in back-propagation 
with the last term [z L- hn(�,z�)] being the error at the output layer. 
However, unlike the delta rule used in back-propagation, this error is 
propagated to the weights through the Kaitaart gain K(n) which updates each 
weight through ihe elltire gradient matrix H(n) and the conditional error 
covariance matrix P(n). In this sense, the Kaitaart algorithm is not a local 
training algorithm. However, the inversion required in (17) has dimension 
equal to the number of outputs 1, not the number of weights M, and thus 
does not grow as weights are added to the problem. 
EXAMPLES AND RESULTS 
To evaluate the output and the convergence properties of the extended 
Kahnan algorithm. we constructed mappings using two-dimensional inputs 
with two or four outputs as shown in Fig. 1. Limiting the input vector to 2 
dimensions allows its IO visualize the decision regions obtained by the net and 
Training Multilayer Percepttons ! 37 
to examine the outputs of any node in the net in a meaningful way. The x- 
and y-axes in Fig. i represent the two inputs, with the origin located at the 
center of the figures. The numbers in the figures represent the different 
output classes. 
3 
2 1 
1 2 
I 
(a) REGIONS (b) XOR 
Figure 1. Output decision regions for two problems 
The training set for each example consisted of 1000 random vectors uniformly 
filling the region. The hyperbolic tangent nonlinearity was used as the 
nonlinear element in the networks. The output corresponding to a class was 
set to 0.9 when the input vector belonged to that class, and to ~0.9 otherwise. 
During training, the weights were adjusted after each data vector was 
presented. Up to 2000 sweeps through the input data were used with the 
stopping criteria desc
